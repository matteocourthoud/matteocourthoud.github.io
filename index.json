[{"authors":null,"categories":null,"content":"I hold a PhD in Economics from the University of Zürich, specializing in industrial organization, econometrics and competition policy.\nI believe in knowledge sharing, and I try to contribute by sharing my code, tutorials and lecture notes. I write on Medium for Towards Data Science.\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"https://matteocourthoud.github.io/author/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/","section":"authors","summary":"I hold a PhD in Economics from the University of Zürich, specializing in industrial organization, econometrics and competition policy.\nI believe in knowledge sharing, and I try to contribute by sharing my code, tutorials and lecture notes.","tags":null,"title":"","type":"authors"},{"authors":null,"categories":null,"content":"⚠️ WORK IN PROGRESS! ⚠️\nWelcome to my Data Science with Python course!\nYou can find all the Jupyter notebook on my Github page here.\nPlease, if you find any typos or mistakes, open a new issue. Or even better, fork the repo and submit a pull request. I am happy to share my work and I am even happier if it can be useful.\nContent Data Structures Lists Tuples Sets Dictionaries Numpy arrays Pandas DataFrames Pyspark DataFrames Data Exploration Import, export data Descriprives and summary statistics Pivot tables and aggregation Data Types Numerical data String data Time data Missing data Data Wrangling Rows: sorting, indexing, \u0026hellip;. Columns: renaming, ordering, \u0026hellip;. Collapse and aggregate Reshape Concatenate and merge Plotting Distributions Time Series Correlations Regression Geographical data Machine Learning Pipeline Data exploration Encoding and normalization Missing values Weighting Prediction Cross-validation Web Scraping Pandas APIs Static Webscraping Dynamic Webscraping TBD What is missing? Let me know! Contacts All feedback is greatly appreciated!\n","date":1633046400,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":1633046400,"objectID":"e74e2a6ba6bd604ca04f081def878330","permalink":"https://matteocourthoud.github.io/course/data-science/","publishdate":"2021-10-01T00:00:00Z","relpermalink":"/course/data-science/","section":"course","summary":"⚠️ WORK IN PROGRESS! ⚠️\nWelcome to my Data Science with Python course!\nYou can find all the Jupyter notebook on my Github page here.\nPlease, if you find any typos or mistakes, open a new issue.","tags":["tutorial","data science","programming","python"],"title":"Data Science with Python","type":"book"},{"authors":null,"categories":null,"content":"Welcome to my notes for the Machine Learning for Economic Analysis course by Damian Kozbur @UZH!\nThe exercise sessions are entirely coded in Python on Jupyter Notebooks. The examples heavily borrow from An Introduction to Statistical Learning by James, Witten, Tibshirani, Friedman and its advanced version Elements of Statistical Learning by Hastie, Tibshirani, Friedman. Other recommended free resources are the documentation of the Python library scikit-learn and Bruce Hansen\u0026rsquo;s Econometrics book.\nYou can find all the Jupyter Notebooks on my Github page HERE.\nPlease, if you find any typos or mistakes, open a new issue. Or even better, fork the repo and submit a pull request. I am happy to share my work and I am even happier if it can be useful.\nContent OLS Regression\nISLR, chapter 3 ESL, chapter 3 Econometrics, chapters 3 and 4 Instrumental Variables\nEconometrics, chapter 12.1-12.12 Nonparametric Regression\nISLR, chapter 7 ESL, chapter 5 Econometrics, chapters 19 and 20 Cross-validation\nISLR, chapter 5 ESL, chapter 7 Lasso and Forward Regression\nISLR, chapter 6 ESL, chapters 3 and 18 Econometrics, chapter 29.2-29.5 Convexity and Optimization\nTrees and Forests\nISLR, chapter 8 ESL, chapters 9, 10, 15, 16 Econometrics, chapter 29.6-29.9 Neural Networks\nESL, chapter 11 Post-Double Selection\nEconometrics, chapter 3.18 Belloni, Chen, Chernozhukov, Hansen (2012) Belloni, Chernozhukov, Hansen (2014) Chernozhukov, Chetverikov, Demirer, Duflo, Hansen, Newey, Robins (2018) Unsupervised Learning\nISLR, chapter 10 ESL, chapter 14 Pre-requisites Students should be familiar with the following concepts:\nMatrix Algebra Econometrics, appendix A.1-A.10 Conditional Expectation and Projection Econometrics, chapter 2.1-2.25 Large Sample Asymptotics Econometrics, chapter 6.1-6.5 Python basics Quant-Econ Tutorial Readings Athey, S., \u0026amp; Imbens, G. W. (n.d.). Machine Learning Methods Economists Should Know About. 62. Belloni, A., Chen, H., Chernozhukov, V., \u0026amp; Hansen, C. B. (2012). Sparse Models and Methods for Optimal Instruments With an Application to Eminent Domain. Econometrica, 80(6), 2369–2429. https://doi.org/10.3982/ECTA9626 Belloni, A., Chernozhukov, V., \u0026amp; Hansen, C. (2014). Inference on Treatment Effects after Selection among High-Dimensional Controls. The Review of Economic Studies, 81(2), 608–650. https://doi.org/10.1093/restud/rdt044 Chernozhukov, V., Chetverikov, D., Demirer, M., Duflo, E., Hansen, C., Newey, W., \u0026amp; Robins, J. (2018). Double/debiased machine learning for treatment and structural parameters. The Econometrics Journal, 21(1), C1–C68. https://doi.org/10.1111/ectj.12097 Franks, A., Miller, A., Bornn, L., \u0026amp; Goldsberry, K. (2015). Characterizing the spatial structure of defensive skill in professional basketball. The Annals of Applied Statistics, 9(1), 94–121. https://doi.org/10.1214/14-AOAS799 Gentzkow, M., Shapiro, J. M., \u0026amp; Taddy, M. (2019). Measuring Group Differences in High‐Dimensional Choices: Method and Application to Congressional Speech. Econometrica, 87(4), 1307–1340. https://doi.org/10.3982/ECTA16566 Kleinberg, J., Lakkaraju, H., Leskovec, J., Ludwig, J., \u0026amp; Mullainathan, S. (2017). Human Decisions and Machine Predictions. The Quarterly Journal of Economics. https://doi.org/10.1093/qje/qjx032 Kleinberg, J., Ludwig, J., Mullainathan, S., \u0026amp; Obermeyer, Z. (2015). Prediction Policy Problems. American Economic Review, 105(5), 491–495. https://doi.org/10.1257/aer.p20151023 Mullainathan, S., \u0026amp; Spiess, J. (2017). Machine Learning: An Applied Econometric Approach. Journal of Economic Perspectives, 31(2), 87–106. https://doi.org/10.1257/jep.31.2.87 Wager, S., \u0026amp; Athey, S. (2018). Estimation and Inference of Heterogeneous Treatment Effects using Random Forests. Journal of the American Statistical Association, 113(523), 1228–1242. https://doi.org/10.1080/01621459.2017.1319839 Sources These exercise sessions heavily borrow from\nJordi Warmenhoven\u0026rsquo;s git repo ISLR-python Quant-Econ website Prof. Damian Kozbur past UZH PhD Econometrics Class Clark Science Center Machine Learning couse UC Berkeley Convex Optimization and Approximation class by Moritz Hardt Morvan Zhou and Yunjey Choi pytorch tutorials Daniel Godoy excellent article on Pytorch in Medium\u0026rsquo;s towardsdatascience ","date":1633046400,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":1633046400,"objectID":"ed1edeaa10df72e3bccd0c8259f6ef0b","permalink":"https://matteocourthoud.github.io/course/ml-econ/","publishdate":"2021-10-01T00:00:00Z","relpermalink":"/course/ml-econ/","section":"course","summary":"Welcome to my notes for the Machine Learning for Economic Analysis course by Damian Kozbur @UZH!\nThe exercise sessions are entirely coded in Python on Jupyter Notebooks. The examples heavily borrow from An Introduction to Statistical Learning by James, Witten, Tibshirani, Friedman and its advanced version Elements of Statistical Learning by Hastie, Tibshirani, Friedman.","tags":["tutorial","economics","programming","julia"],"title":"Machine Learning for Economics","type":"book"},{"authors":null,"categories":null,"content":"⚠️ WORK IN PROGRESS! ⚠️\nWelcome to my notes and code for a graduate course in Empirical Industrial Organization!\nAll code is written in Julia. I borrow heavily from other sources (below), but all mistakes are mine. The RMarkdown notebooks can be found here.\nPlease, if you find any typos or mistakes, open a new issue. Or even better, fork the repo and submit a pull request. I am happy to share my work and I am even happier if it can be useful.\nContent The course will cover the following content\nIntroduction Production Function Estimation Demand Estimation [slides] Demand Identification Merger Analysis Entry and Exit Single-Agent Dynamics [slides] Dynamic Games [slides] Auctions Media The course also covers the baseline replication of the following papers\nLogit Demand [slides] Berry, Levinsohn, Pakes (1995) [slides] Rust (1987) [slides] Sources The course heavily borrows from the following sources\nKohei Kawaguchi’s Empirical IO course in R Gregory Crawford Graduate Empirical IO course at UZH Phil Haile and Mitrsuru Igami Graduate Empirical IO course at Yale More references can be found within each single session.\n","date":1636502400,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":1636502400,"objectID":"3afc2fc1930ae28d1e7933e2e3c47670","permalink":"https://matteocourthoud.github.io/course/empirical-io/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/course/empirical-io/","section":"course","summary":"These are my notes and code for an advanced course in Empirical Industrial Organization. All code is written in Julia. The original notebooks can be found on my Github page.","tags":null,"title":"PhD Industrial Organization","type":"book"},{"authors":null,"categories":null,"content":"Welcome to my lecture notes for graduate Econometrics!\nThese notes were initially born as my personal summary for the PhD Econometrics course of professor Damian Kozbur in Zurich. The first draft was the result of an intense collaborative effort together with Chiara Aina and Paolo Mengano. During the years I have expanded the first draft in order to make it more comprehensive and include Julia code examples. All errors are mine.\nPlease, if you find any typos or mistakes, open a new issue. Or even better, fork the repo and submit a pull request. I am happy to share my work and I am even happier if it can be useful.\nContent My notes cover the following content\nMatrix Algebra [slides] Hansen (2021). \u0026ldquo;Econometrics\u0026rdquo;. Appendix A. Greene (2006). \u0026ldquo;Econometric Analysis\u0026rdquo;. Appendix A: Matrix Algebra. Probability Theory [slides] Greene (2006). \u0026ldquo;Econometric Analysis\u0026rdquo;. Appendix B: Probability and Distribution Theory. Greene (2006). \u0026ldquo;Econometric Analysis\u0026rdquo;. Appendix C: Estimation and Inference. Asymptotic Theory [slides] Hansen (2021). \u0026ldquo;Econometrics\u0026rdquo;. Chapters 6. Wooldridge (2010). \u0026ldquo;Econometric Analysis of Cross Section and Panel Data\u0026rdquo;. Chapter 3: Basic Asymptotic Theory. Halmos (2006). \u0026ldquo;Lectures on Ergodic Theory\u0026rdquo;. Greene (2006). \u0026ldquo;Econometric Analysis\u0026rdquo;. Appendix D: Large Sample Distribution Theory. Hayashi (2000). \u0026ldquo;Econometrics\u0026rdquo;. Chapter 2: Large-Sample Theory. Inference [slides] Hansen (2021). \u0026ldquo;Econometrics\u0026rdquo;. Chapters 9: Hypothesis Testing. OLS Algebra [slides] Hansen (2021). \u0026ldquo;Econometrics\u0026rdquo;. Chapters 3 and 4. Hayashi (2000). \u0026ldquo;Econometrics\u0026rdquo;. Chapter 1: Finite-Sample Properties of OLS. OLS Inference [slides] Hansen (2021). \u0026ldquo;Econometrics\u0026rdquo;. Chapters 7. Hayashi (2000). \u0026ldquo;Econometrics\u0026rdquo;. Chapter 2: Large-Sample Theory. Endogeneity [slides] Hansen (2021). \u0026ldquo;Econometrics\u0026rdquo;. Chapters 12 and 13. Hayashi (2000). \u0026ldquo;Econometrics\u0026rdquo;. Chapter 3: Single-Equation GMM. Belloni, A., Chen, H., Chernozhukov, V., \u0026amp; Hansen, C. B. (2012). Sparse Models and Methods for Optimal Instruments With an Application to Eminent Domain. Econometrica, 80(6), 2369–2429. Non-parametrics [slides] Hansen (2021). \u0026ldquo;Econometrics\u0026rdquo;. Chapters 19, 20 and 21. Newey, W. K. (1997). Convergence rates and asymptotic normality for series estimators. Journal of Econometrics, 79(1), 147–168. Selection [slides] Hansen (2021). \u0026ldquo;Econometrics\u0026rdquo;. Chapter 24. Hastie, Tibshirani, Friedman (2001). \u0026ldquo;The Elements of Statistical Learning\u0026rdquo;. Belloni, A., Chernozhukov, V., \u0026amp; Hansen, C. (2014). Inference on Treatment Effects after Selection among High-Dimensional Controls. The Review of Economic Studies, 81(2), 608–650. Sources Kozbur (2019), PhD Econometrics - Lecture Notes. Hansen (2021), \u0026ldquo;Econometrics\u0026rdquo;. Wooldridge (2010), \u0026ldquo;Econometric Analysis of Cross Section and Panel Data\u0026rdquo;. Greene (2006), \u0026ldquo;Econometric Analysis\u0026rdquo;. Hayashi (2000), \u0026ldquo;Econometrics\u0026rdquo;. ","date":1635465600,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":1635465600,"objectID":"fbda65a0e2851a55117043a99bfb2df3","permalink":"https://matteocourthoud.github.io/course/metrics/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/course/metrics/","section":"course","summary":"These are my notes and code for an advanced course in Econometrics. All code is written in Julia. The original notebooks can be found on my Github page.","tags":null,"title":"PhD Econometrics","type":"book"},{"authors":null,"categories":null,"content":"import numpy as np import pandas as pd For most of this course we are going to work with pandas DataFrames. However, it\u0026rsquo;s important to start with an introduction to the different types of data structures available in Python, their characteristics, their differences and their comparative advantages.\nThe most important data structures in Python are:\nlists tuples sets dictionaries numpy arrays pandas DataFrames pyspark DataFrames Lists A list is a mutable array data structure in Python with the following characteristics:\ncan hold any type of data can hold different types of data at the same time can be modified We can generate lists using square brackets.\nl = [12, \u0026quot;world\u0026quot;, [3,4,5]] print(l) [12, 'world', [3, 4, 5]] Since lists are ordered, we can access their element by calling the position of the element in the list.\nl[0] 12 Since lists are mutable, we can modify their elements.\nl[0] = 'hello' print(l) ['hello', 'world', [3, 4, 5]] We can add elements to a list using .append().\nl.append(23) l ['hello', 'world', [3, 4, 5], 23] We can remove elements by calling del on it.\ndel l[0] print(l) ['world', [3, 4, 5], 23] We can combine two lists using +. Note that this operation does not modify the list but generates a new one.\nl + [23] ['world', [3, 4, 5], 23, 23] We can also generate lists using comprehensions.\nl = [n for n in range(3)] print(l) [0, 1, 2] Comprehensions are a powerful tool!\nl = [n+10 for n in range(10) if (n%2==0) and (n\u0026gt;4)] print(l) [16, 18] Tuples A tuple is an immutable array data structure in Python with the following characteristics:\ncan hold any type of data can hold different types of data at the same time can not be modified We can generate tuples using curve brackets.\n# A list of different data types t = (12, \u0026quot;world\u0026quot;, [3,4,5]) print(t) (12, 'world', [3, 4, 5]) Since tuples are ordered, we can access their element by calling the position of the element in the list.\nt[0] 12 Since tuples are unmutable, we cannot modify their elements.\n# Try to modify element try: t[0] = 'hello' except Exception as e: print(e) 'tuple' object does not support item assignment # Try to add element try: t.append('hello') except Exception as e: print(e) 'tuple' object has no attribute 'append' # Try to remove element try: del t[0] except Exception as e: print(e) 'tuple' object doesn't support item deletion We can combine two tuples using +. Note that this operation does not modify the tuple but generates a new one. Also note that to generate a 1-element tuple we need to insert a comma.\nt + (23,) (12, 'world', [3, 4, 5], 23) We can generate tuples using comprehensions, but we need to specify it\u0026rsquo;s a tuple.\nt = tuple(n for n in range(3)) print(t) (0, 1, 2) Sets A set is a mutable array data structure in Python with the following characteristics:\ncan only hold hashable types can hold different types of data at the same time cannot be modified cannot contain duplicates We can generate using curly brackets.\ns = {12, \u0026quot;world\u0026quot;, (3,4,5)} print(s) {(3, 4, 5), 12, 'world'} Since sets are unordered and unindexed, we cannot access single elements by calling their position.\n# Try to access element by position try: s[0] except Exception as e: print(e) 'set' object is not subscriptable Since sets are unordered, we cannot modify their elements by specifying the position.\n# Try to modify element try: s[0] = 'hello' except Exception as e: print(e) 'set' object does not support item assignment However, since sets are mutable, we can add elements using .add().\ns.add('hello') print(s) {'hello', (3, 4, 5), 12, 'world'} However, we cannot add duplicates.\ns.add('hello') print(s) {'hello', (3, 4, 5), 12, 'world'} We can delete elements of a set using .remove().\ns.remove('hello') print(s) {(3, 4, 5), 12, 'world'} We can also generate sets using comprehensions.\ns = {n for n in range(3)} print(s) {0, 1, 2} Dictionaries A dictionary is a mutable array data structure in Python with the following characteristics:\ncan hold any type can hold different types at the same time can be modified items are named We can generate dictionaries using curly brackets. Since elements are indexed by keys, we have to provide one for each element.\nDictionary keys can be of any hashable type. A hashable object has a hash value that never changes during its lifetime, and it can be compared to other objects. Hashable objects that compare as equal must have the same hash value.\nImmutable types like strings and numbers are hashable and work well as dictionary keys. You can also use tuple objects as dictionary keys as long as they contain only hashable types themselves.\nd = {\u0026quot;first\u0026quot;: 12, 2: \u0026quot;world\u0026quot;, (3,): [3,4,5]} print(d) {'first': 12, 2: 'world', (3,): [3, 4, 5]} Since dictionaries are indexed but not ordered, we can only access elements by the corresponding key. If the corresponding key does not exist, we do not access any element.\ntry: d[0] except Exception as e: print(e) 0 We can access all values of the dictionary using .values().\nd.values() dict_values([12, 'world', [3, 4, 5]]) We can access all keys of the dictionary using .keys().\nd.keys() dict_keys(['first', 2, (3,)]) We can access both values and keys using .items().\nd.items() dict_items([('first', 12), (2, 'world'), ((3,), [3, 4, 5])]) This gives us a list of tuples which we can iterate on.\n[f\u0026quot;{key}: {value}\u0026quot; for key, value in d.items()] ['first: 12', '2: world', '(3,): [3, 4, 5]'] Since dictionaries are mutable, we can modify their elements.\nd[\u0026quot;first\u0026quot;] = 'hello' print(d) {'first': 'hello', 2: 'world', (3,): [3, 4, 5]} If we try to modify an element that does not exist, the element is added to the dictionary.\nd[0] = 'hello' print(d) {'first': 'hello', 2: 'world', (3,): [3, 4, 5], 0: 'hello'} We can remove elements using del.\ndel d[0] print(d) {'first': 'hello', 2: 'world', (3,): [3, 4, 5]} We can cannot combine two dictionaries using +. We can only add one element at the time.\ntry: d + {\u0026quot;fourth\u0026quot;: (1,2)} except Exception as e: print(e) unsupported operand type(s) for +: 'dict' and 'dict' We can also generate dictionaries using comprehensions.\nd = {f\u0026quot;k{n}\u0026quot;: n+1 for n in range(3)} print(d) {'k0': 1, 'k1': 2, 'k2': 3} Numpy Arrays A numpy array is a mutable array data structure in Python with the following characteristics:\ncan hold any type of data can only hold one type at the same time can be modified can be multi-dimensional support matrix operations We can generate numpy arrays are generated the np.array() function on a list.\na = np.array([1,2,3]) print(a) [1 2 3] We can make 2-dimensional arrays (a matrix) as lists of lists.\nm = np.array([[1,2,3] , [4,5,6]]) print(m) [[1 2 3] [4 5 6]] Since numpy arrays are mutable, we can modify elements by calling the index of the numpy array.\nm[0,0] = 89 print(m) [[89 2 3] [ 4 5 6]] We can check the shape of a numpy array using .shape\nm.shape (2, 3) We can expand dimensions of a numpy array using .expand_dims().\na = np.expand_dims(a,1) print(a) [[1] [2] [3]] We can transpose matrices using .T.\na = a.T print(a) [[1 2 3]] We add elements to a numpy array using np.concatenate(). All elements must have the same number of dimensions and must have the same number of elements along the concatenation axis.\nm = np.concatenate((m, a), axis=0) print(m) [[89 2 3] [ 4 5 6] [ 1 2 3]] We cannot remove elements of numpy arrays.\ntry: del a[0] except Exception as e: print(e) cannot delete array elements We can do matrix operations between numpy arrays. For example, we can do multiplication with @.\na @ m array([[100, 18, 24]]) If we use * we get dot (or element-wise) multiplication instead.\na * m array([[89, 4, 9], [ 4, 10, 18], [ 1, 4, 9]]) There is a wide array of functions available in numpy. For example np.invert() inverts a squared matrix.\nnp.invert(m) array([[-90, -3, -4], [ -5, -6, -7], [ -2, -3, -4]]) Comparison Which data structure should you use and why? Let\u0026rsquo;s compare different data types\nK = 100_000 l = [n for n in range(K)] t = tuple(n for n in range(K)) s = {n for n in range(K)} a = np.array([n for n in range(K)]) Size\nWhich data type is more efficient?\nimport sys def compare_size(list_objects): for o in list_objects: print(f\u0026quot;Size of {type(o)}: {sys.getsizeof(o)}\u0026quot;) compare_size([l, t, s, a]) Size of \u0026lt;class 'list'\u0026gt;: 800984 Size of \u0026lt;class 'tuple'\u0026gt;: 800040 Size of \u0026lt;class 'set'\u0026gt;: 4194520 Size of \u0026lt;class 'numpy.ndarray'\u0026gt;: 800104 Size is very similar for lists, tuples and numpy arrays.\nSpeed\nWhich data type is faster?\nimport time def compare_time(list_objects): for o in list_objects: start = time.time() [x**2 for x in o] end = time.time() print(f\u0026quot;Time of {type(o)}: {end - start}\u0026quot;) compare_time([l, t, s, a]) Time of \u0026lt;class 'list'\u0026gt;: 0.021067142486572266 Time of \u0026lt;class 'tuple'\u0026gt;: 0.02071404457092285 Time of \u0026lt;class 'set'\u0026gt;: 0.021012067794799805 Time of \u0026lt;class 'numpy.ndarray'\u0026gt;: 0.010493993759155273 Numpy arrays are faster at math operations.\n","date":1651363200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1651363200,"objectID":"33f7e685bb800e80f98e57773c27fa92","permalink":"https://matteocourthoud.github.io/course/data-science/01_data_structures/","publishdate":"2022-05-01T00:00:00Z","relpermalink":"/course/data-science/01_data_structures/","section":"course","summary":"import numpy as np import pandas as pd For most of this course we are going to work with pandas DataFrames. However, it\u0026rsquo;s important to start with an introduction to the different types of data structures available in Python, their characteristics, their differences and their comparative advantages.","tags":null,"title":"Data Structures","type":"book"},{"authors":null,"categories":null,"content":"This chapter follows closely Chapter 3 of An Introduction to Statistical Learning by James, Witten, Tibshirani, Friedman.\n# Remove warnings import warnings warnings.filterwarnings('ignore') # Import everything import pandas as pd import numpy as np import seaborn as sns import statsmodels.api as sm from sklearn.linear_model import LinearRegression from numpy.linalg import inv from numpy.random import normal as rnorm from statsmodels.stats.outliers_influence import OLSInfluence # Setup matplotlib for graphs import matplotlib.pyplot as plt from mpl_toolkits.mplot3d import axes3d # Set global parameters %matplotlib inline plt.style.use('seaborn-white') plt.rcParams['lines.linewidth'] = 3 plt.rcParams['figure.figsize'] = (10,6) plt.rcParams['figure.titlesize'] = 20 plt.rcParams['axes.titlesize'] = 18 plt.rcParams['axes.labelsize'] = 14 plt.rcParams['legend.fontsize'] = 14 You can inspect all the available global parameter options here.\n1.1 Simple Linear Regression First, let\u0026rsquo;s load the Advertising dataset. It contains information on displays sales (in thousands of units) for a particular product and a list of advertising budgets (in thousands of dollars) for TV, radio, and newspaper media.\nWe open the dataset using the pandas library which is the library for handling datasets and data analysis in Python.\n# Advertisement spending data advertising = pd.read_csv('data/Advertising.csv', usecols=[1,2,3,4]) Let\u0026rsquo;s have a look at the content. We can have a glance at the first rows by using the function head.\n# Preview of the data advertising.head() TV Radio Newspaper Sales 0 230.1 37.8 69.2 22.1 1 44.5 39.3 45.1 10.4 2 17.2 45.9 69.3 9.3 3 151.5 41.3 58.5 18.5 4 180.8 10.8 58.4 12.9 We can have a general overview of the dataset using the function info.\n# Overview of all variables advertising.info() \u0026lt;class 'pandas.core.frame.DataFrame'\u0026gt; RangeIndex: 200 entries, 0 to 199 Data columns (total 4 columns): # Column Non-Null Count Dtype --- ------ -------------- ----- 0 TV 200 non-null float64 1 Radio 200 non-null float64 2 Newspaper 200 non-null float64 3 Sales 200 non-null float64 dtypes: float64(4) memory usage: 6.4 KB We can have more information on the single variables using the function describe.\n# Summary of all variables advertising.describe() TV Radio Newspaper Sales count 200.000000 200.000000 200.000000 200.000000 mean 147.042500 23.264000 30.554000 14.022500 std 85.854236 14.846809 21.778621 5.217457 min 0.700000 0.000000 0.300000 1.600000 25% 74.375000 9.975000 12.750000 10.375000 50% 149.750000 22.900000 25.750000 12.900000 75% 218.825000 36.525000 45.100000 17.400000 max 296.400000 49.600000 114.000000 27.000000 If you just want to call a variable in pandas, you have 3 options:\nuse squared brackets as if the varaible was a component of a dictionary use or dot subscripts as if the variable was a function of the data use the loc function (best practice) # 1. Brackets advertising['TV'] 0 230.1 1 44.5 2 17.2 3 151.5 4 180.8 ... 195 38.2 196 94.2 197 177.0 198 283.6 199 232.1 Name: TV, Length: 200, dtype: float64 # 2. Brackets advertising.TV 0 230.1 1 44.5 2 17.2 3 151.5 4 180.8 ... 195 38.2 196 94.2 197 177.0 198 283.6 199 232.1 Name: TV, Length: 200, dtype: float64 # The loc function advertising.loc[:,'TV'] 0 230.1 1 44.5 2 17.2 3 151.5 4 180.8 ... 195 38.2 196 94.2 197 177.0 198 283.6 199 232.1 Name: TV, Length: 200, dtype: float64 Note that the loc function is more powerful and is generally used to subset lines and columns.\n# Select multiple columns and subset of rows advertising.loc[0:5,['Sales','TV']] Sales TV 0 22.1 230.1 1 10.4 44.5 2 9.3 17.2 3 18.5 151.5 4 12.9 180.8 5 7.2 8.7 Suppose we are interested in the (linear) relationship between sales and tv advertisement.\n$$ sales ≈ \\beta_0 + \\beta_1 TV. $$\nHow are the two two variables related? Visual inspection: scatterplot.\n# Figure 3.1 def make_fig_3_1a(): # Init figure fig, ax = plt.subplots(1,1) ax.set_title('Figure 3.1'); # Plot scatter and best fit line sns.regplot(x=advertising.TV, y=advertising.Sales, ax=ax, order=1, ci=None, scatter_kws={'color':'r', 's':20}) ax.set_xlim(-10,310); ax.set_ylim(ymin=0) ax.legend(['Least Squares Fit','Data']); make_fig_3_1a() Estimating the Coefficients How do we estimate the best fit line? Minimize the Residual Sum of Squares (RSS).\nFirst, suppose we have a dataset $\\mathcal D = {x_i, y_i}_{i=1}^N$. We define the prediction of $y$ based on $X$ as\n$$ \\hat y_i = \\hat \\beta X_i $$\nThe residuals are the unexplained component of $y$\n$$ e_i = y_i - \\hat y_i $$\nOur objective function (to be minimized) is the Resdual Sum of Squares (RSS):\n$$ RSS := \\sum_{n=1}^N e_i^2 $$\nAnd the OLS coefficient is defined as its minimizer:\n$$ \\hat \\beta_{OLS} := \\arg\\min_{\\beta} \\sum_{n=1}^N e_i^2 = \\arg\\min_{\\beta} \\sum_{n=1}^N (y_i - X_i \\beta)^2 $$\nLet\u0026rsquo;s use the sklearn library to fit a linear regression model of Sales on TV advertisement.\n# Define X and y X = advertising.TV.values.reshape(-1,1) y = advertising.Sales.values # Fit linear regressions reg = LinearRegression().fit(X,y) print(reg.intercept_) print(reg.coef_) 7.0325935491276885 [0.04753664] We can visualize the residuals as the vertical distances between the data and the prediction line. The objective function RSS is the sum of the squares of the lengths of vertical lines.\n# Compute predicted values y_hat = reg.predict(X) # Figure 3.1 def make_figure_3_1b(): # Init figure fig, ax = plt.subplots(1,1) ax.set_title('Figure 3.1'); # Add residuals sns.regplot(x=advertising.TV, y=advertising.Sales, ax=ax, order=1, ci=None, scatter_kws={'color':'r', 's':20}) ax.vlines(X, np.minimum(y,y_hat), np.maximum(y,y_hat), linestyle='--', color='k', alpha=0.5, linewidth=1) plt.legend(['Least Squares Fit','Data','Residuals']); make_figure_3_1b() The closed form solution in matrix algebra is $$ \\hat \\beta_{OLS} = (X\u0026rsquo;X)^{-1}(X\u0026rsquo;y) $$\nPython has a series of shortcuts to make the syntax less verbose. However, we still need to import the inv function from numpy. In Matlab it would be (X'*X)^{-1}*(X'*y), almost literal.\n# Compute OLS coefficient with matrix algebra beta = inv(X.T @ X) @ X.T @ y print(beta) [0.08324961] Why is the result different?\nWe are missing one coefficient: the intercept. Our regression now looks like this\n# New figure 1 def make_new_figure_1(): # Init figure fig, ax = plt.subplots(1,1) fig.suptitle('Role of the Intercept') # Add new line on the previous plot sns.regplot(x=advertising.TV, y=advertising.Sales, ax=ax, order=1, ci=None, scatter_kws={'color':'r', 's':10}) ax.plot(X, beta*X, color='g') plt.xlim(-10,310); plt.ylim(ymin=0); ax.legend(['With Intercept', 'Without intercept']); make_new_figure_1() How do we insert an intercept using matrix algebra? We add a column of ones.\n$$ X_1 = [\\boldsymbol{1}, X] $$\n# How to insert intercept? Add constant: column of ones one = np.ones(np.shape(X)) X1 = np.concatenate([one,X],axis=1) print(np.shape(X1)) (200, 2) Now we compute again the coefficients as before.\n$$ \\hat \\beta_{OLS} = (X_1\u0026rsquo;X_1)^{-1}(X_1\u0026rsquo;y) $$\n# Compute beta OLS with intercept beta_OLS = inv(X1.T @ X1) @ X1.T @ y print(beta_OLS) [7.03259355 0.04753664] Now we have indeed obtained the same exact coefficients.\nWhat does minimizing the Residual Sum of Squares means in practice? How does the objective function looks like?\nfrom sklearn.preprocessing import scale # First, scale the data X = scale(advertising.TV, with_mean=True, with_std=False).reshape(-1,1) y = advertising.Sales regr = LinearRegression().fit(X,y) # Create grid coordinates for plotting B0 = np.linspace(regr.intercept_-2, regr.intercept_+2, 50) B1 = np.linspace(regr.coef_-0.02, regr.coef_+0.02, 50) xx, yy = np.meshgrid(B0, B1, indexing='xy') Z = np.zeros((B0.size,B1.size)) # Calculate Z-values (RSS) based on grid of coefficients for (i,j),v in np.ndenumerate(Z): Z[i,j] =((y - (xx[i,j]+X.ravel()*yy[i,j]))**2).sum()/1000 # Minimized RSS min_RSS = r'$\\beta_0$, $\\beta_1$ for minimized RSS' min_rss = np.sum((regr.intercept_+regr.coef_*X - y.values.reshape(-1,1))**2)/1000 min_rss 2.1025305831313514 # Figure 3.2 - Regression coefficients - RSS def make_fig_3_2(): fig = plt.figure(figsize=(15,6)) fig.suptitle('RSS - Regression coefficients') ax1 = fig.add_subplot(121) ax2 = fig.add_subplot(122, projection='3d') # Left plot CS = ax1.contour(xx, yy, Z, cmap=plt.cm.Set1, levels=[2.15, 2.2, 2.3, 2.5, 3]) ax1.scatter(regr.intercept_, regr.coef_[0], c='r', label=min_RSS) ax1.clabel(CS, inline=True, fontsize=10, fmt='%1.1f') # Right plot ax2.plot_surface(xx, yy, Z, rstride=3, cstride=3, alpha=0.3) ax2.contour(xx, yy, Z, zdir='z', offset=Z.min(), cmap=plt.cm.Set1, alpha=0.4, levels=[2.15, 2.2, 2.3, 2.5, 3]) ax2.scatter3D(regr.intercept_, regr.coef_[0], min_rss, c='r', label=min_RSS) ax2.set_zlabel('RSS') ax2.set_zlim(Z.min(),Z.max()) ax2.set_ylim(0.02,0.07) # settings common to both plots for ax in fig.axes: ax.set_xlabel(r'$\\beta_0$') ax.set_ylabel(r'$\\beta_1$') ax.set_yticks([0.03,0.04,0.05,0.06]) ax.legend() make_fig_3_2() Assessing the Accuracy of the Coefficient Estimates How accurate is our regression fit? Suppose we were drawing different (small) samples from the same data generating process, for example\n$$ y_i = 2 + 3x_i + \\varepsilon_i $$\nwhere $x_i \\sim N(0,1)$ and $\\varepsilon \\sim N(0,3)$.\n# Init N = 30; # Sample size K = 100; # Number of simulations beta_hat = np.zeros((2,K)) x = np.linspace(-4,4,N) # Set seed np.random.seed(1) # K simulations for i in range(K): # Simulate data x1 = np.random.normal(0,1,N).reshape([-1,1]) X = np.concatenate([np.ones(np.shape(x1)), x1], axis=1) epsilon = np.random.normal(0,5,N) beta0 = [2,3] y = X @ beta0 + epsilon # Estimate coefficients beta_hat[:,i] = inv(X.T @ X) @ X.T @ y # new figure 2 def make_new_fig_2(): # Init figure fig, ax = plt.subplots(1,1) for i in range(K): # Plot line ax.plot(x, beta_hat[0,i] + x*beta_hat[1,i], color='blue', alpha=0.2, linewidth=1) if i==K-1: ax.plot(x, beta_hat[0,i] + x*beta_hat[1,i], color='blue', alpha=0.2, linewidth=1, label='Estimated Lines') # Plot true line ax.plot(x, 2 + 3*x, color='red', linewidth=3, label='True Line'); ax.set_xlabel('X'); ax.set_ylabel('Y'); ax.legend(); ax.set_xlim(-4,4); make_new_fig_2() The regplot command lets us automatically draw confidence intervals. Let\u0026rsquo;s draw the last simulated dataset with conficence intervals.\nfig, ax = plt.subplots(1,1) # Plot last simulation scatterplot with confidence interval sns.regplot(x=x1, y=y, ax=ax, order=1, scatter_kws={'color':'r', 's':20}); ax.set_xlabel('X'); ax.set_ylabel('Y'); ax.legend(['Best fit','Data', 'Confidence Intervals']); As we can see, depending on the sample, we get a different estimate of the linear relationship between $x$ and $y$. However, there estimates are on average correct. Indeed, we can visualize their distribution.\n# Plot distribution of coefficients plot = sns.jointplot(x=beta_hat[0,:], y=beta_hat[1,:], color='red', edgecolor=\u0026quot;white\u0026quot;); plot.ax_joint.axvline(x=2); plot.ax_joint.axhline(y=3); plot.set_axis_labels('beta_0', 'beta_1'); How do we compute confidence intervals by hand?\n$$ Var(\\hat \\beta_{OLS}) = \\sigma^2 (X\u0026rsquo;X)^{-1} $$\nwhere $\\sigma^2 = Var(\\varepsilon)$. Since we do not know $Var(\\varepsilon)$, we estimate it as $Var(e)$.\n$$ \\hat Var(\\hat \\beta_{OLS}) = \\hat \\sigma^2 (X\u0026rsquo;X)^{-1} $$\nIf we assume the standard errors are normally distributed (or we apply the Central Limit Theorem, assuming $n \\to \\infty$), a 95% confidence interval for the OLS coefficient takes the form\n$$ CI(\\hat \\beta_{OLS}) = \\Big[ \\hat \\beta_{OLS} - 1.96 \\times \\hat SE(\\hat \\beta_{OLS}) \\ , \\ \\hat \\beta_{OLS} + 1.96 \\times \\hat SE(\\hat \\beta_{OLS}) \\Big] $$\nwhere $\\hat SE(\\hat \\beta_{OLS}) = \\sqrt{\\hat Var(\\hat \\beta_{OLS})}$.\n# Import again X and y from example above X = advertising.TV.values.reshape(-1,1) X1 = np.concatenate([np.ones(np.shape(X)), X], axis=1) y = advertising.Sales.values # Compute residual variance X_hat = X1 @ beta_OLS e = y - X_hat sigma_hat = np.var(e) var_beta_OLS = sigma_hat * inv(X1.T @ X1) # Take elements on the diagonal and square them std_beta_OLS = [var_beta_OLS[0,0]**.5, var_beta_OLS[1,1]**.5] print(std_beta_OLS) [0.4555479737400674, 0.0026771203500466564] The statsmodels library allows us to produce nice tables with parameter estimates and standard errors.\n# Table 3.1 \u0026amp; 3.2 est = sm.OLS.from_formula('Sales ~ TV', advertising).fit() est.summary().tables[1] coef std err t P\u003e|t| [0.025 0.975] Intercept 7.0326 0.458 15.360 0.000 6.130 7.935 TV 0.0475 0.003 17.668 0.000 0.042 0.053 Assessing the Accuracy of the Model What metrics can we use to assess whether the model is a good model, in terms of capturing the relationship between the variables?\nFirst, we can compute our objective function: the Residual Sum of Squares (RSS). Lower values of our objective function imply that we got a better fit.\n# RSS with regression coefficients RSS = sum(e**2) print(RSS) 2102.530583131351 The problem with RSS as a metric is that it\u0026rsquo;s hard to compare different regressions since its scale depends on the magnitude of the variables.\nOne measure of fit that does not depend on the magnitude of the variables is $R^2$: the percentage of our explanatory variable explained by the model\n$$ R^2 = 1 - \\frac{\\text{RSS}}{\\text{TSS}} $$\nwhere\n$$ TSS = \\sum_{i=1}^N (y_i - \\bar y)^2 $$\n# TSS TSS = sum( (y-np.mean(y))**2 ) # R2 R2 = 1 - RSS/TSS print(R2) 0.6118750508500709 Can the $R^2$ metric be negative? When?\n2.2 Multiple Linear Regression What if we have more than one explanatory variable? Spoiler: we already did, but one was a constant.\nLet\u0026rsquo;s have a look at the regression of Sales on Radio and TV advertisement expenditure separately.\n# Table 3.3 (1) est = sm.OLS.from_formula('Sales ~ Radio', advertising).fit() est.summary().tables[1] coef std err t P\u003e|t| [0.025 0.975] Intercept 9.3116 0.563 16.542 0.000 8.202 10.422 Radio 0.2025 0.020 9.921 0.000 0.162 0.243 # Table 3.3 (2) est = sm.OLS.from_formula('Sales ~ Newspaper', advertising).fit() est.summary().tables[1] coef std err t P\u003e|t| [0.025 0.975] Intercept 12.3514 0.621 19.876 0.000 11.126 13.577 Newspaper 0.0547 0.017 3.300 0.001 0.022 0.087 It seems that both Radio and Newspapers are positively correlated with Sales. Why don\u0026rsquo;t we estimate a unique regression with both dependent variables?\nEstimating the Regression Coefficients Suppose now we enrich our previous model adding all different forms of advertisement:\n$$ \\text{Sales} = \\beta_0 + \\beta_1 \\text{TV} + \\beta_2 \\text{Radio} + \\beta_3 \\text{Newspaper} + \\varepsilon $$\nWe estimate it using the statsmodels ols library.\n# Table 3.4 est = sm.OLS.from_formula('Sales ~ TV + Radio + Newspaper', advertising).fit() est.summary().tables[1] coef std err t P\u003e|t| [0.025 0.975] Intercept 2.9389 0.312 9.422 0.000 2.324 3.554 TV 0.0458 0.001 32.809 0.000 0.043 0.049 Radio 0.1885 0.009 21.893 0.000 0.172 0.206 Newspaper -0.0010 0.006 -0.177 0.860 -0.013 0.011 Why now it seems that there is no relationship between Sales and Newspaper while the univariate regression told us the opposite?\nLet\u0026rsquo;s explore the correlation between those variables.\n# Table 3.5 - Correlation Matrix advertising.corr() TV Radio Newspaper Sales TV 1.000000 0.054809 0.056648 0.782224 Radio 0.054809 1.000000 0.354104 0.576223 Newspaper 0.056648 0.354104 1.000000 0.228299 Sales 0.782224 0.576223 0.228299 1.000000 Let\u0026rsquo;s try to inspect the relationship visually. Note that now the linear best fit is going to be 3-dimensional. In order to make it visually accessible, we consider only on TV and Radio advertisement expediture as dependent variables. The best fit will be a plane instead of a line.\n# Fit regression est = sm.OLS.from_formula('Sales ~ Radio + TV', advertising).fit() print(est.params) Intercept 2.921100 Radio 0.187994 TV 0.045755 dtype: float64 # Create a coordinate grid Radio = np.arange(0,50) TV = np.arange(0,300) B1, B2 = np.meshgrid(Radio, TV, indexing='xy') # Compute predicted plane Z = np.zeros((TV.size, Radio.size)) for (i,j),v in np.ndenumerate(Z): Z[i,j] =(est.params[0] + B1[i,j]*est.params[1] + B2[i,j]*est.params[2]) # Compute residuals e = est.predict() - advertising.Sales # Figure 3.5 - Multiple Linear Regression def make_fig_3_5(): # Init figure fig = plt.figure() ax = axes3d.Axes3D(fig, auto_add_to_figure=False) fig.add_axes(ax) fig.suptitle('Figure 3.5'); # Plot best fit plane ax.plot_surface(B1, B2, Z, color='k', alpha=0.3) points = ax.scatter3D(advertising.Radio, advertising.TV, advertising.Sales, c=e, cmap=\u0026quot;seismic\u0026quot;, vmin=-5, vmax=5) plt.colorbar(points, cax=fig.add_axes([0.9, 0.1, 0.03, 0.8])) ax.set_xlabel('Radio'); ax.set_xlim(0,50) ax.set_ylabel('TV'); ax.set_ylim(bottom=0) ax.set_zlabel('Sales'); ax.view_init(20, 20) make_fig_3_5() Some Important Questions How do you check whether the model fit well the data with multiple regressors? statmodels and most regression packages automatically outputs more information about the least squares model.\n# Measires of fit est.summary().tables[0] OLS Regression Results Dep. Variable: Sales R-squared: 0.897 Model: OLS Adj. R-squared: 0.896 Method: Least Squares F-statistic: 859.6 Date: Mon, 03 Jan 2022 Prob (F-statistic): 4.83e-98 Time: 18:28:21 Log-Likelihood: -386.20 No. Observations: 200 AIC: 778.4 Df Residuals: 197 BIC: 788.3 Df Model: 2 Covariance Type: nonrobust First measure: the F-test. The F-test tries to answe the question \u0026ldquo;Is There a Relationship Between the Response and Predictors?\u0026rdquo;\nIn particular, it tests the following hypothesis\n$$ H_1: \\text{is at least one coefficient different from zero?} $$\nagainst the null hypothesis\n$$ H_0: \\beta_0 = \\beta_1 = \u0026hellip; = 0 $$\nThis hypothesis test is performed by computing the F-statistic,\n$$ F=\\frac{(\\mathrm{TSS}-\\mathrm{RSS}) / p}{\\operatorname{RSS} /(n-p-1)} $$\nLet\u0026rsquo;s try to compute it by hand.\n# Init X = advertising[['Radio', 'TV']] y = advertising.Sales e = y - est.predict(X) RSS = np.sum(e**2) TSS = np.sum((y - np.mean(y))**2) (n,p) = np.shape(X) # Compute F F = ((TSS - RSS)/p) / (RSS/(n-p-1)) print('F = %.4f' % F) F = 859.6177 A rule of thumb is to reject $H_0$ if $F \u0026gt; 10$.\nWe can also test that a particular subset of coefficients are equal to zero. In that case, we just substitute the Total Sum of Squares (TSS) with the Residual Sum of Squares under the null.\n$$ F=\\frac{(\\mathrm{RSS_0}-\\mathrm{RSS}) / p}{\\operatorname{RSS} /(n-p-1)} $$\ni.e. we perfome the regression under the null hypothesis and we compute\n$$ RSS_0 = \\sum_{n=1}^N (y_i - X_i \\beta)^2 \\quad s.t. \\quad H_0 $$\n2.3 Other Considerations in the Regression Model Qualitative Predictors What if some variables are qualitative instead of quantitative? Let\u0026rsquo;s change dataset and use the credit dataset.\n# Credit ratings dataset credit = pd.read_csv('data/Credit.csv', usecols=list(range(1,12))) This dataset contains information on credit ratings, i.e. each person is assigned a Rating score based on his/her own individual characteristics.\nLet\u0026rsquo;s have a look at data types.\n# Summary credit.info() \u0026lt;class 'pandas.core.frame.DataFrame'\u0026gt; RangeIndex: 400 entries, 0 to 399 Data columns (total 11 columns): # Column Non-Null Count Dtype --- ------ -------------- ----- 0 Income 400 non-null float64 1 Limit 400 non-null int64 2 Rating 400 non-null int64 3 Cards 400 non-null int64 4 Age 400 non-null int64 5 Education 400 non-null int64 6 Gender 400 non-null object 7 Student 400 non-null object 8 Married 400 non-null object 9 Ethnicity 400 non-null object 10 Balance 400 non-null int64 dtypes: float64(1), int64(6), object(4) memory usage: 34.5+ KB As we can see, some variables like Gender, Student or Married are not numeric.\nWe can have a closer look at what these variables look like.\n# Look at data credit.head() Income Limit Rating Cards Age Education Gender Student Married Ethnicity Balance 0 14.891 3606 283 2 34 11 Male No Yes Caucasian 333 1 106.025 6645 483 3 82 15 Female Yes Yes Asian 903 2 104.593 7075 514 4 71 11 Male No No Asian 580 3 148.924 9504 681 3 36 11 Female No No Asian 964 4 55.882 4897 357 2 68 16 Male No Yes Caucasian 331 Let\u0026rsquo;s consider the variable Student. From a quick inspection it looks like it\u0026rsquo;s a binary Yes/No variable. Let\u0026rsquo;s check by listing all its values.\n# What values does the Student variable take? credit['Student'].unique() array(['No', 'Yes'], dtype=object) What happens if you pass a binary varaible to statsmodel? It automatically generates a dummy out of it.\n# Table 3.7 est = sm.OLS.from_formula('Balance ~ Student', credit).fit() est.summary().tables[1] coef std err t P\u003e|t| [0.025 0.975] Intercept 480.3694 23.434 20.499 0.000 434.300 526.439 Student[T.Yes] 396.4556 74.104 5.350 0.000 250.771 542.140 If a variable takes more than one value, statsmodel automatically generates a uniqe dummy for each level (-1).\n# Table 3.8 est = sm.OLS.from_formula('Balance ~ Ethnicity', credit).fit() est.summary().tables[1] coef std err t P\u003e|t| [0.025 0.975] Intercept 531.0000 46.319 11.464 0.000 439.939 622.061 Ethnicity[T.Asian] -18.6863 65.021 -0.287 0.774 -146.515 109.142 Ethnicity[T.Caucasian] -12.5025 56.681 -0.221 0.826 -123.935 98.930 Relaxing the Additive Assumption We have seen that both TV and Radio advertisement are positively associated with Sales. What if there is a synergy? For example it might be that if someone sees an ad both on TV and on the radio, s/he is much more likely to buy the product.\nConsider the following model\n$$ \\text{Sales} ≈ \\beta_0 + \\beta_1 \\text{TV} + \\beta_2 \\text{Radio} + \\beta_3 \\text{TV} \\times \\text{Radio} $$\nwhich can be rewritten as\n$$ \\text{Sales} ≈ \\beta_0 + (\\beta_1 + \\beta_3 \\text{Radio}) \\times \\text{TV} + \\beta_2 \\text{Radio} $$\nLet\u0026rsquo;s estimate the linear regression model, with the intercept.\n# Table 3.9 - Interaction Variables est = sm.OLS.from_formula('Sales ~ TV + Radio + TV*Radio', advertising).fit() est.summary().tables[1] coef std err t P\u003e|t| [0.025 0.975] Intercept 6.7502 0.248 27.233 0.000 6.261 7.239 TV 0.0191 0.002 12.699 0.000 0.016 0.022 Radio 0.0289 0.009 3.241 0.001 0.011 0.046 TV:Radio 0.0011 5.24e-05 20.727 0.000 0.001 0.001 A positive and significant interaction term indicates a hint of a sinergy effect.\nHeterogeneous Effects We can do interactions with qualitative variables as well. Conside the credit rating dataset.\nWhat if Balance depends by Income differently, depending on whether one is a Student or not?\nConsider the following model:\n$$ \\text{Balance} ≈ \\beta_0 + \\beta_1 \\text{Income} + \\beta_2 \\text{Student} + \\beta_3 \\text{Income} \\times \\text{Student} $$\nThe last coefficient $\\beta_3$ should tell us how much Balance increases in Income for Students with respect to non-Students.\nIndeed, we can decompose the regression in the following equivalent way:\n$$ \\text{Balance} ≈ \\beta_0 + \\beta_1 \\text{Income} + \\beta_2 \\text{Student} + \\beta_3 \\text{Income} \\times \\text{Student} $$\nwhich can be interpreted in the following way since Student is a binary variable\nIf the person is not a student $$ \\text{Balance} ≈ \\beta_0 + \\beta_1 \\text{Income} $$\nIf the person is a student $$ \\text{Balance} ≈ (\\beta_0 + \\beta_2) + (\\beta_1 + \\beta_3 ) \\text{Income} $$\nWe are allowing not only for a different intercept for Students, $\\beta_0 \\to \\beta_0 + \\beta_2$, but also for a different impact of Income, $\\beta_1 \\to \\beta_1 + \\beta_3$.\nWe can visually inspect the distribution of Income across the two groups.\n# Divide data into students and non-students x_student = credit.loc[credit.Student=='Yes','Income'] y_student = credit.loc[credit.Student=='Yes','Balance'] x_nonstudent = credit.loc[credit.Student=='No','Income'] y_nonstudent = credit.loc[credit.Student=='No','Balance'] # Make figure 3.8 def make_fig_3_8(): # Init figure fig, ax = plt.subplots(1,1) fig.suptitle('Figure 3.8') # Relationship betweeen income and balance for students and non-students ax.scatter(x=x_nonstudent, y=y_nonstudent, facecolors='None', edgecolors='k', alpha=0.5); ax.scatter(x=x_student, y=y_student, facecolors='r', edgecolors='r', alpha=0.7); ax.legend(['non-student', 'student']); ax.set_xlabel('Income'); ax.set_ylabel('Balance'); make_fig_3_8() It is hard from the scatterplot to see whether there is a different relationship between income and balance for students and non-students.\nLet\u0026rsquo;s fit two separate regressions.\n# Interaction between qualitative and quantative variables est1 = sm.OLS.from_formula('Balance ~ Income + Student', credit).fit() reg1 = est1.params est2 = sm.OLS.from_formula('Balance ~ Income + Student + Income*Student', credit).fit() reg2 = est2.params print('Regression 1 - without interaction term') print(reg1) print('\\nRegression 2 - with interaction term') print(reg2) Regression 1 - without interaction term Intercept 211.142964 Student[T.Yes] 382.670539 Income 5.984336 dtype: float64 Regression 2 - with interaction term Intercept 200.623153 Student[T.Yes] 476.675843 Income 6.218169 Income:Student[T.Yes] -1.999151 dtype: float64 Without the interaction term, the two lines have different levels but the same slope. Introducing an interaction term allows the two groups to have different responses to Income.\nWe can visualize the relationship in a graph.\n# Income (x-axis) income = np.linspace(0,150) # Balance without interaction term (y-axis) student1 = np.linspace(reg1['Intercept']+reg1['Student[T.Yes]'], reg1['Intercept']+reg1['Student[T.Yes]']+150*reg1['Income']) non_student1 = np.linspace(reg1['Intercept'], reg1['Intercept']+150*reg1['Income']) # Balance with iteraction term (y-axis) student2 = np.linspace(reg2['Intercept']+reg2['Student[T.Yes]'], reg2['Intercept']+reg2['Student[T.Yes]']+ 150*(reg2['Income']+reg2['Income:Student[T.Yes]'])) non_student2 = np.linspace(reg2['Intercept'], reg2['Intercept']+150*reg2['Income']) # Figure 3.7 def make_fig_3_7(): fig, (ax1,ax2) = plt.subplots(1,2, figsize=(12,5)) fig.suptitle('Figure 3.7') # Plot best fit with and without interaction ax1.plot(income, student1, 'r', income, non_student1, 'k') ax2.plot(income, student2, 'r', income, non_student2, 'k') titles = ['Dummy', 'Dummy + Interaction'] for ax, t in zip(fig.axes, titles): ax.legend(['student', 'non-student'], loc=2) ax.set_xlabel('Income') ax.set_ylabel('Balance') ax.set_ylim(ymax=1550) ax.set_title(t) make_fig_3_7() Non-Linear Relationships What if we allow for further non-linearities? Let\u0026rsquo;s change dataset again and use the car dataset.\n# Automobile dataset (dropping missing values) auto = pd.read_csv('data/Auto.csv', na_values='?').dropna() This dataset contains information of a wide variety of car models.\nauto.head() mpg cylinders displacement horsepower weight acceleration year origin name 0 18.0 8 307.0 130 3504 12.0 70 1 chevrolet chevelle malibu 1 15.0 8 350.0 165 3693 11.5 70 1 buick skylark 320 2 18.0 8 318.0 150 3436 11.0 70 1 plymouth satellite 3 16.0 8 304.0 150 3433 12.0 70 1 amc rebel sst 4 17.0 8 302.0 140 3449 10.5 70 1 ford torino Suppose we wanted to understand which car caracteristics are correlated with higher efficiency, i.e. mpg (miles per gallon).\nConsider in particular the relationship between mpg and horsepower. It might be a highly non-linear relationship.\n$$ \\text{mpg} ≈ \\beta_0 + \\beta_1 \\text{horsepower} + \\beta_2 \\text{horsepower}^2 + \u0026hellip; ??? $$\nHow many terms should we include?\nLet\u0026rsquo;s look at the data to understand if it naturally suggests non-linearities.\nfig, ax = plt.subplots(1,1) # Plot polinomials of different degree plt.scatter(x=auto.horsepower, y=auto.mpg, facecolors='None', edgecolors='k', alpha=.3) plt.ylim(5,55); plt.xlim(40,240); plt.xlabel('horsepower'); plt.ylabel('mpg'); The relationship looks non-linear but in which way exactly? Let\u0026rsquo;s try to fit polinomials of different degrees.\ndef make_fig_38(): # Figure 3.8 fig, ax = plt.subplots(1,1) ax.set_title('Figure 3.8') # Plot polinomials of different degree plt.scatter(x=auto.horsepower, y=auto.mpg, facecolors='None', edgecolors='k', alpha=.3) sns.regplot(x=auto.horsepower, y=auto.mpg, ci=None, label='Linear', scatter=False, color='orange') sns.regplot(x=auto.horsepower, y=auto.mpg, ci=None, label='Degree 2', order=2, scatter=False, color='lightblue') sns.regplot(x=auto.horsepower, y=auto.mpg, ci=None, label='Degree 5', order=5, scatter=False, color='g') plt.legend() plt.ylim(5,55) plt.xlim(40,240); make_fig_38() As we can see, the tails are highly unstable depending on the specification.\nLet\u0026rsquo;s add a quadratic term\n# Table 3.10 auto['horsepower2'] = auto.horsepower**2 auto.head(3) mpg cylinders displacement horsepower weight acceleration year origin name horsepower2 0 18.0 8 307.0 130 3504 12.0 70 1 chevrolet chevelle malibu 16900 1 15.0 8 350.0 165 3693 11.5 70 1 buick skylark 320 27225 2 18.0 8 318.0 150 3436 11.0 70 1 plymouth satellite 22500 How does the regression change?\nest = sm.OLS.from_formula('mpg ~ horsepower + horsepower2', auto).fit() est.summary().tables[1] coef std err t P\u003e|t| [0.025 0.975] Intercept 56.9001 1.800 31.604 0.000 53.360 60.440 horsepower -0.4662 0.031 -14.978 0.000 -0.527 -0.405 horsepower2 0.0012 0.000 10.080 0.000 0.001 0.001 Non-Linearities How can we assess if there are non-linearities and of which kind? We can look at the residuals.\nIf the residuals show some kind of pattern, probably we could have fit the line better. Moreover, we can use the pattern itself to understand how.\n# Linear fit X = auto.horsepower.values.reshape(-1,1) y = auto.mpg regr = LinearRegression().fit(X, y) auto['pred1'] = regr.predict(X) auto['resid1'] = auto.mpg - auto.pred1 # Quadratic fit X2 = auto[['horsepower', 'horsepower2']] regr.fit(X2, y) auto['pred2'] = regr.predict(X2) auto['resid2'] = auto.mpg - auto.pred2 # Figure 3.9 def make_fig_39(): fig, (ax1,ax2) = plt.subplots(1,2, figsize=(12,5)) fig.suptitle('Figure 3.9') # Left plot sns.regplot(x=auto.pred1, y=auto.resid1, lowess=True, ax=ax1, line_kws={'color':'r', 'lw':1}, scatter_kws={'facecolors':'None', 'edgecolors':'k', 'alpha':0.5}) ax1.hlines(0,xmin=ax1.xaxis.get_data_interval()[0], xmax=ax1.xaxis.get_data_interval()[1], linestyles='dotted') ax1.set_title('Residual Plot for Linear Fit') # Right plot sns.regplot(x=auto.pred2, y=auto.resid2, lowess=True, line_kws={'color':'r', 'lw':1}, ax=ax2, scatter_kws={'facecolors':'None', 'edgecolors':'k', 'alpha':0.5}) ax2.hlines(0,xmin=ax2.xaxis.get_data_interval()[0], xmax=ax2.xaxis.get_data_interval()[1], linestyles='dotted') ax2.set_title('Residual Plot for Quadratic Fit') for ax in fig.axes: ax.set_xlabel('Fitted values') ax.set_ylabel('Residuals') make_fig_39() It looks like the residuals from the linear fit (on the left) exibit a pattern:\npositive values at the tails negative values in the center This suggests a quadratic fit. Indeed, the residuals when we include horsepower^2 (on the right) seem more uniformly centered around zero.\nOutliers Observations with high residuals have a good chance of being highly influentials. However, they do not have to be.\nLet\u0026rsquo;s use the following data generating process:\n$X \\sim N(0,1)$ $\\varepsilon \\sim N(0,0.5)$ $\\beta_0 = 3$ $y = \\beta_0 X + \\varepsilon$ np.random.seed(1) # Generate random y n = 50 X = rnorm(1,1,(n,1)) e = rnorm(0,0.5,(n,1)) b0 = 3 y = X*b0 + e Now let\u0026rsquo;s change observation 20 so that it becomes an outlier, i.e. it has a high residual.\n# Generate outlier X[20] = 1 y[20] = 7 # Short regression without observation number 41 X_small = np.delete(X, 20) y_small = np.delete(y, 20) Let\u0026rsquo;s now plot the data and the residuals\n# Figure 3.12 def make_fig_3_12(): # Init figure fig, (ax1,ax2) = plt.subplots(1,2, figsize=(12,5)) fig.suptitle('Figure 3.12') # Plot 1 ax1.scatter(x=X, y=y, facecolors='None', edgecolors='k', alpha=.5) sns.regplot(x=X, y=y, ax=ax1, order=1, ci=None, scatter=False, line_kws={'color':'r', 'lw':1}) sns.regplot(x=X_small, y=y_small, ax=ax1, order=1, ci=None, scatter=False, line_kws={'color':'b', 'lw':1}) ax1.set_xlabel('X'); ax1.set_ylabel('Y'); ax1.legend(['With obs. 20', 'Without obs. 20'], fontsize=12); # Hihglight outliers ax1.scatter(x=X[20], y=y[20], facecolors='None', edgecolors='r', alpha=1) ax1.annotate(\u0026quot;20\u0026quot;, (1.1, 7), color='r') # Compute fitted values and residuals r = regr.fit(X, y) y_hat = r.predict(X) e = np.abs(y - y_hat) # Plot 2 ax2.scatter(x=y_hat, y=e, facecolors='None', edgecolors='k', alpha=.5) ax2.set_xlabel('Fitted Values'); ax2.set_ylabel('Residuals'); ax2.hlines(0,xmin=ax2.xaxis.get_data_interval()[0], xmax=ax2.xaxis.get_data_interval()[1], linestyles='dotted',color='k') # Highlight outlier ax2.scatter(x=y_hat[20], y=e[20], facecolors='None', edgecolors='r', alpha=1) ax2.annotate(\u0026quot;20\u0026quot;, (2.2, 3.6), color='r'); make_fig_3_12() High Leverage Points A better concept of \u0026ldquo;influential observation\u0026rdquo; is the Leverage, which represents how much an observation is distant from the others in terms of observables.\nThe leverage formula of observation $i$ is\n$$ h_i = x_i (X\u0026rsquo; X)^{-1} x_i' $$\nHowever, leverage alone is not necessarily enough for an observation to being highly influential.\nLet\u0026rsquo;s modify observation 41 so that it has a high leverage.\n# Generate observation with high leverage X[41] = 4 y[41] = 12 # Short regression without observation number 41 X_small = np.delete(X_small, 41) y_small = np.delete(y_small, 41) # Compute leverage H = X @ inv(X.T @ X) @ X.T h = np.diagonal(H) # Compute fitted values and residuals y_hat = X @ inv(X.T @ X) @ X.T @ y e = np.abs(y - y_hat) What happens now that we have added an observation with high leverage? How does the levarage look like?\n# Figure 3.13 def make_fig_3_13(): # Init figure fig, (ax1,ax2) = plt.subplots(1,2, figsize=(12,5)) fig.suptitle('Figure 3.12') # Plot 1 ax1.scatter(x=X, y=y, facecolors='None', edgecolors='k', alpha=.5) ax1.scatter(x=X[[20,41]], y=y[[20,41]], facecolors='None', edgecolors='r', alpha=1) sns.regplot(x=X, y=y, ax=ax1, order=1, ci=None, scatter=False, line_kws={'color':'r', 'lw':1}) sns.regplot(x=X_small, y=y_small, ax=ax1, order=1, ci=None, scatter=False, line_kws={'color':'b', 'lw':1}) ax1.set_xlabel('X'); ax1.set_ylabel('Y'); ax1.axis(xmax=4.5); ax1.legend(['With obs. 20,41', 'Without obs. 20,41']); # Highlight points ax1.annotate(\u0026quot;20\u0026quot;, (1.1, 7), color='r') ax1.annotate(\u0026quot;41\u0026quot;, (3.6, 12), color='r'); # Plot 2 ax2.scatter(x=h, y=e, facecolors='None', edgecolors='k', alpha=.5) ax2.set_xlabel('Leverage'); ax2.set_ylabel('Residuals'); ax2.hlines(0,xmin=ax2.xaxis.get_data_interval()[0], xmax=ax2.xaxis.get_data_interval()[1], linestyles='dotted',color='k') # Highlight outlier ax2.scatter(x=h[[20,41]], y=e[[20,41]], facecolors='None', edgecolors='r', alpha=1); # Highlight points ax2.annotate(\u0026quot;20\u0026quot;, (0, 3.7), color='r') ax2.annotate(\u0026quot;41\u0026quot;, (0.14, 0.4), color='r'); make_fig_3_13() Influential Observations As we have seen, being an outliers or having high leverage alone might be not enough to conclude that an observation is influential.\nWhat really matters is a combination of both: observations with high leverage and high residuals, i.e. observations that are not only different in terms of observables (high leverage) but are also different in terms of their relationship between observables and dependent variable (high residual).\nLet\u0026rsquo;s now modify observation 7 so that it is an outlier and has high leverage.\n# Generate outlier with high leverage X[7] = 4 y[7] = 7 # Short regression without observation number 41 X_small = np.delete(X, 7) y_small = np.delete(y, 7) # Compute leverage H = X @ inv(X.T @ X) @ X.T h = np.diagonal(H) # Compute fitted values and residuals r = regr.fit(X, y) y_hat = r.predict(X) e = np.abs(y - y_hat) Now the best linear fit line has noticeably moved.\ndef make_fig_extra_3(): fig, (ax1,ax2) = plt.subplots(1,2, figsize=(12,5)) # Plot 1 ax1.scatter(x=X, y=y, facecolors='None', edgecolors='k', alpha=.5) ax1.scatter(x=X[[7,20,41]], y=y[[7,20,41]], facecolors='None', edgecolors='r', alpha=1) sns.regplot(x=X, y=y, ax=ax1, order=1, ci=None, scatter=False, line_kws={'color':'r', 'lw':1}) sns.regplot(x=X_small, y=y_small, ax=ax1, order=1, ci=None, scatter=False, line_kws={'color':'b', 'lw':1}) ax1.set_xlabel('X'); ax1.set_ylabel('Y'); ax1.axis(xmax=4.5); ax1.legend(['With obs. 7,20,41', 'Without obs. 7,20,41']); # Highlight points ax1.annotate(\u0026quot;7\u0026quot;, (3.7, 7), color='r') ax1.annotate(\u0026quot;20\u0026quot;, (1.15, 7.05), color='r') ax1.annotate(\u0026quot;41\u0026quot;, (3.6, 12), color='r'); # Plot 2 ax2.scatter(x=h, y=e, facecolors='None', edgecolors='k', alpha=.5) ax2.set_xlabel('Leverage'); ax2.set_ylabel('Residuals'); ax2.hlines(0,xmin=ax2.xaxis.get_data_interval()[0], xmax=ax2.xaxis.get_data_interval()[1], linestyles='dotted',color='k') # Highlight outlier ax2.scatter(x=h[[7,20,41]], y=e[[7,20,41]], facecolors='None', edgecolors='r', alpha=1); # Highlight points ax2.annotate(\u0026quot;7\u0026quot;, (0.12, 4.0), color='r'); ax2.annotate(\u0026quot;20\u0026quot;, (0, 3.8), color='r') ax2.annotate(\u0026quot;41\u0026quot;, (0.12, 0.9), color='r'); make_fig_extra_3() Collinearity Collinearity is the situation in which two dependent varaibles are higly correlated with each other. Algebraically, this is a problem because the $X\u0026rsquo;X$ matrix becomes almost-non-invertible.\nLet\u0026rsquo;s have a look at the ratings dataset.\n# Inspect dataset sns.pairplot(credit[['Age', 'Balance', 'Limit', 'Rating']], height=1.8); If we zoom into the variable Limit, we see that for example it is not very correlated with Age but is very correlated with Rating.\n# Figure 3.14 def make_fig_3_14(): # Init figure fig, (ax1,ax2) = plt.subplots(1,2, figsize=(12,5)) fig.suptitle('Figure 3.14') # Left plot ax1.scatter(credit.Limit, credit.Age, facecolor='None', edgecolor='brown') ax1.set_ylabel('Age') # Right plot ax2.scatter(credit.Limit, credit.Rating, facecolor='None', edgecolor='brown') ax2.set_ylabel('Rating') for ax in fig.axes: ax.set_xlabel('Limit') ax.set_xticks([2000,4000,6000,8000,12000]) make_fig_3_14() If we regress Balance on Limit and Age, the coefficient of Limit is positive and highly significant.\n# Regress balance on limit and age reg1 = sm.OLS.from_formula('Balance ~ Limit + Age', credit).fit() reg1.summary().tables[1] coef std err t P\u003e|t| [0.025 0.975] Intercept -173.4109 43.828 -3.957 0.000 -259.576 -87.246 Limit 0.1734 0.005 34.496 0.000 0.163 0.183 Age -2.2915 0.672 -3.407 0.001 -3.614 -0.969 However, if we regress Balance on Limit and Rating, the coefficient of Limit is now not significant anymore.\n# Regress balance on limit and rating reg2 = sm.OLS.from_formula('Balance ~ Limit + Rating', credit).fit() reg2.summary().tables[1] coef std err t P\u003e|t| [0.025 0.975] Intercept -377.5368 45.254 -8.343 0.000 -466.505 -288.569 Limit 0.0245 0.064 0.384 0.701 -0.101 0.150 Rating 2.2017 0.952 2.312 0.021 0.330 4.074 Looking at the objective function, the Residual Sum of Squares, helps understanding what is the problem.\n# First scale variables y = credit.Balance regr1 = LinearRegression().fit(scale(credit[['Age', 'Limit']].astype('float'), with_std=False), y) regr2 = LinearRegression().fit(scale(credit[['Rating', 'Limit']], with_std=False), y) # Create grid coordinates for plotting B_Age = np.linspace(regr1.coef_[0]-3, regr1.coef_[0]+3, 100) B_Limit = np.linspace(regr1.coef_[1]-0.02, regr1.coef_[1]+0.02, 100) B_Rating = np.linspace(regr2.coef_[0]-3, regr2.coef_[0]+3, 100) B_Limit2 = np.linspace(regr2.coef_[1]-0.2, regr2.coef_[1]+0.2, 100) X1, Y1 = np.meshgrid(B_Limit, B_Age, indexing='xy') X2, Y2 = np.meshgrid(B_Limit2, B_Rating, indexing='xy') Z1 = np.zeros((B_Age.size,B_Limit.size)) Z2 = np.zeros((B_Rating.size,B_Limit2.size)) Limit_scaled = scale(credit.Limit.astype('float'), with_std=False) Age_scaled = scale(credit.Age.astype('float'), with_std=False) Rating_scaled = scale(credit.Rating.astype('float'), with_std=False) # Calculate Z-values (RSS) based on grid of coefficients for (i,j),v in np.ndenumerate(Z1): Z1[i,j] =((y - (regr1.intercept_ + X1[i,j]*Limit_scaled + Y1[i,j]*Age_scaled))**2).sum()/1000000 for (i,j),v in np.ndenumerate(Z2): Z2[i,j] =((y - (regr2.intercept_ + X2[i,j]*Limit_scaled + Y2[i,j]*Rating_scaled))**2).sum()/1000000 # Figure 3.15 def make_fig_3_15(): # Init figure fig, (ax1,ax2) = plt.subplots(1,2, figsize=(12,5)) fig.suptitle('Figure 3.15') # Minimum min_RSS = r'$\\beta_0$, $\\beta_1$ for minimized RSS' # Left plot CS = ax1.contour(X1, Y1, Z1, cmap=plt.cm.Set1, levels=[21.25, 21.5, 21.8]) ax1.scatter(reg1.params[1], reg1.params[2], c='r', label=min_RSS) ax1.clabel(CS, inline=True, fontsize=10, fmt='%1.1f') ax1.set_ylabel(r'$\\beta_{Age}$') # Right plot CS = ax2.contour(X2, Y2, Z2, cmap=plt.cm.Set1, levels=[21.5, 21.8]) ax2.scatter(reg2.params[1], reg2.params[2], c='r', label=min_RSS) ax2.clabel(CS, inline=True, fontsize=10, fmt='%1.1f') ax2.set_ylabel(r'$\\beta_{Rating}$') #ax2.set_xticks([-0.1, 0, 0.1, 0.2]) for ax in fig.axes: ax.set_xlabel(r'$\\beta_{Limit}$') ax.legend() make_fig_3_15() As we can see, in the left plot the minimum is much better defined than in the right plot.\n","date":1646784000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1646784000,"objectID":"39506bbd73c328171beb64584765d130","permalink":"https://matteocourthoud.github.io/course/ml-econ/01_regression/","publishdate":"2022-03-09T00:00:00Z","relpermalink":"/course/ml-econ/01_regression/","section":"course","summary":"This chapter follows closely Chapter 3 of An Introduction to Statistical Learning by James, Witten, Tibshirani, Friedman.\n# Remove warnings import warnings warnings.filterwarnings('ignore') # Import everything import pandas as pd import numpy as np import seaborn as sns import statsmodels.","tags":null,"title":"Linear Regression","type":"book"},{"authors":null,"categories":null,"content":"Basics Matrix Definition A real $n \\times m$ matrix $A$ is an array\n$$ A= \\begin{bmatrix} a_{11} \u0026amp; a_{12} \u0026amp; \\dots \u0026amp; a_{1m} \\newline a_{21} \u0026amp; a_{22} \u0026amp; \\dots \u0026amp; a_{2m} \\newline \\vdots \u0026amp; \\vdots \u0026amp; \\ddots \u0026amp; \\vdots \\newline a_{n1} \u0026amp; a_{n2} \u0026amp; \\dots \u0026amp; a_{nm} \\end{bmatrix} $$\nWe write $[A]_ {ij} = a_ {ij}$ to indicate the $(i,j)$-element of $A$.\nWe will usually take the convention that a real vector $x \\in \\mathbb R^n$ is identified with an $n \\times 1$ matrix.\nThe $n \\times n$ identity matrix $I_n$ is given by\n$$ [I_n] _ {ij} = \\begin{cases} 1 \\ \\ \\ \\text{if} \\ i=j \\newline 0 \\ \\ \\ \\text{if} \\ i \\neq j \\end{cases} $$\nFundamental Operations Two $n \\times m$ matrices, $A,B$, are added element-wise so that $[A+B]_{ij} = [A] _{ij} + [B] _{ij}$. A matrix $A$ can be multiplied by a scalar $c\\in \\mathbb{R}$ in which case we set $[cA]_{ij} = c[A] _{ij}$. An $n \\times m$ matrix $A$ can be multiplied with an $m \\times p$ matrix $B$. The product $AB$ is defined according to the rule $[AB] _ {ij} = \\sum_{k=1}^m [A] _{ik} [B] _{kj}$. An $n \\times n$ matrix is invertible if there exists a matrix $B$ such that $AB=I$. In this case, we use the notational convention of writing $B = A^{-1}$. Matrix transposition is defined by $[A\u0026rsquo;] _{ij} = [A] _{ji}$. Trace and Determinant The trace of a square matrix $A$ with dimension $n \\times n$ is $\\text{tr}(A) = \\sum_{i=1}^n a_{ii}$.\nThe determinant of a square $n \\times n$ matrix A is defined according to one of the following three (equivalent) definitions.\nRecursively as $det(A) = \\sum_{i=1}^n a_{ij} (-1)^{i+j} det([A]{-i,-j})$ where $[A]{-i,-j}$ is the matrix obtained by deleting the $i$th row and the $j$th column. $A \\mapsto det(A)$ under the unique alternating multilinear map on $n \\times n$ matrices such that $I \\mapsto 1$. Linear Independence Vectors $x_1,\u0026hellip;,x_k$ are linearly independent if the only solution to the equation $b_1x_1 + \u0026hellip; + b_k x_k=0, \\ b_j \\in \\mathbb R$, is $b_1=b_2=\u0026hellip;=b_k=0$.\nUseful Identities $(A+B)\u0026rsquo; =A\u0026rsquo;+B'$ $(AB)C = A(BC)$ $A(B+C) = AB+AC$ $(AB\u0026rsquo;) = B\u0026rsquo;A'$ $(A^{-1})\u0026rsquo; = (A\u0026rsquo;)^{-1}$ $(AB)^{-1} = B^{-1}A^{-1}$ $\\text{tr}(cA) = c\\text{tr}(A)$ $\\text{tr}(A+B) = \\text{tr}(A) + \\text{tr}(B)$ $\\text{tr}(AB) =\\text{tr}(BA)$ $det(I)=1$ $det(cA) = c^ndet(A)$ if $A$ is $n \\times n$ and $c \\in \\mathbb R$ $det(A) = det(A\u0026rsquo;)$ $det(AB) = det(A)det(B)$ $det(A^{-1}) = (det(A))^{-1}$ $A^{-1}$ exists iff $det(A) \\neq 0$ $rank(A) = rank(A\u0026rsquo;) = rank(A\u0026rsquo;A) = rank(AA\u0026rsquo;)$ $A^{-1}$ exists iff $rank(A)=n$ for $A$ $n \\times n$ $rank(AB) \\leq \\min \\lbrace rank(A), rank(B) \\rbrace$ Matrix Rank The rank of a matrix, $rank(A)$ is equal to the maximal number of linearly independent rows for $A$.\nLet $A$ be an $n \\times n$ matrix. The $n \\times 1$ vector $x \\neq 0$ is an eigenvector of $A$ with corresponding eigenvalue $\\lambda$ is $Ax = \\lambda x$.\nDefinitions A matrix $A$ is diagonal if $[A]_ {ij} \\neq 0$ only if $i=j$. An $n \\times n$ matrix $A$ is orthogonal if $A\u0026rsquo;A = I$ A matrix $A$ is symmetric if $[A]_ {ij} = [A]_ {ji}$. An $n \\times n$ matrix $A$ is idempotent if $A^2=A$. The matrix of zeros ($[A]_ {ij} =0$ for each $i,j$) is simply denoted 0. An $n \\times n$ matrix $A$ is nilpotent if $A^k=0$ for some integer $k\u0026gt;0$. Spectral Decomposition Spectral Theorem Theorem: Let $A$ be an $n \\times n$ symmetric matrix. Then $A$ can be factored as $A = C \\Lambda C\u0026rsquo;$ where $C$ is orthogonal and $\\Lambda$ is diagonal.\nIf we postmultiply $A$ by $C$, we get\n$AC = C \\Lambda C\u0026rsquo;C$ and $AC = C \\Lambda$. This is a matrix equation which can be split into columns. The $i$th column of the equation reads $A c_i = \\lambda_i c_i$ which corresponds to the definition of eigenvalues and eigenvectors. So if the decomposition exists, then $C$ is the eigenvector matrix and $\\Lambda$ contains the eigenvalues.\nRank and Trace Theorem: The rank of a symmetric matrix equals the number of non zero eigenvalues.\nProof: $rank(A) = rank(C\\Lambda C\u0026rsquo;) = rank(\\Lambda) = | \\lbrace i: \\lambda_i \\neq 0 \\rbrace |$. $$\\tag*{$\\blacksquare$}$$\nTheorem: The nonzero eigenvalues of $AA\u0026rsquo;$ and $A\u0026rsquo;A$ are identical.\nTheorem: The trace of a symmetric matrix equals the sum of its eignevalues.\nProof: $tr(A) = tr(C \\Lambda C\u0026rsquo;) = tr((C \\Lambda)C\u0026rsquo;) = tr(C\u0026rsquo;C \\Lambda) = tr(\\Lambda) = \\sum_ {i=1}^n \\lambda_i.$ $$\\tag*{$\\blacksquare$}$$\nTheorem: The determinant of a symmetric matrix equals the product of its eignevalues.\nProof: $det(A) = det(C \\Lambda C\u0026rsquo;) = det(C)det(\\Lambda)det(C\u0026rsquo;) = det(C)det(C\u0026rsquo;)det(\\Lambda) = det(CC\u0026rsquo;) det(\\Lambda) = det(I)det(\\Lambda) = det(\\Lambda) = \\prod_ {i=1}^n \\lambda_i.$ $$\\tag*{$\\blacksquare$}$$\nEigenvalues Theorem: For any symmetric matrix $A$, the eigenvalues of $A^2$ are the square of the eignevalues of $A$, and the eigenvectors are the same.\nProof: $A = C \\Lambda C\u0026rsquo; \\implies A^2 = C \\Lambda C\u0026rsquo; C \\Lambda C\u0026rsquo; = C \\Lambda I \\Lambda C\u0026rsquo; = C \\Lambda^2 C\u0026rsquo;$ $$\\tag*{$\\blacksquare$}$$\nTheorem: For any symmetric matrix $A$, and any integer $k\u0026gt;0$, the eigenvalues of $A^k$ are the $k$th power of the eigenvalues of $A$, and the eigenvectors are the same.\nTheorem: Any square symmetric matrix $A$ with positive eigenvalues can be written as the product of a lower triangular matrix $L$ and its (upper triangular) transpose $L\u0026rsquo; = U$. That is $A = LU = LL'$\nNote that $$ A = LL\u0026rsquo; = LU = U\u0026rsquo;U = (L\u0026rsquo;)^{-1}L^{-1} = U^{-1}(U\u0026rsquo;)^{-1} $$ where $L^{-1}$ is lower triangular and $U^{ -1}$ is upper trianguar. You can check this for the $2 \\times 2$ case. Also note that the validity of the theorem can be extended to symmetric matrices with non- negative eigenvalues by a limiting argument. However, then the proof is not constructive anymore.\nQuadratic Forms and Definite Matrices Definition A quadratic form in the $n \\times n$ matrix $A$ and $n \\times 1$ vector $x$ is defined by the scalar $x\u0026rsquo;Ax$.\n$A$ is negative definite (ND) if for each $x \\neq 0$, $x\u0026rsquo;Ax \u0026lt; 0$ $A$ is negative semidefinite (NSD) if for each $x \\neq 0$, $x\u0026rsquo;Ax \\leq 0$ $A$ is positive definite (PD) if for each $x \\neq 0$, $x\u0026rsquo;Ax \u0026gt; 0$ $A$ is positive semidefinite (PSD) if for each $x \\neq 0$, $x\u0026rsquo;Ax \\geq 0$ Equivalence Theorem: Let $A$ be a symmetric matrix. Then $A$ is PD(ND) $\\iff$ all of its eigenvalues are positive (negative).\nSome more results:\nIf a symmetric matrix $A$ is PD (PSD, ND, NSD), then $\\text{det}(A) \u0026gt;(\\geq,\u0026lt;,\\leq) 0$. If symmetric matrix $A$ is PD (ND) then $A^{-1}$ is symmetric PD (ND). The identity matrix is PD (since all eigenvalues are equal to 1). Every symmetric idempotent matrix is PSD (since the eigenvalues are only 0 or 1). Theorem: If $A$ is $n\\times k$ with $n\u0026gt;k$ and $rank(A)=k$, then $A\u0026rsquo;A$ is PD and $AA\u0026rsquo;$ is PSD.\nThe semidefinite partial order is defined by $A \\geq B$ iff $A-B$ is PSD.\nTheorem: Let $A$, $B$ be symmetric,square , PD, conformable. Then $A-B$ is PD iff $A^{-1}-B^{-1}$ is PD.\nMatrix Calculus Comformable Matrices We first define matrices blockwise when they are conformable. In particular, we assume that if $A_1, A_2, A_3, A_4$ are matrices with appropriate dimensions then the matrix $$ A = \\begin{bmatrix} A_1 \u0026amp; A_1 \\newline A_3 \u0026amp; A_4 \\end{bmatrix} $$ is defined in the obvious way.\nMatrix Functions Let $F: \\mathbb R^m \\times \\mathbb R^n \\rightarrow \\mathbb R^p \\times \\mathbb R^q$ be a matrix valued function. More precisely, given a real $m \\times n$ matrix $X$, $F(X)$ returns the $p \\times q$ matrix\n$$ \\begin{bmatrix} f_ {11}(X) \u0026amp; \u0026hellip; \u0026amp; f_ {1q}(X) \\newline \\vdots \u0026amp; \\ddots \u0026amp; \\vdots \\newline f_ {p1}(X)\u0026amp; \u0026hellip; \u0026amp; f_ {pq}(X) \\end{bmatrix} $$\nMatrix Derivatives The derivative of $F$ with respect to the matrix $X$ is the $mp \\times nq$ matrix $$ \\frac{\\partial F(X)}{\\partial X} = \\begin{bmatrix} \\frac{\\partial F(X)}{\\partial x_ {11}} \u0026amp; \u0026hellip; \u0026amp; \\frac{\\partial F(X)}{\\partial x_ {1n}} \\newline \\vdots \u0026amp; \\ddots \u0026amp; \\vdots \\newline \\frac{\\partial F(X)}{\\partial x_ {m1}} \u0026amp; \u0026hellip; \u0026amp; \\frac{\\partial F(X)}{\\partial x_ {mn}} \\end{bmatrix} $$ where each $\\frac{\\partial F(X)}{\\partial x_ {ij}}$ is a $p\\times q$ matrix given by\n$$ \\frac{\\partial F(X)}{\\partial x_ {ij}} = \\begin{bmatrix} \\frac{\\partial f_ {11}(X)}{\\partial x_ {ij}} \u0026amp; \u0026hellip; \u0026amp; \\frac{\\partial f_ {1q}(X)}{\\partial x_ {ij}} \\newline \\vdots \u0026amp; \\ddots \u0026amp; \\vdots \\newline \\frac{\\partial f_ {p1}(X)}{\\partial x_ {ij}} \u0026amp; \u0026hellip; \u0026amp; \\frac{\\partial f_ {pq}(X)}{\\partial x_ {ij}} \\end{bmatrix} $$ The most important case is when $F: \\mathbb R^n \\rightarrow \\mathbb R$ since this simplifies the derivation of the least squares estimator. Also, the trickiest thing is to make sure that dimensions are correct.\nUseful Results in Matrix Calculus $\\frac{\\partial b\u0026rsquo;x}{\\partial x}= b$ for $dim(b) = dim(x)$ $\\frac{\\partial B\u0026rsquo;x}{\\partial x}= B$ for arbitrary, conformable $B$ $\\frac{\\partial B\u0026rsquo;x}{\\partial x\u0026rsquo;}= B\u0026rsquo;$ for arbitrary, conformable $B$ $\\frac{\\partial x\u0026rsquo;Ax}{\\partial x} = (A + A\u0026rsquo;)x$ $\\frac{\\partial x\u0026rsquo;Ax}{\\partial A} = xx'$ $\\frac{\\partial x\u0026rsquo;Ax}{\\partial x} = det(A) (A^{-1})'$ $\\frac{\\partial \\ln det(A)}{\\partial A} = (A^{-1})'$ ","date":1635465600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1635465600,"objectID":"48eeb5e589225f9ba0ed86de5ea1d965","permalink":"https://matteocourthoud.github.io/course/metrics/01_matrices/","publishdate":"2021-10-29T00:00:00Z","relpermalink":"/course/metrics/01_matrices/","section":"course","summary":"Basics Matrix Definition A real $n \\times m$ matrix $A$ is an array\n$$ A= \\begin{bmatrix} a_{11} \u0026amp; a_{12} \u0026amp; \\dots \u0026amp; a_{1m} \\newline a_{21} \u0026amp; a_{22} \u0026amp; \\dots \u0026amp; a_{2m} \\newline \\vdots \u0026amp; \\vdots \u0026amp; \\ddots \u0026amp; \\vdots \\newline a_{n1} \u0026amp; a_{n2} \u0026amp; \\dots \u0026amp; a_{nm} \\end{bmatrix} $$","tags":null,"title":"Matrix Algebra","type":"book"},{"authors":null,"categories":null,"content":"import numpy as np import pandas as pd Setup For the scope of this tutorial we are going to use AirBnb Scraped data for the city of Bologna. The data is freely available at Inside AirBnb: http://insideairbnb.com/get-the-data.html.\nA description of all variables in all datasets is avaliable here.\nWe are going to use 2 datasets:\nlisting dataset: contains listing-level information pricing dataset: contains pricing data, over time Importing Data Pandas has a variety of function to import data\npd.read_csv() pd.read_html() pd.read_parquet() Importatly for our purpose, pd.read_csv() can directly import data from the web.\nThe first dataset that we are going to import is the dataset of Airbnb listings in Bologna. It contains listing-level information.\nurl_listings = \u0026quot;http://data.insideairbnb.com/italy/emilia-romagna/bologna/2021-12-17/visualisations/listings.csv\u0026quot; df_listings = pd.read_csv(url_listings) The second dataset that we are going to use is the dataset of calendar prices. This time the dataset is compressed but we can use the compression option to import it directly.\nurl_prices = \u0026quot;http://data.insideairbnb.com/italy/emilia-romagna/bologna/2021-12-17/data/calendar.csv.gz\u0026quot; df_prices = pd.read_csv(url_prices, compression=\u0026quot;gzip\u0026quot;) Inspecting Data Methods\ninfo() head() describe() The first way yo have a quick look at the data is the info() method. If called with the option verbose=False, it gives a quick overview of the dimensions of the data.\ndf_listings.info(verbose=False) \u0026lt;class 'pandas.core.frame.DataFrame'\u0026gt; RangeIndex: 3453 entries, 0 to 3452 Columns: 18 entries, id to license dtypes: float64(4), int64(8), object(6) memory usage: 485.7+ KB If we want to know how the data looks like, we can use the head() method. It prints the first 5 lines of the data by default.\ndf_listings.head() id name host_id host_name neighbourhood_group neighbourhood latitude longitude room_type price minimum_nights number_of_reviews last_review reviews_per_month calculated_host_listings_count availability_365 number_of_reviews_ltm license 0 42196 50 sm Studio in the historic centre 184487 Carlo NaN Santo Stefano 44.48507 11.34786 Entire home/apt 68 3 180 2021-11-12 1.32 1 161 6 NaN 1 46352 A room in Pasolini's house 467810 Eleonora NaN Porto - Saragozza 44.49168 11.33514 Private room 29 1 300 2021-11-30 2.20 2 248 37 NaN 2 59697 COZY LARGE BEDROOM in the city center 286688 Paolo NaN Santo Stefano 44.48817 11.34124 Private room 50 1 240 2020-10-04 2.18 2 327 0 NaN 3 85368 Garden House Bologna 467675 Anna Maria NaN Santo Stefano 44.47834 11.35672 Entire home/apt 126 2 40 2019-11-03 0.34 1 332 0 NaN 4 145779 SINGLE ROOM 705535 Valerio NaN Porto - Saragozza 44.49306 11.33786 Private room 50 10 69 2021-12-05 0.55 9 365 5 NaN We can print a description of the data using describe(). If we have many variables, it\u0026rsquo;s best to print it transposed using the .T attribute.\ndf_listings.describe().T[:5] count mean std min 25% 50% 75% max id 3453.0 2.950218e+07 1.523988e+07 42196.0000 1.748597e+07 3.078707e+07 4.220094e+07 5.385496e+07 host_id 3453.0 1.236424e+08 1.160756e+08 38468.0000 2.550007e+07 8.845438e+07 2.005926e+08 4.354316e+08 neighbourhood_group 0.0 NaN NaN NaN NaN NaN NaN NaN latitude 3453.0 4.449756e+01 1.173569e-02 44.4236 4.449186e+01 4.449699e+01 4.450271e+01 4.455093e+01 longitude 3453.0 1.134509e+01 1.986071e-02 11.2320 1.133732e+01 1.134519e+01 1.135406e+01 1.142027e+01 You can select which variables to display using the include option. include='all' includes also categorical variables.\ndf_listings.describe(include='all').T[:5] count unique top freq mean std min 25% 50% 75% max id 3453.0 NaN NaN NaN 29502177.118158 15239877.346777 42196.0 17485973.0 30787074.0 42200938.0 53854962.0 name 3453 3410 Luxury Industrial Design LOFT, HEPA UV airpuri... 5 NaN NaN NaN NaN NaN NaN NaN host_id 3453.0 NaN NaN NaN 123642405.854619 116075571.230048 38468.0 25500072.0 88454378.0 200592620.0 435431590.0 host_name 3444 747 Andrea 101 NaN NaN NaN NaN NaN NaN NaN neighbourhood_group 0.0 NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN We can get the list of columns using the .columns attribute.\ndf_listings.columns Index(['id', 'name', 'host_id', 'host_name', 'neighbourhood_group', 'neighbourhood', 'latitude', 'longitude', 'room_type', 'price', 'minimum_nights', 'number_of_reviews', 'last_review', 'reviews_per_month', 'calculated_host_listings_count', 'availability_365', 'number_of_reviews_ltm', 'license'], dtype='object') We can get the index using the .index attribute,\ndf_listings.index RangeIndex(start=0, stop=3453, step=1) Data Selection We can access single columns as if the DataFrame was a dictionary.\ndf_listings['price'] 0 68 1 29 2 50 3 126 4 50 ... 3448 32 3449 45 3450 50 3451 134 3452 115 Name: price, Length: 3453, dtype: int64 We can select rows and columns by index, using the .iloc attribute.\ndf_listings.iloc[:7, 5:9] neighbourhood latitude longitude room_type 0 Santo Stefano 44.48507 11.34786 Entire home/apt 1 Porto - Saragozza 44.49168 11.33514 Private room 2 Santo Stefano 44.48817 11.34124 Private room 3 Santo Stefano 44.47834 11.35672 Entire home/apt 4 Porto - Saragozza 44.49306 11.33786 Private room 5 Navile 44.51628 11.33074 Private room 6 Santo Stefano 44.48787 11.35392 Entire home/apt If we want to condition only on rows or columns, we have use : for the unrestricted dimesion, otherwise we get an error.\ndf_listings.iloc[:, 5:9].head() neighbourhood latitude longitude room_type 0 Santo Stefano 44.48507 11.34786 Entire home/apt 1 Porto - Saragozza 44.49168 11.33514 Private room 2 Santo Stefano 44.48817 11.34124 Private room 3 Santo Stefano 44.47834 11.35672 Entire home/apt 4 Porto - Saragozza 44.49306 11.33786 Private room Instead, the .loc attribute allows us to use row and column names.\ndf_listings.loc[:, ['neighbourhood', 'latitude', 'longitude']].head() neighbourhood latitude longitude 0 Santo Stefano 44.48507 11.34786 1 Porto - Saragozza 44.49168 11.33514 2 Santo Stefano 44.48817 11.34124 3 Santo Stefano 44.47834 11.35672 4 Porto - Saragozza 44.49306 11.33786 We can also select ranges.\ndf_listings.loc[:, 'neighbourhood':'room_type'].head() neighbourhood latitude longitude room_type 0 Santo Stefano 44.48507 11.34786 Entire home/apt 1 Porto - Saragozza 44.49168 11.33514 Private room 2 Santo Stefano 44.48817 11.34124 Private room 3 Santo Stefano 44.47834 11.35672 Entire home/apt 4 Porto - Saragozza 44.49306 11.33786 Private room There is an easy way to select numerical columns, the .select_dtypes() function.\ndf_listings.select_dtypes(include=['number']).head() id host_id neighbourhood_group latitude longitude price minimum_nights number_of_reviews reviews_per_month calculated_host_listings_count availability_365 number_of_reviews_ltm 0 42196 184487 NaN 44.48507 11.34786 68 3 180 1.32 1 161 6 1 46352 467810 NaN 44.49168 11.33514 29 1 300 2.20 2 248 37 2 59697 286688 NaN 44.48817 11.34124 50 1 240 2.18 2 327 0 3 85368 467675 NaN 44.47834 11.35672 126 2 40 0.34 1 332 0 4 145779 705535 NaN 44.49306 11.33786 50 10 69 0.55 9 365 5 Other types include\nobject for strings bool for booleans int for integers float for floats (numbers that are not integers) We can also use logical operators to selet rows.\ndf_listings.loc[df_listings['number_of_reviews']\u0026gt;500, :].head() id name host_id host_name neighbourhood_group neighbourhood latitude longitude room_type price minimum_nights number_of_reviews last_review reviews_per_month calculated_host_listings_count availability_365 number_of_reviews_ltm license 52 884148 APOSA FLAT / CITY CENTER - BO 4664996 Vie D'Acqua Di Sandra Maria NaN Santo Stefano 44.49945 11.34566 Entire home/apt 46 1 668 2021-12-11 6.24 5 252 20 NaN 92 1435627 heart of Bologna Piazza Maggiore 7714013 Carlotta NaN Porto - Saragozza 44.49321 11.33569 Entire home/apt 56 2 508 2021-12-12 5.08 1 131 69 NaN 98 1566003 \"i portici di via Piella \" 8325248 Massimo NaN Santo Stefano 44.49855 11.34411 Entire home/apt 51 2 764 2021-12-14 7.62 3 119 120 NaN 131 2282623 S.Orsola zone,parking for free and self check-in 11658074 Cecilia NaN San Donato - San Vitale 44.49328 11.36650 Entire home/apt 38 1 689 2021-10-24 7.20 1 5 72 NaN 175 3216486 Stanza Privata 16289536 Fabio NaN Navile 44.50903 11.34200 Private room 82 1 569 2021-12-05 6.93 1 7 5 NaN We can use logical operations as well. But remember to use paranthesis.\nNote: the and and or expressions do not work in this setting. We have to use \u0026amp; and | instead.\ndf_listings.loc[(df_listings['number_of_reviews']\u0026gt;300) \u0026amp; (df_listings['reviews_per_month']\u0026gt;7), :].head() id name host_id host_name neighbourhood_group neighbourhood latitude longitude room_type price minimum_nights number_of_reviews last_review reviews_per_month calculated_host_listings_count availability_365 number_of_reviews_ltm license 98 1566003 \"i portici di via Piella \" 8325248 Massimo NaN Santo Stefano 44.498550 11.344110 Entire home/apt 51 2 764 2021-12-14 7.62 3 119 120 NaN 131 2282623 S.Orsola zone,parking for free and self check-in 11658074 Cecilia NaN San Donato - San Vitale 44.493280 11.366500 Entire home/apt 38 1 689 2021-10-24 7.20 1 5 72 NaN 204 4166793 Centralissimo a Bologna 8325248 Massimo NaN Santo Stefano 44.500920 11.344560 Entire home/apt 71 2 750 2021-12-10 9.21 3 233 84 NaN 751 15508481 Monolocale in zona fiera /centro 99632788 Walid NaN Navile 44.514462 11.353731 Entire home/apt 64 1 475 2021-12-01 7.56 1 4 48 NaN 773 15886516 Monolocale nel cuore del ghetto ebraico di Bol... 103024123 Catia NaN Santo Stefano 44.495080 11.347220 Entire home/apt 58 1 428 2021-12-15 7.88 1 285 17 NaN For a single column (i.e. a Series), we can get the unique values using the unique() function.\ndf_listings['neighbourhood'].unique() array(['Santo Stefano', 'Porto - Saragozza', 'Navile', 'San Donato - San Vitale', 'Savena', 'Borgo Panigale - Reno'], dtype=object) For multiple columns, we can use the drop_duplicates function.\ndf_listings[['neighbourhood', 'room_type']].drop_duplicates() neighbourhood room_type 0 Santo Stefano Entire home/apt 1 Porto - Saragozza Private room 2 Santo Stefano Private room 5 Navile Private room 7 Navile Entire home/apt 8 Porto - Saragozza Entire home/apt 19 San Donato - San Vitale Private room 24 Savena Private room 36 Borgo Panigale - Reno Entire home/apt 41 San Donato - San Vitale Entire home/apt 70 Porto - Saragozza Hotel room 75 Borgo Panigale - Reno Private room 110 Santo Stefano Hotel room 111 Savena Entire home/apt 388 Porto - Saragozza Shared room 678 Navile Shared room 1393 Savena Shared room 1416 San Donato - San Vitale Shared room 1572 San Donato - San Vitale Hotel room 1637 Santo Stefano Shared room 1751 Navile Hotel room Aggregation and Pivot Tables We can compute statistics by group using .groupby().\ndf_listings.groupby('neighbourhood')[['price', 'reviews_per_month']].mean() price reviews_per_month neighbourhood Borgo Panigale - Reno 83.020548 0.983488 Navile 142.200993 1.156745 Porto - Saragozza 129.908312 1.340325 San Donato - San Vitale 91.618138 0.933011 Santo Stefano 119.441841 1.344810 Savena 69.626016 0.805888 If you want to perform more than one function, maybe on different columns, you can use .aggregate() which can be shortened to .agg(). It takes as argument a dictionary with variables as keys and lists of functions as values.\ndf_listings.groupby('neighbourhood').agg({\u0026quot;reviews_per_month\u0026quot;: [\u0026quot;mean\u0026quot;], \u0026quot;price\u0026quot;: [\u0026quot;min\u0026quot;, np.max]}).reset_index() neighbourhood reviews_per_month price mean min amax 0 Borgo Panigale - Reno 0.983488 9 1429 1 Navile 1.156745 14 5000 2 Porto - Saragozza 1.340325 7 9999 3 San Donato - San Vitale 0.933011 10 1600 4 Santo Stefano 1.344810 11 9999 5 Savena 0.805888 9 680 The problem with this syntax is that it generates a hierarchical structure for variable names, which might not be so easy to work with. In the example above, to access the mean price, you have to use df.price[\u0026quot;min\u0026quot;].\nTo perform variable naming and aggregation and the same time, you can ise the following syntax: agg(output_var = (\u0026quot;input_var\u0026quot;, function)).\ndf_listings.groupby('neighbourhood').agg(mean_reviews=(\u0026quot;reviews_per_month\u0026quot;, \u0026quot;mean\u0026quot;), min_price=(\u0026quot;price\u0026quot;, \u0026quot;min\u0026quot;), max_price=(\u0026quot;price\u0026quot;, np.max)).reset_index() neighbourhood mean_reviews min_price max_price 0 Borgo Panigale - Reno 0.983488 9 1429 1 Navile 1.156745 14 5000 2 Porto - Saragozza 1.340325 7 9999 3 San Donato - San Vitale 0.933011 10 1600 4 Santo Stefano 1.344810 11 9999 5 Savena 0.805888 9 680 We can make pivot tables with the .pivot_table() function. It takes the folling arguments:\nindex: rows columns: columns values: values aggfunc: aggregation function df_listings.pivot_table(index='neighbourhood', columns='room_type', values='price', aggfunc='mean') room_type Entire home/apt Hotel room Private room Shared room neighbourhood Borgo Panigale - Reno 96.700935 NaN 45.487179 NaN Navile 172.140000 1350.000000 68.416107 28.0 Porto - Saragozza 148.410926 102.375000 83.070234 16.5 San Donato - San Vitale 106.775000 55.000000 61.194030 59.0 Santo Stefano 129.990260 103.827586 80.734177 95.4 Savena 86.301370 NaN 46.229167 22.5 ","date":1651363200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1651363200,"objectID":"3a99f2f89f6c5ffad6aa8f0bb403e587","permalink":"https://matteocourthoud.github.io/course/data-science/02_data_exploration/","publishdate":"2022-05-01T00:00:00Z","relpermalink":"/course/data-science/02_data_exploration/","section":"course","summary":"import numpy as np import pandas as pd Setup For the scope of this tutorial we are going to use AirBnb Scraped data for the city of Bologna. The data is freely available at Inside AirBnb: http://insideairbnb.","tags":null,"title":"Data Exploration","type":"book"},{"authors":null,"categories":null,"content":"# Remove warnings import warnings warnings.filterwarnings('ignore') # Import everything import pandas as pd import numpy as np import seaborn as sns import statsmodels.api as sm from numpy.linalg import inv from statsmodels.iolib.summary2 import summary_col from linearmodels.iv import IV2SLS # Import matplotlib for graphs import matplotlib.pyplot as plt from mpl_toolkits.mplot3d import axes3d # Set global parameters %matplotlib inline plt.style.use('seaborn-white') plt.rcParams['lines.linewidth'] = 3 plt.rcParams['figure.figsize'] = (10,6) plt.rcParams['figure.titlesize'] = 20 plt.rcParams['axes.titlesize'] = 18 plt.rcParams['axes.labelsize'] = 14 plt.rcParams['legend.fontsize'] = 14 2.1 Simple Linear Regression In Acemoglu, Johnson, Robinson (2002), \u0026ldquo;The Colonial Origins of Comparative Development\u0026rdquo; the authors wish to determine whether or not differences in institutions can help to explain observed economic outcomes.\nHow do we measure institutional differences and economic outcomes?\nIn this paper,\neconomic outcomes are proxied by log GDP per capita in 1995, adjusted for exchange rates. institutional differences are proxied by an index of protection against expropriation on average over 1985-95, constructed by the Political Risk Services Group. These variables and other data used in the paper are available for download on Daron Acemoglu’s webpage.\nThe original dataset in in Stata .dta format but has been converted to .csv.\nFirst, let\u0026rsquo;s load the data and have a look at it.\n# Load Acemoglu Johnson Robinson Dataset df = pd.read_csv('data/AJR02.csv',index_col=0) df.head() GDP Exprop Mort Latitude Neo Africa Asia Namer Samer logMort Latitude2 1 8.39 6.50 78.20 0.3111 0 1 0 0 0 4.359270 0.096783 2 7.77 5.36 280.00 0.1367 0 1 0 0 0 5.634790 0.018687 3 9.13 6.39 68.90 0.3778 0 0 0 0 1 4.232656 0.142733 4 9.90 9.32 8.55 0.3000 1 0 0 0 0 2.145931 0.090000 5 9.29 7.50 85.00 0.2683 0 0 0 1 0 4.442651 0.071985 Let’s use a scatterplot to see whether any obvious relationship exists between GDP per capita and the protection against expropriation.\n# Plot relationship between GDP and expropriation rate fig, ax = plt.subplots(1,1) ax.set_title('Figure 1: joint distribution of GDP and expropriation') df.plot(x='Exprop', y='GDP', kind='scatter', s=50, ax=ax); The plot shows a fairly strong positive relationship between protection against expropriation and log GDP per capita.\nSpecifically, if higher protection against expropriation is a measure of institutional quality, then better institutions appear to be positively correlated with better economic outcomes (higher GDP per capita).\nGiven the plot, choosing a linear model to describe this relationship seems like a reasonable assumption.\nWe can write our model as\n$$ {GDP}_i = \\beta_0 + \\beta_1 {Exprop}_i + \\varepsilon_i $$\nwhere:\n$ \\beta_0 $ is the intercept of the linear trend line on the y-axis $ \\beta_1 $ is the slope of the linear trend line, representing the marginal effect of protection against risk on log GDP per capita $ \\varepsilon_i $ is a random error term (deviations of observations from the linear trend due to factors not included in the model) The most common technique to estimate the parameters ($ \\beta $’s) of the linear model is Ordinary Least Squares (OLS).\nAs the name implies, an OLS model is solved by finding the parameters that minimize the sum of squared residuals, i.e.\n$$ \\underset{\\hat{\\beta}}{\\min} \\sum^N_{i=1}{\\hat{u}^2_i} $$\nwhere $ \\hat{u}_i $ is the difference between the observation and the predicted value of the dependent variable.\nTo estimate the constant term $ \\beta_0 $, we need to add a column of 1’s to our dataset (consider the equation if $ \\beta_0 $ was replaced with $ \\beta_0 x_i $ and $ x_i = 1 $)\nNow we can construct our model in statsmodels using the OLS function.\nWe will use pandas dataframes with statsmodels, however standard arrays can also be used as arguments\n# Regress GDP on Expropriation Rate reg1 = sm.OLS.from_formula('GDP ~ Exprop', df) type(reg1) statsmodels.regression.linear_model.OLS So far we have simply constructed our model.\nWe need to use .fit() to obtain parameter estimates $ \\hat{\\beta}_0 $ and $ \\hat{\\beta}_1 $\n# Fit regression results = reg1.fit() type(results) statsmodels.regression.linear_model.RegressionResultsWrapper We now have the fitted regression model stored in results.\nTo view the OLS regression results, we can call the .summary() method.\nNote that an observation was mistakenly dropped from the results in the original paper (see the note located in maketable2.do from Acemoglu’s webpage), and thus the coefficients differ slightly.\nresults.summary() OLS Regression Results Dep. Variable: GDP R-squared: 0.540 Model: OLS Adj. R-squared: 0.532 Method: Least Squares F-statistic: 72.71 Date: Mon, 03 Jan 2022 Prob (F-statistic): 4.84e-12 Time: 18:31:09 Log-Likelihood: -68.214 No. Observations: 64 AIC: 140.4 Df Residuals: 62 BIC: 144.7 Df Model: 1 Covariance Type: nonrobust coef std err t P\u003e|t| [0.025 0.975] Intercept 4.6609 0.409 11.402 0.000 3.844 5.478 Exprop 0.5220 0.061 8.527 0.000 0.400 0.644 Omnibus: 7.134 Durbin-Watson: 2.081 Prob(Omnibus): 0.028 Jarque-Bera (JB): 6.698 Skew: -0.784 Prob(JB): 0.0351 Kurtosis: 3.234 Cond. No. 31.2 Warnings:[1] Standard Errors assume that the covariance matrix of the errors is correctly specified. From our results, we see that\nThe intercept $ \\hat{\\beta}_0 = 4.63 $. The slope $ \\hat{\\beta}_1 = 0.53 $. The positive $ \\hat{\\beta}_1 $ parameter estimate implies that. institutional quality has a positive effect on economic outcomes, as we saw in the figure. The p-value of 0.000 for $ \\hat{\\beta}_1 $ implies that the effect of institutions on GDP is statistically significant (using p \u0026lt; 0.05 as a rejection rule). The R-squared value of 0.611 indicates that around 61% of variation in log GDP per capita is explained by protection against expropriation. Using our parameter estimates, we can now write our estimated relationship as\n$$ \\widehat{GDP}_i = 4.63 + 0.53 \\ {Exprop}_i $$\nThis equation describes the line that best fits our data, as shown in Figure 2.\nWe can use this equation to predict the level of log GDP per capita for a value of the index of expropriation protection.\nFor example, for a country with an index value of 6.51 (the average for the dataset), we find that their predicted level of log GDP per capita in 1995 is 8.09.\nmean_expr = np.mean(df['Exprop']) mean_expr 6.5160937500000005 predicted_logpdp95 = results.params[0] + results.params[1] * mean_expr predicted_logpdp95 8.062499999999995 An easier (and more accurate) way to obtain this result is to use .predict() and set $ constant = 1 $ and $ {Exprop}_i = mean_expr $\nresults.predict(exog=[1, mean_expr]) We can obtain an array of predicted $ {GDP}_i $ for every value of $ {Exprop}_i $ in our dataset by calling .predict() on our results.\nPlotting the predicted values against $ {Exprop}_i $ shows that the predicted values lie along the linear line that we fitted above.\nThe observed values of $ {GDP}_i $ are also plotted for comparison purposes\n# Make first new figure def make_new_fig_2(): # Init figure fig, ax = plt.subplots(1,1) ax.set_title('Figure 2: OLS predicted values') # Drop missing observations from whole sample df_plot = df.dropna(subset=['GDP', 'Exprop']) sns.regplot(x=df_plot['Exprop'], y=df_plot['GDP'], ax=ax, order=1, ci=None, line_kws={'color':'r'}) ax.legend(['predicted', 'observed']) ax.set_xlabel('Exprop') ax.set_ylabel('GDP') plt.show() make_new_fig_2() ERROR! Session/line number was not unique in database. History logging moved to new session 305 2.2 Extending the Linear Regression Model So far we have only accounted for institutions affecting economic performance - almost certainly there are numerous other factors affecting GDP that are not included in our model.\nLeaving out variables that affect $ GDP_i $ will result in omitted variable bias, yielding biased and inconsistent parameter estimates.\nWe can extend our bivariate regression model to a multivariate regression model by adding in other factors that may affect $ GDP_i $.\n[AJR01] consider other factors such as:\nthe effect of climate on economic outcomes; latitude is used to proxy this differences that affect both economic performance and institutions, eg. cultural, historical, etc.; controlled for with the use of continent dummies Let’s estimate some of the extended models considered in the paper (Table 2) using data from maketable2.dta\n# Add constant term to dataset df['const'] = 1 # Create lists of variables to be used in each regression X1 = df[['const', 'Exprop']] X2 = df[['const', 'Exprop', 'Latitude', 'Latitude2']] X3 = df[['const', 'Exprop', 'Latitude', 'Latitude2', 'Asia', 'Africa', 'Namer', 'Samer']] # Estimate an OLS regression for each set of variables reg1 = sm.OLS(df['GDP'], X1, missing='drop').fit() reg2 = sm.OLS(df['GDP'], X2, missing='drop').fit() reg3 = sm.OLS(df['GDP'], X3, missing='drop').fit() Now that we have fitted our model, we will use summary_col to display the results in a single table (model numbers correspond to those in the paper)\ninfo_dict={'No. observations' : lambda x: f\u0026quot;{int(x.nobs):d}\u0026quot;} results_table = summary_col(results=[reg1,reg2,reg3], float_format='%0.2f', stars = True, model_names=['Model 1','Model 2','Model 3'], info_dict=info_dict, regressor_order=['const','Exprop','Latitude','Latitude2']) results_table Model 1 Model 2 Model 3 const 4.66*** 4.55*** 5.95*** (0.41) (0.45) (0.68) Exprop 0.52*** 0.49*** 0.40*** (0.06) (0.07) (0.06) Latitude 2.16 0.42 (1.68) (1.47) Latitude2 -2.12 0.44 (2.86) (2.48) Africa -1.06** (0.41) Asia -0.74* (0.42) Namer -0.17 (0.40) Samer -0.12 (0.42) No. observations 64 64 64 2.3 Endogeneity As [AJR01] discuss, the OLS models likely suffer from endogeneity issues, resulting in biased and inconsistent model estimates.\nNamely, there is likely a two-way relationship between institutions an economic outcomes:\nricher countries may be able to afford or prefer better institutions variables that affect income may also be correlated with institutional differences the construction of the index may be biased; analysts may be biased towards seeing countries with higher income having better institutions To deal with endogeneity, we can use two-stage least squares (2SLS) regression, which is an extension of OLS regression.\nThis method requires replacing the endogenous variable $ {Exprop}_i $ with a variable that is:\ncorrelated with $ {Exprop}_i $ not correlated with the error term (ie. it should not directly affect the dependent variable, otherwise it would be correlated with $ u_i $ due to omitted variable bias) We can write our model as\n$$ {GDP}_i = \\beta_0 + \\beta_1 {Exprop}_i + \\varepsilon_i \\ {Exprop}_i = \\delta_0 + \\delta_1 {logMort}_i + v_i $$\nThe new set of regressors logMort is called an instrument, which aims to remove endogeneity in our proxy of institutional differences.\nThe main contribution of [AJR01] is the use of settler mortality rates to instrument for institutional differences.\nThey hypothesize that higher mortality rates of colonizers led to the establishment of institutions that were more extractive in nature (less protection against expropriation), and these institutions still persist today.\nUsing a scatterplot (Figure 3 in [AJR01]), we can see protection against expropriation is negatively correlated with settler mortality rates, coinciding with the authors’ hypothesis and satisfying the first condition of a valid instrument.\n# Dropping NA's is required to use numpy's polyfit df2 = df.dropna(subset=['logMort', 'Exprop']) X = df2['logMort'] y = df2['Exprop'] # Make new figure 2 def make_new_figure_2(): # Init figure fig, ax = plt.subplots(1,1) ax.set_title('Figure 3: First-stage') # Fit a linear trend line sns.regplot(x=X, y=y, ax=ax, order=1, scatter=True, ci=None, line_kws={\u0026quot;color\u0026quot;: \u0026quot;r\u0026quot;}) ax.set_xlim([1.8,8.4]) ax.set_ylim([3.3,10.4]) ax.set_xlabel('Log of Settler Mortality') ax.set_ylabel('Average Expropriation Risk 1985-95'); make_new_figure_2() The second condition may not be satisfied if settler mortality rates in the 17th to 19th centuries have a direct effect on current GDP (in addition to their indirect effect through institutions).\nFor example, settler mortality rates may be related to the current disease environment in a country, which could affect current economic performance.\n[AJR01] argue this is unlikely because:\nThe majority of settler deaths were due to malaria and yellow fever and had a limited effect on local people. The disease burden on local people in Africa or India, for example, did not appear to be higher than average, supported by relatively high population densities in these areas before colonization. As we appear to have a valid instrument, we can use 2SLS regression to obtain consistent and unbiased parameter estimates.\nFirst stage The first stage involves regressing the endogenous variable ($ {Exprop}_i $) on the instrument.\nThe instrument is the set of all exogenous variables in our model (and not just the variable we have replaced).\nUsing model 1 as an example, our instrument is simply a constant and settler mortality rates $ {logMort}_i $.\nTherefore, we will estimate the first-stage regression as\n$$ {Exprop}_i = \\delta_0 + \\delta_1 {logMort}_i + v_i $$\n# Add a constant variable df['const'] = 1 # Fit the first stage regression and print summary results_fs = sm.OLS(df['Exprop'], df.loc[:,['const', 'logMort']], missing='drop').fit() results_fs.summary() OLS Regression Results Dep. Variable: Exprop R-squared: 0.274 Model: OLS Adj. R-squared: 0.262 Method: Least Squares F-statistic: 23.34 Date: Mon, 03 Jan 2022 Prob (F-statistic): 9.27e-06 Time: 18:31:10 Log-Likelihood: -104.69 No. Observations: 64 AIC: 213.4 Df Residuals: 62 BIC: 217.7 Df Model: 1 Covariance Type: nonrobust coef std err t P\u003e|t| [0.025 0.975] const 9.3659 0.611 15.339 0.000 8.145 10.586 logMort -0.6133 0.127 -4.831 0.000 -0.867 -0.360 Omnibus: 0.047 Durbin-Watson: 1.592 Prob(Omnibus): 0.977 Jarque-Bera (JB): 0.154 Skew: 0.060 Prob(JB): 0.926 Kurtosis: 2.792 Cond. No. 19.4 Warnings:[1] Standard Errors assume that the covariance matrix of the errors is correctly specified. We need to retrieve the predicted values of $ {Exprop}_i $ using .predict().\nWe then replace the endogenous variable $ {Exprop}_i $ with the predicted values $ \\widehat{Exprop}_i $ in the original linear model.\nOur second stage regression is thus\n$$ {GDP}_i = \\beta_0 + \\beta_1 \\widehat{Exprop}_i + u_i $$\nSecond stage # Second stage df['predicted_Exprop'] = results_fs.predict() results_ss = sm.OLS.from_formula('GDP ~ predicted_Exprop', df).fit() # Print results_ss.summary() OLS Regression Results Dep. Variable: GDP R-squared: 0.462 Model: OLS Adj. R-squared: 0.453 Method: Least Squares F-statistic: 53.24 Date: Mon, 03 Jan 2022 Prob (F-statistic): 6.58e-10 Time: 18:31:10 Log-Likelihood: -73.208 No. Observations: 64 AIC: 150.4 Df Residuals: 62 BIC: 154.7 Df Model: 1 Covariance Type: nonrobust coef std err t P\u003e|t| [0.025 0.975] Intercept 2.0448 0.830 2.463 0.017 0.385 3.705 predicted_Exprop 0.9235 0.127 7.297 0.000 0.671 1.177 Omnibus: 10.463 Durbin-Watson: 2.052 Prob(Omnibus): 0.005 Jarque-Bera (JB): 10.693 Skew: -0.806 Prob(JB): 0.00476 Kurtosis: 4.188 Cond. No. 57.8 Warnings:[1] Standard Errors assume that the covariance matrix of the errors is correctly specified. The second-stage regression results give us an unbiased and consistent estimate of the effect of institutions on economic outcomes.\nThe result suggests a stronger positive relationship than what the OLS results indicated.\nNote that while our parameter estimates are correct, our standard errors are not and for this reason, computing 2SLS ‘manually’ (in stages with OLS) is not recommended.\nWe can correctly estimate a 2SLS regression in one step using the linearmodels package, an extension of statsmodels\nNote that when using IV2SLS, the exogenous and instrument variables are split up in the function arguments (whereas before the instrument included exogenous variables)\n# IV regression iv = IV2SLS(dependent=df['GDP'], exog=df['const'], endog=df['Exprop'], instruments=df['logMort']).fit() # Print iv.summary IV-2SLS Estimation Summary Dep. Variable: GDP R-squared: 0.2205 Estimator: IV-2SLS Adj. R-squared: 0.2079 No. Observations: 64 F-statistic: 29.811 Date: Mon, Jan 03 2022 P-value (F-stat) 0.0000 Time: 18:31:10 Distribution: chi2(1) Cov. Estimator: robust Parameter Estimates Parameter Std. Err. T-stat P-value Lower CI Upper CI const 2.0448 1.1273 1.8139 0.0697 -0.1647 4.2542 Exprop 0.9235 0.1691 5.4599 0.0000 0.5920 1.2550 Endogenous: ExpropInstruments: logMortRobust Covariance (Heteroskedastic)Debiased: False Given that we now have consistent and unbiased estimates, we can infer from the model we have estimated that institutional differences (stemming from institutions set up during colonization) can help to explain differences in income levels across countries today.\n[AJR01] use a marginal effect of 0.94 to calculate that the difference in the index between Chile and Nigeria (ie. institutional quality) implies up to a 7-fold difference in income, emphasizing the significance of institutions in economic development.\n2.4 Matrix Algebra The OLS parameter $ \\beta $ can also be estimated using matrix algebra and numpy.\nThe linear equation we want to estimate is (written in matrix form)\n$$ y = X\\beta + \\varepsilon $$\n# Init X = df[['const', 'Exprop']].values Z = df[['const', 'logMort']].values y = df['GDP'].values To solve for the unknown parameter $ \\beta $, we want to minimize the sum of squared residuals\n$$ \\underset{\\hat{\\beta}}{\\min} \\ \\hat{\\varepsilon}\u0026rsquo;\\hat{\\varepsilon} $$\nRearranging the first equation and substituting into the second equation, we can write\n$$ \\underset{\\hat{\\beta}}{\\min} \\ (Y - X\\hat{\\beta})\u0026rsquo; (Y - X\\hat{\\beta}) $$\nSolving this optimization problem gives the solution for the $ \\hat{\\beta} $ coefficients\n$$ \\hat{\\beta} = (X\u0026rsquo;X)^{-1}X\u0026rsquo;y $$\n# Compute beta OLS beta_OLS = inv(X.T @ X) @ X.T @ y print(beta_OLS) [4.66087966 0.52203367] As we as see above, the OLS coefficient might suffer from endogeneity bias. We can solve the issue by instrumenting the predicted average expropriation rate with the average settler mortality.\nIf we define settler mortality as $Z$, our full model is\n$$ y = X\\beta + \\varepsilon \\ X = Z\\gamma + \\mu $$\nWhere we refer to the second equation as second stage and to the first equation as the reduced form equation. In our case, since the number of endogenous varaibles is equal to the number of insturments, there are two equivalent estimators that do not suffer from endogeneity bias: 2SLS and IV.\nIV, the one stage estimator\n$$ \\hat \\beta_{IV} = (Z\u0026rsquo;X)^{-1} Z\u0026rsquo; y $$\n# Compute beta IV beta_IV = inv(Z.T @ X) @ Z.T @ y print(beta_IV) [2.0447613 0.92351936] One of the hypothesis behind the IV estimator is the relevance of the instrument, i.e. we have a strong predictor in the first stage. This is the only hypothesis that we can empirically assess by checking the significance of the first stage coefficient.\n$$ \\hat \\gamma = (Z\u0026rsquo; Z)^{-1} Z\u0026rsquo;X \\ \\hat Var (\\hat \\gamma) = \\sigma_u^2 (Z\u0026rsquo; Z)^{-1} $$\nwhere\n$$ u = X - Z \\hat \\gamma $$\n# Estimate first stage coefficient gamma_hat = (inv(Z.T @ Z) @ Z.T @ X) print(gamma_hat[1,1]) -0.613289272386864 # Compute variance of the estimator u = X - Z @ gamma_hat var_gamma_hat = np.var(u) * inv(Z.T @ Z) # Compute standard errors std_gamma_hat = var_gamma_hat[1,1]**.5 print(std_gamma_hat) 0.08834733362858548 # Compute 95% confidence interval CI = [gamma_hat[1,1] - 1.96*std_gamma_hat, gamma_hat[1,1] + 1.96*std_gamma_hat] print(CI) [-0.7864500462988916, -0.4401284984748365] The first stage coefficient is negative and significant, i.e. settler mortality is negatively correlated with the expropriation rate.\nHow does it work when we have more instruments than endogenous variables? Two-State Least Squares.\nRegress $X$ on $Z$ and obtain $\\hat X$: $$ \\hat X = Z (Z\u0026rsquo; Z)^{-1} Z\u0026rsquo;X $$ Regress $Y$ on $\\hat X$ and obtain $\\hat \\beta_{2SLS}$ $$ \\hat \\beta_{2SLS} = (\\hat X\u0026rsquo; \\hat X)^{-1} \\hat X\u0026rsquo; y $$ In our case, just for the sake of exposition, let\u0026rsquo;s generate a second instrument: the settler mortality squared, logMort_2 = logMort^2.\ndf['logMort_2'] = df['logMort']**2 # Define Z Z1 = df[['const', 'logMort', 'logMort_2']].values # Compute beta 2SLS in two steps X_hat = Z1 @ inv(Z1.T @ Z1) @ Z1.T @ X beta_2SLS = inv(X_hat.T @ X_hat) @ X_hat.T @ y print(beta_2SLS) [3.08817432 0.76339075] The 2SLS estimator does not have to be actually estimated in two stages. Combining the two formulas above, we get\n$$ \\hat{\\beta} _ {2SLS} = \\Big( X\u0026rsquo;Z (Z\u0026rsquo;Z)^{-1} Z\u0026rsquo;X \\Big)^{-1} \\Big( X\u0026rsquo;Z (Z\u0026rsquo;Z)^{-1} Z\u0026rsquo;y \\Big) $$\nwhich can be computed in one step.\n# Compute beta 2SLS in one step beta_2SLS = inv(X_hat.T @ Z1 @ inv(Z1.T @ Z1) @ Z1.T @ X_hat) @ X_hat.T @ Z1 @ inv(Z1.T @ Z1) @ Z1.T @ y print(beta_2SLS) [3.08817432 0.76339075] ","date":1646784000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1646784000,"objectID":"1d1a4399e0bf93a4fbd939183d1d975f","permalink":"https://matteocourthoud.github.io/course/ml-econ/02_iv/","publishdate":"2022-03-09T00:00:00Z","relpermalink":"/course/ml-econ/02_iv/","section":"course","summary":"# Remove warnings import warnings warnings.filterwarnings('ignore') # Import everything import pandas as pd import numpy as np import seaborn as sns import statsmodels.api as sm from numpy.linalg import inv from statsmodels.","tags":null,"title":"Instrumental Variables","type":"book"},{"authors":null,"categories":null,"content":"Introduction Setting Oligopoly Supply\nfirms produce differentiated goods/products\nselling to consumers with heterogeneous preferences\nstatic model, complete information\nproducts are given\nequilibrium: NE for each product/market\nCost Function Variable cost of product $j$: $C_j (Q_j , w_{jt} , \\mathbb \\omega_{jt}, \\gamma)$\n$Q_j$: total quantity of good $j$ sold\n$w_{jt}$ observable cost shifters; may include product characteristics $x_{jt}$ that will affect demand (later)\n$\\omega_{jt}$ unobserved cost shifters (“cost shocks”); may be correlated with latent demand shocks (later)\n$\\gamma$: parameters\nNotes\nfor multi-product firms, we’ll assume variable cost additive across products for simplicity we ignore fixed costs: these affect entry/exit/innovation but not pricing, conditional on these things Notation Some other variables\n$J_t$: products/goods/choices in market $t$ (for now $J_t = J$) $P_t = (p_{1t},\u0026hellip;,p_{Jt})$: prices of all goods $\\boldsymbol X_t = ( \\boldsymbol x_{1t} , … , \\boldsymbol x_{Jt})$ : other characteristics of goods affecting demand (observed and unobserved to us) In general\nI use bold for arrays in dimensions that are not $i$ (consumers), $j$ (firms) or $t$ (markets) For example product characteristics $\\boldsymbol x_{jt} = \\lbrace x_{jt}^1,, \u0026hellip;, x_{jt}^K \\rbrace$ I use CAPS for variables aggregated over $j$ (firms) For example vector of prices in market $t$: $P_t = (p_{1t},\u0026hellip;,p_{Jt})$ Equilibrium Pricing Demand system:\n$$ q_{jt} = Q_j ( P_t, \\boldsymbol X_t) \\quad \\text{for} \\quad j = 1,\u0026hellip;,J. $$\nProfit function\n$$ \\pi_{jt} = Q_j (P_t, \\boldsymbol X_t) \\Big[p_{jt} − mc_j (w_{jt}, \\omega_{jt}, \\gamma) \\Big] $$\nFOC wrt to $p_{jt}$:\n$$ p_{jt} = mc_{jt} - Q_j (P_t, \\boldsymbol X_t) \\left(\\frac{\\partial Q_j}{\\partial p_{jt}}\\right)^{-1} $$\nInverse elasticity pricing (i.e., monopoly pricing) against the “residual demand curve” $Q_j (P_t, \\boldsymbol X_t)$:\n$$ \\frac{p_{jt} - mc_{jt}}{p_{jt}} = - \\frac{Q_j (P_t, \\boldsymbol X_t)}{p_{jt}} \\left(\\frac{\\partial Q_j}{\\partial p_{jt}}\\right)^{-1} $$\nWhat do we get? Holding all else fixed, markups/prices depend on the own-price elasticities of residual demand. Equilibrium depends, further, on how a change in price of one good affects the quantities sold of others, i.e., on cross-price demand elasticities\nIf we known demand, we can also perform a small miracle:\nRe-arrange FOC\n$$ mc_{jt} = p_{jt} + Q_j (P_t, \\boldsymbol X_t)\\left(\\frac{\\partial Q_j}{\\partial p_{jt}}\\right)^{-1} $$\nSupply model + estimated demand $\\to$ estimates of marginal costs!\nIf we know demand and marginal costs, we can”predict” a lot of stuff - i.e., give the quantitative implications of the model for counterfactual worlds\nIssues Typically we need to know levels/elasticities of demand at particular points; i.e., effects of one price change holding all else fixed\nThe main challenge: unobserved demand shifters (“demand shocks”) at the level of the good×market (e.g., unobserved product char or market-specific variation in mean tastes for products)\ndemand shocks are among the things that must be held fixed to measure the relevant demand elasticities etc.\nexplicit modeling of these demand shocks central in the applied IO literature following S. Berry, Levinsohn, and Pakes (1995) (often ignored outside this literature).\nKey Challenge The demand of product $j$\n$$ q_{jt} (\\boldsymbol X_{t}, P_t, \\Xi_t) $$\ndepends on:\n$P_t$: $J$-vector of all goods’ prices in market $t$\n$\\boldsymbol X_t$: $J \\times k$ matrix of all non-price observables in market $t$\n$\\Xi_t$: J-vector of demand shocks for all goods in market $t$\nKey insight: we have an endogeneity problem even if prices were exogenous!\nPrice Endogeneity Adds to the Challenge all $J$ endogenous prices are on RHS of demand for each good\nequilibrium pricing implies that each price depends on all demand shocks and all cost shocks\nprices endogenous\ncontrol function generally is not a valid solution\nclear that we need sources of exogenous price variation, but\nwhat exactly is required?\nhow do we proceed?\nBLP: Model Goals of BLP Model of S. Berry, Levinsohn, and Pakes (1995)\nparsimonious specification to generate the distribution $F_U (\\cdot| P, \\Xi)$ of random utilities sufficiently rich heterogeneity in preferences to permit reasonable/flexible substitution patterns be explicit about unobservables, including the nature of endogeneity “problem(s)” use the model to reveal solutions to the identification problem, including appropriate instruments computationally feasible (in early 1990s!) algorithm for consistent estimation of the model and standard errors. Utility Specification Utility of consumer $i$ for product $j$\n$$ u_{ijt} = \\boldsymbol x_{jt} \\boldsymbol \\beta_{it} - \\alpha p_{jt} + \\xi_{jt} + \\epsilon_{ijt} $$\nWhere\n$\\boldsymbol x_{jt}$: $K$-vector of characteristics of product $j$ in market $t$\n$\\boldsymbol \\beta_{it} = (\\beta_{it}^{1}, \u0026hellip;, \\beta_{it}^K)$: vector of tastes for characteristics $1,…,K$ in market $t$\n$\\beta_{it}^k = \\beta_0^k + \\sigma_k \\zeta_{it}^k$\n$\\beta_0^k$: fixed taste for characteristic $k$ (the usual $\\beta$)\n$\\zeta_{it}^k$: random taste, i.i.d. across consumers and markets $t$\n$\\alpha$: price elasticity\n$p_{jt}$ price of product $j$ in market $t$\n$\\xi_{jt}$: unobservable product shock at the level of products $j$ $\\times$ market $t$\n$\\epsilon_{ijt}$: idiosyncratic (and latent) taste\nExogenous and Endogenous Product Characteristics Utility of consumer $i$ for product $j$\n$$ u_{ijt} = \\boldsymbol x_{jt} \\beta_{it} - \\alpha p_{jt} + \\xi_{jt} + \\epsilon_{ijt} $$\nexogenous characteristics: $\\boldsymbol x_{jt} \\perp \\xi_{jt}$\nendogenous characteristics: $p_{jt}$ (usually a scalar, price)\ntypically each $p_{jt}$ will depend on whole vector $\\Xi_t = (\\xi_{1t} , . . . , \\xi_{Jt} )$ and on own costs $mc_{jt}$ and others’ costs $mc_{-jt}$ we need to distinguish true effects of prices on demand from the effects of $\\Xi_t$ ; this will require instruments of course the equation above is not an estimating equation ($u_{ijt}$ not observed) because prices and quantities are all endogenous - indeed determined - simultaneously, you may suspect (correctly) that instruments for prices alone may not suffice. Utility Specification, Rewritten Rewrite\n$$ \\begin{align} u_{ijt} \u0026amp;= \\boldsymbol x_{jt} \\boldsymbol \\beta_{it} - \\alpha p_{jt} + \\xi_{jt} + \\epsilon_{ijt} = \\newline \u0026amp;= \\delta_{jt} + \\nu_{ijt} \\end{align} $$\nwhere\n$\\delta_{jt} = \\boldsymbol x_{jt} \\boldsymbol \\beta_0 - \\alpha p_{jt} + \\xi_{jt}$ mean utility of good $j$ in market $t$ $\\nu_{ijt} = \\sum_{k} x_{jt}^{k} \\sigma^{k} \\zeta_{i t}^{k} + \\epsilon_{ijt} \\equiv \\boldsymbol x_{jt} \\tilde{\\boldsymbol \\beta}{it} + \\epsilon{ijt}$ We split $\\beta_{it}$ into its random ($\\tilde{\\beta}_{it}$) and non-random ($\\beta_0$) part From Consumer Utility to Demand With a continuum of consumers in each market: market shares = choice probabilities\nP.S. continuum not needed, enough that sampling error on choice probs negligible compared to that of moments based on variation across products/markets $$ s_{jt} (\\Delta_t, \\boldsymbol X_t, \\boldsymbol \\sigma) = \\Pr (y_{it} = j) = \\int_{\\mathcal A_j (\\Delta_t)} \\text d F_{\\nu} \\Big(\\nu_{i0t}, \\nu_{i1t}, \u0026hellip; , \\nu_{iJt} \\ \\Big| \\ \\boldsymbol X_t, \\boldsymbol \\sigma \\Big) $$\nwhere $$ \\mathcal A_j(\\Delta_t) = \\Big\\lbrace (\\nu_{i0t}, \\nu_{i1t}, \u0026hellip; , \\nu_{iJt} ) \\in \\mathbb{R}^{J+1}: \\delta_{jt} + \\nu_{ijt} \\geq \\delta_{kt} + \\nu_{ikt} \\ , \\ \\forall k \\Big\\rbrace $$ In words: market share of firm $j$ is the frequency of consumers buying good $j$ Demand is just shares $s_{jt}$ per market size $M_t$ $$ q_{jt} = M_t \\times s_j (\\Delta_t, \\boldsymbol X_t, \\boldsymbol \\sigma) $$\nWhy Random Coefficients? Without random coefficients $$ \\begin{aligned} u_{ijt} \u0026amp;= \\underbrace{\\boldsymbol x_{jt} \\boldsymbol \\beta_0 - \\alpha p_{jt} + \\xi_{jt}} + \\epsilon_{ijt} \\newline \u0026amp;= \\hspace{3.4em} \\delta_{jt} \\hspace{3.4em} + \\epsilon_{ijt} \\end{aligned} $$ If $\\epsilon_{ijt}$ are iid and independent of $(\\boldsymbol X_t, P_t)$, e.g. as in the multinomial logit or probit models,\nproducts differ only in mean utilities $\\delta_{jt}$ $\\to$ market shares depend only on the mean utilities $\\to$ price elasticities (own and cross) depend only on mean utilities too Implication: two products with the same market shares have the same cross elasticities w.r.t. all other products\nDoes this matter? Yes!\nMercedes class-A and Fiat Panda might both have low market shares But realistically should have very different cross-price elasticities w.r.t. BMW series-2 What is the issue?\nModels (like MNL) that have only iid additive taste shocks impose very restrictive relationships between the levels of market shares and the matrix of own and cross-price derivatives\nImpact on counterfactuals! Restrictions only coming from model assumptions (analytical convenience)\nModels always imporse restrictions\nnecessary for estimation but must allow flexibility in the relevant dimensions How do random coefficients help? In reality:\ngoods differ in multiple dimensions consumers have (heterogeneous) preferences over these differences How do random coefficients capture it?\nlarge $\\beta_i^k$ $\\leftrightarrow$ strong taste for characteristic $k$ e.g., maximum speed for sport car Consumer $i$’s first choice likely to have high value of $x^k$ $i$’s second choice too! Mark: cross elasticities are always about 1st vs. 2nd choices Incorporating this allows more sensible substitution patterns\ncompetition is mostly “local” i.e., between firms offering products appealing to the same consumers. Which random coefficients? Which characteristics have random coefficients?\ndummies for subsets of products? S. T. Berry (1994): covers the nested logit as a special case certain horizontal or vertical characteristics? parts of $(\\boldsymbol X_t, P_t)$? In practice\nChoice depends on the application and data set, including instruments Too many RC’s (w.r.t quantity of data available) $\\to$ imprecise estimates of $\\boldsymbol \\sigma$ BLP: Estimation Setting Observables\n$\\boldsymbol X_t$: product characteristics $P_t$: prices $S_t$: observed market shares $\\boldsymbol W_t$: observable cost shifters $\\boldsymbol Z_t$: excluded instruments Sketch of procedure\nstart with demand model alone suppose $ F_{\\nu{=tex}} (\\cdot {=tex} | \\boldsymbol {=tex}X, \\boldsymbol {=tex}\\sigma {=tex})$ is known (i.e., $\\sigma$ known) for each market $t$, find mean utilities $\\Delta_t \\in \\mathbb R$ such that $s_{jt} (\\Delta_t, \\boldsymbol X_t, \\boldsymbol \\sigma) = s^{obs}_{jt} \\ \\forall j$ i.e.,“invert” model at observed market shares to find mean utilities $\\boldsymbol \\delta$ where $s^{obs}_{jt}$ are the observed market shares using IV ,e.g. $\\mathbb E [\\boldsymbol z_{jt} | \\xi_{jt} ] = 0$, estimate the equation Issues What instruments? Will the “inversion” step actually work? What about $\\boldsymbol \\sigma$?? Formal estimator? Computational algorithm(s)? Supply side additional restrictions (moment conditions) help estimation of demand additional parameters: marginal cost function why? may care directly and needed for counterfactuals that change equilibrium quantities unless $mc$ is constant Instruments We need intruments for all endogenous variables—prices and quantities—independently.\nExcluded cost shifters $\\boldsymbol W_t$ (classic)\nUsually: wages, material costs, shipping cost to market $t$, taxes/tariffs, demand shifters from other markets Or proxies for them\nUsually: price of same good in another mkt (“Hausman instruments”) Markup shifters:\nUsually: characteristics of “nearby” markets (“Waldfogel instruments”)\nLogic: income/age/education in San Francisco might affect prices in Oakland but might be independent fo Oakland preferences\nProduct characteristics of other firms in the same market $\\boldsymbol X_{-jt}$\n“BLP instruments” affect quantities directly; affect prices (markups) via equilibrium only Inversion How do we get from market shares to prices??\nGiven x,σ and any positive shares sh, define the following mapping $\\Phi : \\mathbb R^j \\to \\mathbb R^j$ $$ \\Phi (\\Delta_t) = \\Delta_t + \\log\\Big( \\hat S^{obs}_t \\Big) - \\log \\Big( S_t (\\Delta_t, \\boldsymbol X_t, \\boldsymbol \\sigma) \\Big) $$ S. T. Berry (1994): for any nonzero shares sh, Φ is a contraction\nunder mild conditions on the linear random coefficients random utility model extreme value and normal random coeff not necessary What does it imply?\nIt has a unique fixed point: we can compute $\\delta_{jt} = \\delta (S_t, \\boldsymbol X_t, \\boldsymbol \\sigma)$ We can compute the fixed point iterating the contraction from any initial guess $\\Delta_{0t}$ What about $\\sigma$? What we we got?\ninversion result: for any market shares and any $\\boldsymbol \\sigma$, we can find a vector of mean utilities $\\Delta_t$ that rationalizes the data with the BLP model a non-identification result? there is no information about $\\boldsymbol \\sigma$ from market shares? What are we forgetting?\nCross-market variation! We can get the mean utilities $\\delta_{jt} = \\boldsymbol x_{jt} \\boldsymbol \\beta_0 - \\alpha p_{jt} + \\xi_{jt}$ As in OLS, use $\\boldsymbol z_{jt} \\perp \\xi_{jt}$ to get identification of $(\\alpha, \\boldsymbol \\beta_0, \\boldsymbol \\sigma)$ Identification of $\\sigma$ We are trying to estimate $(\\alpha, \\boldsymbol \\beta_0, \\boldsymbol \\sigma)$ from $$ \\mathbb E \\Big[ \\xi_{jt} (\\alpha, \\boldsymbol \\beta_0, \\boldsymbol \\sigma) \\cdot \\boldsymbol z_{jt} \\Big] = \\mathbb E \\Big[ \\big( \\delta_{jt}(\\boldsymbol \\sigma) - \\boldsymbol x_{jt} \\boldsymbol \\beta_0 + \\alpha p_{jt} \\big) \\cdot \\boldsymbol z_{jt} \\Big] $$ What kind of intruments $\\boldsymbol z_{jt}$ do we need?\n$\\boldsymbol x_{jt}$ (for $\\boldsymbol \\beta_0$) intruments for $p_{jt}$ (for $\\alpha$) but also something for $\\boldsymbol \\sigma$! BLP Estimation Steps\nTake guess of parameters $(\\alpha, \\boldsymbol \\beta_0, \\boldsymbol \\sigma)$ From observed market shared $S^{obs}{t}$ and $\\boldsymbol \\sigma$ get mean utilities $\\delta{jt} (\\boldsymbol \\sigma)$ Use also $(\\alpha, \\boldsymbol \\beta_0)$ to get $\\xi_{jt} (\\alpha, \\boldsymbol \\beta_0, \\boldsymbol \\sigma)$ Compute empirical moments $\\frac{1}{JT} \\xi_{jt} (\\alpha, \\boldsymbol \\beta_0, \\boldsymbol \\sigma) \\cdot \\boldsymbol z_{jt}$ The GMM estimator is $(\\hat \\alpha, \\boldsymbol{\\hat{\\beta}_0}, \\boldsymbol{\\hat{\\sigma}})$ that get the empirical moments as close to $0$ as possible.\nIssues\nComputing $S_t (\\Delta_t, \\boldsymbol X_t, \\boldsymbol \\sigma)$ involves a high dimensional integral Use simulation to approximate distribution of random tastes $\\zeta_{it}^k$ P.S. recall that we have decomposed random coefficients $\\beta_{it}^k$ as $\\beta_{it}^k = \\beta_0^k + \\sigma_k \\zeta_{it}^k$ $\\xi_{jt} (\\alpha, \\boldsymbol \\beta_0, \\boldsymbol \\sigma)$ has no closed form solution Compute it via contraction MPEC? Computation Nested fixed point algorithm Sketch of the algorithm\nDraw a vector of consumer tastes Until you have found a minimum for $\\mathbb E \\Big[ \\xi_{jt} (\\alpha, \\boldsymbol \\beta_0, \\boldsymbol \\sigma) \\cdot \\boldsymbol z_{jt} \\Big]$ do Pick a vector of parameter values $(\\alpha, \\boldsymbol \\beta_0, \\boldsymbol \\sigma)$ Initialize mean utilities $\\delta_{jt}^0$ Until $\\big|\\big| \\Delta_{t}^{n+1} - \\Delta_{t}^{n} \\big|\\big| \u0026lt; tolerance$ do Compute implied shares: $s_{jt} (\\Delta_{t}^{n}, \\boldsymbol X_t, \\boldsymbol \\sigma) = \\int \\frac{\\exp \\left[ \\boldsymbol x_{j t} \\boldsymbol{\\tilde{\\beta}}{it}+\\delta{j t}\\right]}{1+\\sum_{j^{\\prime}} \\exp \\left[\\boldsymbol x_{j^{\\prime} t} \\boldsymbol{\\tilde{\\beta}}{it}+\\delta{j\u0026rsquo; t}\\right]} f\\left( \\boldsymbol{\\tilde{\\beta}}{it} \\mid \\theta\\right) d \\tilde{\\beta}{i t}$ Update mean utilities: $\\Delta_{t}^{n+1} = \\Delta_{t}^{n} + \\log\\Big( \\hat S^{obs}t \\Big) - \\log \\Big( S_t (\\Delta{t}^{n}, \\boldsymbol X_t, \\boldsymbol \\sigma) \\Big)$ Compute $\\xi_{jt} = \\delta_{jt} - \\boldsymbol x_{jt} \\boldsymbol \\beta_0 + \\alpha p_{jt}$ Compute $\\mathbb E \\Big[ \\xi_{jt} (\\alpha, \\boldsymbol \\beta_0, \\boldsymbol \\sigma) \\cdot \\boldsymbol z_{jt} \\Big]$ Notes Important to draw shocks outside the optimization routine! Appendix References [references] Berry, Steven T. 1994. “Estimating Discrete-Choice Models of Product Differentiation.” The RAND Journal of Economics, 242–62.\nBerry, Steven, James Levinsohn, and Ariel Pakes. 1995. “Automobile Prices in Market Equilibrium.” Econometrica: Journal of the Econometric Society, 841–90.\n","date":1635465600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1635465600,"objectID":"ce928b138560543460b6303f1927ab29","permalink":"https://matteocourthoud.github.io/course/empirical-io/02_demand_estimation/","publishdate":"2021-10-29T00:00:00Z","relpermalink":"/course/empirical-io/02_demand_estimation/","section":"course","summary":"Introduction Setting Oligopoly Supply\nfirms produce differentiated goods/products\nselling to consumers with heterogeneous preferences\nstatic model, complete information\nproducts are given\nequilibrium: NE for each product/market\nCost Function Variable cost of product $j$: $C_j (Q_j , w_{jt} , \\mathbb \\omega_{jt}, \\gamma)$","tags":null,"title":"Demand Estimation","type":"book"},{"authors":null,"categories":null,"content":"Probability Probability Space A probability space is a triple $(\\Omega, \\mathcal A, P)$ where\n$\\Omega$ is the sample space. $\\mathcal A$ is the $\\sigma$-algebra on $\\Omega$. $P$ is a probability measure. The sample space $\\Omega$ is the space of all possible events.\nWhat is a $\\sigma$-algebra and a probability measure?\nSigma Algebra A nonempty set (of subsets of $\\Omega$) $\\mathcal A \\in 2^\\Omega$ is a sigma algebra ($\\sigma$-algebra) of $\\Omega$ if the following conditions hold:\n$\\Omega \\in \\mathcal A$ If $A \\in \\mathcal A$, then $(\\Omega - A) \\in \\mathcal A$ If $A_1, A_2, \u0026hellip; \\in \\mathcal A$, then $\\bigcup _ {i=1}^{\\infty} A_i \\in \\mathcal A$ The smallest $\\sigma$-algebra is $\\lbrace \\emptyset, \\Omega \\rbrace$ and the largest one is $2^\\Omega$ (in cardinality terms).\nSuppose $\\Omega = \\mathbb R$. Let $\\mathcal{C} = \\lbrace (a, b],-\\infty \\leq a\u0026lt;b\u0026lt;\\infty \\rbrace$. Then the Borel $\\sigma$- algebra on $\\mathbb R$ is defined by $$ \\mathcal B (\\mathbb R) = \\sigma (\\mathcal C) $$\nProbability Measure A probability measure $P: \\mathcal A \\to [0,1]$ is a set function with domain $\\mathcal A$ and codomain $[0,1]$ such that\n$P(A) \\geq 0 \\ \\forall A \\in \\mathcal A$ $P$ is $\\sigma$-additive: is $A_n \\in \\mathcal A$ are pairwise disjoint events ($A_j \\cap A_k = \\emptyset$ for $j \\neq k$), then $$ P\\left(\\bigcup _ {n=1}^{\\infty} A_{n} \\right)=\\sum _ {n=1}^{\\infty} P\\left(A_{n}\\right) $$ $P(\\Omega) = 1$ Properties Some properties of probability measures\n$P\\left(A^{c}\\right)=1-P(A)$ $P(\\emptyset)=0$ For $A, B \\in \\mathcal{A}$, $P(A \\cup B)=P(A)+P(B)-P(A \\cap B)$ For $A, B \\in \\mathcal{A}$, if $A \\subset B$ then $P(A) \\leq P(B)$ For $A_n \\in \\mathcal{A}$, $P \\left(\\cup _ {n=1}^\\infty A_{n} \\right) \\leq \\sum _ {n=1}^\\infty P(A_n)$ For $A_n \\in \\mathcal{A}$, if $A_n \\uparrow A$ then $\\lim _ {n \\to \\infty} P(A_n) = P(A)$ Conditional Probability Let $A, B \\in \\mathcal A$ and $P(B) \u0026gt; 0$, the conditional probability of $A$ given $B$ is $$ P(A | B)=\\frac{P(A \\cap B)}{P(B)} $$\nTwo events $A$ and $B$ are independent if $P(A \\cap B)=P(A) P(B)$.\nLaw of Total Probability Theorem (Law of Total Probability)\nLet $(E_n) _ {n \\geq 1}$ be a finite or countable partition of $\\Omega$. Then, if $A \\in \\mathcal A$, $$ P(A) = \\sum_n P(A | E_n ) P(E_n) $$\nBayes Theorem Theorem (Bayes Theorem)\nLet $(E_n) _ {n \\geq 1}$ be a finite or countable partition of $\\Omega$, and suppose $P(A) \u0026gt; 0$. Then, $$ P(E_n | A) = \\frac{P(A | E_n) P(E_n)}{\\sum_m P(A | E_m) P(E_m)} $$\nFor a single event $E \\in \\Omega$, $$ P(E|A) = \\frac{P(A|E) P(E)}{P(A)} $$\nRandom Variables Definition A random variable $X$ on a probability space $(\\Omega,\\mathcal A, P)$ is a (measurable) mapping $X : \\Omega \\to \\mathbb{R}$ such that $$ \\forall B \\in \\mathcal{B}(\\mathbb{R}), \\quad X^{-1}(B) \\in \\mathcal{A} $$\nThe measurability condition states that the inverse image is a measurable set of $\\Omega$ i.e. $X^{-1}(B) \\in \\mathcal A$. This is essential since probabilities are defined only on $\\mathcal A$.\nIn words, a random variable it’s a mapping from events to real numbers such that each interval on the real line can be mapped back into an element of the sigma algebra (it can be the empty set).\nDistribution Function Let $X$ be a real valued random variable. The distribution function (also called cumulative distribution function) of $X$, commonly denoted $F_X(x)$ is defined by $$ F_X(x) = \\Pr(X \\leq x) $$\nProperties\n$F$ is monotone non-decreasing $F$ is right continuous $\\lim _ {x \\to - \\infty} F(x)=0$ and $\\lim _ {x \\to + \\infty} F(x)=1$ The random variables $(X_1, .. , X_n)$ are independent if and only if $$ F _ {(X_1, \u0026hellip; , X_n)} (x) = \\prod _ {i=1}^n F_{X_i} (x_i) \\quad \\forall x \\in \\mathbb R^n $$\nDensity Function Let $X$ be a real valued random variable. $X$ has a probability density function if there exists $f_X(x)$ such that for all measurable $A \\subset \\mathbb{R}$, $$ P(X \\in A) = \\int_A f_X(x) \\mathrm{d} x $$\nMoments Expected Value The expected value of a random variable, when it exists, is given by $$ \\mathbb{E}[ X ] = \\int_ \\Omega X(\\omega) \\mathrm{d} P $$ When $X$ has a density, then $$ \\mathbb{E} [ X ] = \\int_ \\mathbb{R} x f_X (x) \\mathrm{d} x = \\int _ \\mathbb{R} x \\mathrm{d} F_X (x) $$\nThe empirical expectation (or sample average) is given by $$ \\mathbb{E}_n [x_i] = \\frac{1}{n} \\sum _ {i=1}^N x_i $$\nVariance and Covariance The covariance of two random variables $X$, $Y$ defined on $\\Omega$ is $$ Cov(X, Y ) = \\mathbb{E}[ (X - \\mathbb{E}[ X ]) (Y - \\mathbb{E}[ Y ]) ] = \\mathbb{E}[XY ] - \\mathbb{E}[ X ]E[ Y ] $$ In vector notation, $Cov(X, Y) = \\mathbb{E}[XY\u0026rsquo;] - \\mathbb{E}[ X ]\\mathbb{E}[Y\u0026rsquo;]$.\nThe variance of a random variable $X$, when it exists, is given by $$ Var(X) = \\mathbb{E}[ (X - \\mathbb{E}[ X ])^2 ] = \\mathbb{E}[X^2] - \\mathbb{E}[ X ]^2 $$ In vector notation, $Var(X) = \\mathbb{E}[XX\u0026rsquo;] - \\mathbb{E}[ X ]\\mathbb{E}[X\u0026rsquo;]$.\nProperties Let $X, Y, Z, T \\in \\mathcal{L}^{2}$ and $a, b, c, d \\in \\mathbb{R}$\n$Cov(X, X) = Var(X)$ $Cov(X, Y) = Cov(Y, X)$ $Cov(aX + b, Y) = a \\ Cov(X,Y)$ $Cov(X+Z, Y) = Cov(X,Y) + Cov(Z,Y)$ $Cov(aX + bZ, cY + dT) = ac * Cov(X,Y) + ad * Cov(X,T) + bc * Cov(Z,Y) + bd * Cov(Z,T)$ Let $X, Y \\in \\mathcal L^1$ be independent. Then, $\\mathbb E[XY] = \\mathbb E[ X ] \\mathbb E[ Y ]$.\nIf $X$ and $Y$ are independent, then $Cov(X,Y) = 0$.\nNote that the converse does not hold: $Cov(X,Y) = 0 \\not \\to X \\perp Y$.\nSample Variance The sample variance is given by $$ Var_n (x_i) = \\frac{1}{n} \\sum _ {i=1}^N (x_i - \\bar{x})^2 $$ where $\\bar{x_i} = \\mathbb{E}_n [x_i] = \\frac{1}{n} \\sum _ {i=1}^N x_i$.\nFinite Sample Bias Theorem Theorem: The expected sample variance $\\mathbb{E} [\\sigma^2_n] = \\mathbb{E} \\left[ \\frac{1}{n} \\sum _ {i=1}^N \\left(y_i - \\mathbb{E}_n[ Y ] \\right)^2 \\right]$ gives an estimate of the population variance that is biased by a factor of $\\frac{1}{n}$ and is therefore referred to as biased sample variance.\nProof: $$ \\begin{aligned} \u0026amp;\\mathbb{E}[\\sigma^2_n] = \\mathbb{E} \\left[ \\frac{1}{n} \\sum _ {i=1}^n \\left( y_i - \\mathbb{E}_n [ Y ] \\right)^2 \\right] = \\newline \u0026amp;= \\mathbb{E} \\left[ \\frac{1}{n} \\sum _ {i=1}^n \\left( y_i - \\frac{1}{n} \\sum _ {i=1}^n y_i \\right )^2 \\right] = \\newline \u0026amp;= \\frac{1}{n} \\sum _ {i=1}^n \\mathbb{E} \\left[ y_i^2 - \\frac{2}{n} y_i \\sum _ {j=1}^n y_j + \\frac{1}{n^2} \\sum _ {j=1}^n y_j \\sum _ {k=1}^{n}y_k \\right] = \\newline \u0026amp;= \\frac{1}{n} \\sum _ {i=1}^n \\left[ \\frac{n-2}{n} \\mathbb{E}[y_i^2] - \\frac{2}{n} \\sum _ {j\\neq i} \\mathbb{E}[y_i y_j] + \\frac{1}{n^2} \\sum _ {j=1}^n \\sum _ {k\\neq j} \\mathbb{E}[y_j y_k] + \\frac{1}{n^2} \\sum _ {j=1}^n \\mathbb{E}[y_j^2] \\right] = \\newline \u0026amp;= \\frac{1}{n} \\sum _ {i=1}^n \\left[ \\frac{n-2}{n}(\\mu^2 + \\sigma^2) - \\frac{2}{n} (n-1) \\mu^2 + \\frac{1}{n^2} n(n-1)\\mu^2 + \\frac{1}{n^2} n (\\mu^2 + \\sigma^2)]\\right] = \\newline \u0026amp;= \\frac{n-1}{n} \\sigma^2 \\end{aligned} $$ $$\\tag*{$\\blacksquare$}$$\nInequalities Triangle Inequality: if $\\mathbb{E} [ X ] \u0026lt; \\infty$, then $$ |\\mathbb{E} [ X ] | \\leq \\mathbb{E} [|X|] $$\nMarkov’s Inequality: if $\\mathbb{E}[ X ] \u0026lt; \\infty$, then $$ \\Pr(|X| \u0026gt; t) \\leq \\frac{1}{t} \\mathbb{E}[|X|] $$\nChebyshev’s Inequality: if $\\mathbb{E}[X^2] \u0026lt; \\infty$, then $$ \\Pr(|X- \\mu|\u0026gt; t \\sigma) \\leq \\frac{1}{t^2}\\Leftrightarrow \\Pr(|X- \\mu|\u0026gt; t ) \\leq \\frac{\\sigma^2}{t^2} $$\nCauchy-Schwarz’s Inequality: $$ \\mathbb{E} [|XY|] \\leq \\sqrt{\\mathbb{E}[X^2] \\mathbb{E}[Y^2]} $$\nMinkowski Inequality: $$ \\left( \\sum _ {k=1}^n | x_k + y_k |^p \\right) ^ {\\frac{1}{p}} \\leq \\left( \\sum _ {k=1}^n | x_k |^p \\right) ^ {\\frac{1}{p}} + \\left( \\sum _ {k=1}^n | y_k | ^p \\right) ^ { \\frac{1}{p} } $$\nJensen’s Inequality: if $g( \\cdot)$ is concave (e.g. logarithmic function), then $$ \\mathbb{E}[g(x)] \\leq g(\\mathbb{E}[ X ]) $$ Similarly, if $g(\\cdot)$ is convex (e.g. exponential function), then $$ \\mathbb{E}[g(x)] \\geq g(\\mathbb{E}[ X ]) $$\nLaw of Iterated Expectations Theorem (Law of Iterated Expectations) $$ \\mathbb{E}(Y) = \\mathbb{E}_X [\\mathbb{E}(Y|X)] $$ \u0026gt; This states that the expectation of the conditional expectation is the unconditional expectation. \u0026gt; \u0026gt; In other words the average of the conditional averages is the unconditional average.\nLaw of Total Variance Theorem (Law of Total Variance) $$ Var(Y) = Var_X (\\mathbb{E}[Y |X]) + \\mathbb{E}_X [Var(Y|X)] $$\nSince variances are always non-negative, the law of total variance implies $$ Var(Y) \\geq Var_X (\\mathbb{E}[Y |X]) $$\nDistributions Normal Distribution We say that a random variable $Z$ has the standard normal distribution, or Gaussian, written $Z \\sim N(0,1)$, if it has the density $$ \\phi(x)=\\frac{1}{\\sqrt{2 \\pi}} \\exp \\left(-\\frac{x^{2}}{2}\\right), \\quad-\\infty\u0026lt;x\u0026lt;\\infty $$ If $Z \\sim N(0, 1)$ and $X = \\mu + \\sigma Z$ for $\\mu \\in \\mathbb R$ and $\\sigma \\geq 0$, then $X$ has a univariate normal distribution, written $X \\sim N(\\mu, \\sigma^2)$. By change-of-variables X has the density $$ f(x)=\\frac{1}{\\sqrt{2 \\pi \\sigma^{2}}} \\exp \\left(-\\frac{(x-\\mu)^{2}}{2 \\sigma^{2}}\\right), \\quad-\\infty\u0026lt;x\u0026lt;\\infty $$\nMultinomial Normal Distribution We say that the k -vector Z has a multivariate standard normal distribution, written $Z \\sim N(0, I_k)$ if it has the joint density $$ f(x)=\\frac{1}{(2 \\pi)^{k / 2}} \\exp \\left(-\\frac{x^{\\prime} x}{2}\\right), \\quad x \\in \\mathbb{R}^{k} $$ If $Z \\sim N(0, I_k)$ and $X = \\mu + B Z$, then the k-vector $X$ has a multivariate normal distribution, written $X \\sim N(\\mu, \\Sigma)$ where $\\Sigma = BB\u0026rsquo; \\geq 0$. If $\\sigma \u0026gt; 0$, then by change-of-variables $X$ has the joint density function $$ f(x)=\\frac{1}{(2 \\pi)^{k / 2} \\operatorname{det}(\\Sigma)^{1 / 2}} \\exp \\left(-\\frac{(x-\\mu)^{\\prime} \\Sigma^{-1}(x-\\mu)}{2}\\right), \\quad x \\in \\mathbb{R}^{k} $$\nProperties The expectation and covariance matrix of $X \\sim N(\\mu, \\Sigma)$ are $\\mathbb E = \\mu$ and $Var =\\Sigma$. If $(X,Y)$ are multivariate normal, $X$ and $Y$ are uncorrelated if and only if they are independent. If $X \\sim N(\\mu, \\Sigma)$ and $Y = a + bB$, then $X \\sim N(a + B\\mu, B \\Sigma B\u0026rsquo;)$. If $X \\sim N(0, I_k)$, then $X\u0026rsquo;X \\sim \\chi^2_k$, chi-square with $k$ degrees of freedom. If $X \\sim N(0, \\Sigma)$ with $\\Sigma\u0026gt;0$, then $X\u0026rsquo; \\Sigma X \\sim \\chi_k$ where $k = \\dim (X)$. If $Z \\sim N(0,1)$ and $Q \\sim \\chi^2_k$ are independent then $\\frac{Z}{\\sqrt{Q/k}} \\sim t_k$, student t with k degrees of freedom. Normal Distribution Relatives These distributions are relatives of the normal distribution\n$\\chi^2_q \\sim \\sum _ {i=1}^q Z_i^2$ where $Z_i \\sim N(0,1)$ $t_n \\sim \\frac{Z}{\\sqrt{\\chi^2 _ n}/n }$ $F(n_1 , n_2) \\sim \\frac{\\chi^2 _ {n_1} / n_1}{\\chi^2 _ {n_2}/n_2}$ The $t$ distribution is approximately standard normal but has heavier tails. The approximation is good for $n \\geq 30$: $t_{n\\geq 30} \\sim N(0,1)$\n","date":1635465600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1635465600,"objectID":"a5c296515288d5e000ab88ebc0e2f779","permalink":"https://matteocourthoud.github.io/course/metrics/02_probability/","publishdate":"2021-10-29T00:00:00Z","relpermalink":"/course/metrics/02_probability/","section":"course","summary":"Probability Probability Space A probability space is a triple $(\\Omega, \\mathcal A, P)$ where\n$\\Omega$ is the sample space. $\\mathcal A$ is the $\\sigma$-algebra on $\\Omega$. $P$ is a probability measure.","tags":null,"title":"Probability Theory","type":"book"},{"authors":null,"categories":null,"content":"import numpy as np import pandas as pd Setup For the scope of this tutorial we are going to use AirBnb Scraped data for the city of Bologna. The data is freely available at Inside AirBnb: http://insideairbnb.com/get-the-data.html.\nA description of all variables in all datasets is avaliable here.\nWe are going to use 2 datasets:\nlisting dataset: contains listing-level information pricing dataset: contains pricing data, over time # Import listings data url_listings = \u0026quot;http://data.insideairbnb.com/italy/emilia-romagna/bologna/2021-12-17/visualisations/listings.csv\u0026quot; df_listings = pd.read_csv(url_listings) # Import pricing data url_prices = \u0026quot;http://data.insideairbnb.com/italy/emilia-romagna/bologna/2021-12-17/data/calendar.csv.gz\u0026quot; df_prices = pd.read_csv(url_prices, compression=\u0026quot;gzip\u0026quot;) Numerical Data Methods\n+, -, *, / numpy functions pd.cut() Standard mathematical operations between columns are done row-wise.\ndf_prices['maximum_nights'] - df_prices['minimum_nights'] 0 148 1 357 2 357 3 357 4 357 ... 1260340 1124 1260341 1124 1260342 1124 1260343 1124 1260344 1124 Length: 1260345, dtype: int64 We can use most numpy operations element-wise on a single column.\nnp.log(df_listings['price']) 0 4.219508 1 3.367296 2 3.912023 3 4.836282 4 3.912023 ... 3448 3.465736 3449 3.806662 3450 3.912023 3451 4.897840 3452 4.744932 Name: price, Length: 3453, dtype: float64 We can create a categorical variables from a numerical one using the pd.cut() function.\npd.cut(df_listings['price'], bins = [0, 50, 100, np.inf], labels=['cheap', 'ok', 'expensive']) 0 ok 1 cheap 2 cheap 3 expensive 4 cheap ... 3448 cheap 3449 cheap 3450 cheap 3451 expensive 3452 expensive Name: price, Length: 3453, dtype: category Categories (3, object): ['cheap' \u0026lt; 'ok' \u0026lt; 'expensive'] String Data Methods\n+ .str.replace .str.contains .astype(str) -pd.get_dummies() We can use the + operator between columns, to do pairwise append.\nNote: we cannot do it with strings.\ndf_listings['host_name'] + df_listings['neighbourhood'] 0 CarloSanto Stefano 1 EleonoraPorto - Saragozza 2 PaoloSanto Stefano 3 Anna MariaSanto Stefano 4 ValerioPorto - Saragozza ... 3448 IleanaNavile 3449 FernandaPorto - Saragozza 3450 IleanaNavile 3451 Wonderful ItalySanto Stefano 3452 Wonderful ItalyPorto - Saragozza Length: 3453, dtype: object Pandas Series have a lot of vectorized string functions. You can find a list here.\nFor example, we want to remove the dollar symbol from the price variable in the df_prices dataset.\ndf_prices['price'].str.replace('$', '', regex=False) 0 70.00 1 68.00 2 68.00 3 68.00 4 68.00 ... 1260340 115.00 1260341 115.00 1260342 115.00 1260343 115.00 1260344 115.00 Name: price, Length: 1260345, dtype: object Some of these functions use regular expressions.\nmatch(): Call re.match() on each element, returning a boolean. extract(): Call re.match() on each element, returning matched groups as strings. findall(): Call re.findall() on each element replace(): Replace occurrences of pattern with some other string contains(): Call re.search() on each element, returning a boolean count(): Count occurrences of pattern split(): Equivalent to str.split(), but accepts regexps rsplit() For example, the next code checks whether in the word centre or center are contained in the text description.\ndf_listings['name'].str.contains('centre|center') 0 True 1 False 2 True 3 False 4 False ... 3448 False 3449 False 3450 False 3451 False 3452 False Name: name, Length: 3453, dtype: bool Lastly, we can (try to) convert string variables to numeric using astype(float).\ndf_prices['price'].str.replace('[$|,]', '', regex=True).astype(float) 0 70.0 1 68.0 2 68.0 3 68.0 4 68.0 ... 1260340 115.0 1260341 115.0 1260342 115.0 1260343 115.0 1260344 115.0 Name: price, Length: 1260345, dtype: float64 We can also use it to convert numerics to strings using astype(str).\ndf_listings['id'].astype(str) 0 42196 1 46352 2 59697 3 85368 4 145779 ... 3448 53810648 3449 53820830 3450 53837098 3451 53837654 3452 53854962 Name: id, Length: 3453, dtype: object We can generate dummies from a categorical variable using pd.get_dummies()\npd.get_dummies(df_listings['neighbourhood']).head() Borgo Panigale - Reno Navile Porto - Saragozza San Donato - San Vitale Santo Stefano Savena 0 0 0 0 0 1 0 1 0 0 1 0 0 0 2 0 0 0 0 1 0 3 0 0 0 0 1 0 4 0 0 1 0 0 0 Time Data Methods\npd.to_datetime() .dt.year .df.to_period() pd.to_timedelta() In the df_prices we have a date variable, date. Which format is it in? We can check it with the .dtypes attribute.\ndf_prices['date'].dtypes dtype('O') We can convert a variable into a date using the\ndf_prices['datetime'] = pd.to_datetime(df_prices['date']) Indeed, if we now check the format of the datetime variable, it\u0026rsquo;s datetime.\ndf_prices['datetime'].dtypes dtype('\u0026lt;M8[ns]') Once we have a variable in datetime format, we gain plenty of datetime operations through the dt accessor object for datetime-like properties.\nFor example, we can extract the year using .dt.year. We can do the same with month, week and day.\ndf_prices['datetime'].dt.year 0 2021 1 2021 2 2021 3 2021 4 2021 ... 1260340 2022 1260341 2022 1260342 2022 1260343 2022 1260344 2022 Name: datetime, Length: 1260345, dtype: int64 We can change the level of aggregation of a date using .dt.to_period(). The option M converts to year-month level.\ndf_prices['datetime'].dt.to_period('M') 0 2021-12 1 2021-12 2 2021-12 3 2021-12 4 2021-12 ... 1260340 2022-12 1260341 2022-12 1260342 2022-12 1260343 2022-12 1260344 2022-12 Name: datetime, Length: 1260345, dtype: period[M] We can add or subtract time periods from a date using the pd.to_timedelta() function. We need to specify the unit of measurement with the unit option.\ndf_prices['datetime'] - pd.to_timedelta(3, unit='d') 0 2021-12-14 1 2021-12-14 2 2021-12-15 3 2021-12-16 4 2021-12-17 ... 1260340 2022-12-09 1260341 2022-12-10 1260342 2022-12-11 1260343 2022-12-12 1260344 2022-12-13 Name: datetime, Length: 1260345, dtype: datetime64[ns] Missing Data Methods\n.isna() .dropna() .fillna() The function isna() reports missing values.\ndf_listings.isna().head() id name host_id host_name neighbourhood_group neighbourhood latitude longitude room_type price minimum_nights number_of_reviews last_review reviews_per_month calculated_host_listings_count availability_365 number_of_reviews_ltm license 0 False False False False True False False False False False False False False False False False False True 1 False False False False True False False False False False False False False False False False False True 2 False False False False True False False False False False False False False False False False False True 3 False False False False True False False False False False False False False False False False False True 4 False False False False True False False False False False False False False False False False False True To get a quick description of the amount of missing data in the dataset, we can use\ndf_listings.isna().sum() id 0 name 0 host_id 0 host_name 9 neighbourhood_group 3453 neighbourhood 0 latitude 0 longitude 0 room_type 0 price 0 minimum_nights 0 number_of_reviews 0 last_review 409 reviews_per_month 409 calculated_host_listings_count 0 availability_365 0 number_of_reviews_ltm 0 license 3318 dtype: int64 We can drop missing values using dropna(). It drops all rows with at least one missing value.\ndf_listings.dropna().shape (0, 18) In this case unfortunately, it drops all the rows. If we wa to drop only rows with all missing values, we can use the parameter how='all'.\ndf_listings.dropna(how='all').shape (3453, 18) If we want to drop only missing values for one particular value, we can use the subset option.\ndf_listings.dropna(subset=['reviews_per_month']).shape (3044, 18) We can also fill the missing values instead of dropping them, using fillna().\ndf_listings.fillna(' -- This was NA -- ').head() id name host_id host_name neighbourhood_group neighbourhood latitude longitude room_type price minimum_nights number_of_reviews last_review reviews_per_month calculated_host_listings_count availability_365 number_of_reviews_ltm license 0 42196 50 sm Studio in the historic centre 184487 Carlo -- This was NA -- Santo Stefano 44.48507 11.34786 Entire home/apt 68 3 180 2021-11-12 1.32 1 161 6 -- This was NA -- 1 46352 A room in Pasolini's house 467810 Eleonora -- This was NA -- Porto - Saragozza 44.49168 11.33514 Private room 29 1 300 2021-11-30 2.2 2 248 37 -- This was NA -- 2 59697 COZY LARGE BEDROOM in the city center 286688 Paolo -- This was NA -- Santo Stefano 44.48817 11.34124 Private room 50 1 240 2020-10-04 2.18 2 327 0 -- This was NA -- 3 85368 Garden House Bologna 467675 Anna Maria -- This was NA -- Santo Stefano 44.47834 11.35672 Entire home/apt 126 2 40 2019-11-03 0.34 1 332 0 -- This was NA -- 4 145779 SINGLE ROOM 705535 Valerio -- This was NA -- Porto - Saragozza 44.49306 11.33786 Private room 50 10 69 2021-12-05 0.55 9 365 5 -- This was NA -- We can also make missing values if we want.\ndf_listings.iloc[2, 2] = np.nan df_listings.iloc[:3, :3] id name host_id 0 42196 50 sm Studio in the historic centre 184487.0 1 46352 A room in Pasolini's house 467810.0 2 59697 COZY LARGE BEDROOM in the city center NaN ","date":1651363200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1651363200,"objectID":"ade16101536560376740ad034d1b702a","permalink":"https://matteocourthoud.github.io/course/data-science/03_data_types/","publishdate":"2022-05-01T00:00:00Z","relpermalink":"/course/data-science/03_data_types/","section":"course","summary":"import numpy as np import pandas as pd Setup For the scope of this tutorial we are going to use AirBnb Scraped data for the city of Bologna. The data is freely available at Inside AirBnb: http://insideairbnb.","tags":null,"title":"Data Types","type":"book"},{"authors":null,"categories":null,"content":"# Setup %matplotlib inline from utils.lecture03 import * Dataset For this session, we are mostly going to work with the wage dataset.\ndf = pd.read_csv('data/Wage.csv', index_col=0) df.head(3) year age maritl race education region jobclass health health_ins logwage wage 231655 2006 18 1. Never Married 1. White 1. \u0026lt; HS Grad 2. Middle Atlantic 1. Industrial 1. \u0026lt;=Good 2. No 4.318063 75.043154 86582 2004 24 1. Never Married 1. White 4. College Grad 2. Middle Atlantic 2. Information 2. \u0026gt;=Very Good 2. No 4.255273 70.476020 161300 2003 45 2. Married 1. White 3. Some College 2. Middle Atlantic 1. Industrial 1. \u0026lt;=Good 1. Yes 4.875061 130.982177 This dataset contains information on wages and individual characteristics.\nOur main objective is going to be to explain wages using the observables contained in the dataset.\nPolynomial Regression and Step Functions As we have seen in the first lecture, the most common way to introduce linearities is to replace the standard linear model\n$$ y_i = \\beta_0 + \\beta_1 x_i + \\varepsilon_i $$\nwith a polynomial function\n$$ y_i = \\beta_0 + \\beta_1 x_i + \\beta_2 x_i^2 + \\beta_2 x_i^3 + \u0026hellip; + \\varepsilon_i $$\nExplore the Data Suppose we want to investigate the relationship between wage and age. Let\u0026rsquo;s first plot the two variables.\n# Scatterplot of the data df.plot.scatter('age','wage',color='w', edgecolors='k', alpha=0.3); Polynomials of different degrees The relationship is highly complex and non-linear. Let\u0026rsquo;s expand our linear regression polynomials of different degrees: 1 to 5.\nX_poly1 = PolynomialFeatures(1).fit_transform(df.age.values.reshape(-1,1)) X_poly2 = PolynomialFeatures(2).fit_transform(df.age.values.reshape(-1,1)) X_poly3 = PolynomialFeatures(3).fit_transform(df.age.values.reshape(-1,1)) X_poly4 = PolynomialFeatures(4).fit_transform(df.age.values.reshape(-1,1)) X_poly5 = PolynomialFeatures(5).fit_transform(df.age.values.reshape(-1,1)) Variables Our dependent varaible is going to be a dummy for income above 250.000 USD.\n# Get X and y X = df.age y = df.wage y01 = (df.wage \u0026gt; 250).map({False:0, True:1}).values Polynomia Regression If we run a linear regression on a 4-degree polinomial expansion of age, this is what it looks like`:\n# Fit ols on 4th degree polynomial fit = sm.OLS(y, X_poly4).fit() fit.summary().tables[1] coef std err t P\u003e|t| [0.025 0.975] const -184.1542 60.040 -3.067 0.002 -301.879 -66.430 x1 21.2455 5.887 3.609 0.000 9.703 32.788 x2 -0.5639 0.206 -2.736 0.006 -0.968 -0.160 x3 0.0068 0.003 2.221 0.026 0.001 0.013 x4 -3.204e-05 1.64e-05 -1.952 0.051 -6.42e-05 1.45e-07 Measures of Fit In this case, the single coefficients are not of particular interest. We are mostly interested in the best capturing the relationship between age and wage. How can we pick among thedifferent polynomials?\nWe compare different polynomial degrees. For each regression, we are going to look at a series of metrics:\nabsolute residuals sum of squared residuals the difference in SSR w.r (SSR).t the 0-degree case F statistic # Run regressions fit_1 = sm.OLS(y, X_poly1).fit() fit_2 = sm.OLS(y, X_poly2).fit() fit_3 = sm.OLS(y, X_poly3).fit() fit_4 = sm.OLS(y, X_poly4).fit() fit_5 = sm.OLS(y, X_poly5).fit() # Compare fit sm.stats.anova_lm(fit_1, fit_2, fit_3, fit_4, fit_5, typ=1) df_resid ssr df_diff ss_diff F Pr(\u0026gt;F) 0 2998.0 5.022216e+06 0.0 NaN NaN NaN 1 2997.0 4.793430e+06 1.0 228786.010128 143.593107 2.363850e-32 2 2996.0 4.777674e+06 1.0 15755.693664 9.888756 1.679202e-03 3 2995.0 4.771604e+06 1.0 6070.152124 3.809813 5.104620e-02 4 2994.0 4.770322e+06 1.0 1282.563017 0.804976 3.696820e-01 The polynomial degree 4 seems best.\n# Set polynomial X to 4th degree X_poly = X_poly4 Binary Dependent Variable Since we have a binary dependent variable, it would be best to account for it in our regression framework. One way to do so, is to run a logistic regression.\nHow to interpret a Logistic Regression?\n$$ y = \\mathbb I \\ \\Big( \\beta_0 + \\beta_1 x_i + \\beta_2 x_i^2 + \\beta_2 x_i^3 + \u0026hellip; + \\varepsilon_i \\Big) $$\nwhere $\\mathbb I(\\cdot)$ is an indicator function and now $\\varepsilon_i$ is the error term.\nBinomial Link Functions Depending on the assumed distribution of the error term, we get different results. I list below the error types supported by the Binomial family.\n# List link functions for the Binomial family sm.families.family.Binomial.links [statsmodels.genmod.families.links.logit, statsmodels.genmod.families.links.probit, statsmodels.genmod.families.links.cauchy, statsmodels.genmod.families.links.log, statsmodels.genmod.families.links.cloglog, statsmodels.genmod.families.links.identity] Logit Link Function We are going to pick the logit link, i.e. we are going to assume that the error term is Type 1 Extreme Value (or Gumbel) distributed. It instead we take the usual standard normal distribution assumption for $\\varepsilon_i$, we get probit regression.\n# Pick the logit link for the Binomial family logit_link = sm.families.Binomial(sm.genmod.families.links.logit()) Given the error distribution, we can write the probability that $y=1$ as\n$$ \\Pr(y=1) = \\frac{e^{ \\beta_0 + \\beta_1 x_i + \\beta_2 x_i^2 + \\beta_2 x_i^3 + \u0026hellip; + \\varepsilon_i }}{1 + e^{ \\beta_0 + \\beta_1 x_i + \\beta_2 x_i^2 + \\beta_2 x_i^3 + \u0026hellip; + \\varepsilon_i } } $$\nLogistic Regression We now estimate the regression and plot the estimated relationship between age and wage.\n# Run logistic regression logit_poly = sm.GLM(y01, X_poly, family=logit_link).fit() logit_poly.summary().tables[1] coef std err z P\u003e|z| [0.025 0.975] const -109.5530 47.655 -2.299 0.022 -202.956 -16.150 x1 8.9950 4.187 2.148 0.032 0.789 17.201 x2 -0.2816 0.135 -2.081 0.037 -0.547 -0.016 x3 0.0039 0.002 2.022 0.043 0.000 0.008 x4 -1.949e-05 9.91e-06 -1.966 0.049 -3.89e-05 -6.41e-08 Linear Model Comparison What is the difference with the linear model?\n# Run OLS regression with binary outcome ols_poly = sm.OLS(y01, X_poly).fit() ols_poly.summary().tables[1] coef std err t P\u003e|t| [0.025 0.975] const -0.1126 0.240 -0.468 0.640 -0.584 0.359 x1 0.0086 0.024 0.363 0.717 -0.038 0.055 x2 -0.0002 0.001 -0.270 0.787 -0.002 0.001 x3 3.194e-06 1.23e-05 0.260 0.795 -2.09e-05 2.73e-05 x4 -1.939e-08 6.57e-08 -0.295 0.768 -1.48e-07 1.09e-07 The magnitude of the coefficients is different, but the signs are the same.\nPlot data and predictions Let\u0026rsquo;s plot the estimated curves against the data distribution.\n# Generate predictions x_grid = np.arange(df.age.min(), df.age.max()).reshape(-1,1) X_poly_test = PolynomialFeatures(4).fit_transform(x_grid) y_hat1 = sm.OLS(y, X_poly).fit().predict(X_poly_test) y01_hat1 = logit_poly.predict(X_poly_test) plot_predictions(X, y, x_grid, y01, y_hat1, y01_hat1, 'Figure 7.1: Degree-4 Polynomial') Which is remindful of Using polynomial functions of the features as predictors in a linear model imposes a global structure on the non-linear function of age. We can instead use step functions in order to avoid imposing such a global structure.\nFor example, we could break the range of age into bins, and fit a different constant in each bin.\nStep Functions Building a step function means first picking $K$ cutpoints $c_1 , c_2 , . . . , c_K$ in the range of age, and then construct $K + 1$ new variables\n$$ C_0(age) = \\mathbb I ( age \u0026lt; c_1) \\ C_1(age) = \\mathbb I ( c_1 \u0026lt; age \u0026lt; c_2) \\ C_2(age) = \\mathbb I ( c_2 \u0026lt; age \u0026lt; c_3) \\ \u0026hellip; \\ C_{K-1}(age) = \\mathbb I ( c_{K-1} \u0026lt; age \u0026lt; c_K) \\ C_K(age) = \\mathbb I ( c_K \u0026lt; age) \\ $$\nwhere $\\mathbb I(\\cdot)$ is the indicator function.\nBinning First, we generate the cuts.\n# Generate cuts for the variable age df_cut, bins = pd.cut(df.age, 4, retbins=True, right=True) df_cut.value_counts(sort=False) type(df_cut) pandas.core.series.Series Let\u0026rsquo;s generate a DataFrame out of this series.\n# Generate bins for \u0026quot;age\u0026quot; from the cuts df_steps = pd.concat([df.age, df_cut, df.wage], keys=['age','age_cuts','wage'], axis=1) df_steps.head(5) age age_cuts wage 231655 18 (17.938, 33.5] 75.043154 86582 24 (17.938, 33.5] 70.476020 161300 45 (33.5, 49.0] 130.982177 155159 43 (33.5, 49.0] 154.685293 11443 50 (49.0, 64.5] 75.043154 Dummy Variables Now we can generate different dummy variables out of each bin.\n# Create dummy variables for the age groups df_steps_dummies = pd.get_dummies(df_steps['age_cuts']) # Statsmodels requires explicit adding of a constant (intercept) df_steps_dummies = sm.add_constant(df_steps_dummies) df_steps_dummies.head(5) const (17.938, 33.5] (33.5, 49.0] (49.0, 64.5] (64.5, 80.0] 231655 1.0 1 0 0 0 86582 1.0 1 0 0 0 161300 1.0 0 1 0 0 155159 1.0 0 1 0 0 11443 1.0 0 0 1 0 Stepwise Regression We are now ready to run our regression\n# Generate our new X variable X_step = df_steps_dummies.drop(df_steps_dummies.columns[1], axis=1) # OLS Regression on step functions ols_step = sm.OLS(y, X_step).fit() ols_step.summary().tables[1] coef std err t P\u003e|t| [0.025 0.975] const 94.1584 1.476 63.790 0.000 91.264 97.053 (33.5, 49.0] 24.0535 1.829 13.148 0.000 20.466 27.641 (49.0, 64.5] 23.6646 2.068 11.443 0.000 19.610 27.719 (64.5, 80.0] 7.6406 4.987 1.532 0.126 -2.139 17.420 From the regression outcome we can see that most bin coefficients are significant, except for the last one.\n# Put the test data in the same bins as the training data. bin_mapping = np.digitize(x_grid.ravel(), bins) bin_mapping array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4]) # Get dummies, drop first dummy category, add constant X_step_test = sm.add_constant(pd.get_dummies(bin_mapping).drop(1, axis=1)) X_step_test.head() const 2 3 4 0 1.0 0 0 0 1 1.0 0 0 0 2 1.0 0 0 0 3 1.0 0 0 0 4 1.0 0 0 0 # Step prediction y_hat2 = ols_step.predict(X_step_test) Logistic Step Regression We are going again to run a logistic regression, given that our outcome is binary.\n# Logistic regression on step functions logit_step = sm.GLM(y01, X_step, family=logit_link).fit() y01_hat2 = logit_step.predict(X_step_test) logit_step.summary().tables[1] coef std err z P\u003e|z| [0.025 0.975] const -5.0039 0.449 -11.152 0.000 -5.883 -4.124 (33.5, 49.0] 1.5998 0.474 3.378 0.001 0.672 2.528 (49.0, 64.5] 1.7147 0.488 3.512 0.000 0.758 2.672 (64.5, 80.0] 0.7413 1.102 0.672 0.501 -1.420 2.902 Plotting How does the predicted function looks like?\nplot_predictions(X, y, x_grid, y01, y_hat2, y01_hat2, 'Figure 7.2: Piecewise Constant') Regression Splines Spline regression, or piece-wise polynomial regression, involves fitting separate low-degree polynomials over different regions of $X$. The idea is to have one regression specification but with different coefficients in different parts of the $X$ range. The points where the coefficients change are called knots.\nFor example, we could have a third degree polynomial and splitting the sample in two.\n$$ y_{i}=\\left{\\begin{array}{ll} \\beta_{01}+\\beta_{11} x_{i}+\\beta_{21} x_{i}^{2}+\\beta_{31} x_{i}^{3}+\\epsilon_{i} \u0026amp; \\text { if } x_{i}\u0026lt;c \\ \\beta_{02}+\\beta_{12} x_{i}+\\beta_{22} x_{i}^{2}+\\beta_{32} x_{i}^{3}+\\epsilon_{i} \u0026amp; \\text { if } x_{i} \\geq c \\end{array}\\right. $$\nWe have now two sets of coefficients, one for each subsample.\nGenerally, using more knots leads to a more flexible piecewise polynomial. Also increasing the degree of the polynomial increases the degree of flexibility.\nExample We are now going to plot 4 different examples for the age wage relationship:\nDiscontinuous piecewise cubic Continuous piecewise cubic Quadratic (continuous) Continuous piecewise linear # Cut dataset df_short = df.iloc[:80,:] X_short = df_short.age y_short = df_short.wage x_grid_short = np.arange(df_short.age.min(), df_short.age.max()+1).reshape(-1,1) # 1. Discontinuous piecewise cubic spline1 = \u0026quot;bs(x, knots=(50,50,50,50), degree=3, include_intercept=False)\u0026quot; # 2. Continuous piecewise cubic spline2 = \u0026quot;bs(x, knots=(50,50,50), degree=3, include_intercept=False)\u0026quot; # 3. Quadratic (continuous) spline3 = \u0026quot;bs(x, knots=(%s,%s), degree=2, include_intercept=False)\u0026quot; % (min(df.age), min(df.age)) # 4. Continuous piecewise linear spline4 = \u0026quot;bs(x, knots=(%s,50), degree=1, include_intercept=False)\u0026quot; % min(df.age) Generate Predictions # Generate spline predictions def fit_predict_spline(spline, X, y, x_grid): transformed_x = dmatrix(spline, {\u0026quot;x\u0026quot;: X}, return_type='dataframe') fit = sm.GLM(y, transformed_x).fit() y_hat = fit.predict(dmatrix(spline, {\u0026quot;x\u0026quot;: x_grid}, return_type='dataframe')) return y_hat y_hats = [fit_predict_spline(s, X_short, y_short, x_grid_short) for s in [spline1, spline2, spline3, spline4]] Plotting plot_splines(df_short, x_grid_short, y_hats) Comment The first example makes us think on why would we want out function to be discontinuous. Unless we expect a sudden wage jump at a certain age, we would like the function to be continuous. However, if for example we split age around the retirement age, we might expect a discontinuity.\nThe second example (top right) makes us think on why would we want out function not to be differentiable. Unless we have some specific mechanism in mind, ususally there is a trade-off between making the function non-differentiable or increasing the degree of the polynomial, as the last two examples show us. We get a similar fit with a quadratic fit or a discontinuous linear fit. The main difference is that in the second case we are picking the discontinuity point by hand instead of letting the data choose how to change the slope of the curve.\nThe Spline Basis Representation How can we fit a piecewise degree-d polynomial under the constraint that it (and possibly its first d − 1 derivatives) be continuous?\nThe most direct way to represent a cubic spline is to start off with a basis for a cubic polynomial—namely, x,x2,x3—and then add one truncated power basis function per knot. A truncated power basis function is defined as\n$$ h(x, c)=(x-c)_{+}^{3} = \\Bigg{\\begin{array}{cl} (x-c)^{3} \u0026amp; \\text { if } x\u0026gt;c \\ 0 \u0026amp; \\text { otherwise } \\end{array} $$\nOne can show that adding a term of the form $\\beta_4 h(x, c)$ to the model for a cubic polynomial will lead to a discontinuity in only the third derivative at $c$; the function will remain continuous, with continuous first and second derivatives, at each of the knots.\nCubic Splines One way to specify the spline is using nodes and degrees of freedom.\n# Specifying 3 knots and 3 degrees of freedom spline5 = \u0026quot;bs(x, knots=(25,40,60), degree=3, include_intercept=False)\u0026quot; pred5 = fit_predict_spline(spline5, X, y, x_grid) No Knots When we fit a spline, where should we place the knots?\nThe regression spline is most flexible in regions that contain a lot of knots, because in those regions the polynomial coefficients can change rapidly. Hence, one option is to place more knots in places where we feel the function might vary most rapidly, and to place fewer knots where it seems more stable. While this option can work well, in practice it is common to place knots in a uniform fashion. One way to do this is to specify the desired degrees of freedom, and then have the software automatically place the corresponding number of knots at uniform quantiles of the data.\n# Specifying degree 3 and 6 degrees of freedom spline6 = \u0026quot;bs(x, df=6, degree=3, include_intercept=False)\u0026quot; pred6 = fit_predict_spline(spline6, X, y, x_grid) Natural Splines A natural spline is a regression spline with additional boundary constraints: the function is required to be linear at the boundary (in the region where X is smaller than the smallest knot, or larger than the largest knot). This addi- tional constraint means that natural splines generally produce more stable estimates at the boundaries.\n# Natural spline with 4 degrees of freedom spline7 = \u0026quot;cr(x, df=4)\u0026quot; pred7 = fit_predict_spline(spline7, X, y, x_grid) Comparison # Compare predictons preds = [pred5, pred6, pred7] labels = ['degree 3, knots 3', 'degree 3, degrees of freedom 3', 'natural, degrees of freedom 4'] compare_predictions(X, y, x_grid, preds, labels) Comparison to Polynomial Regression Regression splines often give superior results to polynomial regression. This is because unlike polynomials, which must use a high degree to produce flexible fits, splines introduce flexibility by increasing the number of knots but keeping the degree fixed.\nWe are now fitting a polynomial of degree 15 and a spline with 15 degrees of freedom.\n# Polynomial of degree 15 X_poly15 = PolynomialFeatures(15).fit_transform(df.age.values.reshape(-1,1)) ols_poly_15 = sm.OLS(y, X_poly15).fit() pred8 = ols_poly_15.predict(PolynomialFeatures(15).fit_transform(x_grid)) # Spline with 15 degrees of freedon spline9 = \u0026quot;bs(x, df=15, degree=3, include_intercept=False)\u0026quot; pred9 = fit_predict_spline(spline9, X, y, x_grid) Plotting # Compare predictons preds = [pred8, pred9] labels = ['Polynomial', 'Spline'] compare_predictions(X, y, x_grid, preds, labels) As we can see, despite the two regressions having the same degrees of freedom, the polynomial fit is much more volatile. We can compare them along some dimensions.\nLocal Regression So far we have looked at so-called \u0026ldquo;global methods\u0026rdquo;: methods that try to fit a unique function specification over the whole data. The function specification can be complex, as in the case of splines, but can be expressed globally.\nLocal regression is a different approach for fitting flexible non-linear functions, which involves computing the fit at a target point $x_0$ using only the nearby training observations.\nDetails How does local regression work?\nIngredients: $X$, $y$.\nHow to you output a prediction $\\hat y_i$ at a new point $x_i$?\nTake a number of points in $X$ close to $x_i$: $X_{\\text{close-to-i}}$ Assign a weight to each of there points Fit a weigthed least squares regression of $X_{\\text{close-to-i}}$ on $y_{\\text{close-to-i}}$ Use the estimated coefficients $\\hat \\beta$ to predict $\\hat y_i = \\hat \\beta_0 + \\hat \\beta_1 x_i$ Generate Data # Set seed np.random.seed(1) # Generate data X_sim = np.sort(np.random.uniform(0,1,100)) e = np.random.uniform(-.5,.5,100) y_sim = -4*X_sim**2 + 3*X_sim + e # True Generating process without noise X_grid = np.linspace(0,1,100) y_grid = -4*X_grid**2 + 3*X_grid Plotting Let\u0026rsquo;s visualize the simulated data and the curve without noise.\nplot_simulated_data(X_sim, y_sim, X_grid, y_grid); Fit LL Regression Now we fit a local linear regression.\n# Settings spec = 'll' bandwidth = 0.1 kernel = 'gaussian' # Locally linear regression local_reg = KernelReg(y_sim, X_sim.reshape(-1,1), var_type='c', reg_type=spec, bw=[bandwidth]) y_hat = KernelReg.fit(local_reg) What do the parameters mean?\nvar_type: dependent variable type (c i.e. continuous) reg_type: local regression specification (ll i.e. locally linear) bw : bandwidth length (0.1) ckertype: kernel type (gaussian) Prediction What does the prediction looks like?\nfig, ax = plot_simulated_data(X_sim, y_sim, X_grid, y_grid); make_figure_7_9a(fig, ax, X_sim, y_hat); Details How exactly was the prediction generated? It was generated pointwise. We are now going to look at the prediction at one particular point: $x_i=0.5$.\nWe proceed as follows:\nWe select the focal point: $x_i=0.5$ We select observations close to $\\ x_i$, i.e. $x_{\\text{close to i}} = { x \\in X : |x_i - x| \u0026lt; 0.1 } \\ $ and $ \\ y_{\\text{close to i}} = { y \\in Y : |x_i - x| \u0026lt; 0.1 }$ We apply gaussian weights We run a weighted linear regression of $y_{\\text{close to i}}$ on $x_{\\text{close to i}}$ # Get local X and y x_i = 0.5 close_to_i = (x_i-bandwidth \u0026lt; X_sim) \u0026amp; (X_sim \u0026lt; x_i+bandwidth) X_tilde = X_sim[close_to_i] y_tilde = y_sim[close_to_i] # Get local estimates local_estimate = KernelReg.fit(local_reg, data_predict=[x_i]) y_i_hat = local_estimate[0] beta_i_hat = local_estimate[1] alpha_i_hat = y_i_hat - beta_i_hat*x_i print('Estimates: alpha=%1.4f, beta=%1.4f' % (alpha_i_hat, beta_i_hat)) Estimates: alpha=0.7006, beta=-0.6141 Visualization Now we can use the locally estimated coefficients to predict the value of $\\hat y_i(x_i)$ for $x_i = 0.5$.\n# Build local predictions close_to_i_grid = (x_i-bandwidth \u0026lt; X_grid) \u0026amp; (X_grid \u0026lt; x_i+bandwidth) X_grid_tilde = X_grid[close_to_i_grid].reshape(-1,1) y_grid_tilde = alpha_i_hat + X_grid_tilde*beta_i_hat fig, ax = plot_simulated_data(X_sim, y_sim, X_grid, y_grid); make_figure_7_9a(fig, ax, X_sim, y_hat); make_figure_7_9b(fig, ax, X_tilde, y_tilde, X_grid_tilde, y_grid_tilde, x_i, y_i_hat) Zooming in We can zoom in and look only at the \u0026ldquo;close to i\u0026rdquo; sample.\nsns.regplot(X_tilde, y_tilde); Why is the line upward sloped? We forgot the gaussian weights.\n# Weights w = norm.pdf((X_sim-x_i)/bandwidth) # Estimate LWS mod_wls = sm.WLS(y_sim, sm.add_constant(X_sim), weights=w) results = mod_wls.fit() print('Estimates: alpha=%1.4f, beta=%1.4f' % tuple(results.params)) Estimates: alpha=0.7006, beta=-0.6141 We indeed got the same estimates as before. Note two things:\nthe badwidth defines the scale parameter of the gaussian weights our locally linear regression is acqually global Plotting make_figure_7_9d(X_sim, y_sim, w, results, X_grid, x_i, y_i_hat) Now the slope is indeed negative, as in the locally linear regression.\nGeneralized Additive Models Generalized additive models (GAMs) provide a general framework for extending a standard linear model by allowing non-linear functions of each of the variables, while maintaining additivity.\nGAM for Regression Problems Imagine to extend the general regression framework to some separabily additive model of the form\n$$ y_i = \\beta_0 + \\sum_{k=1}^K \\beta_k f_k(x_{ik}) + \\varepsilon_i $$\nIt is called an additive model because we calculate a separate $f_k$ for each $X_k$, and then add together all of their contributions.\nConsider for example the following model\n$$ \\text{wage} = \\beta_0 + f_1(\\text{year}) + f_2(\\text{age}) + f_3(\\text{education}) + \\varepsilon $$\nExample We are going to use the following functions:\n$f_1$: natural spline with 8 degrees of freedom $f_2$: natural spline with 10 degrees of freedom $f_3$: step function # Set X and y df['education_'] = LabelEncoder().fit_transform(df[\u0026quot;education\u0026quot;]) X = df[['year','age','education_']].to_numpy() y = df[['wage']].to_numpy() ## model linear_gam = LinearGAM(s(0, n_splines=8) + s(1, n_splines=10) + f(2)) linear_gam.gridsearch(X, y); 100% (11 of 11) |########################| Elapsed Time: 0:00:00 Time: 0:00:00 Plotting plot_gam(linear_gam) Pros and Cons Before we move on, let us summarize the advantages of a GAM.\nGAMs allow us to fit a non-linear $f_k$ to each $X_k$, so that we can automatically model non-linear relationships that standard linear regression will miss The non-linear fits can potentially make more accurate predictions Because the model is additive, we can still examine the effect of each $X_k$ on $Y$ separately The smoothness of the function $f_k$ for the variable $X_k$ can be summarized via degrees of freedom. The main limitation of GAMs is that the model is restricted to be additive. With many variables, important interactions can be missed. However, as with linear regression, we can manually add interaction terms to the GAM model by including additional predictors of the form $X_j \\times X_k$.\nGAMs for Classification Problems We can use GAMs also with a binary dependent variable.\n# Binary dependent variable y_binary = (y\u0026gt;250) ## Logit link function logit_gam = LogisticGAM(s(0, n_splines=8) + s(1, n_splines=10) + f(2), fit_intercept=True) logit_gam.gridsearch(X, y_binary); 100% (11 of 11) |########################| Elapsed Time: 0:00:00 Time: 0:00:00 Plotting plot_gam(logit_gam) The results are qualitatively similar to the non-binary case.\n","date":1646784000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1646784000,"objectID":"0f8ac3b5f6c7bc841b2b14305295ef5f","permalink":"https://matteocourthoud.github.io/course/ml-econ/03_nonparametric/","publishdate":"2022-03-09T00:00:00Z","relpermalink":"/course/ml-econ/03_nonparametric/","section":"course","summary":"# Setup %matplotlib inline from utils.lecture03 import * Dataset For this session, we are mostly going to work with the wage dataset.\ndf = pd.read_csv('data/Wage.csv', index_col=0) df.head(3) year age maritl race education region jobclass health health_ins logwage wage 231655 2006 18 1.","tags":null,"title":"Non-Parametric Regression","type":"book"},{"authors":null,"categories":null,"content":"Convergence Sequences A sequence of nonrandom numbers $\\lbrace a_n \\rbrace$ converges to $a$ (has limit $a$) if for all $\\varepsilon\u0026gt;0$, there exists $n _ \\varepsilon$ such that if $n \u0026gt; n_ \\varepsilon$, then $|a_n - a| \u0026lt; \\varepsilon$. We write $a_n \\to a$ as $n \\to \\infty$.\nA sequence of nonrandom numbers $\\lbrace a_n \\rbrace$ is bounded if and only if there is some $B \u0026lt; \\infty$ such that $|a_n| \\leq B$ for all $n=1,2,\u0026hellip;$ Otherwise, we say that $\\lbrace a_n \\rbrace$ is unbounded.\nBig-O and Small-o Notation A sequence of nonrandom numbers $\\lbrace a_n \\rbrace$ is $O(N^\\delta)$ (at most of order $N^\\delta$) if $N^{-\\delta} a_n$ is bounded. When $\\delta=0$, $a_n$ is bounded, and we also write $a_n = O(1)$ (big oh one).\nA sequence of nonrandom numbers $\\lbrace a_n \\rbrace$ is $o(N^\\delta)$ if $N^{-\\delta} a_n \\to 0$. When $\\delta=0$, $a_n$ converges to zero, and we also write $a_n = o(1)$ (little oh one).\nProperties\nif $a_n = o(N^{\\delta})$, then $a_n = O(N^\\delta)$ if $a_n = o(1)$, then $a_n = O(1)$ if each element of a sequence of vectors or matrices is $O(N^\\delta)$, we say the sequence of vectors or matrices is $O(N^\\delta)$ similarly for $o(N^\\delta)$. Convergence in Probability A sequence of random variables $\\lbrace X_n \\rbrace$ converges in probability to a constant $c \\in \\mathbb R$ if for all $\\varepsilon\u0026gt;0$ $$ \\Pr \\big( |X_n - c| \u0026gt; \\varepsilon \\big) \\to 0 \\qquad \\text{ as } n \\to \\infty $$ We write $X_n \\overset{p}{\\to} c$ and say that $a$ is the probability limit (plim) of $X_n$: $\\mathrm{plim} X_n = c$. In the special case where $c=0$, we also say that $\\lbrace X_n \\rbrace$ is $o_p(1)$ (little oh p one). We also write $X_n = o_p(1)$ or $X_n \\overset{p}{\\to} 0$.\nA sequence of random variables $\\lbrace X_n \\rbrace$ is bounded in probability if for every $\\varepsilon\u0026gt;0$, there exists a $B _ \\varepsilon \u0026lt; \\infty$ and an integer $n_ \\varepsilon$ such that $$ \\Pr \\big( |x_ n| \u0026gt; B_ \\varepsilon \\big) \u0026lt; \\varepsilon \\qquad \\text{ for all } n \u0026gt; n_ \\varepsilon $$ We write $X_n = O_p(1)$ ($\\lbrace X_n \\rbrace$ is big oh p one).\nA sequence of random variables $\\lbrace X_n \\rbrace$ is $o_p(a_n)$ where $\\lbrace a_n \\rbrace$ is a nonrandom positive sequence, if $X_n/a_n = o_p(1)$. We write $X_n = o_p(a_n)$.\nA sequence of random variables $\\lbrace X_n \\rbrace$ is $O_p(a_n)$ where $\\lbrace a_n \\rbrace$ is a nonrandom positive sequence, if $X_n/a_n = O_p(1)$. We write $X_n = O_p(a_n)$.\nOther Convergences A sequence of random variables $\\lbrace X_n \\rbrace$ converges almost surely to a constant $c \\in \\mathbb R$ if $$ \\Pr \\big( X_n \\overset{p}{\\to} c \\big) = 1 $$ We write $X_n \\overset{as}{\\to} c$.\nA sequence of random variables $\\lbrace X_n \\rbrace$ converges in mean square to a constant $c \\in \\mathbb R$ if $$ \\mathbb E [(X_n - c)^2] \\to 0 \\qquad \\text{ as } n \\to \\infty $$ We write $X_n \\overset{ms}{\\to} c$.\nLet $\\lbrace X_n \\rbrace$ be a sequence of random variables and $F_n$ be the cumulative distribution function (cdf) of $X_n$. We say that $X_n$ converges in distribution to a random variable $x$ with cdf $F$ if the cdf $F_n$ of $X_n$ converges to the cdf $F$ of $x$ at every continuity point of $F$. We write $X_n \\overset{d}{\\to} x$ and we call $F$ the asymptotic distribution of $X_n$.\nCompare Convergences Lemma: Let $\\lbrace X_n \\rbrace$ be a sequence of random variables and $c \\in \\mathbb R$\n$X_n \\overset{ms}{\\to} c \\ \\Rightarrow \\ X_n \\overset{p}{\\to} c$ $X_n \\overset{as}{\\to} c \\ \\Rightarrow \\ X_n \\overset{p}{\\to} c$ $X_n \\overset{p}{\\to} c \\ \\Rightarrow \\ X_n \\overset{d}{\\to} c$ Note that all the above definitions naturally extend to a sequence of random vectors by requiring element-by-element convergence. For example, a sequence of $K \\times 1$ random vectors $\\lbrace X_n \\rbrace$ converges in probability to a constant $c \\in \\mathbb R^K$ if for all $\\varepsilon\u0026gt;0$ $$ \\Pr \\big( |X _ {nk} - c_k| \u0026gt; \\varepsilon \\big) \\to 0 \\qquad \\text{ as } n \\to \\infty \\quad \\forall k = 1\u0026hellip;K $$\nTheorems Slutsky Theorem Theorem\nLet $\\lbrace X_n \\rbrace$ and $\\lbrace Y_n \\rbrace$ be two sequences of random variables, $x$ a random variable and $c \\in \\mathbb R$ a constant such that $\\lbrace X_n \\rbrace \\overset{d}{\\to} X$ and $\\lbrace Y_n \\rbrace \\overset{p}{\\to} c$. Then\n$X_n + Y_n \\overset{d}{\\to} X + c$ $X_n \\cdot Y_n \\overset{d}{\\to} X \\cdot c$ Continuous Mapping Theorem Theorem\nLet $\\lbrace X_n \\rbrace$ be sequence of $K \\times 1$ random vectors and $g: \\mathbb{R}^K \\to \\mathbb{R}^J$ a continuous function that does not depend on $n$.Then\n$x _n \\overset{as}{\\to} x \\ \\Rightarrow \\ g(X_n) \\overset{as}{\\to} g(x)$ $x _n \\overset{p}{\\to} x \\ \\Rightarrow \\ g(X_n) \\overset{p}{\\to} g(x)$ $x _n \\overset{d}{\\to} x \\ \\Rightarrow \\ g(X_n) \\overset{d}{\\to} g(x)$ Weak Law of Large Numbers Theorem\nLet $\\lbrace x_i \\rbrace _ {i=1}^n$ be a sequence of independent, identically distributed random variables such that $\\mathbb{E}[|x_i|] \u0026lt; \\infty$. Then the sequence satisfies the weak law of large numbers (WLLN): $$ \\mathbb{E}_n[x_i] = \\frac{1}{n} \\sum _ {i=1}^n x_i \\overset{p}{\\to} \\mu \\qquad \\text{ where } \\mu \\equiv \\mathbb{E}[x_i] $$\nIntuitions for the law of large numbers:\nCancellation with high probability. Re-visiting regions of the sample space over and over again. WLLN Proof The independence of the random variables implies no correlation between them, and we have that $$ Var \\left( \\mathbb{E}_n[x_i] \\right) = Var \\left( \\frac{1}{n} \\sum _ {i=1}^n x_i \\right) = \\frac{1}{n^2} Var\\left( \\sum _ {i=1}^n x_i \\right) = \\frac{n \\sigma^2}{n^2} = \\frac{\\sigma^2}{n} $$ Using Chebyshev’s inequality on $\\mathbb{E}_n[x_i]$ results in $$ \\Pr \\big( \\left|\\mathbb{E}_n[x_i]-\\mu \\right| \u0026gt; \\varepsilon \\big) \\leq {\\frac {\\sigma ^{2}}{n\\varepsilon ^{2}}} $$ As $n$ approaches infinity, the right hand side approaches $0$. And by definition of convergence in probability, we have obtained $\\mathbb{E}_n[x_i] \\overset{p}{\\to} \\mu$ as $n \\to \\infty$. $$\\tag*{$\\blacksquare$}$$\nCentral Limit Theorem Lindberg-Levy Central Limit Theorem\nLet $\\lbrace x_i \\rbrace _ {i=1}^n$ be a sequence of independent, identically distributed random variables such that $\\mathbb{E}[x_i^2] \u0026lt; \\infty$, and $\\mathbb{E}[x_i] = \\mu$. Then $\\lbrace x_i \\rbrace$ satisfies the central limit theorem (CLT); that is, $$ \\frac{1}{\\sqrt{n}} \\sum _ {i=1}^{n} (x_i - \\mu) \\overset{d}{\\to} N(0,\\sigma^2) $$ where $\\sigma^2 = Var(x_i) = \\mathbb{E}[x_i x_i\u0026rsquo;]$ is necessarily positive semidefinite.\nCLT Proof (1) Suppose $\\lbrace x_i \\rbrace$ are independent and identically distributed random variables, each with mean $\\mu$ and finite variance $\\sigma^2$. The sum $x_1 + \u0026hellip; + X_n$ has mean $n \\mu$ and variance $n \\sigma^2$.\nConsider the random variable $$ Z_n = \\frac{x_1 + \u0026hellip; + X_n - n\\mu}{\\sqrt{n \\sigma^2}} = \\sum _ {i=1}^n \\frac{x_i - \\mu}{\\sqrt{n \\sigma^2}} = \\sum _ {i=1}^n \\frac{1}{\\sqrt{n}} \\tilde x_i $$\nwhere in the last step we defined the new random variables $\\tilde x_i = \\frac{x_i - \\mu}{\\sigma}$ each with zero mean and unit variance. The characteristic function of $Z_n$ is given by $$ \\varphi _ {Z_n} (t) = \\varphi _ { \\sum _ {i=1}^n \\frac{1}{\\sqrt{n} } \\tilde{x}_i}(t) = \\varphi _ {\\tilde x_1} \\left( \\frac{t}{\\sqrt{n}} \\right) \\times \u0026hellip; \\times \\varphi _ {Y_n} \\left( \\frac{t}{\\sqrt{n}} \\right) = \\left[ \\varphi _ {\\tilde x_1} \\left( \\frac{t}{\\sqrt{n}} \\right) \\right]^n $$\nwhere in the last step we used the fact that all of the $\\tilde{x}_i$ are identically distributed.\nCLT Proof (2) The characteristic function of $\\tilde{x}_1$ is, by Taylor’s theorem, $$ \\varphi _ {\\tilde{x}_1} \\left( \\frac{t}{\\sqrt{n}} \\right) = 1 - \\frac{t^2}{2n} + o \\left( \\frac{t^2}{n} \\right) \\qquad \\text{ for } n \\to \\infty $$\nwhere $o(t^2)$ is “little o notation” for some function of $t$ that goes to zero more rapidly than $t^2$. By the limit of the exponential function, the characteristic function of $Z_n$ equals $$ \\varphi _ {Z_ n}(t) = \\left[ 1 - \\frac{t^2}{2n} + o \\left( \\frac{t^2}{n} \\right) \\right]^n \\to e^{ -\\frac{1}{2}t^2 } \\qquad \\text{ for } n \\to \\infty $$\nNote that all of the higher order terms vanish in the limit $n \\to \\infty$. The right hand side equals the characteristic function of a standard normal distribution $N(0,1)$, which implies through Lévy’s continuity theorem that the distribution of $Z_ n$ will approach $N(0,1)$ as $n \\to \\infty$. Therefore, the sum $x_1 + \u0026hellip; + x_n$ will approach that of the normal distribution $N(n_{\\mu}, n\\sigma^2)$, and the sample average $$ \\mathbb{E}_n [x_i] = \\frac{1}{n} \\sum _ {i=1}^n x_i $$\nconverges to the normal distribution $N(\\mu, \\sigma^2)$, from which the central limit theorem follows. $$\\tag*{$\\blacksquare$}$$\nDelta Method Let $\\lbrace X_n \\rbrace$ be a sequence of independent, identically distributed $K \\times 1$ random vectors such that\n$\\sqrt{n} (X_n - c) \\overset{d}{\\to} Z$ for some fixed $c \\in \\mathbb{R}^K$ and $\\Sigma$ a $K \\times K$ positive definite matrix. Suppose $g : \\mathbb{R}^K \\to \\mathbb{R}^J$ with $J \\leq K$ is continuously differentiable and full rank at $c$, then $$ \\sqrt{n} \\Big[ g(X_n) - g( c ) \\Big] \\overset{d}{\\to} G Z $$\nwhere $G = \\frac{\\partial g( c )}{\\partial x}$ is the $J \\times K$ matrix of partial derivatives evaluated at $c$.\nNote that the most common utilization is with the random variable $\\mathbb E_n [x_i]$. In fact, under the assumptions of the CLT, we have that $$ \\sqrt{n} \\Big[ g \\big( \\mathbb E_n [x_i] \\big) - g(\\mu) \\Big] \\overset{d}{\\to} N(0, G \\Sigma G\u0026rsquo;) $$\nErgodic Theory PPT Let $(\\Omega, \\mathcal{B}, P)$ be a probability space and $T: \\Omega \\rightarrow \\Omega$ a measurable map. $T$ is a probability preserving transformation if the probability of the pre-image of every set is the same as the probability of the set itself, i.e. $\\forall G, \\Pr(T^{-1}(G)) = \\Pr(G)$.\nLet $(\\Omega, \\mathcal{B}, P)$ be a probability space and $T: \\Omega \\rightarrow \\Omega$ a PPT. A set $G \\in \\mathcal{B}$ is invariant if $T^{-1}(G)=G$.\nNote that it does not have to work the other way around: $G \\neq T(G)$.\nLet $(\\Omega, \\mathcal{B}, P)$ be a probability space and $T: \\Omega \\rightarrow \\Omega$ a PPT. $T$ is ergodic if every invariant set $G \\in \\mathcal{B}$ has probability zero or one, i.e. $\\Pr(G) = 0 \\lor \\Pr(G) = 1$.\nPoincarè Recurrence Theorem\nLet $(\\Omega, \\mathcal{B}, P)$ be a probability space and $T: \\Omega \\rightarrow \\Omega$ a PPT. Suppose $A \\in \\mathcal{B}$ is measurable. Then, for almost every $\\omega \\in A$, $T^n(\\omega)\\in A$ for infinitely many $n$.\nProof\nWe follow 5 steps:\nLet $G = \\lbrace \\omega \\in A : T^K(\\omega) \\notin A \\quad \\forall k \u0026gt;0 \\rbrace$: the set of all points of A that never ``return” in A. Note that $\\forall j \\geq 1$, $T^{-j}(G) \\cap G = \\emptyset$. In fact, suppose $\\omega \\in T^{-j}(G)$. Then $\\omega \\notin G$ since otherwise we would have $\\omega \\in G \\subseteq A$ and $\\omega \\in T^J(G) \\subseteq A$ which contradicts the definition of $G$. It follows that $\\forall l,n \\geq 1$, $T^{-l}(G) \\cap T^{-n}(G) = \\emptyset$ Since $T$ is a PPT, $\\Pr(T^{-j}(G)) = \\Pr(G)$ $\\forall j$ Then $$ \\Pr (T^{-1}(G) \\cup T^{-2}(G) \\cup \u0026hellip; \\cup T^{-l}(G)) = l \\cdot \\Pr(G) \\leq 1 \\Rightarrow \\Pr(G) \\leq \\frac{1}{l} \\quad \\Rightarrow \\quad \\lim_ {l \\to \\infty} \\Pr(G) = 0 $$ $$\\tag*{$\\blacksquare$}$$ Comment Halmos: “The recurrence theorem says that under the appropriate conditions on a transformation T almost every point of each measurable set $A$ returns to $A$ infinitely often. It is natural to ask: exactly how long a time do the images of such recurrent points spend in $A$? The precise formulation of the problem runs as follows: given a point $x$ (for present purposes it does not matter whether $x$ is in $A$ or not), and given a positive integer $n$, form the ratio of the number of these points that belong to $A$ to the total number (i.e., to $n$), and evaluate the limit of these ratios as $n$ tends to infinity. It is, of course, not at all obvious in what sense, if any, that limit exists. If $f$ is the characteristic function of $A$ then the ratio just discussed is” $$ \\frac{1}{n} \\sum _ {i=1}^n f(T^{i}x) = \\frac{1}{n} \\sum _ {i=1}^n x_i $$\nErgodic Theorem Theorem\nLet $T$ be an ergodic PPT on $\\Omega$. Let $x$ be a random variable on $\\Omega$ with $\\mathbb{E}[x] \u0026lt; \\infty$. Let $x_i = x \\circ T^i$. Then, $$ \\frac{1}{n} \\sum _ {i=1}^n x_i \\overset{as}{\\to} \\mathbb{E}[x] $$\nTo figure out whether a PPT is ergodic, it’s useful to draw a graph with $T^{-1}(G)$ on the y-axis and $G$ on the x-axis.\nComment From the ergodic theorem, we have that $$ \\lim _ {n \\to \\infty} \\frac{1}{n} \\sum _ {i=1}^n f(T^{i}x) g(x) = f^* (x)g(x) \\quad \\Rightarrow \\quad \\lim _ {n \\to \\infty} \\Pr(T^{-n}G \\cap H) = \\Pr(G)\\Pr(H) $$ where $f^* (x) = \\int f(x) dx = \\mathbb{E}[f]$.\n[Halmos]: We have seen that if a transformation $T$ is ergodic, then $\\Pr(T^{-n}G \\cap H)$ converges in the sense of Cesaro to $\\Pr(G)\\Pr(H)$. The validity of this condition for all $G$ and $H$ is, in fact, equivalent to ergodicity. To prove this, suppose that $A$ is a measurable invariant set, and take both $G$ and $H$ equal to $A$. It follows that $\\Pr(A) = (\\Pr(A))^2$, and hence that $\\Pr(A)$ is either 0 or 1.\nComment 2 The Cesaro convergence condition has a natural intuitive interpretation. We may visualize the transformation $T$ as a particular way of stirring the contents of a vessel (of total volume 1) full of an incompressible fluid, which may be thought of as 90 per cent gin ($G$) and 10 per cent vermouth ($H$). If $H$ is the region originally occupied by the vermouth, then, for any part $G$ of the vessel, the relative amount of vermouth in $G$, after $n$ repetitions of the act of stirring, is given by $\\Pr(T^{-n}G \\cap H)/\\Pr(H)$. The ergodicity of $T$ implies therefore that on the average this relative amount is exactly equal to 10 per cent. In general, in physical situations like this one, one expects to be justified in making a much stronger statement, namely that, after the liquid has been stirred sufficiently often ($n \\to \\infty$), every part $G$ of the container will contain approximately 10 per cent vermouth. In mathematical language this expectation amounts to replacing Cesaro convergence by ordinary convergence, i.e., to the condition $\\lim_ {n\\to \\infty} \\Pr(T^{-n}G \\cap H) = \\Pr(G)\\Pr(H)$. If a transformation $T$ satisfies this condition for every pair $G$ and $H$ of measurable sets, it is called mixing, or, in distinction from a related but slightly weaker concept, strongly mixing.”\nMixing Let $\\lbrace\\Omega, \\mathcal{B}, P \\rbrace$ be a probability space. Let $T$ be a probability preserving transform. Then $T$ is strongly mixing if for every invariant sets $G$,$H \\in \\mathcal{B}$ $$ P(G \\cap T^{-k}H) \\to P(G)P(H) \\quad \\text{ as } k \\to \\infty $$ where $T^{-k}H$ is defined as $T^{-k}H = T^{-1}(\u0026hellip;T^{-1}(T^{-1} H)\u0026hellip;)$ repeated $k$ times.\nLet $\\lbrace X_i\\rbrace _ {i=-\\infty}^{\\infty}$ be a two sided sequence of random variables. Let $\\mathcal{B}_ {-\\infty}^n$ be the sigma algebra generated by $\\lbrace X_i\\rbrace _ {i=-\\infty}^{n}$ and $\\mathcal{B}_ {n+k}^\\infty$ the sigma algebra generated by $\\lbrace X_i \\rbrace _ {i=n+k}^{\\infty}$. Define the mixing coefficient $$ \\alpha(k) = \\sup_ {n \\in \\mathbb{Z}} \\sup_ {G \\in \\mathcal{B}_ {-\\infty}^n} \\sup_ {H \\in \\mathcal{B}_ {n+k}^\\infty} | \\Pr(G \\cap H) - \\Pr(G) \\Pr(H)| $$ $\\lbrace X_i \\rbrace$ is $\\mathbb{\\alpha}$-mixing if $\\alpha(k) \\to 0$ if $k \\to \\infty$.\nNote that mixing implies ergodicity.\nStationarity Let $X_i : \\Omega \\to \\mathbb{R}$ be a (two sided) sequence of random variables with $i \\in \\mathbb{Z}$. $X_i$ is strongly stationary or simply stationary if $$ \\Pr (X _ {i_ 1} \\leq a_ 1 , \u0026hellip; , X _ {i_ k} \\leq a_ k ) = \\Pr (X _ { i _ {1-s}} \\leq a_ 1 , \u0026hellip; , X _ {i _ {k-s}} \\leq a_ k) \\quad \\text{ for every } i_ 1, \u0026hellip;, i_ k, a_ 1, \u0026hellip;, a_ k, s \\in \\mathbb{R}. $$\nLet $X_i : \\Omega \\to \\mathbb{R}$ be a (two sided) sequence of random variables with $i \\in \\mathbb{Z}$. $X_i$ is covariance stationary if $\\mathbb{E}[X_i] = \\mathbb{E}[X_j]$ for every $i,j$ and $\\mathbb{E}[X_i X_j] = \\mathbb{E}[X _ {i+k} X _ {j+k}]$ for all $i,j,k$. All of the second moments above are assumed to exist.\nLet $X_t : \\Omega \\to \\mathbb{R}$ be a sequence of random variables indexed by $t \\in \\mathbb{Z}$ such that $\\mathbb{E}[|X_t|] \u0026lt; 1$ for each $t$. $X_t$ is a martingale if $\\mathbb{E} [X _ t |X _ {t-1} , X _ {t-2} , \u0026hellip;] = X _ t$. $X_t$ is a martingale difference if $\\mathbb{E} [X _ t | X _ {t-1} , X _ {t-2} ,\u0026hellip;] = 0$.\nGordin’s Central Limit Theorem Theorem\nLet $\\lbrace z_i \\rbrace$ be a stationary, $\\alpha$-mixing sequence of random variables. If moreover\n$\\sum_ {m=1}^\\infty \\alpha(m)^{\\frac{\\delta}{2 + \\delta}} \u0026lt; \\infty$ $\\mathbb{E}[z_i] = 0$ $\\mathbb{E}\\Big[ ||z_i || ^ {2+\\delta} \\Big] \u0026lt; \\infty$ Then $$ \\sqrt{n} \\mathbb{E}_n [z_i] \\overset{d}{\\to} N(0,\\Omega) \\quad \\text{ where } \\quad \\Omega = \\lim _ {n \\to \\infty} Var(\\sqrt{n} \\mathbb{E}_n [z_i]) $$\nLet $\\Omega_k = \\mathbb{E}[ z_i z _ {i+k}\u0026rsquo;]$. Then a necessary condition for Gordin’s CLT is covariance summability: $\\sum _ {k=1}^\\infty \\Omega_k \u0026lt; \\infty$.\nErgodic Central Limit Theorem Theorem\nLet $\\lbrace z_i \\rbrace$ be a stationary, ergodic, martingale difference sequence. Then $$ \\sqrt{n} \\mathbb{E}_n [z_i] \\overset{d}{\\to} N(0,\\Omega) \\quad \\text{ where } \\quad \\Omega = \\lim _ {n \\to \\infty} Var(\\sqrt{n}\\mathbb{E}_n[z_i]) $$\n","date":1635465600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1635465600,"objectID":"70668fa894234c30126070b28f67c601","permalink":"https://matteocourthoud.github.io/course/metrics/03_asymptotics/","publishdate":"2021-10-29T00:00:00Z","relpermalink":"/course/metrics/03_asymptotics/","section":"course","summary":"Convergence Sequences A sequence of nonrandom numbers $\\lbrace a_n \\rbrace$ converges to $a$ (has limit $a$) if for all $\\varepsilon\u0026gt;0$, there exists $n _ \\varepsilon$ such that if $n \u0026gt; n_ \\varepsilon$, then $|a_n - a| \u0026lt; \\varepsilon$.","tags":null,"title":"Asymptotic Theory","type":"book"},{"authors":null,"categories":null,"content":"Setup For the scope of this tutorial we are going to use AirBnb Scraped data for the city of Bologna. The data is freely available at Inside AirBnb: http://insideairbnb.com/get-the-data.html.\nA description of all variables in all datasets is avaliable here.\nWe are going to use 2 datasets:\nlisting dataset: contains listing-level information pricing dataset: contains pricing data, over time import numpy as np import pandas as pd # Import listings data url_listings = \u0026quot;http://data.insideairbnb.com/italy/emilia-romagna/bologna/2021-12-17/visualisations/listings.csv\u0026quot; df_listings = pd.read_csv(url_listings) # Import pricing data url_prices = \u0026quot;http://data.insideairbnb.com/italy/emilia-romagna/bologna/2021-12-17/data/calendar.csv.gz\u0026quot; df_prices = pd.read_csv(url_prices, compression=\u0026quot;gzip\u0026quot;) Sorting and Renaming You can sort the data using the sort_values function.\nOptions\nascending: bool or list of bool, default True na_position: {‘first’, ‘last’}, default ‘last’ df_listings.sort_values(by=['name', 'price'], ascending=[False, True], na_position='last').head() id name host_id host_name neighbourhood_group neighbourhood latitude longitude room_type price minimum_nights number_of_reviews last_review reviews_per_month calculated_host_listings_count availability_365 number_of_reviews_ltm license 2280 38601411 🏡Giardino di Annabella-relax in città-casa intera 240803020 Annabella NaN Porto - Saragozza 44.49303 11.31986 Entire home/apt 90 2 53 2021-12-13 1.96 1 76 27 392901 2988 48177313 ❤ Romantic Suite with SPA Bath ❤ 4starbologna.com 239491712 4 Star Bologna NaN Santo Stefano 44.50271 11.34998 Entire home/apt 309 1 1 2021-03-14 0.11 14 344 1 NaN 3302 52367336 ✨House of Alchemy✨ 140013413 Greta NaN Porto - Saragozza 44.49072 11.30890 Entire home/apt 96 2 7 2021-11-28 3.18 1 88 7 NaN 2039 34495335 ♥ Romantic for Couple in Love ♥ | 4 Star Boutique 239491712 4 Star Bologna NaN Santo Stefano 44.50368 11.34972 Entire home/apt 143 1 25 2021-08-20 0.79 14 262 6 NaN 2964 47866124 ♡Amazing Suite with Private SPA ♡ 4starbologna... 239491712 4 Star Bologna NaN Santo Stefano 44.50381 11.34951 Entire home/apt 347 1 2 2021-10-17 0.72 14 337 2 NaN You can remane columns using the rename() function. It takes a dictionary as column argument in the form {\u0026quot;old_name\u0026quot;: \u0026quot;new_name\u0026quot;}.\ndf_listings.rename(columns={'name': 'listing_name', 'id': 'listing_id'}).head() listing_id listing_name host_id host_name neighbourhood_group neighbourhood latitude longitude room_type price minimum_nights number_of_reviews last_review reviews_per_month calculated_host_listings_count availability_365 number_of_reviews_ltm license 0 42196 50 sm Studio in the historic centre 184487 Carlo NaN Santo Stefano 44.48507 11.34786 Entire home/apt 68 3 180 2021-11-12 1.32 1 161 6 NaN 1 46352 A room in Pasolini's house 467810 Eleonora NaN Porto - Saragozza 44.49168 11.33514 Private room 29 1 300 2021-11-30 2.20 2 248 37 NaN 2 59697 COZY LARGE BEDROOM in the city center 286688 Paolo NaN Santo Stefano 44.48817 11.34124 Private room 50 1 240 2020-10-04 2.18 2 327 0 NaN 3 85368 Garden House Bologna 467675 Anna Maria NaN Santo Stefano 44.47834 11.35672 Entire home/apt 126 2 40 2019-11-03 0.34 1 332 0 NaN 4 145779 SINGLE ROOM 705535 Valerio NaN Porto - Saragozza 44.49306 11.33786 Private room 50 10 69 2021-12-05 0.55 9 365 5 NaN Aggregating If we want to count observations across 2 categorical variables, we can use pd.crosstab().\npd.crosstab(df_listings['neighbourhood'], df_listings['room_type']) room_type Entire home/apt Hotel room Private room Shared room neighbourhood Borgo Panigale - Reno 107 0 39 0 Navile 250 3 149 1 Porto - Saragozza 842 16 299 10 San Donato - San Vitale 280 1 134 4 Santo Stefano 924 29 237 5 Savena 73 0 48 2 We can compute statistics by group using groupby().\ndf_listings.groupby('neighbourhood')[['price', 'reviews_per_month']].mean() price reviews_per_month neighbourhood Borgo Panigale - Reno 83.020548 0.983488 Navile 142.200993 1.156745 Porto - Saragozza 129.908312 1.340325 San Donato - San Vitale 91.618138 0.933011 Santo Stefano 119.441841 1.344810 Savena 69.626016 0.805888 If you want to perform more than one function, maybe on different columns, you can use aggregate() which can be shortened to agg(). The sintax is agg(output_var = (\u0026quot;input_var\u0026quot;, function)) and it accepts also numpy functions.\ndf_listings.groupby('neighbourhood').agg(mean_reviews=(\u0026quot;reviews_per_month\u0026quot;, \u0026quot;mean\u0026quot;), min_price=(\u0026quot;price\u0026quot;, \u0026quot;min\u0026quot;), max_price=(\u0026quot;price\u0026quot;, np.max)).reset_index() neighbourhood mean_reviews min_price max_price 0 Borgo Panigale - Reno 0.983488 9 1429 1 Navile 1.156745 14 5000 2 Porto - Saragozza 1.340325 7 9999 3 San Donato - San Vitale 0.933011 10 1600 4 Santo Stefano 1.344810 11 9999 5 Savena 0.805888 9 680 If we want to build a new column by group, we can use transform() on the grouped data. Unfortunately, it does not work as nicely as aggregate() and we have to do one column at the time.\ndf_listings.groupby('neighbourhood')[['price', 'reviews_per_month']].transform('mean').head() price reviews_per_month 0 119.441841 1.344810 1 129.908312 1.340325 2 119.441841 1.344810 3 119.441841 1.344810 4 129.908312 1.340325 Combining Datasets We can concatenate datasets using pd.concat(). It takes as argument a list of dataframes. By default, pd.concat() performs the outer join. We can change it using the join option (in this case, it makes no difference).\ndf_listings1 = df_listings[:2000] np.shape(df_listings1) (2000, 18) df_listings2 = df_listings[1000:] np.shape(df_listings2) (2453, 18) np.shape( pd.concat([df_listings1, df_listings2]) ) (4453, 18) To instead merge dataframes, we can use the pd.merge function.\nOptions\nhow: {‘left’, ‘right’, ‘outer’, ‘inner’, ‘cross’}, default ‘inner’ on: label or list df_merged = pd.merge(df_listings, df_prices, left_on='id', right_on='listing_id', how='inner') df_merged.head() id name host_id host_name neighbourhood_group neighbourhood latitude longitude room_type price_x ... availability_365 number_of_reviews_ltm license listing_id date available price_y adjusted_price minimum_nights_y maximum_nights 0 42196 50 sm Studio in the historic centre 184487 Carlo NaN Santo Stefano 44.48507 11.34786 Entire home/apt 68 ... 161 6 NaN 42196 2021-12-17 f $68.00 $68.00 3 360 1 42196 50 sm Studio in the historic centre 184487 Carlo NaN Santo Stefano 44.48507 11.34786 Entire home/apt 68 ... 161 6 NaN 42196 2021-12-18 f $68.00 $68.00 3 360 2 42196 50 sm Studio in the historic centre 184487 Carlo NaN Santo Stefano 44.48507 11.34786 Entire home/apt 68 ... 161 6 NaN 42196 2021-12-19 f $68.00 $68.00 3 360 3 42196 50 sm Studio in the historic centre 184487 Carlo NaN Santo Stefano 44.48507 11.34786 Entire home/apt 68 ... 161 6 NaN 42196 2021-12-20 f $68.00 $68.00 3 360 4 42196 50 sm Studio in the historic centre 184487 Carlo NaN Santo Stefano 44.48507 11.34786 Entire home/apt 68 ... 161 6 NaN 42196 2021-12-21 f $68.00 $68.00 3 360 5 rows × 25 columns\nAs you can see, since the variable price was present in both datasets, we now have a price.x and a price.y.\nReshaping First, let\u0026rsquo;s compute average prices by neighbourhood and date using the merged dataset.\ndf_long = df_merged.groupby(['neighbourhood', 'date'])['price_x'].agg('mean').reset_index() df_long.head() neighbourhood date price_x 0 Borgo Panigale - Reno 2021-12-17 83.020548 1 Borgo Panigale - Reno 2021-12-18 83.020548 2 Borgo Panigale - Reno 2021-12-19 83.020548 3 Borgo Panigale - Reno 2021-12-20 83.020548 4 Borgo Panigale - Reno 2021-12-21 83.020548 This is what is called long format since it has one or more variables (price_x in this case) stacked vertically along a categorical variable (neighborhood and date here), which acts as index.\nThe alternative is the wide format where we have one separate column for each neighborhood.\nWe can reshape the dataset from long to wide using the pd.pivot() command. d\ndf_wide = pd.pivot(data=df_long, index='date', columns='neighbourhood').reset_index() df_wide.head() date price_x neighbourhood Borgo Panigale - Reno Navile Porto - Saragozza San Donato - San Vitale Santo Stefano Savena 0 2021-12-17 83.020548 142.200993 129.908312 91.618138 119.441841 69.626016 1 2021-12-18 83.020548 142.200993 129.908312 91.618138 119.441841 69.626016 2 2021-12-19 83.020548 142.200993 129.908312 91.618138 119.441841 69.626016 3 2021-12-20 83.020548 142.200993 129.908312 91.618138 119.441841 69.626016 4 2021-12-21 83.020548 142.200993 129.908312 91.618138 119.441841 69.626016 We can reshape the dataset from wide to long using the pd.melt() command. It takes the following arguments\ndata: the dataframe id_vars: the variable that was indexing the old dataset pd.melt(df_wide, id_vars='date', value_name='price').head() date None neighbourhood price 0 2021-12-17 price_x Borgo Panigale - Reno 83.020548 1 2021-12-18 price_x Borgo Panigale - Reno 83.020548 2 2021-12-19 price_x Borgo Panigale - Reno 83.020548 3 2021-12-20 price_x Borgo Panigale - Reno 83.020548 4 2021-12-21 price_x Borgo Panigale - Reno 83.020548 If we do not have MultiIndex columns, but just a common prefix, we can reshape the dataset from wide to long using the pd.wide_to_long() command.\ndf_wide2 = df_wide.copy() df_wide2.columns = [''.join(col) for col in df_wide2.columns] df_wide2.head() date price_xBorgo Panigale - Reno price_xNavile price_xPorto - Saragozza price_xSan Donato - San Vitale price_xSanto Stefano price_xSavena 0 2021-12-17 83.020548 142.200993 129.908312 91.618138 119.441841 69.626016 1 2021-12-18 83.020548 142.200993 129.908312 91.618138 119.441841 69.626016 2 2021-12-19 83.020548 142.200993 129.908312 91.618138 119.441841 69.626016 3 2021-12-20 83.020548 142.200993 129.908312 91.618138 119.441841 69.626016 4 2021-12-21 83.020548 142.200993 129.908312 91.618138 119.441841 69.626016 The pd.wide_to_long() command takes the following arguments\ndata: the dataframe stubnames: the prefixes of the variables that we want to reshape into one i: the variable that was indexing the old dataset j: the name of the new categorical variable that we extract from stubnames suffix: regular expression of the suffix, the default is \\d+, i.e. digits pd.wide_to_long(df_wide2, stubnames='price_x', i='date', j='neighborhood', suffix='\\D+').head() price_x date neighborhood 2021-12-17 Borgo Panigale - Reno 83.020548 2021-12-18 Borgo Panigale - Reno 83.020548 2021-12-19 Borgo Panigale - Reno 83.020548 2021-12-20 Borgo Panigale - Reno 83.020548 2021-12-21 Borgo Panigale - Reno 83.020548 Note that we had to change the suffix to \\D+, i.e. not digits.\nWindow Functions Methods\nshift() expanding() rolling() When we have time series data, we might want to do operations across time. First, let\u0026rsquo;s aggregate the df_price dataset at the year-month level.\ntemp = df_prices.copy() temp['price'] = temp['price'].str.replace('[$|,]', '', regex=True).astype(float) temp['date'] = pd.to_datetime(temp['date']).dt.to_period('M') temp = temp.groupby(['listing_id', 'date'])['price'].mean().reset_index()\\ .sort_values(by=['listing_id', 'date'], ascending=[False, True]) temp.head() listing_id date price 44876 53854962 2021-12 147.400000 44877 53854962 2022-01 137.645161 44878 53854962 2022-02 124.642857 44879 53854962 2022-03 285.096774 44880 53854962 2022-04 115.000000 We can lead or lag one variable using shift().\ntemp['price1'] = temp['price'].shift(1) temp.head(15) listing_id date price price1 44876 53854962 2021-12 147.400000 NaN 44877 53854962 2022-01 137.645161 147.400000 44878 53854962 2022-02 124.642857 137.645161 44879 53854962 2022-03 285.096774 124.642857 44880 53854962 2022-04 115.000000 285.096774 44881 53854962 2022-05 115.000000 115.000000 44882 53854962 2022-06 115.000000 115.000000 44883 53854962 2022-07 115.000000 115.000000 44884 53854962 2022-08 115.000000 115.000000 44885 53854962 2022-09 115.000000 115.000000 44886 53854962 2022-10 115.000000 115.000000 44887 53854962 2022-11 115.000000 115.000000 44888 53854962 2022-12 115.000000 115.000000 44863 53837654 2021-12 184.133333 115.000000 44864 53837654 2022-01 148.741935 184.133333 If we want to lead or lag a variable within a group, we can combine shift() with groupby()\ntemp['price1'] = temp.groupby('listing_id')['price'].shift(1) temp.head(15) listing_id date price price1 44876 53854962 2021-12 147.400000 NaN 44877 53854962 2022-01 137.645161 147.400000 44878 53854962 2022-02 124.642857 137.645161 44879 53854962 2022-03 285.096774 124.642857 44880 53854962 2022-04 115.000000 285.096774 44881 53854962 2022-05 115.000000 115.000000 44882 53854962 2022-06 115.000000 115.000000 44883 53854962 2022-07 115.000000 115.000000 44884 53854962 2022-08 115.000000 115.000000 44885 53854962 2022-09 115.000000 115.000000 44886 53854962 2022-10 115.000000 115.000000 44887 53854962 2022-11 115.000000 115.000000 44888 53854962 2022-12 115.000000 115.000000 44863 53837654 2021-12 184.133333 NaN 44864 53837654 2022-01 148.741935 184.133333 We can perform cumulative operations using the expanding() function\ntemp['avg_cum_price'] = temp['price'].expanding().mean() temp.head(15) listing_id date price price1 avg_cum_price 44876 53854962 2021-12 147.400000 NaN 147.400000 44877 53854962 2022-01 137.645161 147.400000 142.522581 44878 53854962 2022-02 124.642857 137.645161 136.562673 44879 53854962 2022-03 285.096774 124.642857 173.696198 44880 53854962 2022-04 115.000000 285.096774 161.956959 44881 53854962 2022-05 115.000000 115.000000 154.130799 44882 53854962 2022-06 115.000000 115.000000 148.540685 44883 53854962 2022-07 115.000000 115.000000 144.348099 44884 53854962 2022-08 115.000000 115.000000 141.087199 44885 53854962 2022-09 115.000000 115.000000 138.478479 44886 53854962 2022-10 115.000000 115.000000 136.344072 44887 53854962 2022-11 115.000000 115.000000 134.565399 44888 53854962 2022-12 115.000000 115.000000 133.060369 44863 53837654 2021-12 184.133333 NaN 136.708438 44864 53837654 2022-01 148.741935 184.133333 137.510671 To perform cumulative operations within a group, we can combine expanding() with groupby(). Since groups with not enough observations get dropped, we need to merge the dataset back.\ntemp.groupby('listing_id')['price'].expanding().mean().reset_index(level=0).head(15) listing_id price 0 42196 68.000000 1 42196 68.000000 2 42196 68.000000 3 42196 68.000000 4 42196 68.000000 5 42196 68.000000 6 42196 68.000000 7 42196 68.000000 8 42196 68.000000 9 42196 68.000000 10 42196 68.000000 11 42196 68.000000 12 42196 68.000000 13 46352 29.333333 14 46352 29.311828 If we want to perform an operation over a rolling window, we can use the rolling() function\ntemp['avg3_price'] = temp['price'].rolling(3).mean() temp.head(15) listing_id date price price1 avg_cum_price avg3_price 44876 53854962 2021-12 147.400000 NaN 147.400000 NaN 44877 53854962 2022-01 137.645161 147.400000 142.522581 NaN 44878 53854962 2022-02 124.642857 137.645161 136.562673 136.562673 44879 53854962 2022-03 285.096774 124.642857 173.696198 182.461598 44880 53854962 2022-04 115.000000 285.096774 161.956959 174.913210 44881 53854962 2022-05 115.000000 115.000000 154.130799 171.698925 44882 53854962 2022-06 115.000000 115.000000 148.540685 115.000000 44883 53854962 2022-07 115.000000 115.000000 144.348099 115.000000 44884 53854962 2022-08 115.000000 115.000000 141.087199 115.000000 44885 53854962 2022-09 115.000000 115.000000 138.478479 115.000000 44886 53854962 2022-10 115.000000 115.000000 136.344072 115.000000 44887 53854962 2022-11 115.000000 115.000000 134.565399 115.000000 44888 53854962 2022-12 115.000000 115.000000 133.060369 115.000000 44863 53837654 2021-12 184.133333 NaN 136.708438 138.044444 44864 53837654 2022-01 148.741935 184.133333 137.510671 149.291756 ","date":1651363200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1651363200,"objectID":"842c28fe2413586e18023093b371e682","permalink":"https://matteocourthoud.github.io/course/data-science/04_data_wrangling/","publishdate":"2022-05-01T00:00:00Z","relpermalink":"/course/data-science/04_data_wrangling/","section":"course","summary":"Setup For the scope of this tutorial we are going to use AirBnb Scraped data for the city of Bologna. The data is freely available at Inside AirBnb: http://insideairbnb.com/get-the-data.html.\nA description of all variables in all datasets is avaliable here.","tags":null,"title":"Data Wrangling","type":"book"},{"authors":null,"categories":null,"content":"# Remove warnings import warnings warnings.filterwarnings('ignore') # Import import pandas as pd import numpy as np import seaborn as sns import time from numpy.linalg import inv from numpy.random import normal from sklearn.linear_model import LinearRegression from sklearn.metrics import mean_squared_error from sklearn.model_selection import train_test_split, LeaveOneOut, KFold, cross_val_score from sklearn.preprocessing import PolynomialFeatures from sklearn.utils import resample # Import matplotlib for graphs import matplotlib.pyplot as plt from mpl_toolkits.mplot3d import axes3d # Set global parameters %matplotlib inline plt.style.use('seaborn-white') plt.rcParams['lines.linewidth'] = 3 plt.rcParams['figure.figsize'] = (10,6) plt.rcParams['figure.titlesize'] = 20 plt.rcParams['axes.titlesize'] = 18 plt.rcParams['axes.labelsize'] = 14 plt.rcParams['legend.fontsize'] = 14 Resampling methods involve repeatedly drawing samples from a training set and refitting a model of interest on each sample in order to obtain additional information about the fitted model.\n4.1 Cross-Validation Cross-validation can be used to estimate the test error associated with a given statistical learning method in order to evaluate its performance, or to select the appropriate level of flexibility. The process of evaluating a model’s performance is known as model assessment, whereas the process of selecting the proper level of flexibility for a model is known as model selection.\nLet\u0026rsquo;s use the auto dataset we have used for nonparametric models.\n# Load car dataset df1 = pd.read_csv('data/Auto.csv', na_values='?').dropna() df1.head() mpg cylinders displacement horsepower weight acceleration year origin name 0 18.0 8 307.0 130 3504 12.0 70 1 chevrolet chevelle malibu 1 15.0 8 350.0 165 3693 11.5 70 1 buick skylark 320 2 18.0 8 318.0 150 3436 11.0 70 1 plymouth satellite 3 16.0 8 304.0 150 3433 12.0 70 1 amc rebel sst 4 17.0 8 302.0 140 3449 10.5 70 1 ford torino The Validation Set Approach Suppose that we would like to estimate the test error associated with fitting a particular statistical learning method on a set of observations. The validation set approach is a very simple strategy for this task. It involves randomly dividing the available set of observations into two parts\na training set and a validation set or hold-out set The model is fit on the training set, and the fitted model is used to predict the responses for the observations in the validation set. The resulting validation set error rate-typically assessed using MSE in the case of a quantitative response—provides an estimate of the test error rate.\nIn the following example we are are going to compute the MSE fit polynomial of different order (one to ten). We are going to split the data 50-50 across training and test set.\n# Cross-validation function for polynomials def cv_poly(X, y, p_order, r_states, t_prop): start = time.time() # Init scores scores = np.zeros((p_order.size,r_states.size)) # Generate 10 random splits of the dataset for j in r_states: # Split sample in train and test X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=t_prop, random_state=j) # For every polynomial degree for i in p_order: # Generate polynomial X_train_poly = PolynomialFeatures(i+1).fit_transform(X_train) X_test_poly = PolynomialFeatures(i+1).fit_transform(X_test) # Fit regression ols = LinearRegression().fit(X_train_poly, y_train) pred = ols.predict(X_test_poly) scores[i,j]= mean_squared_error(y_test, pred) print('Time elapsed: %.4f seconds' % (time.time()-start)) return scores # Init t_prop = 0.5 p_order = np.arange(10) r_states = np.arange(10) # Get X,y X = df1.horsepower.values.reshape(-1,1) y = df1.mpg.ravel() # Compute scores cv_scores = cv_poly(X, y, p_order, r_states, t_prop) Time elapsed: 0.0277 seconds Let\u0026rsquo;s test the score for polynomials of different orders.\n# Figure 5.2 def make_figure_5_2(): # Init fig, (ax1, ax2) = plt.subplots(1,2,figsize=(14,6)) fig.suptitle('Figure 5.2') # Left plot (first split) ax1.plot(p_order+1,cv_scores[:,0], '-o') ax1.set_title('Random split of the data set') # Right plot (all splits) ax2.plot(p_order+1,cv_scores) ax2.set_title('10 random splits of the data set') for ax in fig.axes: ax.set_ylabel('Mean Squared Error') ax.set_ylim(15,30) ax.set_xlabel('Degree of Polynomial') ax.set_xlim(0.5,10.5) ax.set_xticks(range(2,11,2)); make_figure_5_2() This figure illustrates a first drawback of the validation approach: the estimate of the test error rate can be highly variable, depending on precisely which observations are included in the training set and which observations are included in the validation set.\nThe second drawback of the validation approach is that only a subset of the observations—those that are included in the training set rather than in the validation set—are used to fit the model. Since statistical methods tend to per- form worse when trained on fewer observations, this suggests that the validation set error rate may tend to overestimate the test error rate for the model fit on the entire data set.\nLeave-One-Out Cross-Validation Leave-one-out cross-validation (LOOCV) attempts to address that method’s drawbacks.\nLike the validation set approach, LOOCV involves splitting the set of observations into two parts. However, instead of creating two subsets of comparable size, a single observation $i$ is used for the validation set, and the remaining $n-1$ observations make up the training set. The statistical learning method is fit on the $n−1$ training observations and the MSE is computed using the excluded observation $i$. The procedure is repeated $n$ times, for $i=1,\u0026hellip;,n$.\nThe LOOCV estimate for the test MSE is the average of these $n$ test error estimates:\n$$ \\mathrm{CV}{(n)}=\\frac{1}{n} \\sum{i=1}^{n} \\mathrm{MSE}_{i} $$\nLOOCV has a couple of major advantages over the validation set approach.\nFirst, it has far less bias. In LOOCV, we repeatedly fit the statistical learning method using training sets that contain $n − 1$ observations, almost as many as are in the entire data set. However, this also means that LOOCV is more computationally intense.\nSecond, in contrast to the validation approach which will yield different results when applied repeatedly due to randomness in the training/validation set splits, performing LOOCV multiple times will always yield the same results: there is no randomness in the training/validation set splits.\n# LeaveOneOut CV function for polynomials def loo_cv_poly(X, y, p_order): start = time.time() # Init loo = LeaveOneOut().get_n_splits(y) loo_scores = np.zeros((p_order.size,1)) # For every polynomial degree for i in p_order: # Generate polynomial X_poly = PolynomialFeatures(i+1).fit_transform(X) # Get score loo_scores[i] = cross_val_score(LinearRegression(), X_poly, y, cv=loo, scoring='neg_mean_squared_error').mean() print('Time elapsed: %.4f seconds' % (time.time()-start)) return loo_scores Let\u0026rsquo;s compare the validation set approach against LOO in terms of computational time.\n# Validation set approach cv_scores = cv_poly(X, y, p_order, r_states, t_prop) # Leave One Out CV loo_scores = loo_cv_poly(X, y, p_order) Time elapsed: 0.0270 seconds Time elapsed: 1.1495 seconds As expected, LOOCV is much more computationally intense. Even accounting for the fact that we repeat every the validation set approach 10 times.\nLet\u0026rsquo;s now compare them in terms of accuracy in minimizing the MSE.\n# Make new figure 1 def make_new_figure_1(): # Init fig, ax = plt.subplots(1,1, figsize=(7,6)) # Left plot ax.plot(p_order+1, np.array(loo_scores)*-1, '-o', label='LOOCV') ax.plot(p_order+1, np.mean(cv_scores, axis=1), '-o', c='orange', label='Standard CV') ax.set_ylabel('Mean Squared Error'); ax.set_xlabel('Degree of Polynomial'); ax.set_ylim(15,30); ax.set_xlim(0.5,10.5); ax.set_xticks(range(2,11,2)); ax.legend(); make_new_figure_1() With least squares linear or polynomial regression, an amazing shortcut makes the cost of LOOCV the same as that of a single model fit! The following formula holds:\n$$ \\mathrm{CV}{(n)}=\\frac{1}{n} \\sum{i=1}^{n}\\left(\\frac{y_{i}-\\hat{y}{i}}{1-h{i}}\\right)^{2} $$\nwhere $\\hat y_i$ is the $i^{th}$ fitted value from the original least squares fit, and $h_i$ is the leverage of observation $i$.\nk-Fold Cross-Validation An alternative to LOOCV is k-fold CV. This approach involves the following steps:\nRandomly dividing the set of observations into $k$ groups, or folds, of approximately equal size. The first fold is treated as a validation set, and the method is fit on the remaining $k − 1$ folds. The mean squared error, MSE1, is then computed on the observations in the held-out fold. Steps (1)-(3) are repeated $k$ times; each time, a different group of observations is treated as a validation set. The k-fold CV estimate is computed by averaging these values\n$$ \\mathrm{CV}{(k)}=\\frac{1}{k} \\sum{i=1}^{k} \\mathrm{MSE}_{i} $$\nLOOCV is a special case of k-fold CV in which $k$ is set to equal $n$. In practice, one typically performs k-fold CV using $k = 5$ or $k = 10$.\nThe most obvious advantage is computational. LOOCV requires fitting the statistical learning method $n$ times, while k-fold CV only requires $k$ splits.\n# 10fold CV function for polynomials def k10_cv_poly(X, y, p_order, r_states, folds): start = time.time() # Init k10_scores = np.zeros((p_order.size,r_states.size)) # Generate 10 random splits of the dataset for j in r_states: # For every polynomial degree for i in p_order: # Generate polynomial X_poly = PolynomialFeatures(i+1).fit_transform(X) # Split sample in train and test kf10 = KFold(n_splits=folds, shuffle=True, random_state=j) k10_scores[i,j] = cross_val_score(LinearRegression(), X_poly, y, cv=kf10, scoring='neg_mean_squared_error').mean() print('Time elapsed: %.4f seconds' % (time.time()-start)) return k10_scores Let\u0026rsquo;s now compare 10 fold cross-validation with LOO in terms of computational time.\n# Leave One Out CV loo_scores = loo_cv_poly(X, y, p_order) # 10-fold CV folds = 10 k10_scores = k10_cv_poly(X, y, p_order, r_states, folds) Time elapsed: 1.1153 seconds Time elapsed: 0.3078 seconds Indeed we see that the LOOCV approach is more computationally intense. Even accounting for the fact that we repeat every 10-fold cross-validation 10 times.\nWe can now compare all the methods in terms of accuracy.\n# Figure 5.4 def make_figure_5_4(): fig, (ax1, ax2, ax3) = plt.subplots(1,3,figsize=(17,5)) fig.suptitle('Figure 5.4') # Left plot ax1.plot(p_order+1, np.array(loo_scores)*-1, '-o') ax1.set_title('LOOCV', fontsize=12) # Center plot ax2.plot(p_order+1,k10_scores*-1) ax2.set_title('10-fold CV', fontsize=12) # Right plot ax3.plot(p_order+1, np.array(loo_scores)*-1, '-o', label='LOOCV') ax3.plot(p_order+1, np.mean(cv_scores, axis=1), label='Standard CV') ax3.plot(p_order+1,np.mean(k10_scores,axis=1)*-1, label='10-fold CV') ax3.set_title('Comparison', fontsize=12); ax3.legend(); for ax in fig.axes: ax.set_ylabel('Mean Squared Error') ax.set_ylim(15,30) ax.set_xlabel('Degree of Polynomial') ax.set_xlim(0.5,10.5) ax.set_xticks(range(2,11,2)); make_figure_5_4() 10-fold cross-validation outputs a very similar MSE with respect to LOOCV, but with considerably less computational time.\n4.2 The Bootstrap The bootstrap is a widely applicable and extremely powerful statistical tool that can be used to quantify the uncertainty associated with a given estimator or statistical learning method. In the specific case of linear regression, this is not particularly useful since there exist a formula for the standard errors. However, there are many models (almost all actually) for which there exists no closed for solution to the estimator variance.\nIn pricinple, we would like to draw independent samples from the true data generating process and assessing the uncertainty of an estimator by comparing its values across the different samples. However, this is clearly unfeasible since we do not know the true data generating process.\nWith the bootstrap, rather than repeatedly obtaining independent data sets from the population, we instead obtain distinct data sets by repeatedly sampling observations from the original data set. The power of the bootstrap lies in the fact that it can be easily applied to a wide range of statistical learning methods, including some for which a measure of variability is otherwise difficult to obtain and is not automatically output by statistical software.\nWe are now going to assess its usefulness through simulation. Take the following model:\n$$ y_i = \\beta_0 \\cdot x_i + \\varepsilon_i $$\nwhere $\\beta_0 = 0.6$ and $\\varepsilon \\sim N(0,1)$. We are now going to assess the variance of the OLS estimator $\\hat \\beta$ with the standard formula, simulating different samples and with bootstrap.\n# Set seed np.random.seed(1) # Init simulations = 1000 N = 1000 beta_0 = 0.6 beta_sim = np.zeros((simulations,1)) # Generate X X = normal(0,3,N).reshape(-1,1) # Loop over simulations for i in range(simulations): # Generate y e = normal(0,1,N).reshape(-1,1) y = beta_0*X + e # Estimate beta OLS beta_sim[i] = inv(X.T @ X) @ X.T @ y # Init Bootstrap beta_boot = np.zeros((simulations,1)) # Loop over simulations for i in range(simulations): # Sample y X_sample, y_sample = resample(X, y, random_state=i) # Estimate beta OLS beta_boot[i] = inv(X_sample.T @ X_sample) @ X_sample.T @ y_sample We can first compare the means.\n# Print means print('True value : %.4f' % beta_0) print('Mean Simulations: %.4f' % np.mean(beta_sim)) print('Mean One Sample : %.4f' % beta_sim[-1]) print('Mean Boostrap : %.4f' % np.mean(beta_boot)) True value : 0.6000 Mean Simulations: 0.6003 Mean One Sample : 0.5815 Mean Boostrap : 0.5816 The mean of the bootstrap estimtor is quite off. But this is not its actual purpose: it is designed to assess the uncertainty of an estimator, not its value.\nNow we compare the variances.\n# Print variances print('True std : %.6f' % np.sqrt(inv(X.T @ X))) print('Std Simulations: %.6f' % np.std(beta_sim)) print('Std One Sample : %.6f' % np.sqrt(inv(X.T @ X) * np.var(y - beta_sim[-1]*X))) print('Std Boostrap : %.6f' % np.std(beta_boot)) True std : 0.010737 Std Simulations: 0.010830 Std One Sample : 0.010536 Std Boostrap : 0.010812 Bootstrap gets as close to the true standard deviation of the estimator as the simulation with the true data generating process. Impressive!\nWe can now have a visual inspection.\n# Figure 5.10 def make_figure_5_10(): fig, (ax1, ax2, ax3) = plt.subplots(1,3,figsize=(14,6)) fig.suptitle('Figure 5.10') # Left plot ax1.hist(beta_sim, bins=10, edgecolor='black'); ax1.axvline(x=beta_0, color='r', label='beta_0') ax1.set_xlabel('beta simulated'); # Center plot ax2.hist(beta_boot, bins=10, color='orange', edgecolor='black'); ax2.axvline(x=beta_0, color='r', label='beta_0') ax2.set_xlabel('beta bootstrap'); # Right plot df_bootstrap = pd.DataFrame({'simulated': beta_sim.ravel(), 'bootstrap':beta_boot.ravel()}, index=range(simulations)) ax3 = sns.boxplot(data=df_bootstrap, width=0.5, linewidth=2); ax3.axhline(y=beta_0, color='r', label='beta_0'); make_figure_5_10() As we can see, the bootstrap is a powerful tool to assess the uncertainty of an estimator.\n","date":1646784000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1646784000,"objectID":"55557b8d76a9ab6ec9e78a74df31ab53","permalink":"https://matteocourthoud.github.io/course/ml-econ/04_crossvalidation/","publishdate":"2022-03-09T00:00:00Z","relpermalink":"/course/ml-econ/04_crossvalidation/","section":"course","summary":"# Remove warnings import warnings warnings.filterwarnings('ignore') # Import import pandas as pd import numpy as np import seaborn as sns import time from numpy.linalg import inv from numpy.random import normal from sklearn.","tags":null,"title":"Resampling Methods","type":"book"},{"authors":null,"categories":null,"content":"Statistical Models Definition A statistical model is a set of probability distributions $\\lbrace P \\rbrace$.\nMore precisely, a statistical model over data $D \\in \\mathcal{D}$ is a set of probability distribution over datasets $D$ which takes values in $\\mathcal{D}$.\nSuppose you have regression data $\\lbrace x_i , y_i \\rbrace _ {i=1}^N$ with $x_i \\in \\mathbb{R}^p$ and $y_i \\in \\mathbb{R}$. The statistical model is\n$$ \\Big\\lbrace P : y_i = f(x_i) + \\varepsilon_i, \\ x_i \\sim F_x , \\ \\varepsilon_i \\sim F _\\varepsilon , \\ \\varepsilon_i \\perp x_i , \\ f \\in C^2 (\\mathbb{R}^p) \\Big\\rbrace $$\nIn words: the statistical model is the set of distributions $P$ such that an additive decomposition of $y_i$ as $f(x_i) + \\varepsilon_i$ exists for some $x_i$; where $f$ is twice continuously differentiable.\nA data generating process (DGP) is a single statistical distribution over\nParametrization A statistical model parameterized by $\\theta \\in \\Theta$ is well specified if the data generating process corresponds to some $\\theta_0$ and $\\theta_0 \\in \\Theta$. Otherwise, the statistical model is misspecified.\nA statistical model can be parametrized as $\\mathcal{F} = \\lbrace P_\\theta \\rbrace _ {\\lbrace \\theta \\in \\Theta \\rbrace }$.\nWe can divide statistical models into 3 classes\nParametric: the stochastic features of the model are completly specified up to a finite dimensional parameter: $\\lbrace P_\\theta \\rbrace _ { \\lbrace \\theta \\in \\Theta \\rbrace }$ with $\\Theta \\subseteq \\mathbb{R}^k, k\u0026lt;\\infty$; Semiparametric: it is a partially specified model, e.g., $\\lbrace P_\\theta \\rbrace _ { \\lbrace \\theta \\in \\Theta, \\gamma \\in \\Gamma \\rbrace }$ with $\\Theta$ of finite dimension and $\\Gamma$ of infinite dimension;\nNon parametric: there is no finite dimensional component of the model.\nEstimation Let $\\mathcal{D}$ be the set of possible data realizations. Let $D \\in \\mathcal{D}$ be your data. Let $\\mathcal{F}$ be a statistical model indexed by some parameter $\\theta \\in \\Theta$. An estimator is a map $$ \\mathcal{D} \\to \\mathcal{F} \\quad , \\quad D \\mapsto \\hat{\\theta} $$\nIn words:\nAn estimator is a map from the set of data realizations to the set of statistical models. It takes as inputs a dataset $D$ and outputs a parameter estimate $\\hat \\theta$. Inference Let $\\alpha \u0026gt; 0$ be a small tolerance. Statistical inference is a map into subsets of $\\mathcal{F}$ given by $$ \\mathcal{D} \\to \\mathcal{G} \\subseteq \\mathcal{F}: \\min _ \\theta P_\\theta (\\mathcal{G} | \\theta \\in \\mathcal{G}) \\geq 1-\\alpha $$\nIn words\nInference maps datasets into sets of models The set contains only models that generate the observed data with high probability I.e. at least $1-\\alpha$ Hypotesis Testing Hypothesis A statistical hypothesis $H_0$, is a subset of a statistical model, $\\mathcal K \\subset \\mathcal F$.\nIf $\\mathcal F$ is the statistical model and $\\mathcal K$ is the statistical hypothesis, we use the notation $H_0 : P \\in \\mathcal K$.\nExample\nCommon hypothesis are\nA single coefficient being equal to zero, $\\beta_k = c \\in \\mathbb R$ Multiple linear combination of coefficients being equal to some values: $\\boldsymbol R\u0026rsquo; \\beta = r \\in \\mathbb R^p$ Test A hypothesis test $T$ is a map from the space of datasets to a decision, rejection (0) or acceptance (1) $$ \\mathcal D \\to \\lbrace 0, 1 \\rbrace \\quad, \\quad D \\mapsto T $$\nGenerally, we are interested in understanding whether it is likely that data $D$ are drawn from a model $\\mathcal K$ or not.\nA hypothesis test, $T$ is our tool for deciding whether the hypothesis is consistent with the data.\n$T(D) = 0 \\to$ fail to reject $H_0$ and test inconclusive $T (D) = 1 \\to$ reject $H_0$ and D is inconsistent with any $P \\in \\mathcal K$ Errors Let $\\mathcal K \\subset \\mathcal F$ be a statistical hypothesis and $T$ a hypothesis test.\nA Type I error is an event $T(D)=1$ under $P \\in \\mathcal K$. In words: rejecting the null hypothesis, when it is is true A Type II error is an event $T(D)=0$ under $P \\in \\mathcal K^C$. In words: not rejecting the null hypothesis, when it is false The corresponding probability of a type I error is called size.\nThe corresponding probability of a type II error is called power (against the alternative P).\nType I Error and Test Size Test size is the probability of a Type I error, i.e. $$ \\Pr \\Big[ \\text{ Reject } H_0 \\Big| H_0 \\text{ is true } \\Big] = \\Pr \\Big[ T(D)=1 \\Big| P \\in \\mathcal K \\Big] $$ A primary goal of test construction is to limit the incidence of Type I error by bounding the size of the test.\nIn the dominant approach to hypothesis testing the researcher pre-selects a significance level $\\alpha \\in (0,1)$ and then selects the test so that its size is no larger than $\\alpha$.\nType II Error and Power Test power is the probability of a Type II error, i.e. $$ \\Pr \\Big[ \\text{ Not Reject } H_0 \\Big| H_0 \\text{ is false } \\Big] = \\Pr \\Big[ T(D)=0 \\Big| P \\in \\mathcal K^C \\Big] $$\nIn the dominant approach to hypothesis testing the goal of test construction is to have high power subject to the constraint that the size of the test is lower than the pre-specified significance level.\nStatistical Significance TBD\nP-Values Recap We now summarize the main features of hypothesis testing.\nSelect a significance level $\\alpha$. Select a test statistic $T$ with asymptotic distribution $T\\to \\xi$ under $H_0$. Set the asymptotic critical value $c$ so that 1−G(c)=α, where G is the distribution function of $\\xi$. Calculate the asymptotic p-value p=1−G(T). Reject $H_0$ if T \u0026gt; c, or equivalently p \u0026lt; α. Accept $H_0$ if T ≤ c, or equivalently p ≥ α. Report $p$ to summarize the evidence concerning $H_0$ versus $H_1$. Examples Let’s focus two hypotheses:\n$\\beta_k = c \\in \\mathbb R$ $\\boldsymbol R\u0026rsquo; \\beta = r \\in \\mathbb R^p$ t-test with Known Variance Consider the testing problem $H_0 : \\beta_k = c$, where $c$ is a pre-specified value under the null. Suppose the variance of the esimator $\\hat \\beta_k$ is known.\nThe t-statistic for this problem is defined by $$ n_{k}:=\\frac{\\hat \\beta_{k} - c}{\\sigma_{\\hat \\beta_{k}}} $$ In the testing procedure above, the sampling distribution under the null $H_0$ is given by $$ n_k \\sim N(0,1) $$ Where $N(0,1)$ the standard normal distribution.\nt-test with Unknown Variance Consider the testing problem $H_0 : \\beta_k = c$, where $c$ is a presepecified value under the null. In case the variance of the estimator $\\hat \\beta_k$ is not known, we have to replace it with a consistent estimate $\\hat \\sigma^2_{\\hat \\beta}$\nThe t-statistic for this problem is defined by $$ t_{k}:=\\frac{\\hat \\beta_{k} - c}{\\hat \\sigma_{\\hat \\beta_{k}}} $$ In the testing procedure above, the sampling distribution under the null $H_0$ is given by $$ t_k \\sim t_{n-K} $$ Where $t_{n-K}$ denotes the t-distribution with $n-K$ degress of freedom.\nWald-test Consider the testing problem $\\boldsymbol R\u0026rsquo; \\beta = r$, where $\\boldsymbol R \\in \\mathbb R^{p+K}$ is a pre-specified set of linear combinations and $r \\in \\mathbb R^p$ is a restriction vector. Suppose the variance of the esimator $\\hat \\beta$ is known.\nThe Wald statistic for this problem is given by $$ W := \\frac{(R \\hat \\beta-r)^{\\prime}(R \\hat \\beta-r) }{R\u0026rsquo; \\sigma^{2}{\\hat \\beta} R} $$ In the testing procedure above, the sampling distribution under the null $H_0$ is given by $$ W \\sim \\chi^2{n-K} $$ Where $\\chi^2_{n-K}$ denotes the chi-squared distribution with $n-K$ degress of freedom.\nComments on the Wald test The Wald statistic $W$ is a weighted Euclidean measure of the length of the vector $R \\hat \\beta-r$ The Wald test is intrinsecally 2-sided When $p=1$ then $W = |T|$ , the square of the t-statistic, so hypothesis tests based on $W$ and $|T|$ are equivalent. F-test Consider the testing problem $\\boldsymbol R\u0026rsquo; \\beta = r$, where $\\boldsymbol R \\in \\mathbb R^{p+K}$ is a pre-specified set of linear combinations and $r \\in \\mathbb R^p$ is a restriction vector. In case the variance of the estimator $\\hat \\beta$ is not known, we have to replace it with a consistent estimate $\\hat \\sigma^2_{\\hat \\beta}$.\nThe F-statistic for this problem is given by $$ F := \\frac{(R \\hat \\beta-r)^{\\prime}(R \\hat \\beta-r) / p }{R\u0026rsquo; \\hat \\sigma^{2} _ {\\hat \\beta} R} $$ In the testing procedure above, the sampling distribution under the null $H_0$ is given by $$ F \\sim F_{p, n-K} $$ Where $F_{p, n-K}$ denotes the F-distribution with $n-K$ degress of freedom, with $p$ restrictions.\nF-test Equivalence Consider the testing problem $\\boldsymbol R\u0026rsquo; \\beta = r$, where $\\boldsymbol R \\in \\mathbb R^{p+K}$ is a pre-specified set of linear combinations and $r \\in \\mathbb R^p$ is a restriction vector. Consider two estimators\n$\\hat \\beta_U = \\arg \\min_b \\frac{1}{n} (y - X \\beta)\u0026rsquo; (y - X\\beta)$ $\\hat \\beta_R = \\arg \\min_{b : \\boldsymbol R\u0026rsquo; \\beta = r} \\frac{1}{n} (y - X \\beta)\u0026rsquo; (y - X\\beta)$ Then the F statistic is numerically equivalent to the following expression $$ F = \\frac{\\left(S S R_{R}-S S R_{U}\\right) / p}{S S R_{U} /(n-K)} $$ where SSR is the sum of squared residuals.\nConfidence Intervals TBD\nMinimum Distance Tests TBD\nAsymptotics Estimator Properties Given a sequence of well specified data generating processes $\\mathcal F_n$, each indexed by the same parameter space $\\Theta$, with $\\theta_0$ a component of the true parameter for each $n$.\nThen estimator $\\hat \\theta$ is\nunbiased if $\\mathbb E [\\hat \\theta] = \\theta_0$ consistent if $\\hat \\theta \\overset{p}{\\to} \\theta_0$ asymptotically normal $\\sqrt{n} (\\hat \\theta - \\theta_0) \\overset{d}{\\to} N(0, V)$ for some positive definite $V$ Test Consistency The asymptotic size of a testing procedure is defined as the limiting probability of rejecting $H_0$ when $H_0$ is true. Mathematically, we can write this as $\\lim _ {n \\to \\infty} \\Pr_n ( \\text{reject } H_0 | H_0)$, where the $n$ subscript indexes the sample size.\nA test is said to be consistent against the alternative $H_1$ if the null hypothesis is rejected with probability approaching $1$ when $H_1$ is true: $\\lim _ {N \\to \\infty} \\Pr_N (\\text{reject } H_0 | H_1) \\overset{p}{\\to} 1$.\nConvergence Theorem: Suppose that $\\sqrt{n}(\\hat{\\theta} - \\theta_0) \\overset{d}{\\to} N(0, V)$, where $V$ is positive definite. Then for any non-stochastic $Q\\times P$ matrix $R$, $Q \\leq P$, with rank$( R ) = Q$ $$ \\sqrt{n} R (\\hat{\\theta} - \\theta_0) \\sim N(0, R VR\u0026rsquo;) $$ and $$ [\\sqrt{n}R(\\hat{\\theta} - \\theta_0)]\u0026rsquo;[RVR\u0026rsquo;]^{-1}[\\sqrt{n}R(\\hat{\\theta} - \\theta_0)] \\overset{d}{\\to} \\chi^2_Q $$ In addition, if $\\text{plim} \\hat{V} _n = V$, then $$ (\\hat{\\theta} - \\theta_0)\u0026rsquo; R\u0026rsquo;[R (\\hat{V} _n/n) R\u0026rsquo;]^{-1}R (\\hat{\\theta} - \\theta_0) \\overset{d}{\\to} \\chi^2_Q $$\nWald Statistic For testing the null hypothesis $H_0: R\\theta_0 = r$, where $r$ is a $Q\\times1$ random vector, define the Wald statistic for testing $H_0$ against $H_1 : R\\theta_0 \\neq r$ as $$ W_n = (R\\hat{\\theta} - r)\u0026rsquo;[R (\\hat{V} _n/n) R\u0026rsquo;]^{-1} (R\\hat{\\theta} - r) $$ Under $H_0$, $W_n \\overset{d}{\\to} \\chi^2_Q$. If we abuse the asymptotics and we treat $\\hat{\\theta}$ as being distributed as Normal we get the equation exactly.\n","date":1635465600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1635465600,"objectID":"9b5c711497ddb91c20e1b4cb62202ed9","permalink":"https://matteocourthoud.github.io/course/metrics/04_inference/","publishdate":"2021-10-29T00:00:00Z","relpermalink":"/course/metrics/04_inference/","section":"course","summary":"Statistical Models Definition A statistical model is a set of probability distributions $\\lbrace P \\rbrace$.\nMore precisely, a statistical model over data $D \\in \\mathcal{D}$ is a set of probability distribution over datasets $D$ which takes values in $\\mathcal{D}$.","tags":null,"title":"Inference","type":"book"},{"authors":null,"categories":null,"content":"import numpy as np import pandas as pd import folium import geopandas import contextily import matplotlib as mpl import matplotlib.pyplot as plt import seaborn as sns from src.import_data import import_data For the scope of this tutorial we are going to use AirBnb Scraped data for the city of Bologna. The data is freely available at Inside AirBnb: http://insideairbnb.com/get-the-data.html.\nA description of all variables in all datasets is avaliable here.\nWe are going to use 2 datasets:\nlisting dataset: contains listing-level information pricing dataset: contains pricing data, over time We import and clean them with a script. If you want more details, have a look at the data exploration and data wrangling sections.\ndf_listings, df_prices, df = import_data() Intro The default library for plotting in python is matplotlib. However, a more modern package that builds on top of it, is seaborn.\nWe start by telling the notebook to display the plots inline.\n%matplotlib inline Another important configuration is the plot resulution. We set it to retina to have high resolution plots.\n%config InlineBackend.figure_format = 'retina' You can choose set a general theme using plt.style.use(). The list of themes is available here.\nplt.style.use('seaborn') If you want to further customize some aspects of a theme, you can set some global paramters for all plots. You can find a list of all the options here. If you want to customize all plots in a project in the samy way, you can create a filename.mplstyle file and call it at the beginning of each file as plt.style.use('filename.mplstyle').\nmpl.rcParams['figure.figsize'] = (10,6) mpl.rcParams['axes.labelsize'] = 16 mpl.rcParams['axes.titlesize'] = 18 mpl.rcParams['axes.titleweight'] = 'bold' mpl.rcParams['figure.titlesize'] = 18 mpl.rcParams['figure.titleweight'] = 'bold' mpl.rcParams['axes.titlepad'] = 20 mpl.rcParams['legend.facecolor'] = 'w' Distributions Suppose you have a numerical variable and you want to see how it\u0026rsquo;s distributed. The best option is to use an histogram. Seaborn function is sns.histplot.\ndf_listings['log_price'] = np.log(1+df_listings['mean_price']) sns.histplot(df_listings['log_price'], bins=50)\\ .set(title='Distribution of log-prices'); We can add a smooth kernel density approximation with the kde option.\nsns.histplot(df_listings['log_price'], bins=50, kde=True)\\ .set(title='Distribution of log-prices with density'); If we have a categorical variable, we might want to plot the distribution of the data across its values. We can use a barplot. Seaborn function is sns.countplot() for count data.\nsns.countplot(x=\u0026quot;neighborhood\u0026quot;, data=df_listings)\\ .set(title='Number of observations by neighborhood'); If instead we want to see the distribution of another variable across some group, we can use the sns.barplot() function.\nsns.barplot(x=\u0026quot;neighborhood\u0026quot;, y=\u0026quot;mean_price\u0026quot;, data=df_listings)\\ .set(title='Average price by neighborhood'); We can also use other metrics besides the mean with the estimator option.\nsns.barplot(x=\u0026quot;neighborhood\u0026quot;, y=\u0026quot;mean_price\u0026quot;, data=df_listings, estimator=np.median)\\ .set(title='Median price by neighborhood'); We can also plot the full distribution using, for example boxplots with sns.boxplot(). Boxplots display quartiles and outliers.\nsns.boxplot(x=\u0026quot;neighborhood\u0026quot;, y=\u0026quot;log_price\u0026quot;, data=df_listings)\\ .set(title='Price distribution across neighborhoods'); If we want to see the full distribution, we can use the sns.violinplot() function.\nsns.violinplot(x=\u0026quot;neighborhood\u0026quot;, y=\u0026quot;log_price\u0026quot;, data=df_listings)\\ .set(title='Price distribution across neighborhoods'); Time Series If the dataset has a time dimension, we might want to explore how a variable evolves over time. Seaborn function is sns.lineplot(). If the data has multiple observations for each time period, it will also display a 95% confidence interval around the mean.\nsns.lineplot(data=df, x='date', y='price')\\ .set(title=\u0026quot;Price distribution over time\u0026quot;); We can do the samy by group, with the hue option. We can suppress confidence intervals setting ci=None (making the code much faster).\nsns.lineplot(data=df, x='date', y='price', hue='neighborhood', ci=None)\\ .set(title=\u0026quot;Price distribution over time\u0026quot;); Correlations df_listings[\u0026quot;log_reviews\u0026quot;] = np.log(1 + df_listings[\u0026quot;number_of_reviews\u0026quot;]) df_listings[\u0026quot;log_rpm\u0026quot;] = np.log(1 + df_listings[\u0026quot;reviews_per_month\u0026quot;]) The most intuitive way to plot a correlation between two variables is a scatterplot. Seaborn function is sns.scatterplot()\nsns.scatterplot(data=df_listings, x=\u0026quot;log_rpm\u0026quot;, y=\u0026quot;log_price\u0026quot;, alpha=0.3)\\ .set(title='Prices and Reviews'); We can highlight the best linear approximation adding a line of fit using sns.regplot().\nsns.regplot(x=\u0026quot;log_rpm\u0026quot;, y=\u0026quot;log_price\u0026quot;, data=df_listings, scatter_kws={'alpha':.1}, line_kws={'color':'C1'})\\ .set(title='Price and Reviews'); If we want a more flexible representation of the data, we can use the binscatter package. binscatter splits the data into equally sized bins and displays a scatterplot of the averages.\nThe main difference between a binscatterplot and an histogram is that in a histogram bins have the same width while in a binscatterplot bins have the same number of observations.\nAn advantage of binscatter is that it makes the nature of the data much more transparent, at the cost of hiding some of the background noise.\nimport binscatter # Remove nans temp = df_listings[[\u0026quot;log_rpm\u0026quot;, \u0026quot;log_price\u0026quot;]].dropna() # Binned scatter plot of Wage vs Tenure fig, ax = plt.subplots() ax.binscatter(temp[\u0026quot;log_rpm\u0026quot;], temp[\u0026quot;log_price\u0026quot;]); ax.set_title('Price and Reviews'); As usual, we can split the data by group with the hue option.\nsns.scatterplot(data=df_listings, x=\u0026quot;log_rpm\u0026quot;, y=\u0026quot;log_price\u0026quot;, hue=\u0026quot;room_type\u0026quot;, alpha=0.3)\\ .set(title=\u0026quot;Prices and Ratings, by room type\u0026quot;); We can also add the marginal distributions using the sns.jointplot() function.\nsns.jointplot(data=df_listings, x=\u0026quot;log_rpm\u0026quot;, y=\u0026quot;log_price\u0026quot;, kind=\u0026quot;hex\u0026quot;)\\ .fig.suptitle(\u0026quot;Prices and Reviews, with marginals\u0026quot;) plt.subplots_adjust(top=0.9); If we want to plot correlations (and marginals) of multiple variables, we can use the sns.pairplot() function.\nsns.pairplot(data=df_listings, vars=[\u0026quot;log_rpm\u0026quot;, \u0026quot;log_reviews\u0026quot;, \u0026quot;log_price\u0026quot;], plot_kws={'s':2})\\ .fig.suptitle(\u0026quot;Correlations\u0026quot;); plt.subplots_adjust(top=0.9) We can distinguish across groups with the hue option.\nsns.pairplot(data=df_listings, vars=[\u0026quot;log_rpm\u0026quot;, \u0026quot;log_reviews\u0026quot;, \u0026quot;log_price\u0026quot;], hue='room_type', plot_kws={'s':2})\\ .fig.suptitle(\u0026quot;Correlations, by room type\u0026quot;); plt.subplots_adjust(top=0.9) If we want to plot all the correlations in the data, we can use the sns.heatmap() function on top of a correlation matrix generated by .corr().\n# Plot sns.heatmap(df.corr(), vmin=-1, vmax=1, linewidths=.5, cmap=\u0026quot;RdBu\u0026quot;)\\ .set(title=\u0026quot;Correlations\u0026quot;); Geographical data We can in principle plot geographical data as a simple scatterplot.\nsns.scatterplot(data=df_listings, x=\u0026quot;longitude\u0026quot;, y=\u0026quot;latitude\u0026quot;)\\ .set(title='Listing coordinates'); However, we can do better and do the scatterplot over a map layer.\nFirst, we neeed to convert the latitude and longitude variables into coordinates. We use the library geopandas. Note that the original coordinate system is 4326 (3D) and we need to 3857 (2D).\ngeom = geopandas.points_from_xy(df_listings.longitude, df_listings.latitude) gdf = geopandas.GeoDataFrame( df_listings, geometry=geom, crs=4326).to_crs(3857) We import a map of Bologna using the library contextily.\nbologna = contextily.Place(\u0026quot;Bologna\u0026quot;, source=contextily.providers.Stamen.TonerLite) We are now ready to plot it with the airbnb listings.\nax = bologna.plot() ax.set_ylim([5530000, 5555000]) gdf.plot(ax=ax, c=df_listings['mean_price'], cmap='viridis', alpha=0.8); ","date":1651363200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1651363200,"objectID":"bb056caedc679d8e403fac05680718fb","permalink":"https://matteocourthoud.github.io/course/data-science/05_plotting/","publishdate":"2022-05-01T00:00:00Z","relpermalink":"/course/data-science/05_plotting/","section":"course","summary":"import numpy as np import pandas as pd import folium import geopandas import contextily import matplotlib as mpl import matplotlib.pyplot as plt import seaborn as sns from src.import_data import import_data For the scope of this tutorial we are going to use AirBnb Scraped data for the city of Bologna.","tags":null,"title":"Plotting","type":"book"},{"authors":null,"categories":null,"content":"# Remove warnings import warnings warnings.filterwarnings('ignore') # Import import pandas as pd import numpy as np import time import itertools import statsmodels.api as sm import seaborn as sns from numpy.random import normal, uniform from itertools import combinations from statsmodels.api import add_constant from statsmodels.regression.linear_model import OLS from sklearn.linear_model import LinearRegression, Ridge, Lasso, RidgeCV, LassoCV from sklearn.cross_decomposition import PLSRegression, PLSSVD from sklearn.model_selection import KFold, cross_val_score, train_test_split, LeaveOneOut, ShuffleSplit from sklearn.preprocessing import scale from sklearn.decomposition import PCA from sklearn.metrics import mean_squared_error # Import matplotlib for graphs import matplotlib.pyplot as plt from mpl_toolkits.mplot3d import axes3d # Set global parameters %matplotlib inline plt.style.use('seaborn-white') plt.rcParams['lines.linewidth'] = 3 plt.rcParams['figure.figsize'] = (12,5) plt.rcParams['figure.titlesize'] = 20 plt.rcParams['axes.titlesize'] = 18 plt.rcParams['axes.labelsize'] = 14 plt.rcParams['legend.fontsize'] = 14 When we talk about big data, we do not only talk about bigger sample size, $n$, but also about a larger number of explanatory variables, $p$. However, with ordinary least squares, we are limited by the identification constraint that $p \u0026lt; n$. Moreover, for inference and prediction accuracy, we would actually like to have $k \u0026laquo; n$.\nThis session adresses methods to use a least squares fit in a setting in which the number of regressors, $p$, is large with respect to the sample size, $n$\n5.1 Subset Selection The Subset Selection approach involves identifying a subset of the $p$ predictors that we believe to be related to the response. We then fit a model using least squares on the reduced set of variables.\nLet\u0026rsquo;s load the credit rating dataset.\n# Credit ratings dataset credit = pd.read_csv('data/Credit.csv', usecols=list(range(1,12))) credit.head() Income Limit Rating Cards Age Education Gender Student Married Ethnicity Balance 0 14.891 3606 283 2 34 11 Male No Yes Caucasian 333 1 106.025 6645 483 3 82 15 Female Yes Yes Asian 903 2 104.593 7075 514 4 71 11 Male No No Asian 580 3 148.924 9504 681 3 36 11 Female No No Asian 964 4 55.882 4897 357 2 68 16 Male No Yes Caucasian 331 We are going to look at the relationship between individual characteristics and account Balance in the Credit dataset.\n# X and y X = credit.loc[:, credit.columns != 'Balance'] y = credit.loc[:,'Balance'] Best Subset Selection To perform best subset selection, we fit a separate least squares regression for each possible combination of the $p$ predictors. That is, we fit all $p$ models that contain exactly one predictor, all $p = p(p−1)/2$ models that contain 2 exactly two predictors, and so forth. We then look at all of the resulting models, with the goal of identifying the one that is best.\nClearly the main disadvantage of best subset selection is computational power.\ndef model_selection(X, y, *args): # Init scores = list(itertools.repeat(np.zeros((0,2)), len(args))) # Categorical variables categ_cols = {\u0026quot;Gender\u0026quot;, \u0026quot;Student\u0026quot;, \u0026quot;Married\u0026quot;, \u0026quot;Ethnicity\u0026quot;} # Loop over all admissible number of regressors K = np.shape(X)[1] for k in range(K+1): print(\u0026quot;Computing k=%1.0f\u0026quot; % k, end =\u0026quot;\u0026quot;) # Loop over all combinations for i in combinations(range(K), k): # Subset X X_subset = X.iloc[:,list(i)] # Get dummies for categorical variables if k\u0026gt;0: categ_subset = list(categ_cols \u0026amp; set(X_subset.columns)) X_subset = pd.get_dummies(X_subset, columns=categ_subset, drop_first=True) # Regress reg = OLS(y,add_constant(X_subset)).fit() # Metrics for i,metric in enumerate(args): score = np.reshape([k,metric(reg)], (1,-1)) scores[i] = np.append(scores[i], score, axis=0) print(\u0026quot;\u0026quot;, end=\u0026quot;\\r\u0026quot;) return scores We are going to consider 10 variables and two difference metrics: the Sum of Squares Residuals and $R^2$.\n# Set metrics rss = lambda reg : reg.ssr r2 = lambda reg : reg.rsquared # Compute scores scores = model_selection(X, y, rss, r2) ms_RSS = scores[0] ms_R2 = scores[1] Computing k=10 # Save best scores K = np.shape(X)[1] ms_RSS_best = [np.min(ms_RSS[ms_RSS[:,0]==k,1]) for k in range(K+1)] ms_R2_best = [np.max(ms_R2[ms_R2[:,0]==k,1]) for k in range(K+1)] Let\u0026rsquo;s plot the best scores.\n# Figure 6.1 def make_figure_6_1(): fig, (ax1,ax2) = plt.subplots(1,2) fig.suptitle('Figure 6.1: Best Model Selection') # RSS ax1.scatter(x=ms_RSS[:,0], y=ms_RSS[:,1], facecolors='None', edgecolors='k', alpha=0.5); ax1.plot(range(K+1), ms_RSS_best, c='r'); ax1.scatter(np.argmin(ms_RSS_best), np.min(ms_RSS_best), marker='x', s=300) ax1.set_ylabel('RSS'); # R2 ax2.scatter(x=ms_R2[:,0], y=ms_R2[:,1], facecolors='None', edgecolors='k', alpha=0.5); ax2.plot(range(K+1), ms_R2_best, c='r'); ax2.scatter(np.argmax(ms_R2_best), np.max(ms_R2_best), marker='x', s=300) ax2.set_ylabel('R2'); # All axes; for ax in fig.axes: ax.set_xlabel('Number of Predictors'); ax.set_yticks([]); make_figure_6_1() The figure shows that, as expected, both metrics improve as the number of variables increases; however, from the three-variable model on, there is little improvement in RSS and $R^2$ as a result of including additional predictors.\nForward Stepwise Selection For computational reasons, best subset selection cannot be applied with very large $p$.\nWhile the best subset selection procedure considers all $2^p$ possible models containing subsets of the p predictors, forward step-wise considers a much smaller set of models. Forward stepwise selection begins with a model containing no predictors, and then adds predictors to the model, one-at-a-time, until all of the predictors are in the model. In particular, at each step the variable that gives the greatest additional improvement to the fit is added to the model.\ndef forward_selection(X, y, f): # Init RSS and R2 K = np.shape(X)[1] fms_scores = np.zeros((K,1)) # Categorical variables categ_cols = {\u0026quot;Gender\u0026quot;, \u0026quot;Student\u0026quot;, \u0026quot;Married\u0026quot;, \u0026quot;Ethnicity\u0026quot;} # Loop over p selected_cols = [] for k in range(1,K+1): # Loop over selected columns remaining_cols = [col for col in X.columns if col not in selected_cols] temp_scores = np.zeros((0,1)) # Loop on remaining columns for col in remaining_cols: # Subset X X_subset = X.loc[:,selected_cols + [col]] if k\u0026gt;0: categ_subset = list(categ_cols \u0026amp; set(X_subset.columns)) X_subset = pd.get_dummies(X_subset, columns=categ_subset, drop_first=True) # Regress reg = OLS(y,add_constant(X_subset).values).fit() # Metrics temp_scores = np.append(temp_scores, f(reg)) # Pick best variable best_col = remaining_cols[np.argmin(temp_scores)] print(best_col) selected_cols += [best_col] fms_scores[k-1] = np.min(temp_scores) return fms_scores Let\u0026rsquo;s select the best model according, using the sum of squared residuals as a metric.\nWhat are the most important variables?\n# Forward selection by RSS rss = lambda reg : reg.ssr fms_RSS = forward_selection(X, y, rss) Rating Income Student Limit Cards Age Ethnicity Gender Married Education What happens if we use $R^2$ instead?\n# Forward selection by R2 r2 = lambda reg : -reg.rsquared fms_R2 = -forward_selection(X, y, r2) Rating Income Student Limit Cards Age Ethnicity Gender Married Education Unsurprisingly, both methods select the same models. Why? In the end $R^2$ is just a normalized version of RSS.\nLet\u0026rsquo;s plot the scores of the two methods, for different number of predictors.\n# New figure 1 def make_new_figure_1(): # Init fig, (ax1,ax2) = plt.subplots(1,2) fig.suptitle('Forward Model Selection') # RSS ax1.plot(range(1,K+1), fms_RSS, c='r'); ax1.scatter(np.argmin(fms_RSS)+1, np.min(fms_RSS), marker='x', s=300) ax1.set_ylabel('RSS'); # R2 ax2.plot(range(1,K+1), fms_R2, c='r'); ax2.scatter(np.argmax(fms_R2)+1, np.max(fms_R2), marker='x', s=300) ax2.set_ylabel('R2'); # All axes; for ax in fig.axes: ax.set_xlabel('Number of Predictors'); ax.set_yticks([]); make_new_figure_1() Backward Stepwise Selection Like forward stepwise selection, backward stepwise selection provides an efficient alternative to best subset selection. However, unlike forward stepwise selection, it begins with the full least squares model containing all p predictors, and then iteratively removes the least useful predictor, one-at-a-time.\ndef backward_selection(X, y, f): # Init RSS and R2 K = np.shape(X)[1] fms_scores = np.zeros((K,1)) # Categorical variables categ_cols = {\u0026quot;Gender\u0026quot;, \u0026quot;Student\u0026quot;, \u0026quot;Married\u0026quot;, \u0026quot;Ethnicity\u0026quot;} # Loop over p selected_cols = list(X.columns) for k in range(K,0,-1): # Loop over selected columns temp_scores = np.zeros((0,1)) # Loop on remaining columns for col in selected_cols: # Subset X X_subset = X.loc[:,[x for x in selected_cols if x != col]] if k\u0026gt;1: categ_subset = list(categ_cols \u0026amp; set(X_subset.columns)) X_subset = pd.get_dummies(X_subset, columns=categ_subset, drop_first=True) # Regress reg = OLS(y,add_constant(X_subset).values).fit() # Metrics temp_scores = np.append(temp_scores, f(reg)) # Pick best variable worst_col = selected_cols[np.argmin(temp_scores)] print(worst_col) selected_cols.remove(worst_col) fms_scores[k-1] = np.min(temp_scores) return fms_scores Let\u0026rsquo;s select the best model according, using the sum of squared residuals as a metric.\nWhat are the most important variables?\n# Backward selection by RSS rss = lambda reg : reg.ssr bms_RSS = backward_selection(X, y, rss) Education Married Gender Ethnicity Age Rating Cards Student Income Limit What if we use $R^2$ instead?\n# Backward selection by R2 r2 = lambda reg : -reg.rsquared bms_R2 = -backward_selection(X, y, r2) Education Married Gender Ethnicity Age Rating Cards Student Income Limit The interesting part here is that the the variable Rating that was selected first by forward model selection, is now dropped $5^{th}$ to last. Why? It\u0026rsquo;s probably because it contains a lot of information by itself (hence first in FMS) but it\u0026rsquo;s highly correlated with Student, Income and Limit while these variables are more ortogonal to each other, and hence it gets dropped before them in BMS.\n# Plot correlations sns.pairplot(credit[['Rating','Student','Income','Limit']], height=1.8); If is indeed what we see: Rating and Limit are highly correlated.\nLet\u0026rsquo;s plot the scores for different number of predictors.\n# New figure 2 def make_new_figure_2(): # Init fig, (ax1,ax2) = plt.subplots(1,2) fig.suptitle('Backward Model Selection') # RSS ax1.plot(range(1,K+1), bms_RSS, c='r'); ax1.scatter(np.argmin(bms_RSS)+1, np.min(bms_RSS), marker='x', s=300) ax1.set_ylabel('RSS'); # R2 ax2.plot(range(1,K+1), bms_R2, c='r'); ax2.scatter(np.argmax(bms_R2)+1, np.max(bms_R2), marker='x', s=300) ax2.set_ylabel('R2'); # All axes; for ax in fig.axes: ax.set_xlabel('Number of Predictors'); ax.set_yticks([]); make_new_figure_2() Choosing the Optimal Model So far we have use the trainint error in order to select the model. However, the training error can be a poor estimate of the test error. Therefore, RSS and R2 are not suitable for selecting the best model among a collection of models with different numbers of predictors.\nIn order to select the best model with respect to test error, we need to estimate this test error. There are two common approaches:\nWe can indirectly estimate test error by making an adjustment to the training error to account for the bias due to overfitting. We can directly estimate the test error, using either a validation set approach or a cross-validation approach. Some metrics that account for the trainint error are\nAkaike Information Criterium (AIC) Bayesian Information Criterium (BIC) Adjusted $R^2$ The idea behind all these varaibles is to insert a penalty for the number of parameters used in the model. All these measure have theoretical fundations which are beyond the scope of this session.\nWe are now going to test the three metrics\n# Set metrics aic = lambda reg : reg.aic bic = lambda reg : reg.bic r2a = lambda reg : reg.rsquared_adj # Compute best model selection scores scores = model_selection(X, y, aic, bic, r2a) ms_AIC = scores[0] ms_BIC = scores[1] ms_R2a = scores[2] Computing k=10 # Save best scores ms_AIC_best = [np.min(ms_AIC[ms_AIC[:,0]==k,1]) for k in range(K+1)] ms_BIC_best = [np.min(ms_BIC[ms_BIC[:,0]==k,1]) for k in range(K+1)] ms_R2a_best = [np.max(ms_R2a[ms_R2a[:,0]==k,1]) for k in range(K+1)] We plot the scores for different model selection methods.\n# Figure 6.2 def make_figure_6_2(): # Init fig, (ax1,ax2,ax3) = plt.subplots(1,3, figsize=(16,5)) fig.suptitle('Figure 6.2') # AIC ax1.scatter(x=ms_AIC[:,0], y=ms_AIC[:,1], facecolors='None', edgecolors='k', alpha=0.5); ax1.plot(range(K+1),ms_AIC_best, c='r'); ax1.scatter(np.argmin(ms_AIC_best), np.min(ms_AIC_best), marker='x', s=300) ax1.set_ylabel('AIC'); # BIC ax2.scatter(x=ms_BIC[:,0], y=ms_BIC[:,1], facecolors='None', edgecolors='k', alpha=0.5); ax2.plot(range(K+1), ms_BIC_best, c='r'); ax2.scatter(np.argmin(ms_BIC_best), np.min(ms_BIC_best), marker='x', s=300) ax2.set_ylabel('BIC'); # R2 adj ax3.scatter(x=ms_R2a[:,0], y=ms_R2a[:,1], facecolors='None', edgecolors='k', alpha=0.5); ax3.plot(range(K+1), ms_R2a_best, c='r'); ax3.scatter(np.argmax(ms_R2a_best), np.max(ms_R2a_best), marker='x', s=300) ax3.set_ylabel('R2_adj'); # All axes; for ax in fig.axes: ax.set_xlabel('Number of Predictors'); ax.set_yticks([]); make_figure_6_2() As we can see, all three metrics select more parsimonious models, with BIC being particularly conservative with only 4 variables and $R^2_{adj}$ selecting the larger model with 7 variables.\nValidation and Cross-Validation As an alternative to the approaches just discussed, we can directly estimate the test error using the validation set and cross-validation methods discussed in the previous session.\nThe main problem with cross-validation is the computational burden. We are now going to perform best model selection using the following cross-validation algorithms:\nValidation set approach, 50-50 split, repeated 10 times 5-fold cross-validation 10-fold cross-validation We are not going to perform Leave-One-Out cross-validation for computational reasons.\ndef cv_scores(X, y, *args): # Init scores = list(itertools.repeat(np.zeros((0,2)), len(args))) # Categorical variables categ_cols = {\u0026quot;Gender\u0026quot;, \u0026quot;Student\u0026quot;, \u0026quot;Married\u0026quot;, \u0026quot;Ethnicity\u0026quot;} # Loop over all possible combinations of regressions K = np.shape(X)[1] for k in range(K+1): print(\u0026quot;Computing k=%1.0f\u0026quot; % k, end =\u0026quot;\u0026quot;) for i in combinations(range(K), k): # Subset X X_subset = X.iloc[:,list(i)] # Get dummies for categorical variables if k\u0026gt;0: categ_subset = list(categ_cols \u0026amp; set(X_subset.columns)) X_subset = pd.get_dummies(X_subset, columns=categ_subset, drop_first=True) # Metrics for i,cv_method in enumerate(args): score = cross_val_score(LinearRegression(), add_constant(X_subset), y, cv=cv_method, scoring='neg_mean_squared_error').mean() score_pair = np.reshape([k,score], (1,-1)) scores[i] = np.append(scores[i], score_pair, axis=0) print(\u0026quot;\u0026quot;, end=\u0026quot;\\r\u0026quot;) return scores Let\u0026rsquo;s compute the scores for different model selection methods.\n# Define cv methods vset = ShuffleSplit(n_splits=10, test_size=0.5) kf5 = KFold(n_splits=5, shuffle=True) kf10 = KFold(n_splits=10, shuffle=True) # Get best model selection scores scores = cv_scores(X, y, vset, kf5, kf10) ms_vset = scores[0] ms_kf5 = scores[1] ms_kf10 = scores[2] Computing k=10 # Save best scores ms_vset_best = [np.max(ms_vset[ms_vset[:,0]==k,1]) for k in range(K+1)] ms_kf5_best = [np.max(ms_kf5[ms_kf5[:,0]==k,1]) for k in range(K+1)] ms_kf10_best = [np.max(ms_kf10[ms_kf10[:,0]==k,1]) for k in range(K+1)] We not plot the scores.\n# Figure 6.3 def make_figure_6_3(): # Init fig, (ax1,ax2,ax3) = plt.subplots(1,3, figsize=(16,5)) fig.suptitle('Figure 6.3') # Validation Set ax1.scatter(x=ms_vset[:,0], y=ms_vset[:,1], facecolors='None', edgecolors='k', alpha=0.5); ax1.plot(range(K+1),ms_vset_best, c='r'); ax1.scatter(np.argmax(ms_vset_best), np.max(ms_vset_best), marker='x', s=300) ax1.set_ylabel('Validation Set'); # 5-Fold Cross Validation ax2.scatter(x=ms_kf5[:,0], y=ms_kf5[:,1], facecolors='None', edgecolors='k', alpha=0.5); ax2.plot(range(K+1), ms_kf5_best, c='r'); ax2.scatter(np.argmax(ms_kf5_best), np.max(ms_kf5_best), marker='x', s=300) ax2.set_ylabel('5-Fold Cross Validation'); # 10-Fold Cross-Validation ax3.scatter(x=ms_kf10[:,0], y=ms_kf10[:,1], facecolors='None', edgecolors='k', alpha=0.5); ax3.plot(range(K+1), ms_kf10_best, c='r'); ax3.scatter(np.argmax(ms_kf10_best), np.max(ms_kf10_best), marker='x', s=300) ax3.set_ylabel('10-Fold Cross-Validation'); # All axes; for ax in fig.axes: ax.set_xlabel('Number of Predictors'); ax.set_yticks([]); make_figure_6_3() From the figure we see that each cross-validation method selects a different model and the most accurate one, K-fold CV, select 5 predictors.\n5.2 Shrinkage Methods Model selection methods constrained the number of varaibles before running a linear regression. Shrinkage methods attempt to do the two things simultaneously. In particular they constrain or shrink coefficients by imposing penalties in the objective functions for high values of the parameters.\nRidge Regression The Least Squares Regression minimizes the Residual Sum of Squares\n$$ \\mathrm{RSS}=\\sum_{i=1}^{n}\\left(y_{i}-\\beta_{0}-\\sum_{j=1}^{p} \\beta_{j} x_{i j}\\right)^{2} $$\nThe Ridge Regression objective function instead is\n$$ \\sum_{i=1}^{n}\\left(y_{i}-\\beta_{0}-\\sum_{j=1}^{p} \\beta_{j} x_{i j}\\right)^{2}+\\lambda \\sum_{j=1}^{p} \\beta_{j}^{2}=\\mathrm{RSS}+\\lambda \\sum_{j=1}^{p} \\beta_{j}^{2} $$\nwhere $\\lambda\u0026gt;0$ is a tuning parameter that regulates the extent to which large parameters are penalized.\nIn matrix notation, the objective function is\n$$ ||X\\beta - y||^2_2 + \\alpha ||\\beta||^2_2 $$\nwhich is equivalent to optimizing\n$$ \\frac{1}{N}||X\\beta - y||^2_2 + \\frac{\\alpha}{N} ||\\beta||^2_2 $$\nWe are now going to run Ridge Regression on the Credit dataset trying to explain account Balance with a set of observable individual characteristics.\n# X and y categ_cols = [\u0026quot;Gender\u0026quot;, \u0026quot;Student\u0026quot;, \u0026quot;Married\u0026quot;, \u0026quot;Ethnicity\u0026quot;] X = credit.loc[:, credit.columns != 'Balance'] X = pd.get_dummies(X, columns=categ_cols, drop_first=True) y = credit.loc[:,'Balance'] n = len(credit) We run ridge regression over a range of values for the penalty paramenter $\\lambda$.\n# Init alpha grid n_grid = 100 alphas = 10**np.linspace(-2,5,n_grid).reshape(-1,1) ridge = Ridge() ridge_coefs = [] # Loop over values of alpha for a in alphas: ridge.set_params(alpha=a) ridge.fit(scale(X), y) ridge_coefs.append(ridge.coef_) ridge_coefs = np.reshape(ridge_coefs,(n_grid,-1)) We use linear regression as a comparison.\n# OLS regression ols = LinearRegression().fit(scale(X),y) ols_coefs = ols.coef_; mod_ols = np.linalg.norm(ols_coefs) # Relative magnitude rel_beta = [np.linalg.norm(ridge_coefs[k,:])/mod_ols for k in range(n_grid)] rel_beta = np.reshape(rel_beta, (-1,1)) We plot the results\n# Figure 6.4 def make_figure_6_4(): # Init fig, (ax1,ax2) = plt.subplots(1,2) fig.suptitle('Figure 6.4: Ridge Regression Coefficients') highlight = [0,1,2,7]; # Plot coefficients - absolute ax1.plot(alphas, ridge_coefs[:,highlight], alpha=1) ax1.plot(alphas, ridge_coefs, c='grey', alpha=0.3) ax1.set_xscale('log') ax1.set_xlabel('lambda'); ax1.set_ylabel('Standardized coefficients'); ax1.legend(['Income', 'Limit', 'Rating', 'Student']) # Plot coefficients - relative ax2.plot(rel_beta, ridge_coefs[:,highlight], alpha=1) ax2.plot(rel_beta, ridge_coefs, c='grey', alpha=0.3) ax2.set_xlabel('Relative Beta'); ax2.set_ylabel('Standardized coefficients'); make_figure_6_4() As we decrease $\\lambda$, the Ridge coefficients get larger. Moreover, the variables with the consistently largest coefficients are Income, Limit, Rating and Student.\nBias-Variance Trade-off Ridge regression’s advantage over least squares is rooted in the bias-variance trade-off. As $\\lambda$ increases, the flexibility of the ridge regression fit decreases, leading to decreased variance but increased bias.\n$$ y_0 = f(x_0) + \\varepsilon $$\nRecap: we can decompose the Mean Squared Error of an estimator into two components: the variance and the squared bias:\n$$ \\mathbb E\\left(y_{0}-\\hat{f}\\left(x_{0}\\right)\\right)^{2} = \\mathbb E\\left(f(x_0) + \\varepsilon - \\hat f(x_{0})\\right)^{2} = \\ = \\mathbb E\\left(f(x_0) - \\mathbb E[\\hat f(x_{0})] + \\varepsilon - \\hat f(x_{0}) + \\mathbb E[\\hat f(x_{0})] \\right)^{2} = \\ = \\mathbb E \\left[ \\mathbb E [\\hat{f} (x_{0}) ] - f(x_0) \\right]^2 + \\mathbb E \\left[ \\left( \\hat{f} (x_{0}) - \\mathbb E [\\hat{f} (x_{0})] \\right)^2 \\right] + \\mathbb E[\\varepsilon^2] \\ = \\operatorname{Bias} \\left( \\hat{f} (x_{0}) \\right)^2 + \\operatorname{Var}\\left(\\hat{f}\\left(x_{0}\\right)\\right) + \\operatorname{Var}(\\varepsilon) $$\nThe last term is the variance of the error term, sometimes also called the irreducible error since it\u0026rsquo;s pure noise, and we cannot account for it.\n# Compute var-bias def compute_var_bias(X_train, b0, x0, a, k, n, sim, f): # Init y_hat = np.zeros(sim) coefs = np.zeros((sim, k)) # Loop over simulations for s in range(sim): e_train = normal(0,1,(n,1)) y_train = X_train @ b0 + e_train fit = f(a).fit(X_train, y_train) y_hat[s] = fit.predict(x0) coefs[s,:] = fit.coef_ # Compute MSE, Var and Bias2 e_test = normal(0,1,(sim,1)) y_test = x0 @ b0 + e_test mse = np.mean((y_test - y_hat)**2) var = np.var(y_hat) bias2 = np.mean(x0 @ b0 - y_hat)**2 return [mse, var, bias2], np.mean(coefs, axis=0) np.random.seed(1) # Generate random data n = 50 k = 45 N = 50000 X_train = normal(0.2,1,(n,k)) x0 = normal(0.2,1,(1,k)) e_train = normal(0,1,(n,1)) b0 = uniform(0,1,(k,1)) # Init alpha grid sim = 1000 n_grid = 30 df = pd.DataFrame({'alpha':10**np.linspace(-5,5,n_grid)}) ridge_coefs2 = [] # Init simulations sim = 1000 ridge = lambda a: Ridge(alpha=a, fit_intercept=False) # Loop over values of alpha for i in range(len(df)): print(\u0026quot;Alpha %1.0f/%1.0f\u0026quot; % (i+1,len(df)), end =\u0026quot;\u0026quot;) a = df.loc[i,'alpha'] df.loc[i,['mse','var','bias2']], c = compute_var_bias(X_train, b0, x0, a, k, n, sim, ridge) ridge_coefs2.append(c) print(\u0026quot;\u0026quot;, end=\u0026quot;\\r\u0026quot;) ridge_coefs2 = np.reshape(ridge_coefs2,(n_grid,-1)) Alpha 30/30 # OLS regression y_train = X_train @ b0 + e_train ols = LinearRegression().fit(X_train,y_train) ols_coefs = ols.coef_; mod_ols = np.linalg.norm(ols_coefs) # Relative magnitude rel_beta = [np.linalg.norm(ridge_coefs2[i,:])/mod_ols for i in range(n_grid)] rel_beta = np.reshape(rel_beta, (-1,1)) # Figure 6.5 def make_figure_6_5(): # Init fig, (ax1,ax2) = plt.subplots(1,2) fig.suptitle('Figure 6.5: Ridge Bias-Var decomposition') # MSE ax1.plot(df['alpha'], df[['bias2','var','mse']]); ax1.set_xscale('log'); ax1.set_xlabel('lambda'); ax1.set_ylabel('Mean Squared Error'); ax1.legend(['Bias2','Variance','MSE'], fontsize=12); # MSE ax2.plot(rel_beta, df[['bias2','var','mse']]); ax2.set_xlabel('Relative Beta'); ax2.set_ylabel('Mean Squared Error'); ax2.legend(['Bias2','Variance','MSE'], fontsize=12); make_figure_6_5() Ridge regression has the advantage of shrinking coefficients. However, unlike best subset, forward stepwise, and backward stepwise selection, which will generally select models that involve just a subset of the variables, ridge regression will include all $p$ predictors in the final model.\nLasso solves that problem by using a different penalty function.\nLasso The lasso coefficients minimize the following objective function:\n$$ \\sum_{i=1}^{n}\\left(y_{i}-\\beta_{0}-\\sum_{j=1}^{p} \\beta_{j} x_{i j}\\right)^{2}+\\lambda \\sum_{j=1}^{p}\\left|\\beta_{j}\\right| = \\mathrm{RSS} + \\lambda \\sum_{j=1}^p|\\beta_j| $$\nso that the main difference with respect to ridge regression is the penalty function $\\lambda \\sum_{j=1}^{p}\\left|\\beta_{j}\\right|$ instead of $\\lambda \\sum_{j=1}^p (\\beta_j)^2$.\nA consequence of this objective function is that Lasso is much more likely to shrink coefficients to exactly zero, while Ridge only decreases their magnitude. The reason why lies in the shape of the objective function. You can rewrite the Ridge and Lasso minimization problems as constrained optimization:\nRidge $$ \\underset{\\beta}{\\operatorname{min}} \\ \\left{\\sum_{i=1}^{n}\\left(y_{i}-\\beta_{0}-\\sum_{j=1}^{p} \\beta_{j} x_{i j}\\right)^{2}\\right} \\quad \\text { subject to } \\quad \\sum_{j=1}^{p}\\left|\\beta_{j}\\right| \\leq s $$\nLasso $$ \\underset{\\beta}{\\operatorname{min}} \\ \\left{\\sum_{i=1}^{n}\\left(y_{i}-\\beta_{0}-\\sum_{j=1}^{p} \\beta_{j} x_{i j}\\right)^{2}\\right} \\quad \\text { subject to } \\quad \\sum_{j=1}^{p} \\beta_{j}^{2} \\leq s $$\nIn pictures, constrained optimization problem lookes like this.\nThe red curves represents the contour sets of the RSS. They are elliptical since the objective function is quadratic. The blue area represents the admissible set, i.e. the constraints. As we can see, it is much easier with Lasso to have the constrained optimum on one of the edges of the rhombus.\nWe are now going to repeat the same exercise on the Credit dataset, trying to predict account Balance with a set of obsevable induvidual characteristics, for different values of the penalty paramenter $\\lambda$.\n# X and y categ_cols = [\u0026quot;Gender\u0026quot;, \u0026quot;Student\u0026quot;, \u0026quot;Married\u0026quot;, \u0026quot;Ethnicity\u0026quot;] X = credit.loc[:, credit.columns != 'Balance'] X = pd.get_dummies(X, columns=categ_cols, drop_first=True) y = credit.loc[:,'Balance'] The $\\lambda$ grid is going to be slightly different now.\n# Init alpha grid n_grid = 100 alphas = 10**np.linspace(0,3,n_grid).reshape(-1,1) lasso = Lasso() lasso_coefs = [] # Loop over values of alpha for a in alphas: lasso.set_params(alpha=a) lasso.fit(scale(X), y) lasso_coefs.append(lasso.coef_) lasso_coefs = np.reshape(lasso_coefs,(n_grid,-1)) We run OLS to plot the relative magnitude of the Lasso coefficients.\n# Relative magnitude mod_ols = np.linalg.norm(ols_coefs) rel_beta = [np.linalg.norm(lasso_coefs[i,:])/mod_ols for i in range(n_grid)] rel_beta = np.reshape(rel_beta, (-1,1)) We plot the magnitude of the coefficients $\\beta$\nfor different values of $\\lambda$ for different values of of $||\\beta||$ # Figure 6.6 def make_figure_6_6(): # Init fig, (ax1,ax2) = plt.subplots(1,2) fig.suptitle('Figure 6.6') highlight = [0,1,2,7]; # Plot coefficients - absolute ax1.plot(alphas, lasso_coefs[:,highlight], alpha=1) ax1.plot(alphas, lasso_coefs, c='grey', alpha=0.3) ax1.set_xscale('log') ax1.set_xlabel('lambda'); ax1.set_ylabel('Standardized coefficients'); ax1.legend(['Income', 'Limit', 'Rating', 'Student'], fontsize=12) # Plot coefficients - relative ax2.plot(rel_beta, lasso_coefs[:,highlight], alpha=1) ax2.plot(rel_beta, lasso_coefs, c='grey', alpha=0.3) ax2.set_xlabel('relative mod beta'); ax2.set_ylabel('Standardized coefficients'); make_figure_6_6() Rating seems to be the most important variable, followed by Limit and Student.\nAs with ridge regression, the lasso shrinks the coefficient estimates towards zero. However, in the case of the lasso, the $l_1$ penalty has the effect of forcing some of the coefficient estimates to be exactly equal to zero when the tuning parameter $\\lambda$ is sufficiently large. Hence, much like best subset selection, the lasso performs variable selection.\nWe say that the lasso yields sparse models — that is, models that involve only a subset of the variable\nWe now plot how the choice of $\\lambda$ affects the bias-variance trade-off.\n# Init alpha grid sim = 1000 n_grid = 30 df = pd.DataFrame({'alpha':10**np.linspace(-1,1,n_grid)}) lasso_coefs2 = [] # Init simulations sim = 1000 lasso = lambda a: Lasso(alpha=a, fit_intercept=False) # Loop over values of alpha for i in range(len(df)): print(\u0026quot;Alpha %1.0f/%1.0f\u0026quot; % (i+1,len(df)), end =\u0026quot;\u0026quot;) a = df.loc[i,'alpha'] df.loc[i,['mse','var','bias2']], c = compute_var_bias(X_train, b0, x0, a, k, n, sim, lasso) lasso_coefs2.append(c) print(\u0026quot;\u0026quot;, end=\u0026quot;\\r\u0026quot;) lasso_coefs2 = np.reshape(lasso_coefs2,(n_grid,-1)) Alpha 30/30 # Relative magnitude mod_ols = np.linalg.norm(ols_coefs) rel_beta = [np.linalg.norm(lasso_coefs2[k,:])/mod_ols for k in range(n_grid)] rel_beta = np.reshape(rel_beta, (-1,1)) # OLS regression y_train = X_train @ b0 + e_train ols = LinearRegression().fit(X_train,y_train) ols_coefs = ols.coef_; mod_ols = np.linalg.norm(ols_coefs) # Relative magnitude mod_ols = np.linalg.norm(ols_coefs) rel_beta = [np.linalg.norm(lasso_coefs2[k,:])/mod_ols for k in range(n_grid)] rel_beta = np.reshape(rel_beta, (-1,1)) # Figure 6.8 def make_figure_6_8(): fig, (ax1,ax2) = plt.subplots(1,2, figsize=(12,5)) fig.suptitle('Figure 6.8: Lasso Bias-Var decomposition') # MSE ax1.plot(df['alpha'], df[['bias2','var','mse']]); ax1.set_xscale('log'); ax1.set_xlabel('lambda'); ax1.set_ylabel('Mean Squared Error'); ax1.legend(['Bias2','Variance','MSE'], fontsize=12); # MSE ax2.plot(rel_beta, df[['bias2','var','mse']]); ax2.set_xlabel('Relative Beta'); ax1.set_ylabel('Mean Squared Error'); ax2.legend(['Bias2','Variance','MSE'], fontsize=12); make_figure_6_8() As $\\lambda$ increases the squared bias increases and the variance decreases.\nComparing the Lasso and Ridge Regression In order to obtain a better intuition about the behavior of ridge regression and the lasso, consider a simple special case with $n = p$, and $X$ a diagonal matrix with $1$’s on the diagonal and $0$’s in all off-diagonal elements. To simplify the problem further, assume also that we are performing regression without an intercept.\nWith these assumptions, the usual least squares problem simplifies to the coefficients that minimize\n$$ \\sum_{j=1}^{p}\\left(y_{j}-\\beta_{j}\\right)^{2} $$\nIn this case, the least squares solution is given by\n$$ \\hat \\beta_j = y_j $$\nOne can show that in this setting, the ridge regression estimates take the form\n$$ \\hat \\beta_j^{RIDGE} = \\frac{y_j}{1+\\lambda} $$\nand the lasso estimates take the form\n$$ \\hat{\\beta}{j}^{LASSO}=\\left{\\begin{array}{ll} y{j}-\\lambda / 2 \u0026amp; \\text { if } y_{j}\u0026gt;\\lambda / 2 \\ y_{j}+\\lambda / 2 \u0026amp; \\text { if } y_{j}\u0026lt;-\\lambda / 2 \\ 0 \u0026amp; \\text { if }\\left|y_{j}\\right| \\leq \\lambda / 2 \\end{array}\\right. $$\nWe plot the relationship visually.\nnp.random.seed(3) # Generate random data n = 100 k = n X = np.eye(k) e = normal(0,1,(n,1)) b0 = uniform(-1,1,(k,1)) y = X @ b0 + e # OLS regression reg = LinearRegression().fit(X,y) ols_coefs = reg.coef_; # Ridge regression ridge = Ridge(alpha=1).fit(X,y) ridge_coefs = ridge.coef_; # Ridge regression lasso = Lasso(alpha=0.01).fit(X,y) lasso_coefs = lasso.coef_.reshape(1,-1); # sort order = np.argsort(y.reshape(1,-1), axis=1) y_sorted = np.take_along_axis(ols_coefs, order, axis=1) ols_coefs = np.take_along_axis(ols_coefs, order, axis=1) ridge_coefs = np.take_along_axis(ridge_coefs, order, axis=1) lasso_coefs = np.take_along_axis(lasso_coefs, order, axis=1) # Figure 6.10 def make_figure_6_10(): # Init fig, (ax1,ax2) = plt.subplots(1,2) fig.suptitle('Figure 6.10') # Ridge ax1.plot(y_sorted.T, ols_coefs.T) ax1.plot(y_sorted.T, ridge_coefs.T) ax1.set_xlabel('True Coefficient'); ax1.set_ylabel('Estimated Coefficient'); ax1.legend(['OLS','Ridge'], fontsize=12); # Lasso ax2.plot(y_sorted.T, ols_coefs.T) ax2.plot(y_sorted.T, lasso_coefs.T) ax2.set_xlabel('True Coefficient'); ax2.set_ylabel('Estimated Coefficient'); ax2.legend(['OLS','Lasso'], fontsize=12); make_figure_6_10() We see that ridge regression shrinks every dimension of the data by the same proportion, whereas the lasso hrinks all coefficients toward zero by a similar amount, and sufficiently small coefficients are shrunken all the way to zero.\nSelecting the Tuning Parameter Implementing ridge regression and the lasso requires a method for selecting a value for the tuning parameter $\\lambda$.\nCross-validation provides a simple way to tackle this problem. We choose a grid of $\\lambda$ values, and compute the cross-validation error for each value of $\\lambda$. We then select the tuning parameter value for which the cross-validation error is smallest. Finally, the model is re-fit using all of the available observations and the selected value of the tuning parameter.\n# X and y categ_cols = [\u0026quot;Gender\u0026quot;, \u0026quot;Student\u0026quot;, \u0026quot;Married\u0026quot;, \u0026quot;Ethnicity\u0026quot;] X = credit.loc[:, credit.columns != 'Balance'] X = pd.get_dummies(X, columns=categ_cols, drop_first=True).values y = credit.loc[:,'Balance'] n = len(credit) We are going to use 10-fold CV as cross-validation algorithm.\n# Get MSE def cv_lasso(X,y,a): # Init mse mse = [] # Generate splits kf10 = KFold(n_splits=10, random_state=None, shuffle=False) kf10.get_n_splits(X) # Loop over splits for train_index, test_index in kf10.split(X): X_train, X_test = X[train_index], X[test_index] y_train, y_test = y[train_index], y[test_index] lasso = Lasso(alpha=a).fit(X_train, y_train) y_hat = lasso.predict(X_test) mse.append(mean_squared_error(y_test, y_hat)) return np.mean(mse) # Compute MSE over grid of alphas n_grid = 30 alphas = 10**np.linspace(0,3,n_grid).reshape(-1,1) MSE = [cv_lasso(X,y,a) for a in alphas] What is the optimal $\\lambda$?\n# Find minimum alpha alpha_min = alphas[np.argmin(MSE)] print('Best alpha by 10fold CV:',alpha_min[0]) Best alpha by 10fold CV: 2.592943797404667 We now plot the objective function and the implied coefficients at the optimal $\\lambda$.\n# Get coefficients coefs = [] # Loop over values of alpha for a in alphas: lasso = Lasso(alpha=a).fit(scale(X), y) coefs.append(lasso.coef_) coefs = np.reshape(coefs,(n_grid,-1)) np.shape(coefs) (30, 11) # Figure 6.12 def make_figure_6_12(): # Init fig, (ax1,ax2) = plt.subplots(1,2) fig.suptitle('Figure 6.12: Lasso 10-fold CV') # MSE by LOO CV ax1.plot(alphas, MSE, alpha=1); ax1.axvline(alpha_min, c='k', ls='--') ax1.set_xscale('log') ax1.set_xlabel('lambda'); ax1.set_ylabel('MSE'); highlight = [0,1,2,7]; # Plot coefficients - absolute ax2.plot(alphas, coefs[:,highlight], alpha=1) ax2.plot(alphas, coefs, c='grey', alpha=0.3) ax2.axvline(alpha_min, c='k', ls='--') ax2.set_xscale('log') ax2.set_xlabel('lambda'); ax2.set_ylabel('Standardized coefficients'); ax2.legend(['Income', 'Limit', 'Rating', 'Student'], fontsize=10); make_figure_6_12() ","date":1646784000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1646784000,"objectID":"2f1f6f919e48a777cab5eae3dcdcbc48","permalink":"https://matteocourthoud.github.io/course/ml-econ/05_regularization/","publishdate":"2022-03-09T00:00:00Z","relpermalink":"/course/ml-econ/05_regularization/","section":"course","summary":"# Remove warnings import warnings warnings.filterwarnings('ignore') # Import import pandas as pd import numpy as np import time import itertools import statsmodels.api as sm import seaborn as sns from numpy.random import normal, uniform from itertools import combinations from statsmodels.","tags":null,"title":"Model Selection and Regularization","type":"book"},{"authors":null,"categories":null,"content":"The Gauss Markov Model Definition A statistical model for regression data is the Gauss Markov Model if each of its distributions satisfies the conditions\nLinearity: a statistical model $\\mathcal{F}$ over data $\\mathcal{D}$ satisfies linearity if for each element of $\\mathcal{F}$, the data can be decomposed in $$ \\begin{aligned} y_ i \u0026amp;= \\beta_ 1 x _ {i1} + \\dots + \\beta_ k x _ {ik} + \\varepsilon_ i = x_ i\u0026rsquo;\\beta + \\varepsilon_ i \\newline \\underset{n \\times 1}{\\vphantom{\\beta_ \\beta} y} \u0026amp;= \\underset{n \\times k}{\\vphantom{\\beta}X} \\cdot \\underset{k \\times 1}{\\beta} + \\underset{n \\times 1}{\\vphantom{\\beta}\\varepsilon} \\end{aligned} $$\nStrict Exogeneity: $\\mathbb E [\\varepsilon_i|x_1, \\dots, x_n] = 0, \\forall i$.\nNo Multicollinerity: $\\mathbb E_n [x_i x_i\u0026rsquo;]$ is strictly positive definite almost surely. Equivalent to require $rank(X)=k$ with probability $p \\to 1$. Intuition: no regressor is a linear combination of other regressors.\nSpherical Error Variance: -$\\mathbb E[\\varepsilon_i^2 | x] = \\sigma^2 \u0026gt; 0, \\ \\forall i$ -$\\mathbb E [\\varepsilon_i \\varepsilon_j |x ] = 0, \\ \\forall$ $1 \\leq i \u0026lt; j \\leq n$\nThe Extended Gauss Markov Model also satisfies assumption\nNormal error term: $\\varepsilon|X \\sim N(0, \\sigma^2 I_n)$ and $\\varepsilon \\perp X$. Implications Note that by (2) and (4) you get homoskedasticity: $$ Var(\\varepsilon_i|x) = \\mathbb E[\\varepsilon_i^2|x]- \\mathbb E[\\varepsilon_i|x]^2 = \\sigma^2 I \\qquad \\forall i $$\nStrict exogeneity is not restrictive since it is sufficient to include a constant in the regression to enforce it $$ y_i = \\alpha + x_i\u0026rsquo;\\beta + (\\varepsilon_i - \\alpha) \\quad \\Rightarrow \\quad \\mathbb E[\\varepsilon_i] = \\mathbb E_x [ \\mathbb E[ \\varepsilon_i | x]] = 0 $$ This implies $\\mathbb E[x _ {jk} \\varepsilon_i ] = 0$ by the LIE. These two conditions together imply $Cov (x _ {jk} \\varepsilon_i ) = 0$. Projection A map $\\Pi: V \\to V$ is a projection if $\\Pi \\circ \\Pi = \\Pi$.\nThe Gauss Markov Model assumes that the conditional expectation function (CEF) $f(X) = \\mathbb E[Y|X]$ and the linear projection $g(X) = X \\beta$ coincide.\nCode - DGP This code draws 100 observations from the model $y = 2 x_1 - x_2 + \\varepsilon$ where $x_1, x_2 \\sim U[0,1]$ and $\\varepsilon \\sim N(0,1)$.\n# Set seed Random.seed!(123); # Set the number of observations n = 100; # Set the dimension of X k = 2; # Draw a sample of explanatory variables X = rand(Uniform(0,1), n, k); # Draw the error term σ = 1; ε = rand(Normal(0,1), n, 1) * sqrt(σ); # Set the parameters β = [2; -1]; # Calculate the dependent variable y = X*β + ε; The OLS estimator Definition The sum of squared residuals (SSR) is given by $$ Q_n (\\beta) \\equiv \\frac{1}{n} \\sum _ {i=1}^n \\left( y_i - x_i\u0026rsquo;\\beta \\right)^2 = \\frac{1}{n} (y - X\\beta)\u0026rsquo; (y - X \\beta) $$\nConsider a dataset $\\mathcal{D}$ and define $Q_n(\\beta) = \\mathbb E_n[(y_i - x_i\u0026rsquo;\\beta )^2 ]$. Then the ordinary least squares (OLS) estimator $\\hat \\beta _ {OLS}$ is the value of $\\beta$ that minimizes $Q_n(\\beta)$.\nWhen we can write $D = (y, X)$ in matrix form, then $$ \\hat \\beta _ {OLS} = \\arg \\min_\\beta \\frac{1}{n} (y - X \\beta)\u0026rsquo; (y - X\\beta) $$\nDerivation Theorem\nUnder the assumption that $X$ has full rank, the OLS estimator is unique and it is determined by the normal equations. More explicitly, $\\hat \\beta$ is the OLS estimate precisely when $X\u0026rsquo;X \\hat \\beta = X\u0026rsquo;y$.\nProof\nTaking the FOC: $$ \\frac{\\partial Q_n (\\beta)}{\\partial \\beta} = -\\frac{2}{n} X\u0026rsquo; y + \\frac{2}{n} X\u0026rsquo;X\\beta = 0 \\quad \\Leftrightarrow \\quad X\u0026rsquo;X \\beta = X\u0026rsquo;y $$ Since $(X\u0026rsquo;X)^{-1}$ exists by assumption,\nFinally, $\\frac{\\partial^2 Q_n (\\beta)}{\\partial \\beta \\partial \\beta\u0026rsquo;} = X\u0026rsquo;X/n$ is positive definite since $X\u0026rsquo;X$ is positive semi-definite and $(X\u0026rsquo;X)^{-1}$ exists because $X$ is full rank. Therefore, $Q_n(\\beta)$ minimized at $\\hat \\beta_n$. $$\\tag*{$\\blacksquare$}$$\nThe $k$ equations $X\u0026rsquo;X \\hat \\beta = X\u0026rsquo;y$ are called normal equations.\nFuther Objects Fitted coefficient: $\\hat \\beta _ {OLS} = (X\u0026rsquo;X)^{-1} X\u0026rsquo;y = \\mathbb E_n [x_i x_i\u0026rsquo;] \\mathbb E_n [x_i y_i]$ Fitted residual: $\\hat \\varepsilon_i = y_i - x_i\u0026rsquo;\\hat \\beta$ Fitted value: $\\hat y_i = x_i\u0026rsquo; \\hat \\beta$ Predicted coefficient: $\\hat \\beta _ {-i} = \\mathbb E_n [x _ {-i} x\u0026rsquo; _ {-i}] \\mathbb E_n [x _ {-i} y _ {-i}]$ Prediction error: $\\hat \\varepsilon _ {-i} = y_i - x_i\u0026rsquo;\\hat \\beta _ {-i}$ Predicted value: $\\hat y_i = x_i\u0026rsquo; \\hat \\beta _ {-i}$ Notes on Orthogonality Conditions The normal equations are equivalent to the moment condition $\\mathbb E_n [x_i \\varepsilon_i]= 0$. The algebraic result $\\mathbb E_n [x_i \\hat \\varepsilon_i]= 0$ is called ortogonality property of the OLS residual $\\hat \\varepsilon_i$. If we have included a constant in the regression, $\\mathbb E_n [\\hat \\varepsilon_i] = 0$. $\\mathbb E \\Big[\\mathbb E_n [x_i \\varepsilon_i ] \\Big] = 0$ by strict exogeneity (assumed in GM), but $\\mathbb E_n [x_i \\varepsilon_i] \\ne \\mathbb E [x_i \\varepsilon_i] = 0$. This is why $\\hat \\beta _ {OLS}$ is just an estimate of $\\beta_0$. Calculating OLS is like replacing the $j$ equations $\\mathbb E [x _ {ij} \\varepsilon_i] = 0$ $\\forall j$ with $\\mathbb E_n [x _ {ij} \\varepsilon_i] = 0$ $\\forall j$ and forcing them to hold (remindful of GMM). The Projection Matrix The projection matrix is given by $P = X(X\u0026rsquo;X)^{-1} X\u0026rsquo;$. It has the following properties: - $PX = X$ - $P \\hat \\varepsilon = 0 \\quad$ ($P$, $\\varepsilon$ orthogonal) - $P y = X(X\u0026rsquo;X)^{-1} X\u0026rsquo;y = X\\hat \\beta = \\hat y$ - Symmetric: $P=P\u0026rsquo;$, Idempotent: $PP = P$ - $tr(P) = tr( X(X\u0026rsquo;X)^{-1} X\u0026rsquo;) = tr( X\u0026rsquo;X(X\u0026rsquo;X)^{-1}) = tr(I_k) = k$ - Its diagonal elements are $h_{ii} = x_i (X\u0026rsquo;X)^{-1} x_i\u0026rsquo;$ and are called leverage.\n$h _ {ii} \\in [0,1]$ is a normalized length of the observed regressor vector $x_i$. In the OLS regression framework it captures the relative influence of observation $i$ on the estimated coefficient. Note that $\\sum _ n h_{ii} = k$.\nThe Annihilator Matrix The annihilator matrix is given by $M = I_n - P$. It has the following properties: - $MX = 0 \\quad$ ($M$, $X$ orthogonal) - $M \\hat \\varepsilon = \\hat \\varepsilon$ - $M y = \\hat \\varepsilon$ - Symmetric: $M=M\u0026rsquo;$, idempotent: $MM = M$ - $tr(M) = n - k$ - Its diagonal elements are $1 - h_{ii} \\in [0,1]$\nThen we can equivalently write $\\hat y$ (defined by stacking $\\hat y_i$ into a vector) as $\\hat y = Py$.\nEstimating Beta # Estimate beta β_hat = inv(X'*X)*(X'*y) ## 2×1 Array{Float64,2}: ## 1.8821600407711814 ## -0.9429354944506099 # Equivalent but faster formulation β_hat = (X'*X)\\(X'*y) ## 2×1 Array{Float64,2}: ## 1.8821600407711816 ## -0.9429354944506098 # Even faster (but less intuitive) formulation β_hat = X\\y ## 2×1 Array{Float64,2}: ## 1.8821600407711807 ## -0.9429354944506088 Equivalent Formulation? Generally it’s not true that $$ \\hat \\beta_{OLS} = \\frac{Var(X)}{Cov(X,y)} $$\n# Wrong formulation β_wrong = inv(cov(X)) * cov(X, y) ## 2×1 Array{Float64,2}: ## 1.8490257777704475 ## -0.9709213554007003 Equivalent Formulation (correct) But it’s true if you include a constant, $\\alpha$ $$ y = \\alpha + X \\beta + \\varepsilon $$\n# Correct, with constant α = 3; y1 = α .+ X*β + ε; β_hat1 = [ones(n,1) X] \\ y1 ## 3×1 Array{Float64,2}: ## 3.0362313477745615 ## 1.8490257777704477 ## -0.9709213554007007 β_correct1 = inv(cov(X)) * cov(X, y1) ## 2×1 Array{Float64,2}: ## 1.8490257777704477 ## -0.9709213554007006 Some More Objects # Predicted y y_hat = X*β_hat; # Residuals ε_hat = y - X*β_hat; # Projection matrix P = X * inv(X'*X) * X'; # Annihilator matrix M = I - P; # Leverage h = diag(P); OLS Residuals Homoskedasticity The error is homoskedastic if $\\mathbb E [\\varepsilon^2 | x] = \\sigma^2$ does not depend on $x$. $$ Var(\\varepsilon) = I \\sigma^2 = \\begin{bmatrix} \\sigma^2 \u0026amp; \\dots \u0026amp; 0 \\newline\\newline\n\\vdots \u0026amp; \\ddots \u0026amp; \\vdots \\newline 0 \u0026amp; \\dots \u0026amp; \\sigma^2 \\end{bmatrix} $$\nThe error is heteroskedastic if $\\mathbb E [\\varepsilon^2 | x] = \\sigma^2(x)$ does depend on $x$. $$ Var(\\varepsilon) = I \\sigma_i^2 = \\begin{bmatrix} \\sigma_1^2 \u0026amp; \\dots \u0026amp; 0 \\newline \\vdots \u0026amp; \\ddots \u0026amp; \\vdots \\newline 0 \u0026amp; \\dots \u0026amp; \\sigma_n^2 \\end{bmatrix} $$\nResidual Variance The OLS residual variance can be an object of interest even in a heteroskedastic regression. Its method of moments estimator is given by $$ \\hat \\sigma^2 = \\frac{1}{n} \\sum _ {i=1}^n \\hat \\varepsilon_i^2 $$\nNote that $\\hat \\sigma^2$ can be rewritten as $$ \\hat \\sigma^2 = \\frac{1}{n} \\varepsilon\u0026rsquo; M\u0026rsquo; M \\varepsilon = \\frac{1}{n} tr(\\varepsilon\u0026rsquo; M \\varepsilon) = \\frac{1}{n} tr(M \\varepsilon\u0026rsquo; \\varepsilon) $$\nHowever, the method of moments estimator is a biesed estimator. In fact $$ \\mathbb E[\\hat \\sigma^2 | X] = \\frac{1}{n} \\mathbb E [ tr(M \\varepsilon\u0026rsquo; \\varepsilon) | X] = \\frac{1}{n} tr( M\\mathbb E[\\varepsilon\u0026rsquo; \\varepsilon |X]) = \\frac{1}{n} \\sum _ {i=1}^n (1-h_{ii}) \\sigma^2_i $$\nUnder conditional homoskedasticity, the above expression simplifies to $$ \\mathbb E[\\hat \\sigma^2 | X] = \\frac{1}{n} tr(M) \\sigma^2 = \\frac{n-k}{n} \\sigma^2 $$\nSample Variance The OLS residual sample variance is denoted by $s^2$ and is given by $$ s^2 = \\frac{SSR}{n-k} = \\frac{\\hat \\varepsilon\u0026rsquo;\\hat \\varepsilon}{n-k} = \\frac{1}{n-k}\\sum _ {i=1}^n \\hat \\varepsilon_i^2 $$ Furthermore, the square root of $s^2$, denoted $s$, is called the standard error of the regression (SER) or the standard error of the equation (SEE). Not to be confused with other notions of standard error to be defined later in the course.\nThe sum of squared residuals can be rewritten as: $SSR = \\hat \\varepsilon\u0026rsquo; \\hat \\varepsilon = \\varepsilon\u0026rsquo; M \\varepsilon$.\nThe OLS residual sample variance is an unbiased estimator of the error variance $\\sigma^2$.\nAnother unbiased estimator of $\\sigma^2$ is given by $$ \\bar \\sigma^2 = \\frac{1}{n} \\sum _ {i=1}^n (1-h_{ii})^{-1} \\hat \\varepsilon_i^2 $$\nUncentered R^2 One measure of the variability of the dependent variable $y_i$ is the sum of squares $\\sum _ {i=1}^n y_i^2 = y\u0026rsquo;y$. There is a decomposition: $$ \\begin{aligned} y\u0026rsquo;y \u0026amp;= (\\hat y + e)\u0026rsquo; (\\hat y + \\hat \\varepsilon) \\newline \u0026amp;= \\hat y\u0026rsquo; \\hat y + 2 \\hat y\u0026rsquo; \\hat \\varepsilon + \\hat \\varepsilon\u0026rsquo; \\hat \\varepsilon e \\newline \u0026amp;= \\hat y\u0026rsquo; \\hat y + 2 b\u0026rsquo;X\u0026rsquo;\\hat \\varepsilon + \\hat \\varepsilon\u0026rsquo; \\hat \\varepsilon \\ \\ (\\text{since} \\ \\hat y = Xb) \\newline \u0026amp;= \\hat y\u0026rsquo; \\hat y + \\hat \\varepsilon\u0026rsquo;\\hat \\varepsilon \\ \\ (\\text{since} \\ X\u0026rsquo;\\hat \\varepsilon =0) \\end{aligned} $$\nThe uncentered $\\mathbf{R^2}$ is defined as: $$ R^2 _ {uc} \\equiv 1 - \\frac{\\hat \\varepsilon\u0026rsquo;\\hat \\varepsilon}{y\u0026rsquo;y} = 1 - \\frac{\\mathbb E_n[\\hat \\varepsilon_i^2]}{\\mathbb E_n[y_i^2]} = \\frac{ \\mathbb E [\\hat y_i^2]}{ \\mathbb E [y_i^2]} $$\nCentered R^2 A more natural measure of variability is the sum of centered squares $\\sum _ {i=1}^n (y_i - \\bar y)^2,$ where $\\bar y := \\frac{1}{n}\\sum _ {i=1}^n y_i$. If the regressors include a constant, it can be decomposed as $$ \\sum _ {i=1}^n (y_i - \\bar y)^2 = \\sum _ {i=1}^n (\\hat y_i - \\bar y)^2 + \\sum _ {i=1}^n \\hat \\varepsilon_i^2 $$\nThe coefficient of determination, $\\mathbf{R^2}$, is defined as $$ R^2 \\equiv 1 - \\frac{\\sum _ {i=1}^n \\hat \\varepsilon_i^2}{\\sum _ {i=1}^n (y_i - \\bar y)^2 }= \\frac{ \\sum _ {i=1}^n (\\hat y_i - \\bar y)^2 } { \\sum _ {i=1}^n (y_i - \\bar y)^2} = \\frac{\\mathbb E_n[(\\hat y_i - \\bar y)^2]}{\\mathbb E_n[(y_i - \\bar y)^2]} $$\nAlways use the centered $R^2$ unless you really know what you are doing.\nCode - Variance # Biased variance estimator σ_hat = ε_hat'*ε_hat / n; # Unbiased estimator 1 σ_hat_2 = ε_hat'*ε_hat / (n-k); # Unbiased estimator 2 σ_hat_3 = mean( ε_hat.^2 ./ (1 .- h) ); Code - R^2 # R squared - uncentered R2_uc = (y_hat'*y_hat)/ (y'*y); # R squared y_bar = mean(y); R2 = ((y_hat .- y_bar)'*(y_hat .- y_bar))/ ((y .- y_bar)'*(y .- y_bar)); Finite Sample Properties of OLS Conditional Unbiasedness Theorem\nUnder the GM assumptions (1)-(3), the OLS estimator is conditionally unbiased, i.e. the distribution of $\\hat \\beta _ {OLS}$ is centered at $\\beta_0$: $\\mathbb E [\\hat \\beta | X] = \\beta_0$.\nProof $$ \\begin{aligned} \\mathbb E [\\hat \\beta | X] \u0026amp;= \\mathbb E [ (X\u0026rsquo;X)^{-1} X\u0026rsquo;y | X] = \\newline \u0026amp;= (X\u0026rsquo;X)^{-1} X \u0026rsquo; \\mathbb E [y | X] = \\newline \u0026amp;= (X\u0026rsquo;X)^{-1} X\u0026rsquo; \\mathbb E [X \\beta + \\varepsilon | X] = \\newline \u0026amp;= (X\u0026rsquo;X)^{-1} X\u0026rsquo;X \\beta + (X\u0026rsquo;X)^{-1} X\u0026rsquo; \\mathbb E [\\varepsilon | X] = \\newline \u0026amp;= \\beta \\end{aligned} $$ $$\\tag*{$\\blacksquare$}$$\nOLS Variance Theorem\nUnder the GM assumptions (1)-(3), $Var(\\hat \\beta |X) = \\sigma^2 (X\u0026rsquo;X)^{-1}$.\nProof: $$ \\begin{aligned} Var(\\hat \\beta |X) \u0026amp;= Var( (X\u0026rsquo;X)^{-1} X\u0026rsquo;y|X) = \\newline \u0026amp;= ((X\u0026rsquo;X)^{-1} X\u0026rsquo; ) Var(y|X) ((X\u0026rsquo;X)^{-1} X\u0026rsquo; )\u0026rsquo; = \\newline \u0026amp;= ((X\u0026rsquo;X)^{-1} X\u0026rsquo; ) Var(X\\beta + \\varepsilon|X) ((X\u0026rsquo;X)^{-1} X\u0026rsquo; )\u0026rsquo; = \\newline \u0026amp;= ((X\u0026rsquo;X)^{-1} X\u0026rsquo; ) Var(\\varepsilon|X) ((X\u0026rsquo;X)^{-1} X\u0026rsquo; )\u0026rsquo; = \\newline \u0026amp;= ((X\u0026rsquo;X)^{-1} X\u0026rsquo; ) \\sigma^2 I ((X\u0026rsquo;X)^{-1} X\u0026rsquo; )\u0026rsquo; = \\newline \u0026amp;= \\sigma^2 (X\u0026rsquo;X)^{-1} \\end{aligned} $$ $$\\tag*{$\\blacksquare$}$$\nHigher correlation of the $X$ implies higher variance of the OLS estimator.\nIntuition: individual observations carry less information. You are exploring a smaller region of the $X$ space.\nBLUE Theorem\nUnder the GM assumptions (1)-(3), $Cov (\\hat \\beta, \\hat \\varepsilon ) = 0$.\nTheorem\nUnder the GM assumptions (1)-(3), $\\hat \\beta _ {OLS}$ is the best (most efficient) linear, unbiased estimator (BLUE), i.e., for any unbiased linear estimator $b$: $Var (b|X) \\geq Var (\\hat \\beta |X)$.\nBLUE Proof Consider four steps:\nDefine three objects: (i) $b= Cy$, (ii) $A = (X\u0026rsquo;X)^{-1} X\u0026rsquo;$ such that $\\hat \\beta = A y$, and (iii) $D = C-A$. Decompose $b$ as $$ \\begin{aligned} b \u0026amp;= (D + A) y = \\newline \u0026amp;= Dy + Ay = \\newline\n\u0026amp;= D (X\\beta + \\varepsilon) + \\hat \\beta = \\newline \u0026amp;= DX\\beta + D \\varepsilon + \\hat \\beta \\end{aligned} $$ By assumption, $b$ must be unbiased: $$ \\begin{aligned} \\mathbb E [b|X] \u0026amp;= \\mathbb E [D(X\\beta + \\varepsilon) + Ay |X] = \\newline \u0026amp;= \\mathbb E [DX\\beta|X] + \\mathbb E [D\\varepsilon |X] + \\mathbb E [\\hat \\beta |X] = \\newline \u0026amp;= DX\\beta + D \\mathbb E [\\varepsilon |X] +\\beta \\newline\n\u0026amp;= DX\\beta + \\beta \\end{aligned} $$ Hence, it must be that $DX = 0$ BLUE Proof (2) We know by (2)-(3) that $b = D \\varepsilon + \\hat \\beta$. We can now calculate its variance. $$ \\begin{aligned} Var (b|X) \u0026amp;= Var (\\hat \\beta + D\\varepsilon|X) = \\newline \u0026amp;= Var (Ay + D\\varepsilon|X) = \\newline \u0026amp;= Var (AX\\beta + (D + A)\\varepsilon|X) = \\newline \u0026amp;= Var((D+A)\\varepsilon |X) = \\newline \u0026amp;= (D+A)\\sigma^2 I (D+A)\u0026rsquo; = \\newline \u0026amp;= \\sigma^2 I (DD\u0026rsquo; + AA\u0026rsquo; + DA\u0026rsquo; + AD\u0026rsquo;) = \\newline \u0026amp;= \\sigma^2 I (DD\u0026rsquo; + AA\u0026rsquo;) \\geq \\newline \u0026amp;\\geq \\sigma^2 AA\u0026rsquo;= \\newline \u0026amp;= \\sigma^2 (X\u0026rsquo;X)^{-1} = \\newline \u0026amp;= Var (\\hat \\beta|X) \\end{aligned} $$ since $DA\u0026rsquo;= AD\u0026rsquo; = 0$, $DX = 0$ and $AA\u0026rsquo; = (X\u0026rsquo;X)^{-1}$. $$\\tag*{$\\blacksquare$}$$ $Var(b | X) \\geq Var (\\hat{\\beta} | X)$ is meant in a positive definite sense.\nCode - Variance # Ideal variance of the OLS estimator var_β = σ * inv(X'*X) ## 2×2 Array{Float64,2}: ## 0.0609402 -0.0467732 ## -0.0467732 0.0656808 # Standard errors std_β = sqrt.(diag(var_β)) ## 2-element Array{Float64,1}: ## 0.24686077212177054 ## 0.25628257446345265 ","date":1635465600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1635465600,"objectID":"ff820c54188a6875670d383b4784b4c5","permalink":"https://matteocourthoud.github.io/course/metrics/05_ols_algebra/","publishdate":"2021-10-29T00:00:00Z","relpermalink":"/course/metrics/05_ols_algebra/","section":"course","summary":"The Gauss Markov Model Definition A statistical model for regression data is the Gauss Markov Model if each of its distributions satisfies the conditions\nLinearity: a statistical model $\\mathcal{F}$ over data $\\mathcal{D}$ satisfies linearity if for each element of $\\mathcal{F}$, the data can be decomposed in $$ \\begin{aligned} y_ i \u0026amp;= \\beta_ 1 x _ {i1} + \\dots + \\beta_ k x _ {ik} + \\varepsilon_ i = x_ i\u0026rsquo;\\beta + \\varepsilon_ i \\newline \\underset{n \\times 1}{\\vphantom{\\beta_ \\beta} y} \u0026amp;= \\underset{n \\times k}{\\vphantom{\\beta}X} \\cdot \\underset{k \\times 1}{\\beta} + \\underset{n \\times 1}{\\vphantom{\\beta}\\varepsilon} \\end{aligned} $$","tags":null,"title":"OLS Algebra","type":"book"},{"authors":null,"categories":null,"content":"In this notebook, we are going to build a pipeline for a general prediction problem.\n# Standard Imports from src.utils import * from src.get_feature_names import get_feature_names # Set inline graphs plt.style.use('seaborn') %matplotlib inline %config InlineBackend.figure_format = 'retina' Introduction Usually, in machine learning prediction tasks, the data consists in 3 files:\nX_train.csv y_train.csv X_test.csv The purpose of the exercise is to produce a y_test.csv file, with the predicted values corresponding to the X_test.csv observations.\nThe functions we will write are going to be general and will adapt to any type of dataset, and we will test them on the House Prices Dataset which is a standard dataset for these kind of tasks. The data consists of 2 files:\ntrain.csv test.csv The target variable that we want to predict is SalePrice.\nSetup First we want to import the data.\n# Import data df_train = pd.read_csv(\u0026quot;data/train.csv\u0026quot;) df_test = pd.read_csv(\u0026quot;data/test.csv\u0026quot;) print(f\u0026quot;Training data: {np.shape(df_train)} \\n Testing data: {np.shape(df_test)}\u0026quot;) Training data: (1460, 81) Testing data: (1459, 80) The training data also includes the target variable SalePrice, while, as usual, the testing data does not. We need to separate the training data into two parts:\nX: the features y: the target # Select the features X_train = df_train.drop(['SalePrice'], axis=1) X_test = df_test # Check size print(f\u0026quot;Training features: {np.shape(X_train)} \\n Testing features: {np.shape(X_test)}\u0026quot;) Training features: (1460, 80) Testing features: (1459, 80) # Select the target y_train = df_train['SalePrice'] # Check size print(f\u0026quot;Training target: {np.shape(y_train)}\u0026quot;) Training target: (1460,) It\u0026rsquo;s good practice to immediately set aside a validation sample with 20% of the observations. The purpose of the validation sample is to give us unbiased estimate of the prediction score. Therefore, we want to set it aside as soon as possible, not to be conditioned in any way by it. Possibly, set it away even before data exploration.\nThe more we tune the algorithm based on the feedback received from the validation sample, the more biased our estimate is going to be. Ideally, one would use only cross-validation on the training data and tune only a couple of times using the validation data.\n# Set aside the validation sample X_train, X_validation, y_train, y_validation = train_test_split(X_train, y_train, test_size=0.2) Now we are ready to build and test our pipeline.\nData Exploration First, let\u0026rsquo;s have a quick look at the data.\nX_train.head() Id MSSubClass MSZoning LotFrontage LotArea Street Alley LotShape LandContour Utilities ... ScreenPorch PoolArea PoolQC Fence MiscFeature MiscVal MoSold YrSold SaleType SaleCondition 822 823 60 RL NaN 12394 Pave NaN IR1 Lvl AllPub ... 0 0 NaN NaN NaN 0 10 2007 WD Family 648 649 60 RL 70.0 7700 Pave NaN Reg Lvl AllPub ... 0 0 NaN NaN NaN 0 6 2010 WD Normal 897 898 90 RL 64.0 7018 Pave NaN Reg Lvl AllPub ... 0 0 NaN NaN NaN 0 6 2009 WD Alloca 1131 1132 20 RL 63.0 10712 Pave NaN Reg Lvl AllPub ... 0 0 NaN MnPrv NaN 0 9 2007 Oth Abnorml 1038 1039 160 RM 21.0 1533 Pave NaN Reg Lvl AllPub ... 0 0 NaN NaN NaN 0 5 2009 WD Normal 5 rows × 80 columns\nThe Id column is clearly not useful for prediction, let\u0026rsquo;s drop it from both datasets.\n# Drop Id X_train.drop([\u0026quot;Id\u0026quot;], axis=1, inplace=True) X_test.drop([\u0026quot;Id\u0026quot;], axis=1, inplace=True) Now we want to identify categorical and numerical variables.\n# Save column types numerical_cols = list(X_train.describe().columns) categorical_cols = list(X_train.describe(include=object).columns) print(\u0026quot;There are %i numerical and %i categorical variables\u0026quot; % (len(numerical_cols), len(categorical_cols))) There are 36 numerical and 43 categorical variables Let\u0026rsquo;s start by analyzing the numerical variables.\nX_numerical = X_train.loc[:, numerical_cols] corr = X_numerical.corr() fig, ax = plt.subplots(1, 1, figsize=(10,10)) fig.suptitle(\u0026quot;Correlation between categorical variables\u0026quot;, fontsize=16) cbar_ax = fig.add_axes([.95, .12, .05, .76]) sns.heatmap(corr, vmin=-1, vmax=1, center=0, cmap=sns.diverging_palette(20, 220, n=20), square=True, ax=ax, cbar_ax = cbar_ax) plt.show() For the non/numeric columns, we need a further option.\nunique_values = X_train.describe(include=object).T.unique # Plot fig, ax = plt.subplots(1, 1, figsize=(10,6)) fig.suptitle(\u0026quot;Distribution of unique values for categorical variables\u0026quot;, fontsize=16) sns.histplot(data=unique_values) plt.show(); Let\u0026rsquo;s save the identity of the numerical and categorical columns.\n# Save column types numerical_cols = list(X_train.describe().columns) categorical_cols = list(X_train.describe(include=object).columns) How many missing values are there in the dataset?\nmissing_values = X_train.isnull().sum().sort_values(ascending=True)[-20:] / len(X_train) fig, ax = plt.subplots(figsize=(10,8)) ax.set_title(\u0026quot;Variables with most missing values\u0026quot;, fontsize=16) ax.barh(np.arange(len(missing_values)), missing_values) ax.set_yticks(np.arange(len(missing_values))) ax.set_yticklabels(missing_values.index) ax.set_xlabel('Percentage of missig values') plt.show() Around 10% of each feature is missing. We will have to deal with that.\nPre-processing First, let\u0026rsquo;s process numerical variables. We want to do two things:\ninpute missing values standardize all variables Which imputer should to use? It depends on the type of missing data:\nMissing absolutely at random: as the name says, in this case we believe that missing values are distributed uniformly at random, independently across variables.\nIn this case, the only information on missing values comes from the distribution of non-missing values of the same variable. No information on missing values is contained in other variables. Missing at random: in this case, missing values are random, conditional on values of other observed variables.\nIn this case, information in other variables might help filling missing values. Missing non at random: in this last case, missing values depend on information that we do not observe.\nThis is the most tricky category of missing values since data alone does not tell us which values might be missing. For example, we might have that older women might be less likely to report the age. If we consider the data missing at random (absolutely or not), we would underestimate the missing ages. External information such as the sample population might help. For example, we could estimate the probability of not reporting the age and fill the missing values with the expected age, conditional on age not being reported. So, which imputers are readily available in sklearn for numerical data?\nFor data missing absolutely at random, there is one standard sklearn library: SimpleImputer(). It allows different strategy options such as\n\u0026quot;mean\u0026quot; \u0026quot;median\u0026quot; \u0026quot;most_frequent\u0026quot; For data missing at random, there are multiple sklearn libraries:\nKNNImputer(): uses KNN IterativeImputer(): uses a variety of ML algorithms see comparison here After we have inputed missing values, we want to standardize numerical variables to make the algorithm more efficient and robust to outliers.\nThe two main options for standardization are:\nStandardScaler(): which normalizes each variable to mean zero and unit variance MinMaxScaler(): which normalizes each variable to an interval between zero an one # Inputer for numerical variables num = Pipeline(steps=[ ('ii', IterativeImputer()), ('ss', StandardScaler()) ]) For categorical variables, we do not have to worry about scaling. However, we still need to impute missing values and, crucially, we need to transform them into numerical variables. This process is called encoding.\nWhich imputer should to use?\nFor data missing absolutely at random, the only available strategy option for SimpleImputer() is\n\u0026quot;most_frequent\u0026quot; For data missing at random, we can still use both\nKNNImputer() IterativeImputer() For encoding categorical variables, the standard option is OneHotEncoder() which generates unique binary variables out of every values of the categorical variable.\n# One Hot Encoder for categorical data cat = Pipeline(steps=[ ('si', SimpleImputer(strategy=\u0026quot;most_frequent\u0026quot;)), ('ohe', OneHotEncoder(handle_unknown=\u0026quot;ignore\u0026quot;)), ]) # Preprocess column transformer for preprocessing data preprocess = ColumnTransformer( transformers=[ ('num', num, numerical_cols), ('cat', cat, categorical_cols), ]) Information and components How much information is contained in our dataset? It is a dense or sparse dataset?\nX_clean = num.fit_transform(X_numerical) pca = PCA().fit(X_clean) explained_variance = pca.explained_variance_ratio_ fig, (ax1, ax2) = plt.subplots(1,2, figsize=(15,6)) fig.suptitle('Principal Component Analysis', fontsize=16); # Relative ax1.plot(range(len(explained_variance)), explained_variance) ax1.set_ylabel('Prop. Variance Explained') ax1.set_xlabel('Principal Component'); # Cumulative ax2.plot(range(len(explained_variance)), np.cumsum(explained_variance)) ax2.set_ylabel('Cumulative Variance Explained'); ax2.set_xlabel('Principal Component'); Feature Importance Before starting our prediction analysis, we would like to understand which variables are most important for our prediction problem.\ndef plot_featureimportance(importance, preprocess): df = pd.DataFrame({\u0026quot;names\u0026quot;: get_feature_names(preprocess), \u0026quot;values\u0026quot;: importance}) df = df.sort_values(\u0026quot;values\u0026quot;).iloc[:20, :] # plot fig, ax = plt.subplots(figsize=(10,8)) ax.set_title(\u0026quot;Feature importance\u0026quot;, fontsize=16) sns.barplot(y=\u0026quot;names\u0026quot;, x=\u0026quot;values\u0026quot;, data=df) ax.barh(np.arange(len(df)), df[\u0026quot;values\u0026quot;]) plt.show() We start with linear regression feature importance: we standardize all variables to be mean vero and unit variance, and we run a linear regression over the test set.\ndef featureimportance_lr(X, y): X_clean = preprocess.fit_transform(X) # fit the model model = LinearRegression() model.fit(X_clean, y) # get importance importance = np.abs(model.coef_) plot_featureimportance(importance, preprocess) # Plot linear feature importance featureimportance_lr(X_train, y_train) We now look at regression tree feature importance.\ndef featureimportance_forest(X, y): X_clean = preprocess.fit_transform(X) # fit the model model = RandomForestRegressor() model.fit(X_clean, y) # get importance importance = model.feature_importances_ plot_featureimportance(importance, preprocess) # Plot tree feature importance featureimportance_forest(X_train, y_train) Weighting Another important check to perform concerns weighting. Is the distribution of our objective variable the same in the training and in the test sample? If it is not the case, we might get a poor performance just because our training sample is not representative of our testing sample.\nThis is something that usually we cannot test, since we do not have access to the distribution of the target variable in the test data. However, we might be given the information ex-ante as a warning.\nIn this case, we perform the analysis on the validation set. Since we have selected the validation set at random, we do not expect significant differences.\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15,6)) # Plot 1 sns.histplot(data=y_train, kde=True, ax=ax1) sns.histplot(data=y_validation, kde=True, ax=ax1, color='orange') ax1.set_title(\u0026quot;Density Function of y\u0026quot;, fontsize=16); ax1.legend(['y train', 'y validation']) # Plot 2 sns.histplot(data=y_train, element=\u0026quot;step\u0026quot;, fill=False, cumulative=True, stat=\u0026quot;density\u0026quot;, common_norm=False, ax=ax2) sns.histplot(data=y_validation, element=\u0026quot;step\u0026quot;, fill=False, cumulative=True, stat=\u0026quot;density\u0026quot;, common_norm=False, ax=ax2, color='orange') ax2.set_title(\u0026quot;Cumulative Distribution of y\u0026quot;, fontsize=16); ax2.legend(['y train', 'y validation']); Since the size of the test sample is smaller than the size of the training sample, the two densities are different. However, the distributions indicate that the standardized distributions are the same.\nModel There are many models to choose among.\n# prepare models models = {\u0026quot;Lasso\u0026quot;: Lasso(alpha=100), \u0026quot;Ridge\u0026quot;: BayesianRidge(), \u0026quot;KNN\u0026quot;: KNeighborsRegressor(), \u0026quot;Kernel\u0026quot;: KernelRidge(), \u0026quot;Naive\u0026quot;: GaussianNB(), \u0026quot;SVM\u0026quot;: SVR(), \u0026quot;Ada\u0026quot;: AdaBoostRegressor(), \u0026quot;Tree\u0026quot;: DecisionTreeRegressor(), \u0026quot;Forest\u0026quot;: RandomForestRegressor(), \u0026quot;GBoost\u0026quot;: GradientBoostingRegressor(), \u0026quot;XGBoost\u0026quot;: XGBRegressor(), \u0026quot;LGBoost\u0026quot;: LGBMRegressor()} def evaluate_model(model, name, X, y, cv, scoring): X_clean = preprocess.fit_transform(X) start = time.perf_counter() cv_results = cross_val_score(model, X_clean, y, cv=cv, scoring=scoring) t = time.perf_counter()-start score = {\u0026quot;model\u0026quot;:name, \u0026quot;mean\u0026quot;:-np.mean(cv_results), \u0026quot;std\u0026quot;:np.std(cv_results), \u0026quot;time\u0026quot;:t} print(\u0026quot;%s: %f (%f) in %f seconds\u0026quot; % (name, -np.mean(cv_results), np.std(cv_results), t)) return score def plot_model_scores(scores): fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15,6)) fig.suptitle(\u0026quot;Comparing algorithms\u0026quot;, fontsize=16) # Plot 1 scores.sort_values(\u0026quot;mean\u0026quot;, ascending=False, inplace=True) ax1.set_title(\u0026quot;Mean squared error\u0026quot;, fontsize=16) ax1.barh(range(len(scores)), scores[\u0026quot;mean\u0026quot;], xerr=scores[\u0026quot;std\u0026quot;]) ax1.set_yticks(range(len(scores))) ax1.set_yticklabels([s for s in scores[\u0026quot;model\u0026quot;]]) # Plot 2 scores.sort_values(\u0026quot;time\u0026quot;, ascending=False, inplace=True) ax2.set_title(\u0026quot;Time\u0026quot;, fontsize=16) ax2.barh(range(len(scores)), scores[\u0026quot;time\u0026quot;], color='tab:orange') ax2.set_yticks(range(len(scores))) ax2.set_yticklabels([s for s in scores[\u0026quot;model\u0026quot;]]) plt.show() def compare_models(models): scores = pd.DataFrame() cv = KFold(n_splits=5) scoring = 'neg_mean_squared_error' for name, model in models.items(): score = evaluate_model(model, name, X_validation, y_validation, cv, scoring) scores = scores.append(score, ignore_index=True) return scores scores = compare_models(models) Lasso: 747411443.913101 (462917309.181485) in 0.109821 seconds Ridge: 718774315.061634 (487089023.387329) in 0.472070 seconds KNN: 1756639001.600806 (1476470798.673143) in 0.019063 seconds Kernel: 844681295.934677 (476183041.447080) in 0.085055 seconds Naive: 5254835359.080946 (2916476370.114636) in 0.045415 seconds SVM: 6141030577.726756 (3241262535.954060) in 0.046852 seconds Ada: 1513638885.120911 (1332241015.479751) in 0.306255 seconds Tree: 3258264310.733547 (2139525308.773295) in 0.018476 seconds Forest: 1324403652.968275 (1246235286.003631) in 1.105161 seconds GBoost: 1200654655.518314 (1053677796.098979) in 0.494536 seconds XGBoost: 1819197282.034136 (1587393748.901112) in 0.692401 seconds LGBoost: 1318077152.379926 (1278188928.507894) in 0.157495 seconds plot_model_scores(scores) Pipeline We are now ready to pick a model.\n# Set model model = LGBMRegressor() We need to choose a cross-validation procedure to test our model.\ncv = KFold() Finally, we can combine all the parts into a single pipeline.\nfinal_pipeline = Pipeline(steps=[ ('preprocess', preprocess), ('model', model) ]) Now we can decide which parts of the pipeline to test.\n# Select parameters to explore param_grid = {'preprocess__num__ii': [SimpleImputer(), KNNImputer(), IterativeImputer()], 'preprocess__cat__si__strategy': [\u0026quot;most_frequent\u0026quot;, \u0026quot;constant\u0026quot;], 'model__learning_rate': [0.1, 0.2], 'model__subsample': [1.0, 0.5], 'model__max_depth': [30, -1]} We now generate a grid of parameters we want to search over.\n# Save pipeline grid_search = GridSearchCV(final_pipeline, param_grid, cv=cv, n_jobs=-1, scoring='neg_mean_squared_error', verbose=3) We fit the pipeline and pick the best estimator, from the cross-validation score.\n# Fit pipeline grid_search.fit(X_train, y_train) grid_search.best_estimator_ Fitting 5 folds for each of 48 candidates, totalling 240 fits Pipeline(steps=[('preprocess', ColumnTransformer(transformers=[('num', Pipeline(steps=[('ii', KNNImputer()), ('ss', StandardScaler())]), ['MSSubClass', 'LotFrontage', 'LotArea', 'OverallQual', 'OverallCond', 'YearBuilt', 'YearRemodAdd', 'MasVnrArea', 'BsmtFinSF1', 'BsmtFinSF2', 'BsmtUnfSF', 'TotalBsmtSF', '1stFlrSF', '2ndFlrSF', 'LowQualFinSF', 'GrLivArea', 'BsmtFullBat... 'LotConfig', 'LandSlope', 'Neighborhood', 'Condition1', 'Condition2', 'BldgType', 'HouseStyle', 'RoofStyle', 'RoofMatl', 'Exterior1st', 'Exterior2nd', 'MasVnrType', 'ExterQual', 'ExterCond', 'Foundation', 'BsmtQual', 'BsmtCond', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2', 'Heating', 'HeatingQC', 'CentralAir', 'Electrical', ...])])), ('model', LGBMRegressor(max_depth=30))]) We have three ways of testing the quality of fit of our model:\nscore on the training data score on the validation data score on the test data Score on the training data: this is a biased score since we have picked the model that was best fitting the training data. Kfold cross-validation is efficient in terms of data use, but still evaluates the model over the same data it was trained.\n# Cross/validation score y_train_hat = grid_search.best_estimator_.predict(X_train) train_rmse = mean_squared_error(y_train, y_train_hat, squared=False) print('RMSE on training data :', train_rmse) RMSE on training data : 12151.309344378069 Score on the validation data: this is an unbiased score since we have left out this sample exactly for this purpose. However, be aware that the validation score is unbiased on on the first run. Once we change the grid and pick the algorithm based on previous validation data scores, also this score becomes biased.\n# Validation set score y_validation_hat = grid_search.best_estimator_.predict(X_validation) validation_rmse = mean_squared_error(y_validation, y_validation_hat, squared=False) print('RMSE on validation data :', validation_rmse) RMSE on validation data : 27676.358798908263 Final predictions: we can now use our model to output the predictions.\n# Validation score y_test_hat = grid_search.best_estimator_.predict(X_test) [CV 2/5] END model__learning_rate=0.1, model__max_depth=30, model__subsample=1.0, preprocess__cat__si__strategy=most_frequent, preprocess__num__ii=KNNImputer();, score=-945800410.003 total time= 0.2s [CV 4/5] END model__learning_rate=0.1, model__max_depth=30, model__subsample=1.0, preprocess__cat__si__strategy=most_frequent, preprocess__num__ii=IterativeImputer();, score=-570100776.470 total time= 0.4s [CV 5/5] END model__learning_rate=0.1, model__max_depth=30, model__subsample=1.0, preprocess__cat__si__strategy=constant, preprocess__num__ii=KNNImputer();, score=-1239206018.769 total time= 0.2s [CV 2/5] END model__learning_rate=0.1, model__max_depth=30, model__subsample=0.5, preprocess__cat__si__strategy=most_frequent, preprocess__num__ii=SimpleImputer();, score=-997691407.890 total time= 0.1s [CV 5/5] END model__learning_rate=0.1, model__max_depth=30, model__subsample=0.5, preprocess__cat__si__strategy=most_frequent, preprocess__num__ii=SimpleImputer();, score=-1226710083.913 total time= 0.2s [CV 2/5] END model__learning_rate=0.1, model__max_depth=30, model__subsample=0.5, preprocess__cat__si__strategy=most_frequent, preprocess__num__ii=IterativeImputer();, score=-1024476538.628 total time= 0.4s [CV 3/5] END model__learning_rate=0.1, model__max_depth=30, model__subsample=0.5, preprocess__cat__si__strategy=constant, preprocess__num__ii=KNNImputer();, score=-541737588.393 total time= 0.2s [CV 3/5] END model__learning_rate=0.1, model__max_depth=-1, model__subsample=1.0, preprocess__cat__si__strategy=most_frequent, preprocess__num__ii=SimpleImputer();, score=-513492880.363 total time= 0.1s [CV 1/5] END model__learning_rate=0.1, model__max_depth=-1, model__subsample=1.0, preprocess__cat__si__strategy=most_frequent, preprocess__num__ii=KNNImputer();, score=-1085304295.641 total time= 0.2s [CV 2/5] END model__learning_rate=0.1, model__max_depth=-1, model__subsample=1.0, preprocess__cat__si__strategy=most_frequent, preprocess__num__ii=IterativeImputer();, score=-1024476538.628 total time= 0.4s [CV 2/5] END model__learning_rate=0.1, model__max_depth=-1, model__subsample=1.0, preprocess__cat__si__strategy=constant, preprocess__num__ii=KNNImputer();, score=-1033297772.352 total time= 0.2s [CV 5/5] END model__learning_rate=0.1, model__max_depth=-1, model__subsample=1.0, preprocess__cat__si__strategy=constant, preprocess__num__ii=IterativeImputer();, score=-1260432508.211 total time= 0.4s [CV 1/5] END model__learning_rate=0.1, model__max_depth=-1, model__subsample=0.5, preprocess__cat__si__strategy=most_frequent, preprocess__num__ii=IterativeImputer();, score=-1038744584.272 total time= 0.6s [CV 5/5] END model__learning_rate=0.1, model__max_depth=-1, model__subsample=0.5, preprocess__cat__si__strategy=constant, preprocess__num__ii=IterativeImputer();, score=-1260432508.211 total time= 0.4s [CV 2/5] END model__learning_rate=0.2, model__max_depth=30, model__subsample=1.0, preprocess__cat__si__strategy=most_frequent, preprocess__num__ii=IterativeImputer();, score=-1045009894.967 total time= 0.4s [CV 2/5] END model__learning_rate=0.2, model__max_depth=30, model__subsample=1.0, preprocess__cat__si__strategy=constant, preprocess__num__ii=KNNImputer();, score=-1064162610.680 total time= 0.2s [CV 4/5] END model__learning_rate=0.2, model__max_depth=30, model__subsample=1.0, preprocess__cat__si__strategy=constant, preprocess__num__ii=IterativeImputer();, score=-600837629.536 total time= 0.4s [CV 5/5] END model__learning_rate=0.2, model__max_depth=30, model__subsample=0.5, preprocess__cat__si__strategy=most_frequent, preprocess__num__ii=KNNImputer();, score=-1243376201.323 total time= 0.2s [CV 3/5] END model__learning_rate=0.2, model__max_depth=30, model__subsample=0.5, preprocess__cat__si__strategy=constant, preprocess__num__ii=SimpleImputer();, score=-539187722.319 total time= 0.2s [CV 1/5] END model__learning_rate=0.2, model__max_depth=30, model__subsample=0.5, preprocess__cat__si__strategy=constant, preprocess__num__ii=KNNImputer();, score=-1101298878.719 total time= 0.2s [CV 3/5] END model__learning_rate=0.2, model__max_depth=30, model__subsample=0.5, preprocess__cat__si__strategy=constant, preprocess__num__ii=IterativeImputer();, score=-533278666.873 total time= 0.4s [CV 4/5] END model__learning_rate=0.2, model__max_depth=-1, model__subsample=1.0, preprocess__cat__si__strategy=most_frequent, preprocess__num__ii=KNNImputer();, score=-544430870.856 total time= 0.2s [CV 2/5] END model__learning_rate=0.2, model__max_depth=-1, model__subsample=1.0, preprocess__cat__si__strategy=constant, preprocess__num__ii=SimpleImputer();, score=-1024576434.974 total time= 0.1s [CV 5/5] END model__learning_rate=0.2, model__max_depth=-1, model__subsample=1.0, preprocess__cat__si__strategy=constant, preprocess__num__ii=SimpleImputer();, score=-1271758627.677 total time= 0.2s [CV 1/5] END model__learning_rate=0.2, model__max_depth=-1, model__subsample=1.0, preprocess__cat__si__strategy=constant, preprocess__num__ii=IterativeImputer();, score=-1064924532.138 total time= 0.5s [CV 4/5] END model__learning_rate=0.2, model__max_depth=-1, model__subsample=0.5, preprocess__cat__si__strategy=most_frequent, preprocess__num__ii=IterativeImputer();, score=-643767514.223 total time= 0.4s [CV 4/5] END model__learning_rate=0.2, model__max_depth=-1, model__subsample=0.5, preprocess__cat__si__strategy=constant, preprocess__num__ii=KNNImputer();, score=-590633071.038 total time= 0.2s [CV 3/5] END model__learning_rate=0.1, model__max_depth=30, model__subsample=1.0, preprocess__cat__si__strategy=most_frequent, preprocess__num__ii=KNNImputer();, score=-515317030.231 total time= 0.2s [CV 1/5] END model__learning_rate=0.1, model__max_depth=30, model__subsample=1.0, preprocess__cat__si__strategy=constant, preprocess__num__ii=SimpleImputer();, score=-1102984449.306 total time= 0.1s [CV 4/5] END model__learning_rate=0.1, model__max_depth=30, model__subsample=1.0, preprocess__cat__si__strategy=constant, preprocess__num__ii=SimpleImputer();, score=-543635446.429 total time= 0.2s [CV 4/5] END model__learning_rate=0.1, model__max_depth=30, model__subsample=1.0, preprocess__cat__si__strategy=constant, preprocess__num__ii=KNNImputer();, score=-513174790.496 total time= 0.2s [CV 1/5] END model__learning_rate=0.1, model__max_depth=30, model__subsample=0.5, preprocess__cat__si__strategy=most_frequent, preprocess__num__ii=SimpleImputer();, score=-1094302605.510 total time= 0.2s [CV 4/5] END model__learning_rate=0.1, model__max_depth=30, model__subsample=0.5, preprocess__cat__si__strategy=most_frequent, preprocess__num__ii=SimpleImputer();, score=-530900512.729 total time= 0.1s [CV 4/5] END model__learning_rate=0.1, model__max_depth=30, model__subsample=0.5, preprocess__cat__si__strategy=most_frequent, preprocess__num__ii=KNNImputer();, score=-518644185.752 total time= 0.2s [CV 2/5] END model__learning_rate=0.1, model__max_depth=30, model__subsample=0.5, preprocess__cat__si__strategy=constant, preprocess__num__ii=SimpleImputer();, score=-995475493.332 total time= 0.2s [CV 5/5] END model__learning_rate=0.1, model__max_depth=30, model__subsample=0.5, preprocess__cat__si__strategy=constant, preprocess__num__ii=SimpleImputer();, score=-1261991041.562 total time= 0.2s [CV 1/5] END model__learning_rate=0.1, model__max_depth=30, model__subsample=0.5, preprocess__cat__si__strategy=constant, preprocess__num__ii=IterativeImputer();, score=-1047546222.332 total time= 0.6s [CV 5/5] END model__learning_rate=0.1, model__max_depth=-1, model__subsample=1.0, preprocess__cat__si__strategy=most_frequent, preprocess__num__ii=IterativeImputer();, score=-1219549323.929 total time= 0.4s [CV 3/5] END model__learning_rate=0.1, model__max_depth=-1, model__subsample=1.0, preprocess__cat__si__strategy=constant, preprocess__num__ii=IterativeImputer();, score=-532594533.605 total time= 0.4s [CV 3/5] END model__learning_rate=0.1, model__max_depth=-1, model__subsample=0.5, preprocess__cat__si__strategy=most_frequent, preprocess__num__ii=KNNImputer();, score=-515317030.231 total time= 0.2s [CV 1/5] END model__learning_rate=0.1, model__max_depth=-1, model__subsample=0.5, preprocess__cat__si__strategy=constant, preprocess__num__ii=SimpleImputer();, score=-1102984449.306 total time= 0.1s [CV 4/5] END model__learning_rate=0.1, model__max_depth=-1, model__subsample=0.5, preprocess__cat__si__strategy=constant, preprocess__num__ii=SimpleImputer();, score=-543635446.429 total time= 0.2s [CV 3/5] END model__learning_rate=0.1, model__max_depth=-1, model__subsample=0.5, preprocess__cat__si__strategy=constant, preprocess__num__ii=KNNImputer();, score=-541737588.393 total time= 0.2s [CV 1/5] END model__learning_rate=0.2, model__max_depth=30, model__subsample=1.0, preprocess__cat__si__strategy=most_frequent, preprocess__num__ii=SimpleImputer();, score=-1174830723.077 total time= 0.1s [CV 4/5] END model__learning_rate=0.2, model__max_depth=30, model__subsample=1.0, preprocess__cat__si__strategy=most_frequent, preprocess__num__ii=SimpleImputer();, score=-589395877.632 total time= 0.1s [CV 5/5] END model__learning_rate=0.2, model__max_depth=30, model__subsample=1.0, preprocess__cat__si__strategy=most_frequent, preprocess__num__ii=KNNImputer();, score=-1243376201.323 total time= 0.2s [CV 2/5] END model__learning_rate=0.2, model__max_depth=30, model__subsample=1.0, preprocess__cat__si__strategy=constant, preprocess__num__ii=SimpleImputer();, score=-1024576434.974 total time= 0.1s [CV 5/5] END model__learning_rate=0.2, model__max_depth=30, model__subsample=1.0, preprocess__cat__si__strategy=constant, preprocess__num__ii=SimpleImputer();, score=-1271758627.677 total time= 0.1s [CV 1/5] END model__learning_rate=0.2, model__max_depth=30, model__subsample=1.0, preprocess__cat__si__strategy=constant, preprocess__num__ii=IterativeImputer();, score=-1064924532.138 total time= 0.6s [CV 5/5] END model__learning_rate=0.2, model__max_depth=30, model__subsample=0.5, preprocess__cat__si__strategy=most_frequent, preprocess__num__ii=IterativeImputer();, score=-1249207388.654 total time= 0.3s [CV 1/5] END model__learning_rate=0.2, model__max_depth=30, model__subsample=0.5, preprocess__cat__si__strategy=constant, preprocess__num__ii=IterativeImputer();, score=-1064924532.138 total time= 0.6s [CV 4/5] END model__learning_rate=0.2, model__max_depth=-1, model__subsample=1.0, preprocess__cat__si__strategy=most_frequent, preprocess__num__ii=IterativeImputer();, score=-643767514.223 total time= 0.4s [CV 5/5] END model__learning_rate=0.2, model__max_depth=-1, model__subsample=1.0, preprocess__cat__si__strategy=constant, preprocess__num__ii=KNNImputer();, score=-1210584444.764 total time= 0.2s [CV 3/5] END model__learning_rate=0.2, model__max_depth=-1, model__subsample=0.5, preprocess__cat__si__strategy=most_frequent, preprocess__num__ii=SimpleImputer();, score=-524058509.993 total time= 0.1s [CV 1/5] END model__learning_rate=0.2, model__max_depth=-1, model__subsample=0.5, preprocess__cat__si__strategy=most_frequent, preprocess__num__ii=KNNImputer();, score=-1079192998.183 total time= 0.2s [CV 3/5] END model__learning_rate=0.2, model__max_depth=-1, model__subsample=0.5, preprocess__cat__si__strategy=most_frequent, preprocess__num__ii=IterativeImputer();, score=-576066791.159 total time= 0.4s [CV 3/5] END model__learning_rate=0.2, model__max_depth=-1, model__subsample=0.5, preprocess__cat__si__strategy=constant, preprocess__num__ii=KNNImputer();, score=-564186901.908 total time= 0.2s [CV 3/5] END model__learning_rate=0.1, model__max_depth=30, model__subsample=1.0, preprocess__cat__si__strategy=most_frequent, preprocess__num__ii=SimpleImputer();, score=-513492880.363 total time= 0.2s [CV 1/5] END model__learning_rate=0.1, model__max_depth=30, model__subsample=1.0, preprocess__cat__si__strategy=most_frequent, preprocess__num__ii=IterativeImputer();, score=-1038744584.272 total time= 0.7s [CV 3/5] END model__learning_rate=0.1, model__max_depth=30, model__subsample=0.5, preprocess__cat__si__strategy=most_frequent, preprocess__num__ii=SimpleImputer();, score=-513492880.363 total time= 0.2s [CV 1/5] END model__learning_rate=0.1, model__max_depth=30, model__subsample=0.5, preprocess__cat__si__strategy=most_frequent, preprocess__num__ii=KNNImputer();, score=-1085304295.641 total time= 0.2s [CV 5/5] END model__learning_rate=0.1, model__max_depth=30, model__subsample=0.5, preprocess__cat__si__strategy=most_frequent, preprocess__num__ii=IterativeImputer();, score=-1219549323.929 total time= 0.4s [CV 2/5] END model__learning_rate=0.1, model__max_depth=30, model__subsample=0.5, preprocess__cat__si__strategy=constant, preprocess__num__ii=IterativeImputer();, score=-1016845641.469 total time= 0.4s [CV 2/5] END model__learning_rate=0.1, model__max_depth=-1, model__subsample=1.0, preprocess__cat__si__strategy=most_frequent, preprocess__num__ii=KNNImputer();, score=-945800410.003 total time= 0.2s [CV 4/5] END model__learning_rate=0.1, model__max_depth=-1, model__subsample=1.0, preprocess__cat__si__strategy=most_frequent, preprocess__num__ii=IterativeImputer();, score=-570100776.470 total time= 0.4s [CV 2/5] END model__learning_rate=0.1, model__max_depth=-1, model__subsample=1.0, preprocess__cat__si__strategy=constant, preprocess__num__ii=IterativeImputer();, score=-1016845641.469 total time= 0.3s [CV 2/5] END model__learning_rate=0.1, model__max_depth=-1, model__subsample=0.5, preprocess__cat__si__strategy=most_frequent, preprocess__num__ii=KNNImputer();, score=-945800410.003 total time= 0.2s [CV 4/5] END model__learning_rate=0.1, model__max_depth=-1, model__subsample=0.5, preprocess__cat__si__strategy=most_frequent, preprocess__num__ii=IterativeImputer();, score=-570100776.470 total time= 0.4s [CV 5/5] END model__learning_rate=0.1, model__max_depth=-1, model__subsample=0.5, preprocess__cat__si__strategy=constant, preprocess__num__ii=KNNImputer();, score=-1239206018.769 total time= 0.2s [CV 3/5] END model__learning_rate=0.2, model__max_depth=30, model__subsample=1.0, preprocess__cat__si__strategy=most_frequent, preprocess__num__ii=SimpleImputer();, score=-524058509.993 total time= 0.1s [CV 1/5] END model__learning_rate=0.2, model__max_depth=30, model__subsample=1.0, preprocess__cat__si__strategy=most_frequent, preprocess__num__ii=KNNImputer();, score=-1079192998.183 total time= 0.2s [CV 3/5] END model__learning_rate=0.2, model__max_depth=30, model__subsample=1.0, preprocess__cat__si__strategy=most_frequent, preprocess__num__ii=IterativeImputer();, score=-576066791.159 total time= 0.4s [CV 4/5] END model__learning_rate=0.2, model__max_depth=30, model__subsample=1.0, preprocess__cat__si__strategy=constant, preprocess__num__ii=KNNImputer();, score=-590633071.038 total time= 0.2s [CV 2/5] END model__learning_rate=0.2, model__max_depth=30, model__subsample=0.5, preprocess__cat__si__strategy=most_frequent, preprocess__num__ii=SimpleImputer();, score=-1031476671.446 total time= 0.2s [CV 5/5] END model__learning_rate=0.2, model__max_depth=30, model__subsample=0.5, preprocess__cat__si__strategy=most_frequent, preprocess__num__ii=SimpleImputer();, score=-1267733161.543 total time= 0.1s [CV 1/5] END model__learning_rate=0.2, model__max_depth=30, model__subsample=0.5, preprocess__cat__si__strategy=most_frequent, preprocess__num__ii=IterativeImputer();, score=-1051746575.599 total time= 0.6s [CV 4/5] END model__learning_rate=0.2, model__max_depth=30, model__subsample=0.5, preprocess__cat__si__strategy=constant, preprocess__num__ii=IterativeImputer();, score=-600837629.536 total time= 0.4s [CV 1/5] END model__learning_rate=0.2, model__max_depth=-1, model__subsample=1.0, preprocess__cat__si__strategy=most_frequent, preprocess__num__ii=IterativeImputer();, score=-1051746575.599 total time= 0.6s [CV 3/5] END model__learning_rate=0.2, model__max_depth=-1, model__subsample=1.0, preprocess__cat__si__strategy=constant, preprocess__num__ii=IterativeImputer();, score=-533278666.873 total time= 0.4s [CV 3/5] END model__learning_rate=0.2, model__max_depth=-1, model__subsample=0.5, preprocess__cat__si__strategy=most_frequent, preprocess__num__ii=KNNImputer();, score=-540602961.526 total time= 0.2s [CV 2/5] END model__learning_rate=0.2, model__max_depth=-1, model__subsample=0.5, preprocess__cat__si__strategy=constant, preprocess__num__ii=SimpleImputer();, score=-1024576434.974 total time= 0.2s [CV 5/5] END model__learning_rate=0.2, model__max_depth=-1, model__subsample=0.5, preprocess__cat__si__strategy=constant, preprocess__num__ii=SimpleImputer();, score=-1271758627.677 total time= 0.1s [CV 5/5] END model__learning_rate=0.2, model__max_depth=-1, model__subsample=0.5, preprocess__cat__si__strategy=constant, preprocess__num__ii=KNNImputer();, score=-1210584444.764 total time= 0.2s [CV 1/5] END model__learning_rate=0.1, model__max_depth=30, model__subsample=1.0, preprocess__cat__si__strategy=most_frequent, preprocess__num__ii=KNNImputer();, score=-1085304295.641 total time= 0.2s [CV 5/5] END model__learning_rate=0.1, model__max_depth=30, model__subsample=1.0, preprocess__cat__si__strategy=most_frequent, preprocess__num__ii=IterativeImputer();, score=-1219549323.929 total time= 0.4s [CV 2/5] END model__learning_rate=0.1, model__max_depth=30, model__subsample=1.0, preprocess__cat__si__strategy=constant, preprocess__num__ii=IterativeImputer();, score=-1016845641.469 total time= 0.4s [CV 2/5] END model__learning_rate=0.1, model__max_depth=30, model__subsample=0.5, preprocess__cat__si__strategy=most_frequent, preprocess__num__ii=KNNImputer();, score=-945800410.003 total time= 0.2s [CV 1/5] END model__learning_rate=0.1, model__max_depth=30, model__subsample=0.5, preprocess__cat__si__strategy=constant, preprocess__num__ii=SimpleImputer();, score=-1102984449.306 total time= 0.1s [CV 4/5] END model__learning_rate=0.1, model__max_depth=30, model__subsample=0.5, preprocess__cat__si__strategy=constant, preprocess__num__ii=SimpleImputer();, score=-543635446.429 total time= 0.3s [CV 5/5] END model__learning_rate=0.1, model__max_depth=30, model__subsample=0.5, preprocess__cat__si__strategy=constant, preprocess__num__ii=IterativeImputer();, score=-1260432508.211 total time= 0.4s [CV 3/5] END model__learning_rate=0.1, model__max_depth=-1, model__subsample=1.0, preprocess__cat__si__strategy=most_frequent, preprocess__num__ii=IterativeImputer();, score=-519293006.931 total time= 0.4s [CV 4/5] END model__learning_rate=0.1, model__max_depth=-1, model__subsample=1.0, preprocess__cat__si__strategy=constant, preprocess__num__ii=KNNImputer();, score=-513174790.496 total time= 0.2s [CV 2/5] END model__learning_rate=0.1, model__max_depth=-1, model__subsample=0.5, preprocess__cat__si__strategy=most_frequent, preprocess__num__ii=SimpleImputer();, score=-997691407.890 total time= 0.1s [CV 5/5] END model__learning_rate=0.1, model__max_depth=-1, model__subsample=0.5, preprocess__cat__si__strategy=most_frequent, preprocess__num__ii=SimpleImputer();, score=-1226710083.913 total time= 0.2s [CV 5/5] END model__learning_rate=0.1, model__max_depth=-1, model__subsample=0.5, preprocess__cat__si__strategy=most_frequent, preprocess__num__ii=KNNImputer();, score=-1224677483.241 total time= 0.2s [CV 3/5] END model__learning_rate=0.1, model__max_depth=-1, model__subsample=0.5, preprocess__cat__si__strategy=constant, preprocess__num__ii=SimpleImputer();, score=-529011858.629 total time= 0.1s [CV 1/5] END model__learning_rate=0.1, model__max_depth=-1, model__subsample=0.5, preprocess__cat__si__strategy=constant, preprocess__num__ii=KNNImputer();, score=-1060914076.633 total time= 0.2s [CV 3/5] END model__learning_rate=0.1, model__max_depth=-1, model__subsample=0.5, preprocess__cat__si__strategy=constant, preprocess__num__ii=IterativeImputer();, score=-532594533.605 total time= 0.4s [CV 3/5] END model__learning_rate=0.2, model__max_depth=30, model__subsample=1.0, preprocess__cat__si__strategy=most_frequent, preprocess__num__ii=KNNImputer();, score=-540602961.526 total time= 0.2s [CV 1/5] END model__learning_rate=0.2, model__max_depth=30, model__subsample=1.0, preprocess__cat__si__strategy=constant, preprocess__num__ii=SimpleImputer();, score=-1088255251.747 total time= 0.1s [CV 4/5] END model__learning_rate=0.2, model__max_depth=30, model__subsample=1.0, preprocess__cat__si__strategy=constant, preprocess__num__ii=SimpleImputer();, score=-599626460.012 total time= 0.1s [CV 5/5] END model__learning_rate=0.2, model__max_depth=30, model__subsample=1.0, preprocess__cat__si__strategy=constant, preprocess__num__ii=KNNImputer();, score=-1210584444.764 total time= 0.2s [CV 3/5] END model__learning_rate=0.2, model__max_depth=30, model__subsample=0.5, preprocess__cat__si__strategy=most_frequent, preprocess__num__ii=SimpleImputer();, score=-524058509.993 total time= 0.1s [CV 1/5] END model__learning_rate=0.2, model__max_depth=30, model__subsample=0.5, preprocess__cat__si__strategy=most_frequent, preprocess__num__ii=KNNImputer();, score=-1079192998.183 total time= 0.2s [CV 3/5] END model__learning_rate=0.2, model__max_depth=30, model__subsample=0.5, preprocess__cat__si__strategy=most_frequent, preprocess__num__ii=IterativeImputer();, score=-576066791.159 total time= 0.4s [CV 4/5] END model__learning_rate=0.2, model__max_depth=30, model__subsample=0.5, preprocess__cat__si__strategy=constant, preprocess__num__ii=KNNImputer();, score=-590633071.038 total time= 0.2s [CV 2/5] END model__learning_rate=0.2, model__max_depth=-1, model__subsample=1.0, preprocess__cat__si__strategy=most_frequent, preprocess__num__ii=SimpleImputer();, score=-1031476671.446 total time= 0.2s [CV 5/5] END model__learning_rate=0.2, model__max_depth=-1, model__subsample=1.0, preprocess__cat__si__strategy=most_frequent, preprocess__num__ii=SimpleImputer();, score=-1267733161.543 total time= 0.1s [CV 5/5] END model__learning_rate=0.2, model__max_depth=-1, model__subsample=1.0, preprocess__cat__si__strategy=most_frequent, preprocess__num__ii=KNNImputer();, score=-1243376201.323 total time= 0.2s [CV 3/5] END model__learning_rate=0.2, model__max_depth=-1, model__subsample=1.0, preprocess__cat__si__strategy=constant, preprocess__num__ii=SimpleImputer();, score=-539187722.319 total time= 0.2s [CV 1/5] END model__learning_rate=0.2, model__max_depth=-1, model__subsample=1.0, preprocess__cat__si__strategy=constant, preprocess__num__ii=KNNImputer();, score=-1101298878.719 total time= 0.2s [CV 4/5] END model__learning_rate=0.2, model__max_depth=-1, model__subsample=1.0, preprocess__cat__si__strategy=constant, preprocess__num__ii=IterativeImputer();, score=-600837629.536 total time= 0.4s [CV 5/5] END model__learning_rate=0.2, model__max_depth=-1, model__subsample=0.5, preprocess__cat__si__strategy=most_frequent, preprocess__num__ii=KNNImputer();, score=-1243376201.323 total time= 0.2s [CV 1/5] END model__learning_rate=0.2, model__max_depth=-1, model__subsample=0.5, preprocess__cat__si__strategy=constant, preprocess__num__ii=SimpleImputer();, score=-1088255251.747 total time= 0.1s [CV 4/5] END model__learning_rate=0.2, model__max_depth=-1, model__subsample=0.5, preprocess__cat__si__strategy=constant, preprocess__num__ii=SimpleImputer();, score=-599626460.012 total time= 0.2s [CV 2/5] END model__learning_rate=0.2, model__max_depth=-1, model__subsample=0.5, preprocess__cat__si__strategy=constant, preprocess__num__ii=IterativeImputer();, score=-1034331933.536 total time= 0.3s [CV 4/5] END model__learning_rate=0.1, model__max_depth=30, model__subsample=1.0, preprocess__cat__si__strategy=most_frequent, preprocess__num__ii=SimpleImputer();, score=-530900512.729 total time= 0.1s [CV 3/5] END model__learning_rate=0.1, model__max_depth=30, model__subsample=1.0, preprocess__cat__si__strategy=most_frequent, preprocess__num__ii=IterativeImputer();, score=-519293006.931 total time= 0.3s [CV 2/5] END model__learning_rate=0.1, model__max_depth=30, model__subsample=1.0, preprocess__cat__si__strategy=constant, preprocess__num__ii=KNNImputer();, score=-1033297772.352 total time= 0.2s [CV 4/5] END model__learning_rate=0.1, model__max_depth=30, model__subsample=1.0, preprocess__cat__si__strategy=constant, preprocess__num__ii=IterativeImputer();, score=-551768174.727 total time= 0.4s [CV 5/5] END model__learning_rate=0.1, model__max_depth=30, model__subsample=0.5, preprocess__cat__si__strategy=most_frequent, preprocess__num__ii=KNNImputer();, score=-1224677483.241 total time= 0.2s [CV 3/5] END model__learning_rate=0.1, model__max_depth=30, model__subsample=0.5, preprocess__cat__si__strategy=constant, preprocess__num__ii=SimpleImputer();, score=-529011858.629 total time= 0.2s [CV 1/5] END model__learning_rate=0.1, model__max_depth=30, model__subsample=0.5, preprocess__cat__si__strategy=constant, preprocess__num__ii=KNNImputer();, score=-1060914076.633 total time= 0.2s [CV 4/5] END model__learning_rate=0.1, model__max_depth=30, model__subsample=0.5, preprocess__cat__si__strategy=constant, preprocess__num__ii=IterativeImputer();, score=-551768174.727 total time= 0.3s [CV 4/5] END model__learning_rate=0.1, model__max_depth=-1, model__subsample=1.0, preprocess__cat__si__strategy=most_frequent, preprocess__num__ii=KNNImputer();, score=-518644185.752 total time= 0.2s [CV 2/5] END model__learning_rate=0.1, model__max_depth=-1, model__subsample=1.0, preprocess__cat__si__strategy=constant, preprocess__num__ii=SimpleImputer();, score=-995475493.332 total time= 0.1s [CV 5/5] END model__learning_rate=0.1, model__max_depth=-1, model__subsample=1.0, preprocess__cat__si__strategy=constant, preprocess__num__ii=SimpleImputer();, score=-1261991041.562 total time= 0.1s [CV 5/5] END model__learning_rate=0.1, model__max_depth=-1, model__subsample=1.0, preprocess__cat__si__strategy=constant, preprocess__num__ii=KNNImputer();, score=-1239206018.769 total time= 0.2s [CV 3/5] END model__learning_rate=0.1, model__max_depth=-1, model__subsample=0.5, preprocess__cat__si__strategy=most_frequent, preprocess__num__ii=SimpleImputer();, score=-513492880.363 total time= 0.2s [CV 1/5] END model__learning_rate=0.1, model__max_depth=-1, model__subsample=0.5, preprocess__cat__si__strategy=most_frequent, preprocess__num__ii=KNNImputer();, score=-1085304295.641 total time= 0.2s [CV 3/5] END model__learning_rate=0.1, model__max_depth=-1, model__subsample=0.5, preprocess__cat__si__strategy=most_frequent, preprocess__num__ii=IterativeImputer();, score=-519293006.931 total time= 0.4s [CV 1/5] END model__learning_rate=0.1, model__max_depth=-1, model__subsample=0.5, preprocess__cat__si__strategy=constant, preprocess__num__ii=IterativeImputer();, score=-1047546222.332 total time= 0.6s [CV 4/5] END model__learning_rate=0.2, model__max_depth=30, model__subsample=1.0, preprocess__cat__si__strategy=most_frequent, preprocess__num__ii=IterativeImputer();, score=-643767514.223 total time= 0.3s [CV 3/5] END model__learning_rate=0.2, model__max_depth=30, model__subsample=1.0, preprocess__cat__si__strategy=constant, preprocess__num__ii=KNNImputer();, score=-564186901.908 total time= 0.2s [CV 1/5] END model__learning_rate=0.2, model__max_depth=30, model__subsample=0.5, preprocess__cat__si__strategy=most_frequent, preprocess__num__ii=SimpleImputer();, score=-1174830723.077 total time= 0.1s [CV 4/5] END model__learning_rate=0.2, model__max_depth=30, model__subsample=0.5, preprocess__cat__si__strategy=most_frequent, preprocess__num__ii=SimpleImputer();, score=-589395877.632 total time= 0.1s [CV 4/5] END model__learning_rate=0.2, model__max_depth=30, model__subsample=0.5, preprocess__cat__si__strategy=most_frequent, preprocess__num__ii=KNNImputer();, score=-544430870.856 total time= 0.2s [CV 4/5] END model__learning_rate=0.2, model__max_depth=30, model__subsample=0.5, preprocess__cat__si__strategy=most_frequent, preprocess__num__ii=IterativeImputer();, score=-643767514.223 total time= 0.4s [CV 2/5] END model__learning_rate=0.2, model__max_depth=30, model__subsample=0.5, preprocess__cat__si__strategy=constant, preprocess__num__ii=IterativeImputer();, score=-1034331933.536 total time= 0.4s [CV 3/5] END model__learning_rate=0.2, model__max_depth=-1, model__subsample=1.0, preprocess__cat__si__strategy=most_frequent, preprocess__num__ii=KNNImputer();, score=-540602961.526 total time= 0.2s [CV 1/5] END model__learning_rate=0.2, model__max_depth=-1, model__subsample=1.0, preprocess__cat__si__strategy=constant, preprocess__num__ii=SimpleImputer();, score=-1088255251.747 total time= 0.1s [CV 4/5] END model__learning_rate=0.2, model__max_depth=-1, model__subsample=1.0, preprocess__cat__si__strategy=constant, preprocess__num__ii=SimpleImputer();, score=-599626460.012 total time= 0.2s [CV 4/5] END model__learning_rate=0.2, model__max_depth=-1, model__subsample=1.0, preprocess__cat__si__strategy=constant, preprocess__num__ii=KNNImputer();, score=-590633071.038 total time= 0.2s [CV 2/5] END model__learning_rate=0.2, model__max_depth=-1, model__subsample=0.5, preprocess__cat__si__strategy=most_frequent, preprocess__num__ii=SimpleImputer();, score=-1031476671.446 total time= 0.2s [CV 5/5] END model__learning_rate=0.2, model__max_depth=-1, model__subsample=0.5, preprocess__cat__si__strategy=most_frequent, preprocess__num__ii=SimpleImputer();, score=-1267733161.543 total time= 0.1s [CV 1/5] END model__learning_rate=0.2, model__max_depth=-1, model__subsample=0.5, preprocess__cat__si__strategy=most_frequent, preprocess__num__ii=IterativeImputer();, score=-1051746575.599 total time= 0.5s [CV 3/5] END model__learning_rate=0.2, model__max_depth=-1, model__subsample=0.5, preprocess__cat__si__strategy=constant, preprocess__num__ii=IterativeImputer();, score=-533278666.873 total time= 0.3s [CV 2/5] END model__learning_rate=0.1, model__max_depth=30, model__subsample=1.0, preprocess__cat__si__strategy=most_frequent, preprocess__num__ii=SimpleImputer();, score=-997691407.890 total time= 0.2s [CV 5/5] END model__learning_rate=0.1, model__max_depth=30, model__subsample=1.0, preprocess__cat__si__strategy=most_frequent, preprocess__num__ii=KNNImputer();, score=-1224677483.241 total time= 0.2s [CV 3/5] END model__learning_rate=0.1, model__max_depth=30, model__subsample=1.0, preprocess__cat__si__strategy=constant, preprocess__num__ii=SimpleImputer();, score=-529011858.629 total time= 0.1s [CV 1/5] END model__learning_rate=0.1, model__max_depth=30, model__subsample=1.0, preprocess__cat__si__strategy=constant, preprocess__num__ii=KNNImputer();, score=-1060914076.633 total time= 0.2s [CV 3/5] END model__learning_rate=0.1, model__max_depth=30, model__subsample=1.0, preprocess__cat__si__strategy=constant, preprocess__num__ii=IterativeImputer();, score=-532594533.605 total time= 0.3s [CV 3/5] END model__learning_rate=0.1, model__max_depth=30, model__subsample=0.5, preprocess__cat__si__strategy=most_frequent, preprocess__num__ii=KNNImputer();, score=-515317030.231 total time= 0.1s [CV 3/5] END model__learning_rate=0.1, model__max_depth=30, model__subsample=0.5, preprocess__cat__si__strategy=most_frequent, preprocess__num__ii=IterativeImputer();, score=-519293006.931 total time= 0.4s [CV 2/5] END model__learning_rate=0.1, model__max_depth=30, model__subsample=0.5, preprocess__cat__si__strategy=constant, preprocess__num__ii=KNNImputer();, score=-1033297772.352 total time= 0.2s [CV 3/5] END model__learning_rate=0.1, model__max_depth=30, model__subsample=0.5, preprocess__cat__si__strategy=constant, preprocess__num__ii=IterativeImputer();, score=-532594533.605 total time= 0.4s [CV 1/5] END model__learning_rate=0.1, model__max_depth=-1, model__subsample=1.0, preprocess__cat__si__strategy=most_frequent, preprocess__num__ii=IterativeImputer();, score=-1038744584.272 total time= 0.6s [CV 4/5] END model__learning_rate=0.1, model__max_depth=-1, model__subsample=1.0, preprocess__cat__si__strategy=constant, preprocess__num__ii=IterativeImputer();, score=-551768174.727 total time= 0.4s [CV 2/5] END model__learning_rate=0.1, model__max_depth=-1, model__subsample=0.5, preprocess__cat__si__strategy=most_frequent, preprocess__num__ii=IterativeImputer();, score=-1024476538.628 total time= 0.4s [CV 2/5] END model__learning_rate=0.1, model__max_depth=-1, model__subsample=0.5, preprocess__cat__si__strategy=constant, preprocess__num__ii=KNNImputer();, score=-1033297772.352 total time= 0.2s [CV 4/5] END model__learning_rate=0.1, model__max_depth=-1, model__subsample=0.5, preprocess__cat__si__strategy=constant, preprocess__num__ii=IterativeImputer();, score=-551768174.727 total time= 0.4s [CV 4/5] END model__learning_rate=0.2, model__max_depth=30, model__subsample=1.0, preprocess__cat__si__strategy=most_frequent, preprocess__num__ii=KNNImputer();, score=-544430870.856 total time= 0.2s [CV 3/5] END model__learning_rate=0.2, model__max_depth=30, model__subsample=1.0, preprocess__cat__si__strategy=constant, preprocess__num__ii=SimpleImputer();, score=-539187722.319 total time= 0.1s [CV 1/5] END model__learning_rate=0.2, model__max_depth=30, model__subsample=1.0, preprocess__cat__si__strategy=constant, preprocess__num__ii=KNNImputer();, score=-1101298878.719 total time= 0.2s [CV 3/5] END model__learning_rate=0.2, model__max_depth=30, model__subsample=1.0, preprocess__cat__si__strategy=constant, preprocess__num__ii=IterativeImputer();, score=-533278666.873 total time= 0.4s [CV 3/5] END model__learning_rate=0.2, model__max_depth=30, model__subsample=0.5, preprocess__cat__si__strategy=most_frequent, preprocess__num__ii=KNNImputer();, score=-540602961.526 total time= 0.2s [CV 1/5] END model__learning_rate=0.2, model__max_depth=30, model__subsample=0.5, preprocess__cat__si__strategy=constant, preprocess__num__ii=SimpleImputer();, score=-1088255251.747 total time= 0.1s [CV 4/5] END model__learning_rate=0.2, model__max_depth=30, model__subsample=0.5, preprocess__cat__si__strategy=constant, preprocess__num__ii=SimpleImputer();, score=-599626460.012 total time= 0.1s [CV 2/5] END model__learning_rate=0.2, model__max_depth=30, model__subsample=0.5, preprocess__cat__si__strategy=constant, preprocess__num__ii=KNNImputer();, score=-1064162610.680 total time= 0.2s [CV 5/5] END model__learning_rate=0.2, model__max_depth=30, model__subsample=0.5, preprocess__cat__si__strategy=constant, preprocess__num__ii=IterativeImputer();, score=-1226474594.655 total time= 0.4s [CV 2/5] END model__learning_rate=0.2, model__max_depth=-1, model__subsample=1.0, preprocess__cat__si__strategy=most_frequent, preprocess__num__ii=IterativeImputer();, score=-1045009894.967 total time= 0.4s [CV 2/5] END model__learning_rate=0.2, model__max_depth=-1, model__subsample=1.0, preprocess__cat__si__strategy=constant, preprocess__num__ii=KNNImputer();, score=-1064162610.680 total time= 0.2s [CV 5/5] END model__learning_rate=0.2, model__max_depth=-1, model__subsample=1.0, preprocess__cat__si__strategy=constant, preprocess__num__ii=IterativeImputer();, score=-1226474594.655 total time= 0.4s [CV 2/5] END model__learning_rate=0.2, model__max_depth=-1, model__subsample=0.5, preprocess__cat__si__strategy=most_frequent, preprocess__num__ii=IterativeImputer();, score=-1045009894.967 total time= 0.4s [CV 1/5] END model__learning_rate=0.2, model__max_depth=-1, model__subsample=0.5, preprocess__cat__si__strategy=constant, preprocess__num__ii=KNNImputer();, score=-1101298878.719 total time= 0.2s [CV 4/5] END model__learning_rate=0.2, model__max_depth=-1, model__subsample=0.5, preprocess__cat__si__strategy=constant, preprocess__num__ii=IterativeImputer();, score=-600837629.536 total time= 0.3s [CV 1/5] END model__learning_rate=0.1, model__max_depth=30, model__subsample=1.0, preprocess__cat__si__strategy=most_frequent, preprocess__num__ii=SimpleImputer();, score=-1094302605.510 total time= 0.2s [CV 4/5] END model__learning_rate=0.1, model__max_depth=30, model__subsample=1.0, preprocess__cat__si__strategy=most_frequent, preprocess__num__ii=KNNImputer();, score=-518644185.752 total time= 0.2s [CV 2/5] END model__learning_rate=0.1, model__max_depth=30, model__subsample=1.0, preprocess__cat__si__strategy=constant, preprocess__num__ii=SimpleImputer();, score=-995475493.332 total time= 0.1s [CV 5/5] END model__learning_rate=0.1, model__max_depth=30, model__subsample=1.0, preprocess__cat__si__strategy=constant, preprocess__num__ii=SimpleImputer();, score=-1261991041.562 total time= 0.1s [CV 1/5] END model__learning_rate=0.1, model__max_depth=30, model__subsample=1.0, preprocess__cat__si__strategy=constant, preprocess__num__ii=IterativeImputer();, score=-1047546222.332 total time= 0.6s [CV 4/5] END model__learning_rate=0.1, model__max_depth=30, model__subsample=0.5, preprocess__cat__si__strategy=most_frequent, preprocess__num__ii=IterativeImputer();, score=-570100776.470 total time= 0.3s [CV 4/5] END model__learning_rate=0.1, model__max_depth=30, model__subsample=0.5, preprocess__cat__si__strategy=constant, preprocess__num__ii=KNNImputer();, score=-513174790.496 total time= 0.2s [CV 2/5] END model__learning_rate=0.1, model__max_depth=-1, model__subsample=1.0, preprocess__cat__si__strategy=most_frequent, preprocess__num__ii=SimpleImputer();, score=-997691407.890 total time= 0.1s [CV 5/5] END model__learning_rate=0.1, model__max_depth=-1, model__subsample=1.0, preprocess__cat__si__strategy=most_frequent, preprocess__num__ii=SimpleImputer();, score=-1226710083.913 total time= 0.1s [CV 5/5] END model__learning_rate=0.1, model__max_depth=-1, model__subsample=1.0, preprocess__cat__si__strategy=most_frequent, preprocess__num__ii=KNNImputer();, score=-1224677483.241 total time= 0.2s [CV 3/5] END model__learning_rate=0.1, model__max_depth=-1, model__subsample=1.0, preprocess__cat__si__strategy=constant, preprocess__num__ii=SimpleImputer();, score=-529011858.629 total time= 0.1s [CV 1/5] END model__learning_rate=0.1, model__max_depth=-1, model__subsample=1.0, preprocess__cat__si__strategy=constant, preprocess__num__ii=KNNImputer();, score=-1060914076.633 total time= 0.2s [CV 1/5] END model__learning_rate=0.1, model__max_depth=-1, model__subsample=1.0, preprocess__cat__si__strategy=constant, preprocess__num__ii=IterativeImputer();, score=-1047546222.332 total time= 0.6s [CV 5/5] END model__learning_rate=0.1, model__max_depth=-1, model__subsample=0.5, preprocess__cat__si__strategy=most_frequent, preprocess__num__ii=IterativeImputer();, score=-1219549323.929 total time= 0.4s [CV 2/5] END model__learning_rate=0.1, model__max_depth=-1, model__subsample=0.5, preprocess__cat__si__strategy=constant, preprocess__num__ii=IterativeImputer();, score=-1016845641.469 total time= 0.4s [CV 2/5] END model__learning_rate=0.2, model__max_depth=30, model__subsample=1.0, preprocess__cat__si__strategy=most_frequent, preprocess__num__ii=KNNImputer();, score=-1017028960.525 total time= 0.2s [CV 5/5] END model__learning_rate=0.2, model__max_depth=30, model__subsample=1.0, preprocess__cat__si__strategy=most_frequent, preprocess__num__ii=IterativeImputer();, score=-1249207388.654 total time= 0.4s [CV 2/5] END model__learning_rate=0.2, model__max_depth=30, model__subsample=1.0, preprocess__cat__si__strategy=constant, preprocess__num__ii=IterativeImputer();, score=-1034331933.536 total time= 0.4s [CV 2/5] END model__learning_rate=0.2, model__max_depth=30, model__subsample=0.5, preprocess__cat__si__strategy=most_frequent, preprocess__num__ii=KNNImputer();, score=-1017028960.525 total time= 0.2s [CV 2/5] END model__learning_rate=0.2, model__max_depth=30, model__subsample=0.5, preprocess__cat__si__strategy=constant, preprocess__num__ii=SimpleImputer();, score=-1024576434.974 total time= 0.1s [CV 5/5] END model__learning_rate=0.2, model__max_depth=30, model__subsample=0.5, preprocess__cat__si__strategy=constant, preprocess__num__ii=SimpleImputer();, score=-1271758627.677 total time= 0.2s [CV 5/5] END model__learning_rate=0.2, model__max_depth=30, model__subsample=0.5, preprocess__cat__si__strategy=constant, preprocess__num__ii=KNNImputer();, score=-1210584444.764 total time= 0.2s [CV 3/5] END model__learning_rate=0.2, model__max_depth=-1, model__subsample=1.0, preprocess__cat__si__strategy=most_frequent, preprocess__num__ii=SimpleImputer();, score=-524058509.993 total time= 0.2s [CV 1/5] END model__learning_rate=0.2, model__max_depth=-1, model__subsample=1.0, preprocess__cat__si__strategy=most_frequent, preprocess__num__ii=KNNImputer();, score=-1079192998.183 total time= 0.2s [CV 3/5] END model__learning_rate=0.2, model__max_depth=-1, model__subsample=1.0, preprocess__cat__si__strategy=most_frequent, preprocess__num__ii=IterativeImputer();, score=-576066791.159 total time= 0.4s [CV 3/5] END model__learning_rate=0.2, model__max_depth=-1, model__subsample=1.0, preprocess__cat__si__strategy=constant, preprocess__num__ii=KNNImputer();, score=-564186901.908 total time= 0.2s [CV 1/5] END model__learning_rate=0.2, model__max_depth=-1, model__subsample=0.5, preprocess__cat__si__strategy=most_frequent, preprocess__num__ii=SimpleImputer();, score=-1174830723.077 total time= 0.1s [CV 4/5] END model__learning_rate=0.2, model__max_depth=-1, model__subsample=0.5, preprocess__cat__si__strategy=most_frequent, preprocess__num__ii=SimpleImputer();, score=-589395877.632 total time= 0.1s [CV 4/5] END model__learning_rate=0.2, model__max_depth=-1, model__subsample=0.5, preprocess__cat__si__strategy=most_frequent, preprocess__num__ii=KNNImputer();, score=-544430870.856 total time= 0.3s [CV 3/5] END model__learning_rate=0.2, model__max_depth=-1, model__subsample=0.5, preprocess__cat__si__strategy=constant, preprocess__num__ii=SimpleImputer();, score=-539187722.319 total time= 0.2s [CV 2/5] END model__learning_rate=0.2, model__max_depth=-1, model__subsample=0.5, preprocess__cat__si__strategy=constant, preprocess__num__ii=KNNImputer();, score=-1064162610.680 total time= 0.2s [CV 5/5] END model__learning_rate=0.2, model__max_depth=-1, model__subsample=0.5, preprocess__cat__si__strategy=constant, preprocess__num__ii=IterativeImputer();, score=-1226474594.655 total time= 0.3s [CV 5/5] END model__learning_rate=0.1, model__max_depth=30, model__subsample=1.0, preprocess__cat__si__strategy=most_frequent, preprocess__num__ii=SimpleImputer();, score=-1226710083.913 total time= 0.2s [CV 2/5] END model__learning_rate=0.1, model__max_depth=30, model__subsample=1.0, preprocess__cat__si__strategy=most_frequent, preprocess__num__ii=IterativeImputer();, score=-1024476538.628 total time= 0.4s [CV 3/5] END model__learning_rate=0.1, model__max_depth=30, model__subsample=1.0, preprocess__cat__si__strategy=constant, preprocess__num__ii=KNNImputer();, score=-541737588.393 total time= 0.2s [CV 5/5] END model__learning_rate=0.1, model__max_depth=30, model__subsample=1.0, preprocess__cat__si__strategy=constant, preprocess__num__ii=IterativeImputer();, score=-1260432508.211 total time= 0.4s [CV 1/5] END model__learning_rate=0.1, model__max_depth=30, model__subsample=0.5, preprocess__cat__si__strategy=most_frequent, preprocess__num__ii=IterativeImputer();, score=-1038744584.272 total time= 0.4s [CV 5/5] END model__learning_rate=0.1, model__max_depth=30, model__subsample=0.5, preprocess__cat__si__strategy=constant, preprocess__num__ii=KNNImputer();, score=-1239206018.769 total time= 0.1s [CV 1/5] END model__learning_rate=0.1, model__max_depth=-1, model__subsample=1.0, preprocess__cat__si__strategy=most_frequent, preprocess__num__ii=SimpleImputer();, score=-1094302605.510 total time= 0.1s [CV 4/5] END model__learning_rate=0.1, model__max_depth=-1, model__subsample=1.0, preprocess__cat__si__strategy=most_frequent, preprocess__num__ii=SimpleImputer();, score=-530900512.729 total time= 0.2s [CV 3/5] END model__learning_rate=0.1, model__max_depth=-1, model__subsample=1.0, preprocess__cat__si__strategy=most_frequent, preprocess__num__ii=KNNImputer();, score=-515317030.231 total time= 0.2s [CV 1/5] END model__learning_rate=0.1, model__max_depth=-1, model__subsample=1.0, preprocess__cat__si__strategy=constant, preprocess__num__ii=SimpleImputer();, score=-1102984449.306 total time= 0.1s [CV 4/5] END model__learning_rate=0.1, model__max_depth=-1, model__subsample=1.0, preprocess__cat__si__strategy=constant, preprocess__num__ii=SimpleImputer();, score=-543635446.429 total time= 0.1s [CV 3/5] END model__learning_rate=0.1, model__max_depth=-1, model__subsample=1.0, preprocess__cat__si__strategy=constant, preprocess__num__ii=KNNImputer();, score=-541737588.393 total time= 0.2s [CV 1/5] END model__learning_rate=0.1, model__max_depth=-1, model__subsample=0.5, preprocess__cat__si__strategy=most_frequent, preprocess__num__ii=SimpleImputer();, score=-1094302605.510 total time= 0.1s [CV 4/5] END model__learning_rate=0.1, model__max_depth=-1, model__subsample=0.5, preprocess__cat__si__strategy=most_frequent, preprocess__num__ii=SimpleImputer();, score=-530900512.729 total time= 0.2s [CV 4/5] END model__learning_rate=0.1, model__max_depth=-1, model__subsample=0.5, preprocess__cat__si__strategy=most_frequent, preprocess__num__ii=KNNImputer();, score=-518644185.752 total time= 0.2s [CV 2/5] END model__learning_rate=0.1, model__max_depth=-1, model__subsample=0.5, preprocess__cat__si__strategy=constant, preprocess__num__ii=SimpleImputer();, score=-995475493.332 total time= 0.1s [CV 5/5] END model__learning_rate=0.1, model__max_depth=-1, model__subsample=0.5, preprocess__cat__si__strategy=constant, preprocess__num__ii=SimpleImputer();, score=-1261991041.562 total time= 0.1s [CV 4/5] END model__learning_rate=0.1, model__max_depth=-1, model__subsample=0.5, preprocess__cat__si__strategy=constant, preprocess__num__ii=KNNImputer();, score=-513174790.496 total time= 0.2s [CV 2/5] END model__learning_rate=0.2, model__max_depth=30, model__subsample=1.0, preprocess__cat__si__strategy=most_frequent, preprocess__num__ii=SimpleImputer();, score=-1031476671.446 total time= 0.1s [CV 5/5] END model__learning_rate=0.2, model__max_depth=30, model__subsample=1.0, preprocess__cat__si__strategy=most_frequent, preprocess__num__ii=SimpleImputer();, score=-1267733161.543 total time= 0.1s [CV 1/5] END model__learning_rate=0.2, model__max_depth=30, model__subsample=1.0, preprocess__cat__si__strategy=most_frequent, preprocess__num__ii=IterativeImputer();, score=-1051746575.599 total time= 0.6s [CV 5/5] END model__learning_rate=0.2, model__max_depth=30, model__subsample=1.0, preprocess__cat__si__strategy=constant, preprocess__num__ii=IterativeImputer();, score=-1226474594.655 total time= 0.4s [CV 2/5] END model__learning_rate=0.2, model__max_depth=30, model__subsample=0.5, preprocess__cat__si__strategy=most_frequent, preprocess__num__ii=IterativeImputer();, score=-1045009894.967 total time= 0.4s [CV 3/5] END model__learning_rate=0.2, model__max_depth=30, model__subsample=0.5, preprocess__cat__si__strategy=constant, preprocess__num__ii=KNNImputer();, score=-564186901.908 total time= 0.2s [CV 1/5] END model__learning_rate=0.2, model__max_depth=-1, model__subsample=1.0, preprocess__cat__si__strategy=most_frequent, preprocess__num__ii=SimpleImputer();, score=-1174830723.077 total time= 0.1s [CV 4/5] END model__learning_rate=0.2, model__max_depth=-1, model__subsample=1.0, preprocess__cat__si__strategy=most_frequent, preprocess__num__ii=SimpleImputer();, score=-589395877.632 total time= 0.2s [CV 2/5] END model__learning_rate=0.2, model__max_depth=-1, model__subsample=1.0, preprocess__cat__si__strategy=most_frequent, preprocess__num__ii=KNNImputer();, score=-1017028960.525 total time= 0.2s [CV 5/5] END model__learning_rate=0.2, model__max_depth=-1, model__subsample=1.0, preprocess__cat__si__strategy=most_frequent, preprocess__num__ii=IterativeImputer();, score=-1249207388.654 total time= 0.4s [CV 2/5] END model__learning_rate=0.2, model__max_depth=-1, model__subsample=1.0, preprocess__cat__si__strategy=constant, preprocess__num__ii=IterativeImputer();, score=-1034331933.536 total time= 0.4s [CV 2/5] END model__learning_rate=0.2, model__max_depth=-1, model__subsample=0.5, preprocess__cat__si__strategy=most_frequent, preprocess__num__ii=KNNImputer();, score=-1017028960.525 total time= 0.2s [CV 5/5] END model__learning_rate=0.2, model__max_depth=-1, model__subsample=0.5, preprocess__cat__si__strategy=most_frequent, preprocess__num__ii=IterativeImputer();, score=-1249207388.654 total time= 0.4s [CV 1/5] END model__learning_rate=0.2, model__max_depth=-1, model__subsample=0.5, preprocess__cat__si__strategy=constant, preprocess__num__ii=IterativeImputer();, score=-1064924532.138 total time= 0.5s ","date":1651363200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1651363200,"objectID":"e8c0f32bd58d0f20f450a3facf21546b","permalink":"https://matteocourthoud.github.io/course/data-science/06_ml_pipeline/","publishdate":"2022-05-01T00:00:00Z","relpermalink":"/course/data-science/06_ml_pipeline/","section":"course","summary":"In this notebook, we are going to build a pipeline for a general prediction problem.\n# Standard Imports from src.utils import * from src.get_feature_names import get_feature_names # Set inline graphs plt.","tags":null,"title":"Machine Learning Pipeline","type":"book"},{"authors":null,"categories":null,"content":"# Remove warnings import warnings warnings.filterwarnings('ignore') # Import import autograd.numpy as np from autograd import grad import seaborn as sns # Import matplotlib for graphs import matplotlib.pyplot as plt from mpl_toolkits.mplot3d import axes3d # Set global parameters %matplotlib inline plt.style.use('seaborn-white') plt.rcParams['lines.linewidth'] = 3 plt.rcParams['figure.figsize'] = (10,6) plt.rcParams['figure.titlesize'] = 20 plt.rcParams['axes.titlesize'] = 18 plt.rcParams['axes.labelsize'] = 14 plt.rcParams['legend.fontsize'] = 14 # Function to plot errors def error_plot(ys, yscale='log'): plt.figure() plt.xlabel('Step') plt.ylabel('Error') plt.yscale(yscale) plt.plot(range(len(ys)), ys) 6.1 Gradient Descent We start with a basic implementation of projected gradient descent.\ndef gradient_descent(init, steps, grad, proj=lambda x: x): \u0026quot;\u0026quot;\u0026quot;Projected gradient descent. Inputs: initial: starting point steps: list of scalar step sizes grad: function mapping points to gradients proj (optional): function mapping points to points Returns: List of all points computed by projected gradient descent. \u0026quot;\u0026quot;\u0026quot; xs = [init] for step in steps: xs.append(proj(xs[-1] - step * grad(xs[-1]))) return xs Note that this implementation keeps around all points computed along the way. This is clearly not what you would do on large instances. We do this for illustrative purposes to be able to easily inspect the computed sequence of points.\nWarm-up: Optimizing a quadratic As a toy example, let\u0026rsquo;s optimize $$f(x)=\\frac12|x|^2,$$ which has the gradient map $\\nabla f(x)=x.$\ndef quadratic(x): return 0.5*x.dot(x) def quadratic_gradient(x): return x Note the function is $1$-smooth and $1$-strongly convex. Our theorems would then suggest that we use a constant step size of $1.$ If you think about it, for this step size the algorithm will actually find the optimal solution in just one step.\nx0 = np.random.normal(0, 1, (1000)) _, x1 = gradient_descent(x0, [1.0], quadratic_gradient) Indeed, it does.\nx1.all() == 0 True Let\u0026rsquo;s say we don\u0026rsquo;t have the right learning rate.\nxs = gradient_descent(x0, [0.1]*50, quadratic_gradient) # Plot errors along steps error_plot([quadratic(x) for x in xs]) Constrained Optimization Let\u0026rsquo;s say we want to optimize the function inside some affine subspace. Recall that affine subspaces are convex sets. Below we pick a random low dimensional affine subspace $b+U$ and define the corresponding linear projection operator.\n# U is an orthonormal basis of a random 100-dimensional subspace. U = np.linalg.qr(np.random.normal(0, 1, (1000, 100)))[0] b = np.random.normal(0, 1, 1000) def proj(x): \u0026quot;\u0026quot;\u0026quot;Projection of x onto an affine subspace\u0026quot;\u0026quot;\u0026quot; return b + U.dot(U.T).dot(x-b) x0 = np.random.normal(0, 1, (1000)) xs = gradient_descent(x0, [0.1]*50, quadratic_gradient, proj) # the optimal solution is the projection of the origin x_opt = proj(0) Let\u0026rsquo;s plot the results.\nerror_plot([quadratic(x) for x in xs]) plt.plot(range(len(xs)), [quadratic(x_opt)]*len(xs), label='$\\\\frac{1}{2}|\\!|x_{\\mathrm{opt}}|\\!|^2$') plt.legend(); The orangle line shows the optimal error, which the algorithm reaches quickly.\nThe iterates also converge to the optimal solution in domain as the following plot shows.\nerror_plot([np.linalg.norm(x_opt-x)**2 for x in xs]) Least Squares One of the most fundamental data analysis tools is linear least squares. Given an $m\\times n$ matrix $A$ and a vector $b$ our goal is to find a vector $x\\in\\mathbb{R}^n$ that minimizes the following objective:\n$$f(x) = \\frac 1{2m}\\sum_{i=1}^m (a_i^\\top x - b_j)^2 =\\frac1{2m}\\|Ax-b\\|^2$$ We can verify that $\\nabla f(x) = A^\\top(Ax-b)$ and $\\nabla^2 f(x) = A^\\top A.$\nHence, the objective is $\\beta$-smooth with $\\beta=\\lambda_{\\mathrm{max}}(A^\\top A)$, and $\\alpha$-strongly convex with $\\alpha=\\lambda_{\\mathrm{min}}(A^\\top A)$.\ndef least_squares(A, b, x): \u0026quot;\u0026quot;\u0026quot;Least squares objective.\u0026quot;\u0026quot;\u0026quot; return (0.5/m) * np.linalg.norm(A.dot(x)-b)**2 def least_squares_gradient(A, b, x): \u0026quot;\u0026quot;\u0026quot;Gradient of least squares objective at x.\u0026quot;\u0026quot;\u0026quot; return A.T.dot(A.dot(x)-b)/m Overdetermined case $m\\ge n$ m, n = 1000, 100 A = np.random.normal(0, 1, (m, n)) x_opt = np.random.normal(0, 1, n) noise = np.random.normal(0, 0.1, m) b = A.dot(x_opt) + noise objective = lambda x: least_squares(A, b, x) gradient = lambda x: least_squares_gradient(A, b, x) Convergence in Objective x0 = np.random.normal(0, 1, n) xs = gradient_descent(x0, [0.1]*100, gradient) error_plot([objective(x) for x in xs]) plt.plot(range(len(xs)), [np.linalg.norm(noise)**2]*len(xs), label='noise level') plt.plot(range(len(xs)), [least_squares(A,b,x_opt)]*len(xs), label='optimal') plt.legend(); Convergence in Domain error_plot([np.linalg.norm(x-x_opt)**2 for x in xs]) Underdetermined Case $m \u0026lt; n$ In the underdetermined case, the least squares objective is inevitably not strongly convex, since $A^\\top A$ is a rank deficient matrix and hence $\\lambda_{\\mathrm{min}}(A^\\top A)=0.$\nm, n = 100, 1000 A = np.random.normal(0, 1, (m, n)) b = np.random.normal(0, 1, m) # The least norm solution is given by the pseudo-inverse x_opt = np.linalg.pinv(A).dot(b) objective = lambda x: least_squares(A, b, x) gradient = lambda x: least_squares_gradient(A, b, x) x0 = np.random.normal(0, 1, n) xs = gradient_descent(x0, [0.1]*100, gradient) Results.\nerror_plot([objective(x) for x in xs]) plt.plot(range(len(xs)), [least_squares(A,b,x_opt)]*len(xs), label='optimal') plt.legend(); While we quickly reduce the error, we don\u0026rsquo;t actually converge in domain to the least norm solution. This is just because the function is no longer strongly convex in the underdetermined case.\nerror_plot([np.linalg.norm(x-x_opt)**2 for x in xs], yscale='linear') plt.plot(range(len(xs)), [np.linalg.norm(x_opt)**2]*len(xs), label='$|\\!|x_{\\mathrm{opt}}|\\!|^2$') plt.legend(); $\\ell_2$-regularized least squares In the underdetermined case, it is often desirable to restore strong convexity of the objective function by adding an $\\ell_2^2$-penality, also known as Tikhonov regularization, $\\ell_2$-regularization, or weight decay.\n$$\\frac1{2m}\\|Ax-b\\|^2 + \\frac{\\alpha}2\\|x\\|^2$$ Note: With this modification the objective is $\\alpha$-strongly convex again.\ndef least_squares_l2(A, b, x, alpha=0.1): return least_squares(A, b, x) + (alpha/2) * x.dot(x) def least_squares_l2_gradient(A, b, x, alpha=0.1): return least_squares_gradient(A, b, x) + alpha * x Let\u0026rsquo;s create a least squares instance.\nm, n = 100, 1000 A = np.random.normal(0, 1, (m, n)) b = A.dot(np.random.normal(0, 1, n)) objective = lambda x: least_squares_l2(A, b, x) gradient = lambda x: least_squares_l2_gradient(A, b, x) Note that we can find the optimal solution to the optimization problem in closed form without even running gradient descent by computing $x_{\\mathrm{opt}}=(A^\\top+\\alpha I)^{-1}A^\\top b.$ Please verify that this point is indeed optimal.\nx_opt = np.linalg.inv(A.T.dot(A) + 0.1*np.eye(1000)).dot(A.T).dot(b) Here\u0026rsquo;s how gradient descent fares.\nx0 = np.random.normal(0, 1, n) xs = gradient_descent(x0, [0.1]*500, gradient) We plot the descent.\nerror_plot([objective(x) for x in xs]) plt.plot(range(len(xs)), [least_squares_l2(A,b,x_opt)]*len(xs), label='optimal') plt.legend(); You see that the error doesn\u0026rsquo;t decrease below a certain level due to the regularization term. This is not a bad thing. In fact, the regularization term gives as strong convexity which leads to convergence in domain again:\nxs = gradient_descent(x0, [0.1]*500, gradient) error_plot([np.linalg.norm(x-x_opt)**2 for x in xs]) plt.plot(range(len(xs)), [np.linalg.norm(x_opt)**2]*len(xs), label='squared norm of $x_{\\mathrm{opt}}$') plt.legend(); The Magic of Implicit Regularization Sometimes simply running gradient descent from a suitable initial point has a regularizing effect on its own without introducing an explicit regularization term.\nWe will see this below where we revisit the unregularized least squares objective, but initialize gradient descent from the origin rather than a random gaussian point.\n# We initialize from 0 x0 = np.zeros(n) # Note this is the gradient w.r.t. the unregularized objective! gradient = lambda x: least_squares_gradient(A, b, x) xs = gradient_descent(x0, [0.1]*50, gradient) error_plot([np.linalg.norm(x_opt-x)**2 for x in xs], yscale='linear') plt.plot(range(len(xs)), [np.linalg.norm(x_opt)**2]*len(xs), label='$|\\!|x_{\\mathrm{opt}}|\\!|^2$') plt.legend(); Incredible! We converge to the minimum norm solution!\nImplicit regularization is a deep phenomenon that\u0026rsquo;s an active research topic in learning and optimization. It\u0026rsquo;s exciting that we see it play out in this simple least squares problem already!\nLASSO LASSO is the name for $\\ell_1$-regularized least squares regression:\n$$\\frac1{2m}\\|Ax-b\\|^2 + \\alpha\\|x\\|_1$$ We will see that LASSO is able to fine sparse solutions if they exist. This is a common motivation for using an $\\ell_1$-regularizer.\ndef lasso(A, b, x, alpha=0.1): return least_squares(A, b, x) + alpha * np.linalg.norm(x, 1) def ell1_subgradient(x): \u0026quot;\u0026quot;\u0026quot;Subgradient of the ell1-norm at x.\u0026quot;\u0026quot;\u0026quot; g = np.ones(x.shape) g[x \u0026lt; 0.] = -1.0 return g def lasso_subgradient(A, b, x, alpha=0.1): \u0026quot;\u0026quot;\u0026quot;Subgradient of the lasso objective at x\u0026quot;\u0026quot;\u0026quot; return least_squares_gradient(A, b, x) + alpha*ell1_subgradient(x) m, n = 100, 1000 A = np.random.normal(0, 1, (m, n)) x_opt = np.zeros(n) x_opt[:10] = 1.0 b = A.dot(x_opt) x0 = np.random.normal(0, 1, n) xs = gradient_descent(x0, [0.1]*500, lambda x: lasso_subgradient(A, b, x)) error_plot([lasso(A, b, x) for x in xs]) plt.figure() plt.title('Comparison of initial, optimal, and computed point') idxs = range(50) plt.plot(idxs, x0[idxs], '--', color='#aaaaaa', label='initial') plt.plot(idxs, x_opt[idxs], 'r-', label='optimal') plt.plot(idxs, xs[-1][idxs], 'g-', label='final') plt.xlabel('Coordinate') plt.ylabel('Value') plt.legend(); As promised, LASSO correctly identifies the significant coordinates of the optimal solution. This is why, in practice, LASSO is a popular tool for feature selection.\nPlay around with this plot to inspect other points along the way, e.g., the point that achieves lowest objective value. Why does the objective value go up even though we continue to get better solutions?\nSupport Vector Machines In a linear classification problem, we\u0026rsquo;re given $m$ labeled points $(a_i, y_i)$ and we wish to find a hyperplane given by a point $x$ that separates them so that\n$\\langle a_i, x\\rangle \\ge 1$ when $y_i=1$, and $\\langle a_i, x\\rangle \\le -1$ when $y_i = -1$ The smaller the norm $|x|$ the larger the margin between positive and negative instances. Therefore, it makes sense to throw in a regularizer that penalizes large norms. This leads to the objective.\n$$\\frac 1m \\sum_{i=1}^m \\max\\{1-y_i(a_i^\\top x), 0\\} + \\frac{\\alpha}2\\|x\\|^2$$ def hinge_loss(z): return np.maximum(1.-z, np.zeros(z.shape)) def svm_objective(A, y, x, alpha=0.1): \u0026quot;\u0026quot;\u0026quot;SVM objective.\u0026quot;\u0026quot;\u0026quot; m, _ = A.shape return np.mean(hinge_loss(np.diag(y).dot(A.dot(x))))+(alpha/2)*x.dot(x) z = np.linspace(-2, 2, 100) plt.figure() plt.plot(z, hinge_loss(z)); def hinge_subgradient(z): g = np.zeros(z.shape) g[z \u0026lt; 1] = -1. return g def svm_subgradient(A, y, x, alpha=0.1): g1 = hinge_subgradient(np.diag(y).dot(A.dot(x))) g2 = np.diag(y).dot(A) return g1.dot(g2) + alpha*x plt.figure() plt.plot(z, hinge_subgradient(z)); m, n = 1000, 100 A = np.vstack([np.random.normal(0.1, 1, (m//2, n)), np.random.normal(-0.1, 1, (m//2, n))]) y = np.hstack([np.ones(m//2), -1.*np.ones(m//2)]) x0 = np.random.normal(0, 1, n) xs = gradient_descent(x0, [0.01]*100, lambda x: svm_subgradient(A, y, x, 0.05)) error_plot([svm_objective(A, y, x) for x in xs]) Let\u0026rsquo;s see if averaging out the solutions gives us an improved function value.\nxavg = 0.0 for x in xs: xavg += x svm_objective(A, y, xs[-1]), svm_objective(A, y, xavg/len(xs)) (1.0710162653835846, 0.9069593413738611) We can also look at the accuracy of our linear model for predicting the labels. From how we defined the data, we can see that the all ones vector is the highest accuracy classifier in the limit of infinite data (very large $m$). For a finite data set, the accuracy could be even higher due to random fluctuations.\ndef accuracy(A, y, x): return np.mean(np.diag(y).dot(A.dot(x))\u0026gt;0) plt.figure() plt.ylabel('Accuracy') plt.xlabel('Step') plt.plot(range(len(xs)), [accuracy(A, y, x) for x in xs]) plt.plot(range(len(xs)), [accuracy(A, y, np.ones(n))]*len(xs), label='Population optimum') plt.legend(); We see that the accuracy spikes pretty early and drops a bit as we train for too long.\nSparse Inverse Covariance Estimation Given a positive semidefinite matrix $S\\in\\mathbb{R}^{n\\times n}$ the objective function in sparse inverse covariance estimation is as follows:\n$$ \\min_{X\\in\\mathbb{R}^{n\\times n}, X\\succeq 0} \\langle S, X\\rangle - \\log\\det(X) + \\alpha\\|X\\|_1$$ Here, we define $$\\langle S, X\\rangle = \\mathrm{trace}(S^\\top X)$$ and $$|X|1 = \\sum{ij}|X_{ij}|.$$\nTypically, we think of the matrix $S$ as a sample covariance matrix of a set of vectors $a_1,\\dots, a_m,$ defined as: $$ S = \\frac1{m-1}\\sum_{i=1}^n a_ia_i^\\top $$ The example also highlights the utility of automatic differentiation as provided by the autograd package that we\u0026rsquo;ll regularly use. In a later lecture we will understand exactly how automatic differentiation works. For now we just treat it as a blackbox that gives us gradients.\nnp.random.seed(1337) def sparse_inv_cov(S, X, alpha=0.1): return (np.trace(S.T.dot(X)) - np.log(np.linalg.det(X)) + alpha * np.sum(np.abs(X))) n = 5 A = np.random.normal(0, 1, (n, n)) S = A.dot(A.T) objective = lambda X: sparse_inv_cov(S, X) # autograd provides a \u0026quot;gradient\u0026quot;, yay! gradient = grad(objective) We also need to worry about the projection onto the positive semidefinite cone, which corresponds to truncating eigenvalues.\ndef projection(X): \u0026quot;\u0026quot;\u0026quot;Projection onto positive semidefinite cone.\u0026quot;\u0026quot;\u0026quot; es, U = np.linalg.eig(X) es[es\u0026lt;0] = 0. return U.dot(np.diag(es).dot(U.T)) A0 = np.random.normal(0, 1, (n,n)) X0 = A0.dot(A0.T) Xs = gradient_descent(X0, [0.01]*500, gradient, projection) error_plot([objective(X) for X in Xs]) Going crazy with autograd Just for fun, we\u0026rsquo;ll go through a crazy example below. We can use autograd not just for getting gradients for natural objectives, we can in principle also use it to tune hyperparameters of our optimizer, like the step size schedulde.\nBelow we see how we can find a better 10-step learning rate schedules for optimizing a quadratic. This is mostly just for illustrative purposes (although some researchers are exploring these kinds of ideas more seriously).\nx0 = np.random.normal(0, 1, 1000) def f(x): return 0.5*np.dot(x,x) def optimizer(steps): \u0026quot;\u0026quot;\u0026quot;Optimize a quadratic with the given steps.\u0026quot;\u0026quot;\u0026quot; xs = gradient_descent(x0, steps, grad(f)) return f(xs[-1]) The function optimizer is a non-differentiable function of its input steps. Nontheless, autograd will provide a gradient that we can stick into gradient descent. That is, we\u0026rsquo;re tuning gradient descent with gradient descent.\ngrad_optimizer = grad(optimizer) initial_steps = np.abs(np.random.normal(0, 0.1, 10)) better_steps = gradient_descent(initial_steps, [0.001]*500, grad_optimizer) error_plot([optimizer(steps) for steps in better_steps]) As we can see, the learning rate schedules improve dramatically over time. Of course, we already know from the first example that there is a step size schedule that converges in one step. Interestingly, the last schedule we find here doesn\u0026rsquo;t look at all like what we might expect:\nplt.figure() plt.xticks(range(len(better_steps[-1]))) plt.ylabel('Step size') plt.xlabel('Step number') plt.plot(range(len(better_steps[-1])), better_steps[-1]); ","date":1646784000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1646784000,"objectID":"95b7f39b86cd1dae5bb9b292f50b65a4","permalink":"https://matteocourthoud.github.io/course/ml-econ/06_convexity/","publishdate":"2022-03-09T00:00:00Z","relpermalink":"/course/ml-econ/06_convexity/","section":"course","summary":"# Remove warnings import warnings warnings.filterwarnings('ignore') # Import import autograd.numpy as np from autograd import grad import seaborn as sns # Import matplotlib for graphs import matplotlib.pyplot as plt from mpl_toolkits.","tags":null,"title":"Convexity and Optimization","type":"book"},{"authors":null,"categories":null,"content":"Instrumental Variables Endogeneity We say that there is endogeneity in the linear regression model if $\\mathbb E[x_i \\varepsilon_i] \\neq 0$.\nThe random vector $z_i$ is an instrumental variable in the linear regression model if the following conditions are met.\nExclusion restriction: the instruments are uncorrelated with the regression error $$ \\mathbb E_n[z_i \\varepsilon_i] = 0 $$ almost surely, i.e. with probability $p \\to 1$. Rank condition: no linearly redundant instruments $$ \\mathbb E_n[z_i z_i\u0026rsquo;] \\neq 0 $$ almost surely, i.e. with probability $p \\to 1$. Relevance condition (need $L \u0026gt; K$): $$ rank \\ (\\mathbb E_n[z_i x_i\u0026rsquo;]) = K $$ almost surely, i.e. with probability $p \\to 1$. IV and 2SLS Let $K = dim(x_i)$ and $L = dim(z_i)$. We say that the model is just-identified if $L = K$ (method: IV) and over-identified if $L \u0026gt; K$ (method: 2SLS).\nAssume $z_i$ satisfies the instrumental variable assumptions above and $dim(z_i) = dim(x_i)$, then the instrumental variables (IV) estimator $\\hat{\\beta} _ {IV}$ is given by $$ \\begin{aligned} \\hat{\\beta} _ {IV} \u0026amp;= \\mathbb E_n[z_i x_i\u0026rsquo;]^{-1} \\mathbb E_n[z_i y_i] = \\newline \u0026amp;= \\left( \\frac{1}{n} \\sum _ {i=1}^n z_i x_i\\right)^{-1} \\left( \\frac{1}{n} \\sum _ {i=1}^n z_i y_i\\right) = \\newline \u0026amp;= (Z\u0026rsquo;X)^{-1} (Z\u0026rsquo;y) \\end{aligned} $$\nAssume $z_i$ satisfies the instrumental variable assumptions above and $dim(z_i) \u0026gt; dim(x_i)$, then the two-stage-least squares (2SLS) estimator $\\hat{\\beta} _ {2SLS}$ is given by $$ \\hat{\\beta} _ {2SLS} = \\Big( X\u0026rsquo;Z (Z\u0026rsquo;Z)^{-1} Z\u0026rsquo;X \\Big)^{-1} \\Big( X\u0026rsquo;Z (Z\u0026rsquo;Z)^{-1} Z\u0026rsquo;y \\Big) $$ Where $\\hat{x}_i$ is the predicted $x_i$ from the first stage regression of $x_i$ on $z_i$. This is equivalent to the IV estimator using $\\hat{x}_i$ as an instrument for $x_i$.\n2SLS Algebra The estimator is called two-stage-least squares since it can be rewritten as an IV estimator that uses $\\hat{X}$ as instrument: $$ \\begin{aligned} \\hat{\\beta} _ {\\text{2SLS}} \u0026amp;= \\Big( X\u0026rsquo;Z (Z\u0026rsquo;Z)^{-1} Z\u0026rsquo;X \\Big)^{-1} \\Big( X\u0026rsquo;Z (Z\u0026rsquo;Z)^{-1} Z\u0026rsquo;y \\Big) = \\newline \u0026amp;= (\\hat{X}\u0026rsquo; X)^{-1} \\hat{X}\u0026rsquo; y = \\newline \u0026amp;= \\mathbb E_n[\\hat{x}_i x_i\u0026rsquo;]^{-1} \\mathbb E_n[\\hat{x}_i y_i] \\end{aligned} $$\nMoreover it can be rewritten as $$ \\begin{aligned} \\hat{\\beta} _ {\\text{2SLS}} \u0026amp;= (\\hat{X}\u0026rsquo; X)^{-1} \\hat{X}\u0026rsquo; y = \\newline \u0026amp;= (X\u0026rsquo; P_Z X)^{-1} X\u0026rsquo; P_Z y = \\newline \u0026amp;= (X\u0026rsquo; P_Z P_Z X)^{-1} X\u0026rsquo; P_Z y = \\newline \u0026amp;= (\\hat{X}\u0026rsquo; \\hat{X})^{-1} \\hat{X}\u0026rsquo; y = \\newline \u0026amp;= \\mathbb E_n [\\hat{x}_i \\hat{x}_i]^{-1} \\mathbb E_n[\\hat{x}_i y_i] \\end{aligned} $$\nRule of Thumb How to the test the relevance condition? Rule of thumb: $F$-test in the first stage $\u0026gt;10$ (joint test on $z_i$).\nProblem: as $n \\to \\infty$, with finite $L$, $F \\to \\infty$ (bad rule of thumb).\nEquivalence Theorem\nIf $K=L$, $\\hat{\\beta} _ {\\text{2SLS}} = \\hat{\\beta} _ {\\text{IV}}$.\nProof\nIf $K=L$, $X\u0026rsquo;Z$ and $Z\u0026rsquo;X$ are squared matrices and, by the relevance condition, non-singular (invertible). $$ \\begin{aligned} \\hat{\\beta} _ {\\text{2SLS}} \u0026amp;= \\Big( X\u0026rsquo;Z (Z\u0026rsquo;Z)^{-1} Z\u0026rsquo;X \\Big)^{-1} \\Big( X\u0026rsquo;Z (Z\u0026rsquo;Z)^{-1} Z\u0026rsquo;y \\Big) = \\newline \u0026amp;= (Z\u0026rsquo;X)^{-1} (Z\u0026rsquo;Z) (X\u0026rsquo;Z)^{-1} X\u0026rsquo;Z (Z\u0026rsquo;Z)^{-1} Z\u0026rsquo;y = \\newline \u0026amp;= (Z\u0026rsquo;X)^{-1} (Z\u0026rsquo;Z) (Z\u0026rsquo;Z)^{-1} Z\u0026rsquo;y = \\newline \u0026amp;= (Z\u0026rsquo;X)^{-1} (Z\u0026rsquo;y) = \\newline \u0026amp;= \\hat{\\beta} _ {\\text{IV}} \\end{aligned} $$ $$\\tag*{$\\blacksquare$}$$\nDemand Example Example from Hayiashi (2000) page 187: demand and supply simultaneous equations. $$ \\begin{aligned} \u0026amp; q_i^D(p_i) = \\alpha_0 + \\alpha_1 p_i + u_i \\newline \u0026amp; q_i^S(p_i) = \\beta_0 + \\beta_1 p_i + v_i \\end{aligned} $$\nWe have an endogeneity problem. To see why, we solve the system of equations for $(p_i, q_i)$: $$ \\begin{aligned} \u0026amp; p_i = \\frac{\\beta_0 - \\alpha_0}{\\alpha_1 - \\beta_1} + \\frac{v_i - u_i}{\\alpha_1 - \\beta_1 } \\newline \u0026amp; q_i = \\frac{\\alpha_1\\beta_0 - \\alpha_0 \\beta_1}{\\alpha_1 - \\beta_1} + \\frac{\\alpha_1 v_i - \\beta_1 u_i}{\\alpha_1 - \\beta_1 } \\end{aligned} $$\nDemand Example (2) Then the price variable is not independent from the error term in neither equation: $$ \\begin{aligned} \u0026amp; Cov(p_i, u_i) = - \\frac{Var(u_i)}{\\alpha_1 - \\beta_1 } \\newline \u0026amp; Cov(p_i, v_i) = \\frac{Var(v_i)}{\\alpha_1 - \\beta_1 } \\end{aligned} $$\nAs a consequence, the OLS estimators are not consistent: $$ \\begin{aligned} \u0026amp; \\hat{\\alpha} _ {1, OLS} \\overset{p}{\\to} \\alpha_1 + \\frac{Cov(p_i, u_i)}{Var(p_i)} \\newline \u0026amp; \\hat{\\beta} _ {1, OLS} \\overset{p}{\\to} \\beta_1 + \\frac{Cov(p_i, v_i)}{Var(p_i)} \\end{aligned} $$\nDemand Example (3) In general, running regressing $q$ on $p$ you estimate $$ \\begin{aligned} \\hat{\\gamma} _ {OLS} \u0026amp;\\overset{p}{\\to} \\frac{Cov(p_i, q_i)}{Var(p_i)} = \\newline \u0026amp;= \\frac{\\alpha_1 Var(v_i) + \\beta_1 Var(u_i)}{(\\alpha_1 - \\beta_1)^2} \\left( \\frac{Var(v_i) + Var(u_i)}{(\\alpha_1 - \\beta_1)^2} \\right)^{-1} = \\newline \u0026amp;= \\frac{\\alpha_1 Var(v_i) + \\beta_1 Var(u_i)}{Var(v_i) + Var(u_i)} \\end{aligned} $$ Which is neither $\\alpha_1$ nor $\\beta_1$ but a variance weighted average of the two.\nDemand Example (4) Suppose we have a supply shifter $z_i$ such that\n$\\mathbb E[z_i v_i] \\neq 0$ $\\mathbb E[z_i u_i] = 0$. We combine the second condition and $\\mathbb E[u_i] = 0$ to get a system of 2 equations in 2 unknowns: $\\alpha_0$ and $\\alpha_1$. $$ \\begin{aligned} \u0026amp; \\mathbb E[z_i u_i] = \\mathbb E[ z_i (q_i^D(p_i) - \\alpha_0 - \\alpha_1 p_i) ] = 0 \\newline \u0026amp; \\mathbb E[u_i] = \\mathbb E[q_i^D(p_i) - \\alpha_0 - \\alpha_1 p_i] = 0\n\\end{aligned} $$\nWe could try to solve for the vector $\\alpha$ that solves $$ \\begin{aligned} \u0026amp; \\mathbb E_n[z_i (q_i^D - x_i\\alpha)] = 0 \\newline \u0026amp; \\mathbb E_n[z_i q_i^D] - \\mathbb E_n[z_ix_i\\alpha] = 0 \\end{aligned} $$\nIf $\\mathbb E_n[z_ix_i]$ is invertible, we get $\\hat{\\alpha} = \\mathbb E_n[z_ix_i]^{-1} \\mathbb E_n[z_i q^D_i]$ which is indeed the IV estimator of $\\alpha$ using $z_i$ as an instrument for the endogenous variable $p_i$.\nCode - DGP This code draws 100 observations from the model $y = 2 x_1 - x_2 + \\varepsilon$ where $x_1, x_2 \\sim U[0,1]$ and $\\varepsilon \\sim N(0,1)$.\n# Set seed Random.seed!(123); # Set the number of observations n = 100; # Set the dimension of Z l = 3; # Draw instruments Z = rand(Uniform(0,1), n, l); # Correlation matrix for error terms S = [1 0.8; 0.8 1]; # Endogenous X γ = [2 0; 0 -1; -1 3]; ε = rand(Normal(0,1), n, 2) * cholesky(S).U; X = Z*γ .+ ε[:,1]; # Calculate y y = X*β .+ ε[:,2]; Code - IV # Estimate beta OLS β_OLS = (X'*X)\\(X'*y) ## 2-element Array{Float64,1}: ## 2.335699233358403 ## -0.8576266209987325 # IV: l=k=2 instruments Z_IV = Z[:,1:k]; β_IV = (Z_IV'*X)\\(Z_IV'*y) ## 2-element Array{Float64,1}: ## 1.6133344277861439 ## -0.6678537395714547 # Calculate standard errors ε_hat = y - X*β_IV; V_NHC_IV = var(ε_hat) * inv(Z_IV'*X)*Z_IV'*Z_IV*inv(Z_IV'*X); V_HC0_IV = inv(Z_IV'*X)*Z_IV' * (I(n) .* ε_hat.^2) * Z_IV*inv(Z_IV'*X); Code - 2SLS # 2SLS: l=3 instruments Pz = Z*inv(Z'*Z)*Z'; β_2SLS = (X'*Pz*X)\\(X'*Pz*y) ## 2-element Array{Float64,1}: ## 1.904553638377971 ## -0.8810907510370429 # Calculate standard errors ε_hat = y - X*β_2SLS; V_NCH_2SLS = var(ε_hat) * inv(X'*Pz*X); V_HC0_2SLS = inv(X'*Pz*X)*X'*Pz * (I(n) .* ε_hat.^2) *Pz*X*inv(X'*Pz*X); GMM Setting We have a system of $L$ moment conditions $$ \\begin{aligned} \u0026amp; \\mathbb E[g_1(\\omega_i, \\delta_0)] = 0 \\newline \u0026amp; \\vdots \\newline \u0026amp; \\mathbb E[g_L(\\omega_i, \\delta_0)] = 0 \\end{aligned} $$\nIf $L = \\dim (\\delta_0)$, no problem. If $L \u0026gt; \\dim (\\delta_0)$, there may be no solution to the system of equations.\nOptions There are two possibilities.\nFirst Solution: add moment conditions until the system is identified $$ \\mathbb E[ a\u0026rsquo; g(\\omega_i, \\delta_0)] = 0 $$ Solve $\\mathbb E[Ag(\\omega_i, \\delta)] = 0$ for $\\hat{\\delta}$. How to choose $A$? Such that it minimizes $Var(\\hat{\\delta})$. Second Solution: generalized method of moments (GMM) $$ \\begin{aligned} \\hat{\\delta} _ {GMM} \u0026amp;= \\arg \\min _ \\delta \\quad \\Big| \\Big| \\mathbb E_n [ g(\\omega_i, \\delta) ] \\Big| \\Big| = \\newline \u0026amp;= \\arg \\min _ \\delta \\quad n \\mathbb E_n[g(\\omega_i, \\delta)]\u0026rsquo; W \\mathbb E_n [g(\\omega_i, \\delta)] \\end{aligned} $$ The choice of $A$ and $W$ are closely related to each other.\n1-step GMM Since $J(\\delta,W)$ is a quadratic form, a closed form solution exists: $$ \\hat{\\delta}(W) = \\Big(\\mathbb E_n[z_i x_i\u0026rsquo;] W \\mathbb E_n[z_i x_i\u0026rsquo;] \\Big)^{-1}\\mathbb E_n[z_i x_i\u0026rsquo;] W \\mathbb E_n[z_i y_i] $$\nAssumptions for consistency of the GMM estimator given data $\\mathcal D = \\lbrace y_i, x_i, z_i \\rbrace _ {i=1}^n$:\nLinearity: $y_i = x_i\\gamma_0 + \\varepsilon_i$ IID: $(y_i, x_i, z_i)$ iid Orthogonality: $\\mathbb E [z_i(y_i - x_i\\gamma_0)] = \\mathbb E[z_i \\varepsilon_i] = 0$ Rank identification: $\\Sigma_{xz} = \\mathbb E[z_i x_i\u0026rsquo;]$ has full rank Convergence Theorem\nUnder linearity, independence, orthogonality and rank conditions, if $\\hat{W} \\overset{p}{\\to} W$ positive definite, then $$ \\hat{\\delta}(\\hat{W}) \\to \\delta(W) $$ If in addition to the above assumption, $\\sqrt{n} \\mathbb E_n [g(\\omega_i, \\delta_0)] \\overset{d}{\\to} N(0,S)$ for a fixed positive definite $S$, then $$ \\sqrt{n} (\\hat{\\delta} (\\hat{W}) - \\delta(W)) \\overset{d}{\\to} N(0,V) $$ where $V = (\\Sigma\u0026rsquo; _ {xz} W \\Sigma _ {xz})^{-1} \\Sigma _ {xz} W S W \\Sigma _ {xz}(\\Sigma\u0026rsquo; _ {xz} W \\Sigma _ {xz})^{-1}$.\nFinally, if a consistent estimator $\\hat{S}$ of $S$ is available, then using sample analogues $\\hat{\\Sigma}_{xz}$ it follows that $$ \\hat{V} \\overset{p}{\\to} V $$\nIf $W = S^{-1}$ then $V$ reduces to $V = (\\Sigma\u0026rsquo; _ {xz} W \\Sigma _ {xz})^{-1}$. Moreover, $(\\Sigma\u0026rsquo; _ {xz} W \\Sigma _ {xz})^{-1}$ is the smallest possible form of $V$, in a positive definite sense.\nTherefore, to have an efficient estimator, you want to construct $\\hat{W}$ such that $\\hat{W} \\overset{p}{\\to} S^{-1}$.\n2-step GMM Estimation steps:\nChoose an arbitrary weighting matrix $\\hat{W}_{init}$ (usually the identity matrix $I_K$) Estimate $\\hat{\\delta} _ {init}(\\hat{W} _ {init})$ Estimate $\\hat{S}$ (asymptotic variance of the moment condition) Estimate $\\hat{\\delta}(\\hat{S}^{-1})$ On the procedure:\nThis estimator achieves the semiparametric efficiency bound. This strategy works only if $\\hat{S} \\overset{p}{\\to} S$ exists. For iid cases: we can use $\\hat{\\delta} = \\mathbb E_n[(\\hat{\\varepsilon}_i z_i)(\\hat{\\varepsilon}_i z_i) \u0026rsquo; ]$ where $\\hat{\\varepsilon}_i = y_i - x_i \\hat{\\delta}(\\hat{W} _ {init})$. Code - 1-step GMM # GMM 1-step: inefficient weighting matrix W_1 = I(l); # Objective function gmm_1(b) = ( y - X*b )' * Z * W_1 * Z' * ( y - X*b ); # Estimate GMM β_gmm_1 = optimize(gmm_1, β_OLS).minimizer ## 2-element Array{Float64,1}: ## 1.91556882526808 ## -0.8769689391885799 # Standard errors GMM ε_hat = y - X*β_gmm_1; S_hat = Z' * (I(n) .* ε_hat.^2) * Z; d_hat = -X'*Z; V_gmm_1 = inv(d_hat * inv(S_hat) * d_hat') ## 2×2 Array{Float64,2}: ## 0.0158497 -0.00346601 ## -0.00346601 0.00616531 Code - 2-step GMM # GMM 2-step: efficient weighting matrix W_2 = inv(S_hat); # Objective function gmm_2(b) = ( y - X*b )' * Z * W_2 * Z' * ( y - X*b ); # Estimate GMM β_gmm_2 = optimize(gmm_2, β_OLS).minimizer ## 2-element Array{Float64,1}: ## 1.905326742963115 ## -0.881808949213345 # Standard errors GMM ε_hat = y - X*β_gmm_2; S_hat = Z' * (I(n) .* ε_hat.^2) * Z; d_hat = -X'*Z; V_gmm_2 = inv(d_hat * inv(S_hat) * d_hat') ## 2×2 Array{Float64,2}: ## 0.0162603 -0.00357632 ## -0.00357632 0.00631259 Testing Overidentifying Restrictions If the equations are exactly identified, then it is possible to choose $\\delta$ so that all the elements of the sample moments $\\mathbb E_n[g(\\omega_i; \\delta)]$ are zero and thus that the distance $$ J(\\delta, \\hat{W}) = n \\mathbb E_n[g(\\omega_i, \\delta)]\u0026rsquo; \\hat{W} \\mathbb E_n[g(\\omega_i, \\delta)] $$ is zero. (The $\\delta$ that does it is the IV estimator.)\nIf the equations are overidentified, i.e. $L$ (number of instruments) $\u0026gt; K$ (number of equations), then the distance cannot be zero exactly in general, but we would expect the minimized distance to be close to zero.\nNaive Test Suppose your model is overidentified ($L \u0026gt; K$) and you use the following naive testing procedure:\nEstimate $\\hat{\\delta}$ using a subset of dimension $K$ of instruments $\\lbrace z_1 , .. , z_K\\rbrace$ for $\\lbrace x_1 , \u0026hellip; , x_K\\rbrace$ Set $\\hat{\\varepsilon}_i = y_i - x_i \\hat{\\delta} _ {\\text{GMM}}$ Infer the size of the remaining $L-K$ moment conditions $\\mathbb E[z _{i, K+1} \\varepsilon_i], \u0026hellip;, \\mathbb E[z _{i, L} \\varepsilon_i]$ looking at their empirical counterparts $\\mathbb E_n[z _{i, K+1} \\hat{\\varepsilon}_i], \u0026hellip;, \\mathbb E_n[z _{i, L} \\hat{\\varepsilon}_i]$ Reject exogeneity if the empirical expectations are high. How high? Calculate p-values. Example If you have two invalid instruments and you use one to test the validity of the other, it might happen by chance that you don’t reject it.\nModel: $y_i = x_i + \\varepsilon_i$ and $x_i = \\frac{1}{2} z _{i1} - \\frac{1}{2} z _{i2} + u_i$\nHave $$ Cov (z _{i1}, z _{i2}, \\varepsilon_i, u_i) = \\begin{bmatrix} 1 \u0026amp; 0 \u0026amp; 0 \u0026amp; 0 \\newline 0 \u0026amp; 1 \u0026amp; 0 \u0026amp; 0 \\newline 0 \u0026amp; 0 \u0026amp; 1 \u0026amp; 0.5 \\newline 0 \u0026amp; 0 \u0026amp; 0.5 \u0026amp; 1 \\end{bmatrix} $$\nYou want to test whether the second instrument is valid (is not since $\\mathbb E[z_2 \\varepsilon] \\neq 0$). You use $z_1$ and estimate $\\hat{\\beta} \\to$ the estimator is consistent.\nYou obtain $\\mathbb E_n[z _{i2} \\hat{\\varepsilon}_i] \\simeq 0$ even if $z_2$ is invalid\nProblem: you are using an invalid instrument in the first place.\nHansen’s Test Theorem: We are interested in testing $H_0: \\mathbb E[z_i \\varepsilon_i] = 0$ against $H_1: \\mathbb E[z_i \\varepsilon_i] \\neq 0$. Suppose $\\hat{S} \\overset{p}{\\to} S$. Then $$ J(\\hat{\\delta}(\\hat{S}^{-1}) , \\hat{S}^{-1}) \\overset{d}{\\to} \\chi^2 _ {L-K} $$ For $c$ satisfying $\\alpha = 1- G_{L - K} ( c )$, $\\Pr(J\u0026gt;c | H_0) \\to \\alpha$ so the test reject $H_0$ if $J \u0026gt; c$ has asymptotic size $\\alpha$.\nComments The degrees of freedom of the asymptotic distribution are the number of overidentifying restrictions. This is a specification test, testing whether all model assumptions are true jointly. Only when we are confident that about the other assumptions, can we interpret a large $J$ statistic as evidence for the endogeneity of some of the $L$ instruments included in $x$. Unlike the tests we have encountered so far, the test is not consistent against some failures of the orthogonality conditions (that is, it is not consistent against some fixed elements of the alternative). Several papers in the July 1996 issue of JBES report that the finite-sample null rejection probability of the test can far exceed the nominal significance level $\\alpha$. Special Case: Conditional Homoskedasticity The main implication of conditional homoskedasticity is that efficient GMM becomes 2SLS. With efficient GMM estimation, the weighting matrix is $\\hat{S}^{-1} = \\mathbb En [z_i z_i\u0026rsquo; \\varepsilon_i^2]^{-1}$. With conditional homoskedasticity, the efficient weighting matrix is $\\mathbb E_n[z_iz_i\u0026rsquo;]^{-1} \\sigma^{-2}$, or equivalently $\\mathbb E_n[z_iz_i\u0026rsquo;]^{-1}$. Then, the GMM estimator becomes $$ \\hat{\\delta}(\\hat{S}^{-1}) = \\Big(\\mathbb E_n[z_i x_i\u0026rsquo;]\u0026rsquo; \\underbrace{\\mathbb E_n[z_iz_i\u0026rsquo;]^{-1} \\mathbb E[z_i x_i\u0026rsquo;]} _ {\\text{ols of } x_i \\text{ on }z_i} \\Big)^{-1}\\mathbb E_n[z_i x_i\u0026rsquo;]\u0026rsquo; \\underbrace{\\mathbb E_n[z_iz_i\u0026rsquo;]^{-1} \\mathbb E[z_i y_i\u0026rsquo;]} _ {\\text{ols of } y_i \\text{ on }z_i}= \\hat{\\delta} _ {2SLS} $$\nProof: Consider the matrix notation. $$ \\begin{aligned} \\hat{\\delta} \\left( \\frac{Z\u0026rsquo;Z}{n}\\right) \u0026amp;= \\left( \\frac{X\u0026rsquo;Z}{n} \\left( \\frac{Z\u0026rsquo;Z}{n}\\right)^{-1} \\frac{Z\u0026rsquo;X}{n} \\right)^{-1} \\frac{X\u0026rsquo;Z}{n} \\left( \\frac{Z\u0026rsquo;Z}{n}\\right)^{-1} \\frac{Z\u0026rsquo;Y}{n} = \\newline \u0026amp;= \\left( X\u0026rsquo;Z(Z\u0026rsquo;Z)^{-1} Z\u0026rsquo;X \\right)^{-1} X\u0026rsquo;Z(Z\u0026rsquo;Z)^{-1} Z\u0026rsquo;Y = \\newline \u0026amp;= \\left(X\u0026rsquo;P_ZX\\right)^{-1} X\u0026rsquo;P_ZY = \\newline \u0026amp;= \\left(X\u0026rsquo;P_ZP_ZX\\right)^{-1} X\u0026rsquo;P_ZY = \\newline \u0026amp;= \\left(\\hat{X}\u0026rsquo;_Z \\hat{X}_Z\\right)^{-1} \\hat{X}\u0026rsquo;_ZY = \\newline \u0026amp;= \\hat{\\delta} _ {2SLS} \\end{aligned} $$ $$\\tag*{$\\blacksquare$}$$\nSmall-Sample Properties of 2SLS Theorem: When the number of instruments is equal to the sample size ($L = n$), then $\\hat{\\delta} _ {2SLS} = \\hat{\\delta} _ {OLS}$\nProof: We have a perfect prediction problem. The first stage estimated coefficient $\\hat{\\gamma}$ is such that it solves the normal equations: $\\hat{\\gamma} = z_i^{-1} x_i$. Then $$ \\begin{aligned} \\hat{\\delta} _ {2SLS} \u0026amp;= \\mathbb E_n[\\hat{x}_i x\u0026rsquo;_i]^{-1} \\mathbb E_n[\\hat{x}_i y_i] = \\newline \u0026amp;= \\mathbb E_n[z_i z_i^{-1} x_i x\u0026rsquo;_i]^{-1} \\mathbb E_n[z_i z_i^{-1} x_i y_i] = \\newline \u0026amp;= \\mathbb E_n[x_i x\u0026rsquo;_i]^{-1} \\mathbb E_n[x_i y_i] = \\newline \u0026amp;= \\hat{\\delta} _ {OLS} \\end{aligned} $$ $$\\tag*{$\\blacksquare$}$$\nYou have this overfitting problem in general when the number of instruments is large relative to the sample size. This problem arises even if the instruments are valid.\nExample from Angrist (1992) They regress wages on years of schooling. Problem: endogeneity: both variables are correlated with skills which are unobserved. Solution: instrument years of schooling with the quarter of birth. Idea: if born in the first three quarters, can attend school from the year of your sixth birthday. Otherwise, you have to wait one more year. Problem: quarters of birth are three dummies. In order to ``improve the first stage fit” they interact them with year of birth (180 effective instruments) and also with the state (1527 effective instruments). This mechanically increases the $R^2$ but also increases the bias of the 2SLS estimator. Solutions: LIML, JIVE, RJIVE (Hansen et al., 2014), Post-Lasso (Belloni et al., 2012). Example from Angrist (1992) Many Instrument Robust Estimation Issue Why having too many instruments is problematic? As the number of instruments increases, the estimated coefficient gets closer to OLS which is biased. As seen in the theorem above, for $L=n$, the two estimators coincide.\nLIML An alternative method to estimate the parameters of the structural equation is by maximum likelihood. Anderson and Rubin (1949) derived the maximum likelihood estimator for the joint distribution of $(y_i, x_i)$. The estimator is known as limited information maximum likelihood, or LIML.\nThis estimator is called “limited information” because it is based on the structural equation for $(y_i, x_i)$ combined with the reduced form equation for $x_i$. If maximum likelihood is derived based on a structural equation for $x_i$ as well, then this leads to what is known as full information maximum likelihood (FIML). The advantage of the LIML approach relative to FIML is that the former does not require a structural model for $x_i$, and thus allows the researcher to focus on the structural equation of interest - that for $y_i$.\nK-class Estimators The k-class estimators have the form $$ \\hat{\\delta}(\\alpha) = (X\u0026rsquo; P_Z X - \\alpha X\u0026rsquo; X)^{-1} (X\u0026rsquo; P_Z Y - \\alpha X\u0026rsquo; Y) $$\nThe limited information maximum likelihood estimator LIML is the k-class estimator $\\hat{\\delta}(\\alpha)$ where $$ \\alpha = \\lambda_{min} \\Big( ([X\u0026rsquo; , Y]^{-1} [X\u0026rsquo; , Y])^{-1} [X\u0026rsquo; , Y]^{-1} P_Z [X\u0026rsquo; , Y] \\Big) $$\nIf $\\alpha = 0$ then $\\hat{\\delta} _ {\\text{LIML}} = \\hat{\\delta} _ {\\text{2SLS}}$ while for $\\alpha \\to \\infty$, $\\hat{\\delta} _ {\\text{LIML}} \\to \\hat{\\delta} _ {\\text{OLS}}$.\nComments on LIML The particular choice of $\\alpha$ gives a many instruments robust estimate The LIML estimator has no finite sample moments. $\\mathbb E[\\delta(\\alpha_{LIML})]$ does not exist in general In simulation studies performs well Has good asymptotic properties Asymptotically the LIML estimator has the same distribution as 2SLS. However, they can have quite different behaviors in finite samples. There is considerable evidence that the LIML estimator has superior finite sample performance to 2SLS when there are many instruments or the reduced form is weak. However, on the other hand there is worry that since the LIML estimator is derived under normality it may not be robust in non-normal settings.\nJIVE The Jacknife IV procedure is the following\nRegress $\\lbrace x_j \\rbrace _ {j \\neq i}$ on $\\lbrace z_j \\rbrace _ {j \\neq i}$ and estimate $\\pi_{-i}$ (leave the $i^{th}$ observation out). Form $\\hat{x}_i = \\hat{\\pi} _ {-i} z_i$. Run IV using $\\hat{x}_i$ as instruments. $$ \\hat{\\delta} _ {JIVE} = \\mathbb E_n[\\hat{x}_i x_i\u0026rsquo;]^{-1} \\mathbb E_n[\\hat{x}_i y_i\u0026rsquo;] $$ Comments on JIVE: Prevents overfitting. With many instruments you get bad out of sample prediction which implies low correlation between $\\hat{x}_i$ and $x_i$: $\\mathbb E_n[\\hat{x}_i x_i\u0026rsquo;] \\simeq 0$. Use lasso/ridge regression in the first stage in case of too many instruments. Hausman Test Here we consider testing the validity of OLS. OLS is generally preferred to IV in terms of precision. Many researchers only doubt the (joint) validity of the regressor $z_i$ instead of being certain that it is invalid (in the sense of not being predetermined). So then they wish to choose between OLS and 2SLS, assuming that they have an instrument vector $x_i$ whose validity is not in question. Further, assume for simplicity that $L = K$ so that the efficient GMM estimator is the IV estimator.\nThe Hausman test statistic $$ H \\equiv n (\\hat{\\delta} _ {IV} - \\hat{\\delta} _ {OLS})\u0026rsquo; [\\hat{Avar} (\\hat{\\delta} _ {IV} - \\hat{\\delta} _ {OLS})]^{-1} (\\hat{\\delta} _ {IV} - \\hat{\\delta} _ {OLS}) $$ is asymptotically distributed as a $\\chi^2_{L-s}$ under the null where $s = | z_i \\cup x_i |$: the number of regressors that are retained as instruments in $x_i$.\nComments In general, the idea of the Hausman test is the following. If you have two estimators, one which is efficient under $H_0$ but inconsistent under $H_1$ (in this case, OLS), and another which is consistent under $H_1$ (in this case, IV), then construct a test as a quadratic form in the differences of the estimators. Another classic example arises in panel data with the hypothesis $H_0$ of unconditional strict exogeneity. In that case, under $H_0$ Random Effects estimators are efficient but under $H_1$ they are inconsistent. Fixed Effects estimators instead are consistent under $H_1$.\nThe Hausman test statistic can be used as a pretest procedure: select either OLS or IV according to the outcome of the test. Although widely used, this pretest procedure is not advisable. When the null is false, it is still possible that the test accepts the null (committing a Type 2 error). In particular, this can happen with a high probability when the sample size is small and/or when the regressor $z_i$ is almost valid. In such an instance, estimation and also inference will be based on incorrect methods. Therefore, the overall properties of the Hausman pretest procedure are undesirable.\nThe Hausman test is an example of a specification test. There are many other specification tests. One could for example test for conditional homoskedasticity. Unlike for the OLS case, there does not exist a convenient test for conditional homoskedasticity for the GMM case. A test statistic that is asymptotically chi-squared under the null is available but is extremely cumbersome; see White (1982, note 2). If in doubt, it is better to use the more generally valid inference methods that allow for conditional heteroskedasticity. Similarly, there does not exist a convenient test for serial correlation for the GMM case. If in doubt, it is better to use the more generally valid inference methods that allow for serial correlation; for example, when data are collected over time (that is, time-series data).\n","date":1635465600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1635465600,"objectID":"07162f3680824234e3d24605eb09ffd5","permalink":"https://matteocourthoud.github.io/course/metrics/06_endogeneity/","publishdate":"2021-10-29T00:00:00Z","relpermalink":"/course/metrics/06_endogeneity/","section":"course","summary":"Instrumental Variables Endogeneity We say that there is endogeneity in the linear regression model if $\\mathbb E[x_i \\varepsilon_i] \\neq 0$.\nThe random vector $z_i$ is an instrumental variable in the linear regression model if the following conditions are met.","tags":null,"title":"Endogeneity","type":"book"},{"authors":null,"categories":null,"content":"Asymptotic Theory of the OLS Estimator OLS Consistency Theorem: Assume that $(x_i, y_i) _ {i=1}^n$ i.i.d. , $\\mathbb E[x_i x_i\u0026rsquo;] = Q$ positive definite, $\\mathbb E[x_i x_i\u0026rsquo;] \u0026lt; \\infty$ and $\\mathbb E [y_i^2] \u0026lt; \\infty$, then $\\hat \\beta _ {OLS}$ is a consistent estimator of $\\beta_0$, i.e. $\\hat \\beta = \\mathbb E_n [x_i x_i\u0026rsquo;] \\mathbb E_n [x_i y_i]\\overset{p}{\\to} \\beta_0$.\nProof:\nWe consider 4 steps:\n$\\mathbb E_n [x_i x_i\u0026rsquo;] \\xrightarrow{p} \\mathbb E [x_i x_i\u0026rsquo;]$ by WLLN since $x_i x_i\u0026rsquo;$ iid and $\\mathbb E[x_i x_i\u0026rsquo;] \u0026lt; \\infty$. $\\mathbb E_n [x_i y_i] \\xrightarrow{p} \\mathbb E [x_i y_i]$ by WLLN, due to $x_i y_i$ iid, Cauchy-Schwarz and finite second moments of $x_i$ and $y_i$ $$ \\mathbb E \\left[ x_i y_i \\right] \\leq \\sqrt{ \\mathbb E[x_i^2] \\mathbb E[y_i^2]} \u0026lt; \\infty $$ $\\mathbb E_n [x_i x_i\u0026rsquo;]^{-1} \\xrightarrow{p} \\mathbb E [x_i x_i\u0026rsquo;]^{-1}$ by CMT. $\\mathbb E_n [x_i x_i\u0026rsquo;]^{-1} \\mathbb E_n [x_i y_i] \\xrightarrow{p} \\mathbb E [x_i x_i\u0026rsquo;]^{-1} \\mathbb E [x_i y_i] = \\beta$ by CMT. $$\\tag*{$\\blacksquare$}$$ Variance and Assumptions Now we are going to investigate the variance of $\\hat \\beta _ {OLS}$ progressively relaxing the underlying assumptions.\nGaussian error term. Homoskedastic error term. Heteroskedastic error term. Heteroskedastic and autocorrelated error term. Gaussian Error Term Theorem: Under the GM assumption (1)-(5), $\\hat \\beta - \\beta |X \\sim N(0, \\sigma^2 (X\u0026rsquo;X)^{-1})$\nProof:\nWe follow 2 steps:\nWe can rewrite $\\hat \\beta$ as $$ \\begin{aligned} \\hat \\beta \u0026amp; = (X\u0026rsquo;X)^{-1} X\u0026rsquo;y = (X\u0026rsquo;X)^{-1} X\u0026rsquo;(X\\beta + \\varepsilon) \\newline \u0026amp;= \\beta + (X\u0026rsquo;X)^{-1} X\u0026rsquo; \\varepsilon = \\newline \u0026amp;= \\beta + \\mathbb E_n [x_i x_i\u0026rsquo;]^{-1} \\mathbb E_n [x_i \\varepsilon_i] \\end{aligned} $$ Therefore: $\\hat \\beta-\\beta = \\mathbb E_n [x_i x_i\u0026rsquo;]^{-1} \\mathbb E_n [x_i \\varepsilon_i]$. $$ \\begin{aligned} \\hat \\beta-\\beta |X \u0026amp; \\sim (X\u0026rsquo;X)^{-1} X\u0026rsquo; N(0, \\sigma^2 I_n) = \\newline \u0026amp;= N(0, \\sigma^2 (X\u0026rsquo;X)^{-1} X\u0026rsquo;X (X\u0026rsquo;X)^{-1}) = \\newline \u0026amp;= N(0, \\sigma^2 (X\u0026rsquo;X)^{-1}) \\end{aligned} $$ $$\\tag*{$\\blacksquare$}$$ Does it make sense to assume that $\\varepsilon$ is gaussian? Not much. But does it make sense that $\\hat \\beta$ is gaussian? Yes, because it’s an average.\nHomoskedastic Error Term Theorem: Under the assumptions of the previous theorem, plus $\\mathbb E[x^4] \u0026lt; \\infty$, the OLS estimate has an asymptotic normal distribution: $\\hat \\beta|X \\overset{d}{\\to} N(\\beta, \\sigma^2 (X\u0026rsquo;X)^{-1})$.\nProof: $$ \\sqrt{n} (\\hat \\beta - \\beta ) = \\underbrace{\\mathbb E_n [x_i x_i\u0026rsquo;]^{-1}} _ {\\xrightarrow{p} Q^{-1} } \\underbrace{\\sqrt{n} \\mathbb E_n [x_i \\varepsilon_i ]} _ {\\xrightarrow{d} N(0, \\Omega)} \\rightarrow N(0, \\Sigma ) $$ where in general $\\Omega = Var (x_i \\varepsilon_i) = \\mathbb E [(x_i \\varepsilon_i)^2]$ and $\\Sigma = Q^{-1} \\Omega Q^{-1}$. $$\\tag*{$\\blacksquare$}$$\nGiven that $Q = \\mathbb E [x_i x_i\u0026rsquo;]$ is unobserved, we estimate it with $\\hat{Q} = \\mathbb E_n [x_i x_i\u0026rsquo;]$. Since we have assumed homoskedastic error term, we have $\\Omega = \\sigma^2 (X\u0026rsquo;X)^{-1}$. Since we do not observe $\\sigma^2$ we estimate it as $\\hat{\\sigma}^2 = \\mathbb E_n[\\hat{\\varepsilon}_i^2]$.\nThe terms $x_i \\varepsilon_i$ are called scores and we can already see their central importance for inference.\nHeteroskedastic Error Term Assumption: $\\mathbb E [\\varepsilon_i x_i \\varepsilon_j\u0026rsquo; x_j\u0026rsquo;] = 0$, for all $j \\ne i$ and $\\mathbb E [\\varepsilon_i^4] \\leq \\infty$, $\\mathbb E [|| x_i||^4] \\leq C \u0026lt; \\infty$ a.s.\nTheorem: Under GM assumptions (1)-(4) plus heteroskedastic error term, the following estimators are consistent, i.e. $\\hat{\\Sigma}\\xrightarrow{p} \\Sigma$.\nNote that we are only looking at $\\Omega$ of the $\\Sigma = Q^{-1} \\Omega Q^{-1}$ matrix.\nHC0: use the observed residual $\\hat{\\varepsilon}_i$ $$ \\Omega _ {HC0} = \\mathbb E_n [x_i x_i\u0026rsquo; \\hat{\\varepsilon}_i^2] $$ When $k$ is too big relative to $n$ – i.e., $k/n \\rightarrow c \u0026gt;0$ – $\\hat{\\varepsilon}_i^2$ are too small ($\\Omega _ {HC0}$ biased towards zero). $\\Omega _ {HC1}$, $\\Omega _ {HC2}$ and $\\Omega _ {HC3}$ try to correct this small sample bias. HC1: degree of freedom correction (default robust in Stata) $$ \\Omega _ {HC1} = \\frac{1}{n - k }\\mathbb E_n [x_i x_i\u0026rsquo; \\hat{\\varepsilon}_i^2] $$ HC2: use standardized residuals $$ \\Omega _ {HC2} = \\mathbb E_n [x_i x_i\u0026rsquo; \\hat{\\varepsilon}_i^2 (1-h _ {ii})^{-1}] $$ where $h _ {ii} = [X(X\u0026rsquo;X)^{-1} X\u0026rsquo;] _ {ii}$ is the leverage of the $i^{th}$ observation. A large $h _ {ii}$ means that observation $i$ is unusual in the sense that the regressor $x_i$ is far from its sample mean. HC3: use prediction error, equivalent to Jack-knife estimator, i.e., $\\mathbb E_n [x_i x_i\u0026rsquo; \\hat{\\varepsilon} _ {(-i)}^2]$ $$ \\Omega _ {HC3} = \\mathbb E_n [x_i x_i\u0026rsquo; \\hat{\\varepsilon}_i^2 (1-h _ {ii})^{-2}] $$ This estimator does not overfit when $k$ is relatively big with respect to $n$. Idea: you exclude the corresponding observation when estimating a particular $\\varepsilon_i$: $\\hat{\\varepsilon}_i = y_i - x_i\u0026rsquo; \\hat \\beta _ {-i}$. HC0 Consistency Theorem\nUnder regularity conditions HC0 is consistent, i.e. $\\hat{\\Sigma} _ {HC0} \\overset{p}{\\to} \\Sigma$. $$ \\hat{\\Sigma} = \\hat{Q}^{-1} \\hat{\\Omega} \\hat{Q}^{-1} \\xrightarrow{p} \\Sigma \\qquad \\text{ with } \\hat{\\Omega} = \\mathbb E_n [x_i x_i\u0026rsquo; \\hat{\\varepsilon}_i^2] \\quad \\text{ and } \\hat{Q} = \\mathbb E_n [x_i x_i\u0026rsquo;]^{-1} $$\nWhy is the proof relevant? You cannot directly apply the WLLN to $\\hat \\Sigma$.\nProof\nFor the case $\\mathrm{dim}(x_i) =1$.\n$\\hat{Q}^{-1} \\xrightarrow{p} Q^{-1}$ by WLLN since $x_i$ is iid, $\\mathbb E[x_i^4] \u0026lt; \\infty$ $\\bar{\\Omega} = \\mathbb E_n [\\varepsilon_i^2 x_i x_i\u0026rsquo;] \\xrightarrow{p} \\Omega$ by WLLN since $\\mathbb E_n [\\varepsilon_i^4] \u0026lt; c$ and $x_i$ bounded. By the triangle inequality, $$ | \\hat{\\Omega} - \\hat{\\Omega}| \\leq \\underbrace{|\\Omega - \\bar{\\Omega}|} _ {\\overset{p}{\\to} 0} + \\underbrace{|\\bar{\\Omega} - \\hat{\\Omega}|} _ {\\text{WTS:} \\overset{p}{\\to} 0} $$ We want to show $|\\bar{\\Omega} - \\hat{\\Omega}| \\overset{p}{\\to} 0$ $$ \\begin{aligned} |\\bar{\\Omega} - \\hat{\\Omega}| \u0026amp;= \\mathbb E_n [\\varepsilon_i^2 x_i^2] - \\mathbb E_n [\\hat{\\varepsilon}_i^2 x_i^2] = \\newline \u0026amp;= \\mathbb E_n [\\left( \\varepsilon_i^2 - \\hat{\\varepsilon}_i^2 \\right) x_i^2] \\leq \\newline \u0026amp; \\leq \\mathbb E_n \\left[ \\left( \\varepsilon_i^2 - \\hat{\\varepsilon}_i^2 \\right)^2\\right]^{\\frac{1}{2}} \\mathbb E_n [x_i^4]^{\\frac{1}{2}} \\end{aligned} $$ where $\\mathbb E_n [x_i^4]^{\\frac{1}{2}} \\xrightarrow{p} \\mathbb E [x_i^4]^{\\frac{1}{2}}$ by $x_i$ bounded, iid and CMT. We want to show that $\\mathbb E_n \\left[ \\left( \\varepsilon_i^2 - \\hat{\\varepsilon}_i^2 \\right)^2\\right] \\leq \\eta$ with $\\eta \\rightarrow 0$. Let $L = \\max_i |\\hat{\\varepsilon}_i - \\varepsilon_i|$ (RV depending on $n$), with $L \\xrightarrow{p} 0$ since $$ |\\hat{\\varepsilon}_i - \\varepsilon_i| = |x_i \\hat \\beta - x_i \\beta| \\leq |x_i||\\hat \\beta - \\beta|\\xrightarrow{p} c \\cdot 0 $$ We can depompose $$ \\begin{aligned} \\left(\\varepsilon_i^2 - \\hat{\\varepsilon}_i^2 \\right)^2 \u0026amp; = \\left(\\varepsilon_i - \\hat{\\varepsilon}_i \\right)^2 \\left(\\varepsilon_i + \\hat{\\varepsilon}_i \\right)^2 \\leq \\newline\n\u0026amp; \\leq \\left(\\varepsilon_i + \\hat{\\varepsilon}_i \\right)^2 L^2 = \\newline \u0026amp;= \\left(2\\varepsilon_i - \\varepsilon_i + \\hat{\\varepsilon}_i \\right)^2 L^2\\leq \\newline \u0026amp; \\leq \\left( 2(2\\varepsilon_i)^2 + 2(\\hat{\\varepsilon}_i - \\varepsilon_i)^2 \\right)^2 L^2 \\leq \\newline \u0026amp; \\leq (8 \\varepsilon_i^2 + 2 L^2) L^2 \\end{aligned} $$ Hence $$ \\mathbb E \\left[ \\left(\\varepsilon_i^2 - \\hat{\\varepsilon}_i^2 \\right)^2 \\right] \\leq L^2 \\left( 8 \\mathbb E_n [ \\varepsilon_i^2] + 2 \\mathbb E_n [L^2] \\right) \\xrightarrow{p}0 $$ $$\\tag*{$\\blacksquare$}$$ Heteroskedastic and Autocorrelated Error Term Assumption\nThere esists a $\\bar{d}$ such that:\n$\\mathbb E[\\varepsilon_i x_i \\varepsilon\u0026rsquo; _ {i-d} x\u0026rsquo; _ {i-d}] \\neq 0 \\quad$ for $d \\leq \\bar{d}$ $\\mathbb E[\\varepsilon_i x_i \\varepsilon\u0026rsquo; _ {i-d} x\u0026rsquo; _ {i-d}] = 0 \\quad$ for $d \u0026gt; \\bar{d}$ Intuition: observations far enough from each other are not correlated.\nWe can express the variance of the score as $$ \\begin{aligned} \\Omega_n \u0026amp;= Var(\\sqrt{n} \\mathbb E_n[x_i \\varepsilon_i]) = \\newline \u0026amp;= \\mathbb E \\left[ \\left( \\frac{1}{n} \\sum _ {i=1}^n x_i \\varepsilon_i \\right) \\left( \\frac{1}{n} \\sum _ {j=1}^n x_j \\varepsilon_j \\right) \\right] = \\newline \u0026amp;= \\frac{1}{n} \\sum _ {i=1}^n \\sum _ {j=1}^n \\mathbb E[x_i \\varepsilon_i x_j\u0026rsquo; \\varepsilon_j\u0026rsquo;] = \\newline \u0026amp;= \\frac{1}{n} \\sum _ {i=1}^n \\sum _ {j : |i-j|\\leq \\bar{d}} \\mathbb E[x_i \\varepsilon_i x_j\u0026rsquo; \\varepsilon_j\u0026rsquo;] = \\newline \u0026amp;= \\frac{1}{n} \\sum _ {d=0}^{\\bar{d}} \\sum _ {i = d}^{n} \\mathbb E[x_i \\varepsilon_i x _ {i-d}\u0026rsquo; \\varepsilon _ {i-d}\u0026rsquo;] \\end{aligned} $$\nWe estimate $\\Omega_n$ by $$ \\hat{\\Omega}_n = \\frac{1}{n} \\sum _ {d=0}^{\\bar{d}} \\sum _ {i = d}^{n} x_i \\hat{\\varepsilon}_i x _ {i-d}\u0026rsquo; \\hat{\\varepsilon} _ {i-d}' $$\nTheorem\nIf $\\bar{d}$ is a fixed integer, then $$ \\hat{\\Omega}_n - \\Omega_n \\overset{p}{\\to} 0 $$\nWhat if $\\bar{d}$ does not exist (all $x_i, x_j$ are correlated)? $$ \\hat{\\Omega}_n = \\frac{1}{n} \\sum _ {d=0}^{n} \\sum _ {i = d}^{n} x_i \\hat{\\varepsilon}_i x _ {i-d}\u0026rsquo; \\hat{\\varepsilon} _ {i-d}\u0026rsquo; = n \\mathbb E_n[x_i \\hat{\\varepsilon}_i]^2 = 0 $$ By the orthogonality property of the OLS residual.\nHAC with Uniform Kernel $$ \\hat{\\Omega}_h = \\frac{1}{n} \\sum _ {i,j} x_i \\hat{\\varepsilon}_i x_j\u0026rsquo; \\hat{\\varepsilon}_j\u0026rsquo; \\mathbb{I} \\lbrace |i-j| \\leq h \\rbrace $$ where $h$ is the bandwidth of the kernel. The bandwidth is chosen such that $\\mathbb E[x_i \\varepsilon_i x _ {i-d}\u0026rsquo; \\varepsilon _ {i-d}\u0026rsquo; ]$ is small for $d \u0026gt; h$. How small? Small enough for the estimates to be consistent.\nHAC with General Kernel $$ \\hat{\\Omega}^{HAC} _ {k,h} = \\frac{1}{n} \\sum _ {i,j} x_i \\hat{\\varepsilon}_i x_j\u0026rsquo; \\hat{\\varepsilon}_j\u0026rsquo; k \\left( \\frac{|i-j|}{n} \\right) $$\nHAC Consistency Theorem If the joint distribution is stationary and $\\alpha$-mixing with $\\sum _ {k=1}^\\infty k^2 \\alpha(k) \u0026lt; \\infty$ and\n$\\mathbb E[ | x _ {ij} \\varepsilon_i |^\\nu ] \u0026lt; \\infty$ $\\forall \\nu$ $\\hat{\\varepsilon}_i = y_i - x_i\u0026rsquo; \\hat \\beta$ for some $\\hat \\beta \\overset{p}{\\to} \\beta_0$ $k$ smooth, symmetric, $k(0) \\to \\infty$ as $z \\to \\infty$, $\\int k^2 \u0026lt; \\infty$ $\\frac{h}{n} \\to 0$ $h \\to \\infty$ Then the HAC estimator is consistent. $$ \\hat{\\Omega}^{HAC} _ {k,h} - \\Omega_n \\overset{p}{\\to} 0 $$\nComments We want to choose $h$ small relative to $n$ in order to avoid estimation problems. But we also want to choose $h$ large so that the remainder is small: $$ \\begin{aligned} \\Omega_n \u0026amp;= Var(\\sqrt{n} \\mathbb E_n[x_i \\varepsilon_i]) = \\newline \u0026amp;= \\underbrace{\\frac{1}{n} \\sum _ {i,j : |i-j|\\leq h} \\mathbb E[x_i \\varepsilon_i x_j\u0026rsquo; \\varepsilon_j\u0026rsquo;]} _ {\\Omega^h_n} + \\underbrace{\\frac{1}{n} \\sum _ {i,j : |i-j|\u0026gt; h} \\mathbb E[x_i \\varepsilon_i x_j\u0026rsquo; \\varepsilon_j\u0026rsquo;]} _ {\\text{remainder: } R_n} = \\newline \u0026amp;= \\Omega_n^h + R_n \\end{aligned} $$\nIn particular, HAC theory requires: $$ \\hat{\\Omega}^{HAC} \\overset{p}{\\to} \\Omega \\quad \\text{ if } \\quad \\begin{cases} \u0026amp; \\frac{h}{n} \\to 0 \\newline \u0026amp; h \\to \\infty \\end{cases} $$\nBut in practice, long-run estimation implies $\\frac{h}{n} \\simeq 0$ which is not ``safe” in the sense that it does not imply $R_n \\simeq 0$. On the other hand, if $h \\simeq n$, $\\hat{\\Omega}^{HAC}$ does not converge in probability because it’s too noisy.\nChoice of h How to choose $h$? Look at the score autocorrelation function (ACF).\nIt looks like after 10 periods the empirical autocorrelation is quite small but still not zero.\nFixed b Asymptotics [Neave, 1970]: “When proving results on the asymptotic behavior of estimates of the spectrum of a stationary time series, it is invariably assumed that as the sample size $n$ tends to infinity, so does the truncation point $h$, but at a slower rate, so that $\\frac{h}{n}$ tends to zero. This is a convenient assumption mathematically in that, in particular, it ensures consistency of the estimates, but it is unrealistic when such results are used as approximations to the finite case where the value of $\\frac{h}{n}$ cannot be zero.””\nFixed b Theorem Theorem\nUnder regularity conditions, $$ \\sqrt{n} \\Big( V^{HAC} _ {k,h} \\Big)(\\hat \\beta - \\beta_0) \\overset{d}{\\to} F $$\nThe asymptotic critical values of the $F$ statistic depend on the choice of the kernel. In order to do hypothesis testing, Kiefer and Vogelsang(2005) provide critical value functions for the t-statistic for each kernel-confidence level combination using a cubic equation: $$ cv(b) = a_0 + a_1 b + a_2 b^2 + a_3 b^3 $$\nExample Example for the Bartlett kernel:\nFixed G Asymptotics [Bester, 2013]: “Cluster covariance estimators are routinely used with data that has a group structure with independence assumed across groups. Typically, inference is conducted in such settings under the assumption that there are a large number of these independent groups.””\n“However, with enough weakly dependent data, we show that groups can be chosen by the researcher so that group-level averages are approximately independent. Intuitively, if groups are large enough and well shaped (e.g. do not have gaps), the majority of points in a group will be far from other groups, and hence approximately independent of observations from other groups provided the data are weakly dependent. The key prerequisite for our methods is the researcher’s ability to construct groups whose averages are approximately independent. As we show later, this often requires that the number of groups be kept relatively small, which is why our main results explicitly consider a fixed (small) number of groups.””\nAssumption Assumption Suppose you have data $D = (y _ {it} , x _ {it}) _ {i=1, t=1}^{N, T}$ where $y _ {it} = x _ {it}\u0026rsquo; \\beta + \\alpha_i + \\varepsilon _ {it}$ where $i$ indexes the observational unit and $t$ indexes time (could also be space).\nLet $$ \\begin{aligned} \u0026amp; \\tilde{y} _ {it} = y _ {it} - \\frac{1}{T} \\sum _ {t=1}^T y _ {it} \\newline \u0026amp; \\tilde{x} _ {it} = x _ {it} - \\frac{1}{T} \\sum _ {t=1}^T x _ {it} \\newline \u0026amp; \\tilde{\\varepsilon} _ {it} = \\varepsilon _ {it} - \\frac{1}{T} \\sum _ {t=1}^T \\varepsilon _ {it} \\end{aligned} $$ Then $$ \\tilde{y} _ {it} = \\tilde{x} _ {it}\u0026rsquo; \\beta + \\tilde{\\varepsilon} _ {it} $$\nThe $\\tilde{\\varepsilon} _ {it}$ are by construction correlated between each other even if the original $\\varepsilon$ was iid. The cluster score variance estimator is given by: $$ \\hat{\\Omega}^{CL} = \\frac{1}{T-1} \\sum _ {i=1}^n \\sum _ {t=1}^T \\sum _ {s=1}^T \\tilde{x} _ {it} \\hat{\\tilde{\\varepsilon}} _ {it} \\tilde{x} _ {is} \\hat{\\tilde{\\varepsilon}} _ {is} $$\nIt’s very similar too the HAC estimator since we have dependent cross-products here as well. However, here we do not consider the $i \\times j$ cross-products. We only have time-dependency (state).\nComments (1) On $T$ and $n$:\nIf $T$ is fixed and $n \\to \\infty$, then the number of cross-products considered is much smaller than the total number of cross-products. If $T \u0026raquo; n$ issues arise since the number of cross products considered is close to the total number of cross products. As in HAC estimation, this is a problem because it implies that the algebraic estimate of the cluster score variance gets close to zero because of the orthogonality property of the residuals. The panel assumption is that observations across individuals are not correlated. Strategy: as in HAC, we want to limit the correlation across clusters (individuals). We hope that observations are negligibly dependent between cluster sufficiently distant from each other.\nComments (2) Classical cluster robust estimator: $$ \\hat{\\Omega}^{CL} = \\frac{1}{n} \\sum _ {i=1}^n x_i \\varepsilon_i x_j\u0026rsquo; \\varepsilon_j\u0026rsquo; \\mathbb{I} \\lbrace i,j \\text{ in the same cluster} \\rbrace $$\nOn clusters:\nIf the number of observations near a boundary is small relative to the sample size, ignoring the dependence should not affect inference too adversely. The higher the dimension of the data, the easier it is to have observations near boundaries (curse of dimensionality). We would like to have few clusters in order to make less independence assumptions. However, few clusters means bigger blocks and hence a larger number of cross-products to estimate. If the number of cross-products is too large (relative to the sample size), $\\hat{\\Omega}^{CL}$ does not converge Theorem: Under regularity conditions: $$ \\hat{t} \\overset{d}{\\to} \\sqrt{\\frac{G}{G-1}} t _ {G-1} $$\nCode - DGP This code draws 100 observations from the model $y = 2 x_1 - x_2 + \\varepsilon$ where $x_1, x_2 \\sim U[0,1]$ and $\\varepsilon \\sim N(0,1)$.\n# Set seed Random.seed!(123); # Set the number of observations n = 100; # Set the dimension of X k = 2; # Draw a sample of explanatory variables X = rand(Uniform(0,1), n, k); # Draw the error term σ = 1; ε = rand(Normal(0,1), n, 1) * sqrt(σ); # Set the parameters β = [2; -1]; # Calculate the dependent variable y = X*β + ε; Ideal Estimate # OLS estimator β_hat = (X'*X)\\(X'*y); # Residuals ε_hat = y - X*β_hat; # Homoskedastic standard errors std_h = var(ε_hat) * inv(X'*X); # Projection matrix P = X * inv(X'*X) * X'; # Leverage h = diag(P); HC Estimates # HC0 variance and standard errors Ω_hc0 = X' * (I(n) .* ε_hat.^2) * X; std_hc0 = sqrt.(diag(inv(X'*X) * Ω_hc0 * inv(X'*X))) ## 2-element Array{Float64,1}: ## 0.24691300271914793 ## 0.28044707935951835 # HC1 variance and standard errors Ω_hc1 = n/(n-k) * X' * (I(n) .* ε_hat.^2) * X; std_hc1 = sqrt.(diag(inv(X'*X) * Ω_hc1 * inv(X'*X))) ## 2-element Array{Float64,1}: ## 0.24941979797977423 ## 0.2832943308272532 # HC2 variance and standard errors Ω_hc2 = X' * (I(n) .* ε_hat.^2 ./ (1 .- h)) * X; std_hc2 = sqrt.(diag(inv(X'*X) * Ω_hc2 * inv(X'*X))) ## 2-element Array{Float64,1}: ## 0.2506509902982869 ## 0.2850878737103963 # HC3 variance and standard errors Ω_hc3 = X' * (I(n) .* ε_hat.^2 ./ (1 .- h).^2) * X; std_hc3 = sqrt.(diag(inv(X'*X) * Ω_hc3 * inv(X'*X))) ## 2-element Array{Float64,1}: ## 0.25446321015850176 ## 0.2898264779289438 # Note what happens if you allow for full autocorrelation omega_full = X'*ε_hat*ε_hat'*X; Inference Hypothesis Testing In order to do inference on $\\hat \\beta$ we need to know its distribution. We have two options: (i) assume gaussian error term (extended GM) or (ii) rely on asymptotic approximations (CLT).\nA statistical hypothesis is a subset of a statistical model, $\\mathcal K \\subset \\mathcal F$. A hypothesis test is a map $\\mathcal D \\rightarrow \\lbrace 0,1 \\rbrace$, $D \\mapsto T$. If $\\mathcal F$ is the statistical model and $\\mathcal K$ is the statistical hypothesis, we use the notation $H_0: \\Pr \\in \\mathcal K$.\nGenerally, we are interested in understanding whether it is likely that data $D$ are drawn from $\\mathcal K$ or not.\nA hypothesis test, $T$ is our tool for deciding whether the hypothesis is consistent with the data. $T(D)= 0$ implies fail to reject $H_0$ and test inconclusive $T(D)=1$ $\\implies$ reject $H_0$ and $D$ is inconsistent with any $\\Pr \\in \\mathcal K$.\nLet $\\mathcal K \\subseteq \\mathcal F$ be a statistical hypothesis and $T$ a hypothesis test.\nSuppose $\\Pr \\in \\mathcal K$. A Type I error (relative to $\\Pr$) is an event $T(D)=1$ under $\\Pr$. Suppose $\\Pr \\in \\mathcal K^c$. A Type II error (relative to $\\Pr$) is an event $T(D)=0$ under $\\Pr$. The corresponding probability of a type I error is called size. The corresponding probability of a type II error is called power (against the alternative $\\Pr$).\nIn this section, we are interested in testing three hypotheses, under the assumptions of linearity, strict exogeneity, no multicollinearity, normality on the error term. They are:\n$H_0: \\beta _ {0k} = \\bar \\beta _ {0k}$ (single coefficient, $\\bar \\beta _ {0k} \\in \\mathbb R$, $k \\leq K$) $a\u0026rsquo; \\beta_0 = c$ (linear combination, $a \\in \\mathbb R^K, c \\in \\mathbb R$) $R \\beta_0 = r$ (linear restrictions, $R \\in \\mathbb R^{p \\times K}$, full rank, $r \\in \\mathbb R^p$) Testing Problem Consider the testing problem $H_0: \\beta _ {0k} = \\bar \\beta _ {0k}$ where $\\bar \\beta _ {0k}$ is a pre-specified value under the null. The t-statistic for this problem is defined by $$ t_k:= \\frac{b_k - \\bar \\beta _ {0k}}{SE(b_k)}, \\ \\ SE(b_k):= \\sqrt{s^2 [(X\u0026rsquo;X)^{-1}] _ {kk}} $$\nTheorem: In the testing procedure above, the sampling distribution under the null $H_0$ is given by $$ t_k|X \\sim t _ {n-k} \\ \\ \\text{and so} \\ \\ t_k \\sim t _ {n-k} $$\n$t _ {(n-K)}$ denotes the t-distribution with $(n-k)$ degress of freedom. The test can be one sided or two sided. The above sampling distribution can be used to construct a confidence interval.\nExample We want to asses whether or not the ``true” coefficient $\\beta_0$ equals a specific value $\\hat \\beta$. Specifically, we are interested in testing $H_0$ against $H_1$, where:\nNull Hypothesis: $H_0: \\beta_0 = \\hat \\beta$ Alternative Hypothesis: $H_1: \\beta_0 \\ne \\hat \\beta$. Hence, we are interested in a statistic informative about $H_1$, which is the Wald test statistic $$ |T^*| = \\bigg| \\frac{\\hat \\beta - \\beta_0}{\\sigma(\\hat \\beta)}\\bigg| \\sim N(0,1) $$\nHowever, the true variance $\\sigma^2(\\hat \\beta )$ is not known and has to be estimated. Therefore we plug in the sample variance $\\hat \\sigma^2(\\hat \\beta) = \\frac{n}{n-1} \\mathbb E_n[\\hat e_i^2]$ and we use $$ |T| = \\bigg| \\frac{\\hat \\beta - \\beta_0}{\\hat \\sigma (\\hat \\beta)}\\bigg| \\sim t _ {(n-k)} $$\nComments Hypothesis testing is like proof by contradiction. Imagine the sampling distribution was generated by $\\beta$. If it is highly improbable to observe $\\hat \\beta$ given $\\beta_0 = \\beta$ then we reject the hypothesis that the sampling distribution was generated by $\\beta$.\nThen, given a realized value of the statistic $|T|$, we take the following decision:\nDo not reject $H_0$: it is consistent with random variation under true $H_0$—i.e., $|T|$ small as it has an exact student t distribution with $(n-k)$ degree of freedom in the normal regression model. Reject $H_0$ in favor of $H_1$: $|T| \u0026gt; c$, with $c$ being the critical values selected to control for false rejections: $\\Pr(|t _ {n-k}| \\geq c) = \\alpha$. Moreover, you can also reject $H_0$ if the p-value $p$ is such that: $p \u0026lt; \\alpha$. Comments (2) The probability of false rejection is decreasing in $c$, i.e. the critical value for a given significant level. $$ \\begin{aligned} \\Pr (\\text{Reject } H_0 | H_0) \u0026amp; = \\Pr (|T|\u0026gt; c | H_0 ) = \\newline \u0026amp; = \\Pr (T \u0026gt; c | H_0 ) + \\Pr (T \u0026lt; -c | H_0 ) = \\newline \u0026amp; = 1 - F(c) + F(-c) = 2(1-F(c)) \\end{aligned} $$\nExample: Consider the testing problem $H_0: a\u0026rsquo;\\beta_0=c$ where $a$ is a pre-specified linear combination under study. The t-statistic for this problem is defined by: $$ t_k:= \\frac{a\u0026rsquo;b - c}{SE(a\u0026rsquo;b)}, \\ \\ SE(a\u0026rsquo;b):= \\sqrt{s^2 a\u0026rsquo;(X\u0026rsquo;X)^{-1}a} $$\nt Stat Theorem\nIn the testing procedure above, the sampling distribution under the null $H_0$ is given by $$ t_a|X \\sim t _ {n-K} \\quad\\text{and so} \\quad t_a \\sim t _ {n-K} $$\nLike in the previous test, $t _ {(n-K)}$ denotes the t-distribution with $(n-K)$ degress of freedom. The test can again be one sided or two sided. The above sampling distribution can be used to construct a confidence interval\nF Stat Example\nConsider the testing problem $$ H_0: R \\beta_0 = r $$ where $R \\in \\mathbb R^{p \\times k}$ is a presepecified set of linear combinations and $r \\in \\mathbb R^p$ is a restriction vector.\nThe F-statistic for this problem is given by $$ F:= \\frac{(Rb-r)\u0026rsquo;[R(X\u0026rsquo;X)R\u0026rsquo;]^{-1}(Rb-r)/p }{s^2} $$\nTheorem\nFor the problem, the sampling distribution of the F-statistic under the null $H_0:$ $$ F|X \\sim F _ {p,n-K} \\ \\ \\text{and so} \\ \\ F \\sim F _ {p,n-K} $$\nThe test is intrinsically two-sided. The above sampling distribution can be used to construct a confidence interval.\nEquivalence Theorem\nConsider the testing problem $H_0: R \\beta_0 = r$ where $R \\in \\mathbb R^{p\\times K}$ is a presepecified set of linear combinations and $r \\in \\mathbb R^p$ is a restriction vector.\nConsider the restricted least squares estimator, denoted $\\hat \\beta_R$: $\\hat \\beta_R: = \\text{arg} \\min _ { \\beta: R \\beta = r } Q( \\beta)$. Let $SSR_U = Q(b), \\ \\ SSR_R=Q(\\hat \\beta_R)$. Then the $F$ statistic is numerically equivalent to the following expression: $F = \\frac{(SSR_R - SSR_U)/p}{SSR_U/(n-K)}$.\nConfidence Intervals A confidence interval at $(1-\\alpha)$ is a random set $C$ such that $$ \\Pr(\\beta_0 \\in C) \\geq 1- \\alpha $$ i.e. the probability that $C$ covers the true value $\\beta$ is fixed at $(1-\\alpha)$.\nSince $C$ is not known, it has to be estimated ($\\hat{C}$). We construct confidence intervals such that:\nthey are symmetric around $\\hat \\beta$; their length is proportional to $\\sigma(\\hat \\beta) = \\sqrt{Var(\\hat \\beta)}$. A CI is equivalent to the set of parameter values such that the t-statistic is less than $c$, i.e., $$ \\hat{C} = \\bigg\\lbrace \\beta: |T(\\beta) | \\leq c \\bigg\\rbrace = \\bigg\\lbrace \\beta: - c\\leq \\frac{\\beta - \\hat \\beta}{\\sigma(\\hat \\beta)} \\leq c \\bigg\\rbrace $$\nIn practice, to construct a 95% confidence interval for a single coefficient estimate $\\hat \\beta_j$, we use the fact that $$ \\Pr \\left( \\frac{| \\hat \\beta_j - \\beta _ {0,j} |}{ \\sqrt{\\sigma^2 [(X\u0026rsquo;X)^{-1}] _ {jj} }} \u0026gt; 1.96 \\right) = 0.05 $$\nCode # t-test for beta=0 t = abs.(β_hat ./ (std_hc1)); # p-value p_val = 1 .- cdf.(Normal(0,1), t); # F statistic of joint significance SSR_u = ε_hat'*ε_hat; SSR_r = y'*y; F = (SSR_r - SSR_u)/k / (SSR_u/(n-k)); # 95# confidente intervals conf_int = [β_hat - 1.96*std_hc1, β_hat + 1.96*std_hc1]; ","date":1635465600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1635465600,"objectID":"3a3dba8681825325823978a98a7af4ac","permalink":"https://matteocourthoud.github.io/course/metrics/06_ols_inference/","publishdate":"2021-10-29T00:00:00Z","relpermalink":"/course/metrics/06_ols_inference/","section":"course","summary":"Asymptotic Theory of the OLS Estimator OLS Consistency Theorem: Assume that $(x_i, y_i) _ {i=1}^n$ i.i.d. , $\\mathbb E[x_i x_i\u0026rsquo;] = Q$ positive definite, $\\mathbb E[x_i x_i\u0026rsquo;] \u0026lt; \\infty$ and $\\mathbb E [y_i^2] \u0026lt; \\infty$, then $\\hat \\beta _ {OLS}$ is a consistent estimator of $\\beta_0$, i.","tags":null,"title":"OLS Inference","type":"book"},{"authors":null,"categories":null,"content":"import os import re import time import requests import pandas as pd from bs4 import BeautifulSoup from pprint import pprint from selenium import webdriver There is no silver bullet to getting info from the internet. The coding requirements in these notes start easy and will gradually become more demanding. We will cover the following web scraping techniques:\nPandas APIs Scraping static webpages with BeautifulSoup Scraping dynamic wepages with Selenium Pandas The Pandas library has a very useful webscraping command: read_html. The read_html command works for webpages that contain tables that are particularly well behaved. Let\u0026rsquo;s see an example: https://en.wikipedia.org/wiki/List_of_countries_by_GDP_(nominal)\nAt first glance, it seems that there are three tables in this Wikipedia page:\ndata from the IMF data from the World Bank data from the UN Let\u0026rsquo;s see which tables pandas recognizes.\n# Scrape all tables from Wikipedia page url = 'https://en.wikipedia.org/wiki/List_of_countries_by_GDP_(nominal)' df_list = pd.read_html(url) # Check number of tables on the page print(len(df_list)) 7 Apparently Pandas has found 10 tables in this webpage. Let\u0026rsquo;s see what is their content.\n# Check headers of each table for df in df_list: print(df.shape) (1, 1) (1, 3) (216, 9) (9, 2) (7, 2) (13, 2) (2, 2) It seems that pandas has found many more tables that we could see. The ones that are of interest to us are probably the 3rd, 4th and 5th. But that are the others? Let\u0026rsquo;s look at the them.\n# Check first df_list[0].head() 0 0 Largest economies by nominal GDP in 2021[1] # Check second df_list[1].head() 0 1 2 0 .mw-parser-output .legend{page-break-inside:av... $750 billion – $1 trillion $500–50 billion $25... $50–100 billion $25–50 billion $5–25 billion \u0026lt;... Apparently, the first two are simply picture captions.\n# Check third df_list[2].head() Country/Territory Subregion Region IMF[1] United Nations[12] World Bank[13][14] Country/Territory Subregion Region Estimate Year Estimate Year Estimate Year 0 United States Northern America Americas 22939580.0 2021 20893746.0 2020 20936600.0 2020 1 China Eastern Asia Asia 16862979.0 [n 2]2021 14722801.0 [n 3]2020 14722731.0 2020 2 Japan Eastern Asia Asia 5103110.0 2021 5057759.0 2020 4975415.0 2020 3 Germany Western Europe Europe 4230172.0 2021 3846414.0 2020 3806060.0 2020 4 United Kingdom Western Europe Europe 3108416.0 2021 2764198.0 2020 2707744.0 2020 This is clearly what we were looking for. A part from the footnotes, the table is already clean and organized.\nIf we knew the name of the table, we could directly retrieve it. However, we will see more about it in the next lecture.\nSpecific Libraries Sometimes, there are libraries that are already written down to do the scraping for you. Each one is tailored for a specific website and they are usually userwritten and prone to bugs and errors. However, they are often efficient and save you the time to worry about getting around some website-specific issues.\nOne example is the pytrends library for scraping Google Trends. Let\u0026rsquo;s first install it\npip3 install pytrends Let\u0026rsquo;s see how it works. Imagine we want to do the following search:\nwords \u0026ldquo;python\u0026rdquo;, \u0026ldquo;matlab\u0026rdquo;, \u0026ldquo;stata\u0026rdquo; the the second half of in 2019 daily in the US We can get more details on how pytrends works here. The important thing to know is that if you query a time period of more than 200 days, Google will give you weekly results, instead of daily.\n# Pytrends search from pytrends.request import TrendReq # Set parameters words = ['python', 'matlab', 'stata'] timeframe = '2019-07-01 2019-12-31' country = 'US' # Get data pytrend = TrendReq() pytrend.build_payload(kw_list=words, timeframe=timeframe, geo=country) df_trends = pytrend.interest_over_time() # Plot trends_plot = df_trends.plot.line() Apparently people don\u0026rsquo;t code during the weekend\u0026hellip;.\nAPIs From Wikipedia\nAn application programming interface (API) is an interface or communication protocol between different parts of a computer program intended to simplify the implementation and maintenance of software.\nIn practice, it means that the are some webpages that are structured not to be user-readable but to be computer-readable. Let\u0026rsquo;s see one example.\nGoogle provides many APIs for its services. However, they now all need identification, which means that you have to log in into your Google account and request an API key from there. This allows Google to monitor your behavior since the number of API requests is limited and beyond a certain treshold, one need to pay (a lot).\nThere are however some free APIs. One\nLet\u0026rsquo;s have a look at one of these: zippopotam. Zippopotam lets you retrieve location information from a zip code in the US. Other countries are supported as well.\n# Let's search the department locatiton import requests zipcode = '90210' url = 'https://api.zippopotam.us/us/'+zipcode response = requests.get(url) data = response.json() data {'post code': '90210', 'country': 'United States', 'country abbreviation': 'US', 'places': [{'place name': 'Beverly Hills', 'longitude': '-118.4065', 'state': 'California', 'state abbreviation': 'CA', 'latitude': '34.0901'}]} Data is in JSON (JavaScript Object Notation) format which is basically a nested dictionary-list format. Indeed, we see that in our case, data is a dictionary where the last elements is a list with one element - another dictionary.\n# Check type of value for d in data.values(): print(type(d)) # Check list length print(len(data['places'])) # Check type of content of list print(type(data['places'][0])) \u0026lt;class 'str'\u0026gt; \u0026lt;class 'str'\u0026gt; \u0026lt;class 'str'\u0026gt; \u0026lt;class 'list'\u0026gt; 1 \u0026lt;class 'dict'\u0026gt; The part that could be interesting to us is contained in the places category. We can easily extract it and transform it into a dataframe.\n# Add zipcode to data data['places'][0]['zipcode'] = zipcode # Export data df = pd.DataFrame(data['places']) df place name longitude state state abbreviation latitude zipcode 0 Beverly Hills -118.4065 California CA 34.0901 90210 Static Webscraping We have so far used pre-made tools in order to do web-scraping. When the website contains the data in a nice table or an API is available, we do not need to worry much and we can directly retrieve the data. However, most of web scraping is much more complicated. Data is often the product of webscraping and is not readily available. Moreover, sometimes webscraping knowledge can supplement the need to pay for an API.\nHTTP What happens when you open a page on the internet? In short, your web browser is sending a request to the website that, in turn, sends back a reply/response. The exchange of messages is complex but its core involves a HyperText Transfer Protocol (HTTP) request message to a web server, followed by a HTTP response (or reply). All static webscraping is build on HTTP so let\u0026rsquo;s have a closer look.\nAn HTTP message essentially has 4 components:\nA request line A number of request headers An empty line An optional message Example\nA request message could be\nGET /hello.htm HTTP/1.1 The response would be\nHTTP/1.1 200 OK Date: Sun, 10 Oct 2010 23:26:07 GMT Server: Apache/2.2.8 (Ubuntu) mod_ssl/2.2.8 OpenSSL/0.9.8g Last-Modified: Sun, 26 Sep 2010 22:04:35 GMT ETag: \u0026quot;45b6-834-49130cc1182c0\u0026quot; Accept-Ranges: bytes Content-Length: 12 Connection: close Content-Type: text/html \u0026lt;html\u0026gt; \u0026lt;body\u0026gt; \u0026lt;h1\u0026gt;Hello, World!\u0026lt;/h1\u0026gt; \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt; So, in this case the parts are:\nThe request line HTTP/1.1 200 OK The request headers Date: Sun, 10 Oct 2010 23:26:07 GMT Server: Apache/2.2.8 (Ubuntu) mod_ssl/2.2.8 OpenSSL/0.9.8g Last-Modified: Sun, 26 Sep 2010 22:04:35 GMT ETag: \u0026quot;45b6-834-49130cc1182c0\u0026quot; Accept-Ranges: bytes Content-Length: 12 Connection: close Content-Type: text/html The empty line The optional message \u0026lt;html\u0026gt; \u0026lt;body\u0026gt; \u0026lt;h1\u0026gt;Hello, World!\u0026lt;/h1\u0026gt; \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt; We are interested in the optional message, which is essentially the content of the page we want to scrape. The content is usually written in HTML which is not a proper programming language but rather a typesetting language since it is the language underlying web pages and is usually generated from other programming languages.\nRequests There are many different packages in python to send requests to a web page and read its response. The most user-friendly is the requests package. You can find plenty of useful information on the requests library on its website: https://requests.readthedocs.io/en/master/.\nWe are now going to have a look at a simple example: http://pythonscraping.com/pages/page1.html.\n# Request a simple web page url1 = 'http://pythonscraping.com/pages/page1.html' response = requests.get(url1) print(response) \u0026lt;Response [200]\u0026gt; We are (hopefully) getting a \u0026lt;Response [200]\u0026gt; message. In short, what we got is the status code of the request we sent to the website. The status code is a 3-digit code and essentially there are two broad categories of status codes:\n2XX: success 4XX, 5XX: failure It can be useful to know this codes as they are a fast way to check whether your request has failed or not. When webscraping the most common reasons you get an error are\nThe link does not exist: wither the link is old/expired or you misspelled it and hence there is no page to request You have been \u0026ldquo;caught\u0026rdquo;. This is pretty common when webscraping and happens every time you are too aggressive with your scraping. How much \u0026ldquo;aggressive\u0026rdquo; is \u0026ldquo;too agrressive\u0026rdquo; depends on the website. Usually big tech websites are particularly hard to scrape and anything that is \u0026ldquo;faster than human\u0026rdquo; gets blocked. Sometimes also slow but persistent requests get blocked as well. We have now analyzed the response status but, what is actually the response content? Let\u0026rsquo;s inspect the response object more in detail.\n# Print response attributes dir(response) ['__attrs__', '__bool__', '__class__', '__delattr__', '__dict__', '__dir__', '__doc__', '__enter__', '__eq__', '__exit__', '__format__', '__ge__', '__getattribute__', '__getstate__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__iter__', '__le__', '__lt__', '__module__', '__ne__', '__new__', '__nonzero__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__setstate__', '__sizeof__', '__str__', '__subclasshook__', '__weakref__', '_content', '_content_consumed', '_next', 'apparent_encoding', 'close', 'connection', 'content', 'cookies', 'elapsed', 'encoding', 'headers', 'history', 'is_permanent_redirect', 'is_redirect', 'iter_content', 'iter_lines', 'json', 'links', 'next', 'ok', 'raise_for_status', 'raw', 'reason', 'request', 'status_code', 'text', 'url'] We are actually interested in the text of the response.\n# Print response content response.text '\u0026lt;html\u0026gt;\\n\u0026lt;head\u0026gt;\\n\u0026lt;title\u0026gt;A Useful Page\u0026lt;/title\u0026gt;\\n\u0026lt;/head\u0026gt;\\n\u0026lt;body\u0026gt;\\n\u0026lt;h1\u0026gt;An Interesting Title\u0026lt;/h1\u0026gt;\\n\u0026lt;div\u0026gt;\\nLorem ipsum dolor sit amet, consectetur adipisicing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollit anim id est laborum.\\n\u0026lt;/div\u0026gt;\\n\u0026lt;/body\u0026gt;\\n\u0026lt;/html\u0026gt;\\n' This is the whole content of the table. There is a large chunk of text and other parts which look more obscure. In order to understand the structure of the page, we need to have a closer look at the language in which the webpage is written: HTML. We will do it in the next section.\nHowever, let\u0026rsquo;s first analyze the other relevant components of the response. We have already had a look at the status. Let\u0026rsquo;s inspect the headers.\n# Print response headers response.headers {'Server': 'nginx', 'Date': 'Thu, 10 Feb 2022 11:11:41 GMT', 'Content-Type': 'text/html', 'Content-Length': '361', 'Connection': 'keep-alive', 'X-Accel-Version': '0.01', 'Last-Modified': 'Sat, 09 Jun 2018 19:15:58 GMT', 'ETag': '\u0026quot;234-56e3a58a63780-gzip\u0026quot;', 'Accept-Ranges': 'bytes', 'Vary': 'Accept-Encoding', 'Content-Encoding': 'gzip', 'X-Powered-By': 'PleskLin'} From the headers we can see\nthe present date the name of the server hosting the page the last time the page was modified other stuff Let\u0026rsquo;s now look at the headers of our request.\n# Request headers def check_headers(r): test_headers = dict(zip(r.request.headers.keys(), r.request.headers.values())) pprint(test_headers) check_headers(response) {'Accept': '*/*', 'Accept-Encoding': 'gzip, deflate, br', 'Connection': 'keep-alive', 'User-Agent': 'python-requests/2.27.1'} The headers of our request are pretty minimal. In order to see what normal headers look like, go to https://www.whatismybrowser.com/developers/what-http-headers-is-my-browser-sending\nNormal headers look something like:\n{'Accept': 'text/html,application/xhtml+xml,application/xml;q = 0.9, image / ' 'webp, * / *;q = 0.8', 'Accept-Encoding': 'gzip, deflate, br', 'Accept-Language': 'en-US,en;q=0.9,it-IT;q=0.8,it;q=0.7,de-DE;q=0.6,de;q=0.5', 'Connection': 'keep-alive', 'Host': 'www.whatismybrowser.com', 'Referer': 'http://localhost:8888/', 'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_14_6) ' 'AppleWebKit/537.36 (KHTML, like Gecko) Chrome/79.0.3945.88 ' 'Safari/537.36'} The most important difference is that the requests model default User-Agent is python-requests/2.22.0 which means that we are walking around the web with a big WARNING: web scrapers sign. This is the simplest way to get caught and blocked by a website. Luckily, we can easily change our headers in order to be more subtle.\n# Change headers headers = {\u0026quot;User-Agent\u0026quot;: \u0026quot;Mozilla/5.0\u0026quot;, \u0026quot;Accept\u0026quot;: \u0026quot;webp, * / *;q = 0.8\u0026quot;, \u0026quot;Accept-Language\u0026quot;: \u0026quot;en-US,en;q=0.9\u0026quot;, \u0026quot;Accept-Encoding\u0026quot;: \u0026quot;br, gzip, deflate\u0026quot;, \u0026quot;Referer\u0026quot;: \u0026quot;https://www.google.ch/\u0026quot;} # Test if change worked response = requests.get(url1, headers=headers) check_headers(response) {'Accept': 'webp, * / *;q = 0.8', 'Accept-Encoding': 'br, gzip, deflate', 'Accept-Language': 'en-US,en;q=0.9', 'Connection': 'keep-alive', 'Referer': 'https://www.google.ch/', 'User-Agent': 'Mozilla/5.0'} Nice! Now we are a little more stealthy.\nYou might now be asking yourself what are the ethical limits of webscraping. Information on the internet is public but scraping a website imposes a workload on the website\u0026rsquo;s server. If the website is not protected against aggressive scrapers (most websites are), your activity could significantly slower the website or even crash it.\nUsually websites include their policies for scraping in a text file named robots.txt.\nLet\u0026rsquo;s have a look at the robots.txt file of http://pythonscraping.com/.\n# Read robots.txt response = requests.get('http://pythonscraping.com/robots.txt') print(response.text) # # robots.txt # # This file is to prevent the crawling and indexing of certain parts # of your site by web crawlers and spiders run by sites like Yahoo! # and Google. By telling these \u0026quot;robots\u0026quot; where not to go on your site, # you save bandwidth and server resources. # # This file will be ignored unless it is at the root of your host: # Used: http://example.com/robots.txt # Ignored: http://example.com/site/robots.txt # # For more information about the robots.txt standard, see: # http://www.robotstxt.org/robotstxt.html # # For syntax checking, see: # http://www.frobee.com/robots-txt-check User-agent: * Crawl-delay: 10 # Directories Disallow: /includes/ Disallow: /misc/ Disallow: /modules/ Disallow: /profiles/ Disallow: /scripts/ Disallow: /themes/ # Files Disallow: /CHANGELOG.txt Disallow: /cron.php Disallow: /INSTALL.mysql.txt Disallow: /INSTALL.pgsql.txt Disallow: /INSTALL.sqlite.txt Disallow: /install.php Disallow: /INSTALL.txt Disallow: /LICENSE.txt Disallow: /MAINTAINERS.txt Disallow: /update.php Disallow: /UPGRADE.txt Disallow: /xmlrpc.php # Paths (clean URLs) Disallow: /admin/ Disallow: /comment/reply/ Disallow: /filter/tips/ Disallow: /node/add/ Disallow: /search/ Disallow: /user/register/ Disallow: /user/password/ Disallow: /user/login/ Disallow: /user/logout/ # Paths (no clean URLs) Disallow: /?q=admin/ Disallow: /?q=comment/reply/ Disallow: /?q=filter/tips/ Disallow: /?q=node/add/ Disallow: /?q=search/ Disallow: /?q=user/password/ Disallow: /?q=user/register/ Disallow: /?q=user/login/ Disallow: /?q=user/logout/ As we can see, this robots.txt file mostly deals with crawlers, i.e. scripts that are designed to recover the structure of a website by exploring it. Crawlers are mostly used by browsers that want to index websites.\nNow we have explored most of the issues around HTTP requests. We can now proceed to what we are interested in: the content of the web page. In order to do that, we need to know the language in which wabpages are written: HTML.\nHTML Hypertext Markup Language (HTML) is the standard markup language for documents designed to be displayed in a web browser. Web browsers receive HTML documents from a web server or from local storage and render the documents into multimedia web pages. HTML describes the structure of a web page semantically and originally included cues for the appearance of the document.\nHTML elements are delineated by tags, written using angle brackets.\nTags Tags are the cues that HTML uses to surround content and provide information about its nature. There is a very large amount of tags but some of the most common are:\n\u0026lt;head\u0026gt; and \u0026lt;body\u0026gt; for head and body of the page \u0026lt;p\u0026gt; for paragraphs \u0026lt;br\u0026gt; for line breaks \u0026lt;table\u0026gt; for tables. These are the ones that pandas reads. However, we have seen that not all elements that look like tables are actually \u0026lt;table\u0026gt; and viceversa. Table elements are tagged as \u0026lt;th\u0026gt; (table header), \u0026lt;tr\u0026gt; (table row) and \u0026lt;td\u0026gt; (table data: a cell) \u0026lt;img\u0026gt; for images \u0026lt;h1\u0026gt; to \u0026lt;h6\u0026gt; for headers (titles and subtitles) \u0026lt;div\u0026gt; dor divisions, i.e. for grouping elements \u0026lt;a\u0026gt; for hyperlinks \u0026lt;ul\u0026gt; and \u0026lt;ol\u0026gt; for unordered and ordered lists where list elements are tagged as \u0026lt;li\u0026gt; Let\u0026rsquo;s have a look at the previous page\n# Inspect HTML response.text '#\\n# robots.txt\\n#\\n# This file is to prevent the crawling and indexing of certain parts\\n# of your site by web crawlers and spiders run by sites like Yahoo!\\n# and Google. By telling these \u0026quot;robots\u0026quot; where not to go on your site,\\n# you save bandwidth and server resources.\\n#\\n# This file will be ignored unless it is at the root of your host:\\n# Used: http://example.com/robots.txt\\n# Ignored: http://example.com/site/robots.txt\\n#\\n# For more information about the robots.txt standard, see:\\n# http://www.robotstxt.org/robotstxt.html\\n#\\n# For syntax checking, see:\\n# http://www.frobee.com/robots-txt-check\\n\\nUser-agent: *\\nCrawl-delay: 10\\n# Directories\\nDisallow: /includes/\\nDisallow: /misc/\\nDisallow: /modules/\\nDisallow: /profiles/\\nDisallow: /scripts/\\nDisallow: /themes/\\n# Files\\nDisallow: /CHANGELOG.txt\\nDisallow: /cron.php\\nDisallow: /INSTALL.mysql.txt\\nDisallow: /INSTALL.pgsql.txt\\nDisallow: /INSTALL.sqlite.txt\\nDisallow: /install.php\\nDisallow: /INSTALL.txt\\nDisallow: /LICENSE.txt\\nDisallow: /MAINTAINERS.txt\\nDisallow: /update.php\\nDisallow: /UPGRADE.txt\\nDisallow: /xmlrpc.php\\n# Paths (clean URLs)\\nDisallow: /admin/\\nDisallow: /comment/reply/\\nDisallow: /filter/tips/\\nDisallow: /node/add/\\nDisallow: /search/\\nDisallow: /user/register/\\nDisallow: /user/password/\\nDisallow: /user/login/\\nDisallow: /user/logout/\\n# Paths (no clean URLs)\\nDisallow: /?q=admin/\\nDisallow: /?q=comment/reply/\\nDisallow: /?q=filter/tips/\\nDisallow: /?q=node/add/\\nDisallow: /?q=search/\\nDisallow: /?q=user/password/\\nDisallow: /?q=user/register/\\nDisallow: /?q=user/login/\\nDisallow: /?q=user/logout/\\n' The response looks a little bit messy and not really readable.\nBeautifulSoup is a python library that renders http responses in a user friendly format and helps recovering elements from tags and attributes.\npip3 install bs4 Let\u0026rsquo;s have a look.\n# Make response readable soup = BeautifulSoup(response.text, 'lxml') print(soup) \u0026lt;html\u0026gt;\u0026lt;body\u0026gt;\u0026lt;p\u0026gt;# # robots.txt # # This file is to prevent the crawling and indexing of certain parts # of your site by web crawlers and spiders run by sites like Yahoo! # and Google. By telling these \u0026quot;robots\u0026quot; where not to go on your site, # you save bandwidth and server resources. # # This file will be ignored unless it is at the root of your host: # Used: http://example.com/robots.txt # Ignored: http://example.com/site/robots.txt # # For more information about the robots.txt standard, see: # http://www.robotstxt.org/robotstxt.html # # For syntax checking, see: # http://www.frobee.com/robots-txt-check User-agent: * Crawl-delay: 10 # Directories Disallow: /includes/ Disallow: /misc/ Disallow: /modules/ Disallow: /profiles/ Disallow: /scripts/ Disallow: /themes/ # Files Disallow: /CHANGELOG.txt Disallow: /cron.php Disallow: /INSTALL.mysql.txt Disallow: /INSTALL.pgsql.txt Disallow: /INSTALL.sqlite.txt Disallow: /install.php Disallow: /INSTALL.txt Disallow: /LICENSE.txt Disallow: /MAINTAINERS.txt Disallow: /update.php Disallow: /UPGRADE.txt Disallow: /xmlrpc.php # Paths (clean URLs) Disallow: /admin/ Disallow: /comment/reply/ Disallow: /filter/tips/ Disallow: /node/add/ Disallow: /search/ Disallow: /user/register/ Disallow: /user/password/ Disallow: /user/login/ Disallow: /user/logout/ # Paths (no clean URLs) Disallow: /?q=admin/ Disallow: /?q=comment/reply/ Disallow: /?q=filter/tips/ Disallow: /?q=node/add/ Disallow: /?q=search/ Disallow: /?q=user/password/ Disallow: /?q=user/register/ Disallow: /?q=user/login/ Disallow: /?q=user/logout/ \u0026lt;/p\u0026gt;\u0026lt;/body\u0026gt;\u0026lt;/html\u0026gt; First of all, what is the html5lib option? It\u0026rsquo;s the parser. In short, there are often small mistakes/variations in HTML and each parser interprets it differently. In principles, the latest HTML standard is HTML5, therefore the html5lib parser should be the most \u0026ldquo;correct\u0026rdquo; parser. It might happen that the same code does not work for another person if you use a different parser.\nThis is much better but it can be improved.\n# Prettify response print(soup.prettify()) \u0026lt;html\u0026gt; \u0026lt;body\u0026gt; \u0026lt;p\u0026gt; # # robots.txt # # This file is to prevent the crawling and indexing of certain parts # of your site by web crawlers and spiders run by sites like Yahoo! # and Google. By telling these \u0026quot;robots\u0026quot; where not to go on your site, # you save bandwidth and server resources. # # This file will be ignored unless it is at the root of your host: # Used: http://example.com/robots.txt # Ignored: http://example.com/site/robots.txt # # For more information about the robots.txt standard, see: # http://www.robotstxt.org/robotstxt.html # # For syntax checking, see: # http://www.frobee.com/robots-txt-check User-agent: * Crawl-delay: 10 # Directories Disallow: /includes/ Disallow: /misc/ Disallow: /modules/ Disallow: /profiles/ Disallow: /scripts/ Disallow: /themes/ # Files Disallow: /CHANGELOG.txt Disallow: /cron.php Disallow: /INSTALL.mysql.txt Disallow: /INSTALL.pgsql.txt Disallow: /INSTALL.sqlite.txt Disallow: /install.php Disallow: /INSTALL.txt Disallow: /LICENSE.txt Disallow: /MAINTAINERS.txt Disallow: /update.php Disallow: /UPGRADE.txt Disallow: /xmlrpc.php # Paths (clean URLs) Disallow: /admin/ Disallow: /comment/reply/ Disallow: /filter/tips/ Disallow: /node/add/ Disallow: /search/ Disallow: /user/register/ Disallow: /user/password/ Disallow: /user/login/ Disallow: /user/logout/ # Paths (no clean URLs) Disallow: /?q=admin/ Disallow: /?q=comment/reply/ Disallow: /?q=filter/tips/ Disallow: /?q=node/add/ Disallow: /?q=search/ Disallow: /?q=user/password/ Disallow: /?q=user/register/ Disallow: /?q=user/login/ Disallow: /?q=user/logout/ \u0026lt;/p\u0026gt; \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt; This is much better. Now the tree structure of the HTML page is clearly visible and we can visually separate the different elements.\nIn particular, the structure of the page is:\npage head with ttle: \u0026ldquo;A Useful Page\u0026rdquo; page body with level 1 header \u0026ldquo;An Interesting Title\u0026rdquo; a division with text \u0026ldquo;Lorem ipsum\u0026hellip;\u0026rdquo; How do we work with these elements? Suppose we want to recover the title and the text. The requests library has some useful functions.\n# Find the title url = 'http://pythonscraping.com/pages/page1.html' response = requests.get(url) soup = BeautifulSoup(response.text, 'lxml') soup.find('title') \u0026lt;title\u0026gt;A Useful Page\u0026lt;/title\u0026gt; # Extract text soup.find('title').text 'A Useful Page' # Find all h1 elements soup.find_all('h1') [\u0026lt;h1\u0026gt;An Interesting Title\u0026lt;/h1\u0026gt;] # Find all title or h1 elements soup.find_all(['title','h1']) [\u0026lt;title\u0026gt;A Useful Page\u0026lt;/title\u0026gt;, \u0026lt;h1\u0026gt;An Interesting Title\u0026lt;/h1\u0026gt;] Regular Expressions Note that there is always a more direct alternative: using regular expressions directly on the response!\n# Find the title re.findall('\u0026lt;title\u0026gt;(.*)\u0026lt;/title\u0026gt;', response.text)[0] 'A Useful Page' # Find all h1 elements re.findall('\u0026lt;h1\u0026gt;(.*)\u0026lt;/h1\u0026gt;', response.text) ['An Interesting Title'] # Find all title or h1 elements [x[1] for x in re.findall('\u0026lt;(title|h1)\u0026gt;(.*)\u0026lt;', response.text)] ['A Useful Page', 'An Interesting Title'] This was a very simple page and there was not so much to look for. Let\u0026rsquo;s now look at a more realistic example.\nAttributes Let\u0026rsquo;s inspect a slightly more complicated page: http://pythonscraping.com/pages/page3.html.\nIn this page, there is much more content than in the previous one. There seems to be a table, there are images, hyperlinks, etc\u0026hellip; It\u0026rsquo;s the perfect playground. Let\u0026rsquo;s have a look at what does the HTML code look like.\n# Inspect HTML code url2 = 'http://pythonscraping.com/pages/page3.html' response = requests.get(url2) soup = BeautifulSoup(response.text,'lxml') print(soup.prettify()) \u0026lt;html\u0026gt; \u0026lt;head\u0026gt; \u0026lt;style\u0026gt; img{ width:75px; } table{ width:50%; } td{ margin:10px; padding:10px; } .wrapper{ width:800px; } .excitingNote{ font-style:italic; font-weight:bold; } \u0026lt;/style\u0026gt; \u0026lt;/head\u0026gt; \u0026lt;body\u0026gt; \u0026lt;div id=\u0026quot;wrapper\u0026quot;\u0026gt; \u0026lt;img src=\u0026quot;../img/gifts/logo.jpg\u0026quot; style=\u0026quot;float:left;\u0026quot;/\u0026gt; \u0026lt;h1\u0026gt; Totally Normal Gifts \u0026lt;/h1\u0026gt; \u0026lt;div id=\u0026quot;content\u0026quot;\u0026gt; Here is a collection of totally normal, totally reasonable gifts that your friends are sure to love! Our collection is hand-curated by well-paid, free-range Tibetan monks. \u0026lt;p\u0026gt; We haven't figured out how to make online shopping carts yet, but you can send us a check to: \u0026lt;br/\u0026gt; 123 Main St. \u0026lt;br/\u0026gt; Abuja, Nigeria We will then send your totally amazing gift, pronto! Please include an extra $5.00 for gift wrapping. \u0026lt;/p\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;table id=\u0026quot;giftList\u0026quot;\u0026gt; \u0026lt;tr\u0026gt; \u0026lt;th\u0026gt; Item Title \u0026lt;/th\u0026gt; \u0026lt;th\u0026gt; Description \u0026lt;/th\u0026gt; \u0026lt;th\u0026gt; Cost \u0026lt;/th\u0026gt; \u0026lt;th\u0026gt; Image \u0026lt;/th\u0026gt; \u0026lt;/tr\u0026gt; \u0026lt;tr class=\u0026quot;gift\u0026quot; id=\u0026quot;gift1\u0026quot;\u0026gt; \u0026lt;td\u0026gt; Vegetable Basket \u0026lt;/td\u0026gt; \u0026lt;td\u0026gt; This vegetable basket is the perfect gift for your health conscious (or overweight) friends! \u0026lt;span class=\u0026quot;excitingNote\u0026quot;\u0026gt; Now with super-colorful bell peppers! \u0026lt;/span\u0026gt; \u0026lt;/td\u0026gt; \u0026lt;td\u0026gt; $15.00 \u0026lt;/td\u0026gt; \u0026lt;td\u0026gt; \u0026lt;img src=\u0026quot;../img/gifts/img1.jpg\u0026quot;/\u0026gt; \u0026lt;/td\u0026gt; \u0026lt;/tr\u0026gt; \u0026lt;tr class=\u0026quot;gift\u0026quot; id=\u0026quot;gift2\u0026quot;\u0026gt; \u0026lt;td\u0026gt; Russian Nesting Dolls \u0026lt;/td\u0026gt; \u0026lt;td\u0026gt; Hand-painted by trained monkeys, these exquisite dolls are priceless! And by \u0026quot;priceless,\u0026quot; we mean \u0026quot;extremely expensive\u0026quot;! \u0026lt;span class=\u0026quot;excitingNote\u0026quot;\u0026gt; 8 entire dolls per set! Octuple the presents! \u0026lt;/span\u0026gt; \u0026lt;/td\u0026gt; \u0026lt;td\u0026gt; $10,000.52 \u0026lt;/td\u0026gt; \u0026lt;td\u0026gt; \u0026lt;img src=\u0026quot;../img/gifts/img2.jpg\u0026quot;/\u0026gt; \u0026lt;/td\u0026gt; \u0026lt;/tr\u0026gt; \u0026lt;tr class=\u0026quot;gift\u0026quot; id=\u0026quot;gift3\u0026quot;\u0026gt; \u0026lt;td\u0026gt; Fish Painting \u0026lt;/td\u0026gt; \u0026lt;td\u0026gt; If something seems fishy about this painting, it's because it's a fish! \u0026lt;span class=\u0026quot;excitingNote\u0026quot;\u0026gt; Also hand-painted by trained monkeys! \u0026lt;/span\u0026gt; \u0026lt;/td\u0026gt; \u0026lt;td\u0026gt; $10,005.00 \u0026lt;/td\u0026gt; \u0026lt;td\u0026gt; \u0026lt;img src=\u0026quot;../img/gifts/img3.jpg\u0026quot;/\u0026gt; \u0026lt;/td\u0026gt; \u0026lt;/tr\u0026gt; \u0026lt;tr class=\u0026quot;gift\u0026quot; id=\u0026quot;gift4\u0026quot;\u0026gt; \u0026lt;td\u0026gt; Dead Parrot \u0026lt;/td\u0026gt; \u0026lt;td\u0026gt; This is an ex-parrot! \u0026lt;span class=\u0026quot;excitingNote\u0026quot;\u0026gt; Or maybe he's only resting? \u0026lt;/span\u0026gt; \u0026lt;/td\u0026gt; \u0026lt;td\u0026gt; $0.50 \u0026lt;/td\u0026gt; \u0026lt;td\u0026gt; \u0026lt;img src=\u0026quot;../img/gifts/img4.jpg\u0026quot;/\u0026gt; \u0026lt;/td\u0026gt; \u0026lt;/tr\u0026gt; \u0026lt;tr class=\u0026quot;gift\u0026quot; id=\u0026quot;gift5\u0026quot;\u0026gt; \u0026lt;td\u0026gt; Mystery Box \u0026lt;/td\u0026gt; \u0026lt;td\u0026gt; If you love suprises, this mystery box is for you! Do not place on light-colored surfaces. May cause oil staining. \u0026lt;span class=\u0026quot;excitingNote\u0026quot;\u0026gt; Keep your friends guessing! \u0026lt;/span\u0026gt; \u0026lt;/td\u0026gt; \u0026lt;td\u0026gt; $1.50 \u0026lt;/td\u0026gt; \u0026lt;td\u0026gt; \u0026lt;img src=\u0026quot;../img/gifts/img6.jpg\u0026quot;/\u0026gt; \u0026lt;/td\u0026gt; \u0026lt;/tr\u0026gt; \u0026lt;/table\u0026gt; \u0026lt;div id=\u0026quot;footer\u0026quot;\u0026gt; © Totally Normal Gifts, Inc. \u0026lt;br/\u0026gt; +234 (617) 863-0736 \u0026lt;/div\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt; As we can see, now the page is much more complicated than before. An important distintion is that now some tags have classes. For example, the first \u0026lt;img\u0026gt; tag now has a class src and a class style.\n\u0026lt;img src=\u0026quot;../img/gifts/logo.jpg\u0026quot; style=\u0026quot;float:left;\u0026quot;\u0026gt; Moreover, even though BeautifulSoup is formatting the page in a nicer way, it\u0026rsquo;s still pretty hard to go through it. How can one locate one specific element? And, most importantly, if you know the element only graphically, how do you recover the equivalent in the HTML code?\nThe best way is to use the inspect function from Chrome. Firefox has an equivalent function. Let\u0026rsquo;s inspect the original page.\nSuppose now you want to recover all item names. Let\u0026rsquo;s inspect the first. The corresponding line looks like this:\n\u0026lt;tr class=\u0026quot;gift\u0026quot; id=\u0026quot;gift1\u0026quot;\u0026gt; \u0026lt;td\u0026gt; Vegetable Basket \u0026lt;/td\u0026gt; \u0026lt;td\u0026gt; This vegetable basket is the perfect gift for your health conscious (or overweight) friends! \u0026lt;span class=\u0026quot;excitingNote\u0026quot;\u0026gt; Now with super-colorful bell peppers! \u0026lt;/span\u0026gt; \u0026lt;/td\u0026gt; \u0026lt;td\u0026gt; $15.00 \u0026lt;/td\u0026gt; \u0026lt;td\u0026gt; Let\u0026rsquo;s see some alternative ways.\n# Select the first td element soup.find('td') \u0026lt;td\u0026gt; Vegetable Basket \u0026lt;/td\u0026gt; # Select the first td element of the second tr element (row) second_row = soup.find_all('tr')[1] second_row.find('td') \u0026lt;td\u0026gt; Vegetable Basket \u0026lt;/td\u0026gt; # Select the first element of the table with id=\u0026quot;giftList\u0026quot; table = soup.find('table', {\u0026quot;id\u0026quot;:\u0026quot;giftList\u0026quot;}) table.find('td') \u0026lt;td\u0026gt; Vegetable Basket \u0026lt;/td\u0026gt; The last is the most robust way to scrape. In fact, the first two methods are likely to fail if the page gets modified. If another td element gets added on top of the table, the code will recover something else entirely. In general it\u0026rsquo;s a good practice, to look if the element we want to scrape can be identified by some attribute that is likely to be invariant to changes to other parts of the web page. In this case, the table with id=\u0026quot;giftList\u0026quot; is likely to be our object of interest even if another table id added, for example.\nLet\u0026rsquo;s say no we want to recover the whole table. What would you do?\nimport pandas as pd # Shortcut df = pd.read_html(url2)[0] df Item Title Description Cost Image 0 Vegetable Basket This vegetable basket is the perfect gift for ... $15.00 NaN 1 Russian Nesting Dolls Hand-painted by trained monkeys, these exquisi... $10,000.52 NaN 2 Fish Painting If something seems fishy about this painting, ... $10,005.00 NaN 3 Dead Parrot This is an ex-parrot! Or maybe he's only resting? $0.50 NaN 4 Mystery Box If you love suprises, this mystery box is for ... $1.50 NaN # Scraping with response table = soup.find('table', {\u0026quot;id\u0026quot;:\u0026quot;giftList\u0026quot;}) # Create empty dataframe col_names = [x.text.strip() for x in table.find_all('th')] df = pd.DataFrame(columns=col_names) # Loop over rows and append them to dataframe for row in table.find_all('tr')[1:]: columns = [x.text.strip() for x in row.find_all('td')] df_row = dict(zip(col_names, columns)) df = df.append(df_row, ignore_index=True) df /var/folders/xy/szm3b7211cj16ldwcywbvfzdqv7yhd/T/ipykernel_92159/3999490009.py:12: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead. df = df.append(df_row, ignore_index=True) /var/folders/xy/szm3b7211cj16ldwcywbvfzdqv7yhd/T/ipykernel_92159/3999490009.py:12: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead. df = df.append(df_row, ignore_index=True) /var/folders/xy/szm3b7211cj16ldwcywbvfzdqv7yhd/T/ipykernel_92159/3999490009.py:12: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead. df = df.append(df_row, ignore_index=True) /var/folders/xy/szm3b7211cj16ldwcywbvfzdqv7yhd/T/ipykernel_92159/3999490009.py:12: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead. df = df.append(df_row, ignore_index=True) /var/folders/xy/szm3b7211cj16ldwcywbvfzdqv7yhd/T/ipykernel_92159/3999490009.py:12: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead. df = df.append(df_row, ignore_index=True) Item Title Description Cost Image 0 Vegetable Basket This vegetable basket is the perfect gift for ... $15.00 1 Russian Nesting Dolls Hand-painted by trained monkeys, these exquisi... $10,000.52 2 Fish Painting If something seems fishy about this painting, ... $10,005.00 3 Dead Parrot This is an ex-parrot! Or maybe he's only resting? $0.50 4 Mystery Box If you love suprises, this mystery box is for ... $1.50 # Compact alternative table = soup.find('table', {\u0026quot;id\u0026quot;:\u0026quot;giftList\u0026quot;}) content = [[x.text.strip() for x in row.find_all(['th','td'])] for row in table.find_all('tr')] df = pd.DataFrame(content[1:], columns=content[0]) df Item Title Description Cost Image 0 Vegetable Basket This vegetable basket is the perfect gift for ... $15.00 1 Russian Nesting Dolls Hand-painted by trained monkeys, these exquisi... $10,000.52 2 Fish Painting If something seems fishy about this painting, ... $10,005.00 3 Dead Parrot This is an ex-parrot! Or maybe he's only resting? $0.50 4 Mystery Box If you love suprises, this mystery box is for ... $1.50 We have now seen how to scrape a simple but realistic webpage. Let\u0026rsquo;s proceed with a practical example.\nCSS Selectors One alternative way of doing exactly the same thing is to use select. The select function is very similar to find_all but has a different syntax. In particular, to search an element with a certain tag and attribute, we have to pass the following input:\nsoup.select(tag[attribute=\u0026quot;attribute_name\u0026quot;]) # Select the first element of the table whose id contains \u0026quot;List\u0026quot; table = soup.select('table[id*=\u0026quot;List\u0026quot;]')[0] table.find('td') \u0026lt;td\u0026gt; Vegetable Basket \u0026lt;/td\u0026gt; Forms and post requests When you are scraping, you sometimes have to fill-in forms, either to log-in into an account, or to input the arguments for a search query. Often forms are dynamic objects, but not always. Sometimes we can fill in forms also using the requests library. In whis section we see a simple example.\nShortcut Often we can bypass forms, if the form redirects us to another page whose URL contains the parameters of the form. These are \u0026ldquo;well-behaved\u0026rdquo; forms and are actually quite frequent.\nWe can find a simple example at: http://www.webscrapingfordatascience.com/basicform/. This form takes as input a bunch of information and when we click on \u0026ldquo;Submit my information\u0026rdquo;, we get exactly the same page but with a different URL that contains the information we have inserted.\nSuppose I insert the following information:\nYour gender: \u0026ldquo;Male\u0026rdquo; Food you like: \u0026ldquo;Pizza!\u0026rdquo; and \u0026ldquo;Fries please\u0026rdquo; We should get the following url: http://www.webscrapingfordatascience.com/basicform/?name=\u0026gender=M\u0026pizza=like\u0026fries=like\u0026haircolor=black\u0026comments=\nWe can decompose the url in various components, separated by one \u0026ldquo;?\u0026rdquo; and multiple \u0026ldquo;\u0026amp;\u0026rdquo;:\nhttp://www.webscrapingfordatascience.com/basicform/ name= gender=M pizza=like fries=like haircolor=black comments= We can clearly see a pattern: the first component is the cose of the url and the other components are the form options. The ones we didn\u0026rsquo;t fill have the form option= while the ones we did fill are option=value. Knowing the syntax of a particular form we could fill it ourselves.\nFor example, we could remove the fries and change the hair color to brown: http://www.webscrapingfordatascience.com/basicform/?name=\u0026gender=M\u0026pizza=like\u0026fries=\u0026haircolor=brown\u0026comments=\nMoreover, most forms work even if you remove the empty options. For example, the url above is equivalent to:http://www.webscrapingfordatascience.com/basicform/?gender=M\u0026amp;pizza=like\u0026amp;haircolor=brown\u0026amp;comments=\nOne way to scrape websites with such forms is to create a string with the url with all the empty options and fill them using string formatting functions.\n# Building form url url_core = 'http://www.webscrapingfordatascience.com/basicform/?' url_options = 'name=%s\u0026amp;gender=%s\u0026amp;pizza=%s\u0026amp;fries=%s\u0026amp;haircolor=%s\u0026amp;comments=%s' options = ('','M','like','','brown','') url = url_core + url_options % options print(url) http://www.webscrapingfordatascience.com/basicform/?name=\u0026amp;gender=M\u0026amp;pizza=like\u0026amp;fries=\u0026amp;haircolor=brown\u0026amp;comments= An alternative way is to name the options. This alternative is more verbose but more precise and does not require you to provide always all the options, even if empty.\n# Alternative 1 url_core = 'http://www.webscrapingfordatascience.com/basicform/?' url_options = 'name={name}\u0026amp;gender={gender}\u0026amp;pizza={pizza}\u0026amp;fries={fries}\u0026amp;haircolor={haircolor}\u0026amp;comments={comments}' options = { 'name': '', 'gender': 'M', 'pizza': 'like', 'fries': '', 'haircolor': 'brown', 'comments': '' } url = url_core + url_options.format(**options) print(url) http://www.webscrapingfordatascience.com/basicform/?name=\u0026amp;gender=M\u0026amp;pizza=like\u0026amp;fries=\u0026amp;haircolor=brown\u0026amp;comments= Lastly, one could build the url on the go.\n# Alternative 2 url = 'http://www.webscrapingfordatascience.com/basicform/?' options = { 'gender': 'M', 'pizza': 'like', 'haircolor': 'brown', } for key, value in options.items(): url += key + '=' + value + '\u0026amp;' print(url) http://www.webscrapingfordatascience.com/basicform/?gender=M\u0026amp;pizza=like\u0026amp;haircolor=brown\u0026amp; Post forms Sometimes however, forms do not provide nice URLs as output. This is particularly true for login forms. There is however still a method, for some of them, to deal with them.\nFor this section, we will use the same form example as before: http://www.webscrapingfordatascience.com/postform2/.\nThis looks like the same form but now when the user clicks on \u0026ldquo;Submit my information\u0026rdquo;, we get a page with a summary of the information. The biggest difference however, is that the output URL is exactly the same. Hence, we cannot rely on the same URL-bulding strategy as before.\nIf we inspect the page, we observe the following line at the very beginning\n\u0026lt;form method=\u0026quot;POST\u0026quot;\u0026gt; [...] \u0026lt;/form\u0026gt; And inside there are various input fields:\n\u0026lt;input type=\u0026quot;text\u0026quot;\u0026gt; for name \u0026lt;input type=\u0026quot;radio\u0026quot;\u0026gt; for gender \u0026lt;input type=\u0026quot;checkbox\u0026quot;\u0026gt; for food \u0026lt;select\u0026gt;...\u0026lt;/select\u0026gt; for the hair color \u0026lt;textarea\u0026gt;...\u0026lt;/textarea\u0026gt; for comments These are all fields with which we can interact using the response package. The main difference is that we won\u0026rsquo;t use the get method to get the response from the URL but we will use the post method to post our form parameters and get a response.\nIf we input the following options:\ngender: male pizza: yes hair color: brown hair and we click \u0026ldquo;Submit my information\u0026rdquo; we get to a page with the following text:\nThanks for submitting your information Here's a dump of the form data that was submitted: array(5) { [\u0026quot;name\u0026quot;]=\u0026gt; string(0) \u0026quot;\u0026quot; [\u0026quot;gender\u0026quot;]=\u0026gt; string(1) \u0026quot;M\u0026quot; [\u0026quot;pizza\u0026quot;]=\u0026gt; string(4) \u0026quot;like\u0026quot; [\u0026quot;haircolor\u0026quot;]=\u0026gt; string(5) \u0026quot;brown\u0026quot; [\u0026quot;comments\u0026quot;]=\u0026gt; string(0) \u0026quot;\u0026quot; } We will not try to get to the same page using the requests package.\n# URL url = 'http://www.webscrapingfordatascience.com/postform2/' # Options options = { 'gender': 'M', 'pizza': 'like', 'haircolor': 'brown', } # Post request response = requests.post(url, data=options) print(response.text) \u0026lt;html\u0026gt; \u0026lt;body\u0026gt; \u0026lt;h2\u0026gt;Thanks for submitting your information\u0026lt;/h2\u0026gt; \u0026lt;p\u0026gt;Here's a dump of the form data that was submitted:\u0026lt;/p\u0026gt; \u0026lt;pre\u0026gt;array(3) { [\u0026quot;gender\u0026quot;]=\u0026gt; string(1) \u0026quot;M\u0026quot; [\u0026quot;pizza\u0026quot;]=\u0026gt; string(4) \u0026quot;like\u0026quot; [\u0026quot;haircolor\u0026quot;]=\u0026gt; string(5) \u0026quot;brown\u0026quot; } \u0026lt;/pre\u0026gt; \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt; We have obtained exactly what we wanted! However, sometimes, websites block direct post requests.\nOne simple example is: http://www.webscrapingfordatascience.com/postform3/.\n# Post request url = 'http://www.webscrapingfordatascience.com/postform3/' response = requests.post(url, data=options) print(response.text) \u0026lt;html\u0026gt; \u0026lt;body\u0026gt; Are you trying to submit information from somewhere else? \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt; What happened? If we inspect the page, we can see that there is a new line at the beginning:\n\u0026lt;input type=\u0026quot;hidden\u0026quot; name=\u0026quot;protection\u0026quot; value=\u0026quot;2c17abf5d5b4e326bea802600ff88405\u0026quot;\u0026gt; Now the form contains one more value - protection which is conventiently hidden. In order to bypass the protection, we need to provide the correct protection value to the form.\n# Post request url = 'http://www.webscrapingfordatascience.com/postform3/' response = requests.get(url) # Get out the value for protection soup = BeautifulSoup(response.text, 'lxml') options['protection'] = soup.find('input', attrs={'name': 'protection'}).get('value') # Post request response = requests.post(url, data=options) print(response.text) \u0026lt;html\u0026gt; \u0026lt;body\u0026gt; \u0026lt;h2\u0026gt;Thanks for submitting your information\u0026lt;/h2\u0026gt; \u0026lt;p\u0026gt;Here's a dump of the form data that was submitted:\u0026lt;/p\u0026gt; \u0026lt;pre\u0026gt;array(4) { [\u0026quot;gender\u0026quot;]=\u0026gt; string(1) \u0026quot;M\u0026quot; [\u0026quot;pizza\u0026quot;]=\u0026gt; string(4) \u0026quot;like\u0026quot; [\u0026quot;haircolor\u0026quot;]=\u0026gt; string(5) \u0026quot;brown\u0026quot; [\u0026quot;protection\u0026quot;]=\u0026gt; string(32) \u0026quot;16c87fc858e4d9fcb8d9c920b699388d\u0026quot; } \u0026lt;/pre\u0026gt; \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt; Indeed, now the post request was successful.\nProxies We have discussed at the beginning how to be more subtle while scraping, by changing headers. In this section we will explore one step forward in anonimity: proxies.\nWhen we send an HTTP request, first the request is sent to a proxy server. The important thing is that the destination web server will which is the origin proxy server. Therefore, when one destination web server sees too many requests coming from one machine, it will block the proxy server.\nHow can we change proxy? There are many websites that offer proxies for money but there are also some that offer proxies for free. The problem with free proxies (but often also with premium ones) is that there are many users using the same proxy, hence they are\nslow blocked fast by many websites Nevertheless, it might be still useful to know how to change proxies.\nGet proxy list One website where we can get some free proxies to use for scraping is https://free-proxy-list.net/.\nIf we open the page, we see that there is a long list of proxies, from different countries and with different characteristics. Importantly, we are mostly interested in https proxies. We are now going to retrieve a list of them. Note that the proxy list of this website is updated quite often. However, free proxies usually \u0026ldquo;expire\u0026rdquo; even faster.\n# Retrieve proxy list def get_proxies(): response = requests.get('https://free-proxy-list.net/') soup = BeautifulSoup(response.text, 'lxml') table = soup.find('table', {'class':'table'}) proxies = [] rows = table.find_all('tr') for row in rows: cols = row.find_all('td') if len(cols)\u0026gt;0: line = [col.text for col in cols] if line[6]=='yes': proxies += [line[0]+':'+line[1]] return proxies len(get_proxies()) 176 We have found many proxies. How do we use them? We have to provide them as an argment to a requests session.\n# Test proxies url = 'https://www.google.com' proxies = get_proxies() for proxy in proxies[:10]: try: response = session.get(url, proxies={\u0026quot;https\u0026quot;: proxy}, timeout=5) print(response) except Exception as e: print(type(e)) \u0026lt;class 'NameError'\u0026gt; \u0026lt;class 'NameError'\u0026gt; \u0026lt;class 'NameError'\u0026gt; \u0026lt;class 'NameError'\u0026gt; \u0026lt;class 'NameError'\u0026gt; \u0026lt;class 'NameError'\u0026gt; \u0026lt;class 'NameError'\u0026gt; \u0026lt;class 'NameError'\u0026gt; \u0026lt;class 'NameError'\u0026gt; \u0026lt;class 'NameError'\u0026gt; Yes, most proxies were extremely slow (and consider we are opening Google\u0026hellip;) and we got a ConnetTimeout error. Other proxies worked and for one or two of the others we might have got a ProxyError.\nDynamic Webscraping Let\u0026rsquo;s try to scrape the quotes from this link: http://www.webscrapingfordatascience.com/simplejavascript/. It seems like a straightforward job.\n# Scrape javascript page url = 'http://www.webscrapingfordatascience.com/simplejavascript/' response = requests.get(url) print(response.text) \u0026lt;html\u0026gt; \u0026lt;head\u0026gt; \u0026lt;script src=\u0026quot;https://code.jquery.com/jquery-3.2.1.min.js\u0026quot;\u0026gt;\u0026lt;/script\u0026gt; \u0026lt;script\u0026gt; $(function() { document.cookie = \u0026quot;jsenabled=1\u0026quot;; $.getJSON(\u0026quot;quotes.php\u0026quot;, function(data) { var items = []; $.each(data, function(key, val) { items.push(\u0026quot;\u0026lt;li id='\u0026quot; + key + \u0026quot;'\u0026gt;\u0026quot; + val + \u0026quot;\u0026lt;/li\u0026gt;\u0026quot;); }); $(\u0026quot;\u0026lt;ul/\u0026gt;\u0026quot;, { html: items.join(\u0026quot;\u0026quot;) }).appendTo(\u0026quot;body\u0026quot;); }); }); \u0026lt;/script\u0026gt; \u0026lt;/head\u0026gt; \u0026lt;body\u0026gt; \u0026lt;h1\u0026gt;Here are some quotes\u0026lt;/h1\u0026gt; \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt; Weird. Our response does not contain the quotes on the page, even though they are clearly visible when we open it in our browser.\nSelenium Selenium is a python library that emulates a browser and lets us see pages exactly as with a normal browser. This is the most user-friendly way to do web scraping, however it has a huge cost: speed. This is by far the slowest way to do web scraping.\nAfter installing selenium, we need to download a browser to simulate. We will use Google\u0026rsquo;s chromedriver. You can download it from here: https://sites.google.com/a/chromium.org/chromedriver/. Make sure to select \u0026ldquo;latest stable release\u0026rdquo; and not \u0026ldquo;latest beta release\u0026rdquo;.\nMove the downloaded chromedriver in the current directory (\u0026quot;/11-python-webscraping\u0026quot; for me). We will now try open the url above with selenium and see if we can scrape the quotes in it.\n# Set your chromedriver name chromedriver_name = '/chromedriver_mac' # Open url path = os.getcwd() print(path) driver = webdriver.Chrome(path+chromedriver_name) /Users/mcourt/Dropbox/Projects/Data-Science-Python/notebooks /var/folders/xy/szm3b7211cj16ldwcywbvfzdqv7yhd/T/ipykernel_92159/2846782857.py:4: DeprecationWarning: executable_path has been deprecated, please pass in a Service object driver = webdriver.Chrome(path+chromedriver_name) Awesome! Now, if everything went smooth, you should have a new Chrome window with a banner that says \u0026ldquo;Chrome is being controlled by automated test software\u0026rdquo;. We can now open the web page and check that the list appears.\n# Open url url = 'http://www.webscrapingfordatascience.com/simplejavascript/' driver.get(url) Again, if averything went well, we are now abl to see our page with all the quotes in it. How do we scrape them?\nIf we inspect the elements of the list with the right-click inspect option, we should see something like:\n\u0026lt;html\u0026gt;\u0026lt;head\u0026gt; \u0026lt;script src=\u0026quot;https://code.jquery.com/jquery-3.2.1.min.js\u0026quot;\u0026gt;\u0026lt;/script\u0026gt; \u0026lt;script\u0026gt; $(function() { document.cookie = \u0026quot;jsenabled=1\u0026quot;; $.getJSON(\u0026quot;quotes.php\u0026quot;, function(data) { var items = []; $.each(data, function(key, val) { items.push(\u0026quot;\u0026lt;li id='\u0026quot; + key + \u0026quot;'\u0026gt;\u0026quot; + val + \u0026quot;\u0026lt;/li\u0026gt;\u0026quot;); }); $(\u0026quot;\u0026lt;ul/\u0026gt;\u0026quot;, { html: items.join(\u0026quot;\u0026quot;) }).appendTo(\u0026quot;body\u0026quot;); }); }); \u0026lt;/script\u0026gt; \u0026lt;/head\u0026gt; \u0026lt;body\u0026gt; \u0026lt;h1\u0026gt;Here are some quotes\u0026lt;/h1\u0026gt; \u0026lt;ul\u0026gt;\u0026lt;li id=\u0026quot;0\u0026quot;\u0026gt;Every strike brings me closer to the next home run. –Babe Ruth\u0026lt;/li\u0026gt;\u0026lt;li id=\u0026quot;1\u0026quot;\u0026gt;The two most important days in your life are the day you are born and the day you find out why. –Mark Twain\u0026lt;/li\u0026gt;\u0026lt;li id=\u0026quot;2\u0026quot;\u0026gt;Whatever you can do, or dream you can, begin it. Boldness has genius, power and magic in it. –Johann Wolfgang von Goethe\u0026lt;/li\u0026gt;\u0026lt;/ul\u0026gt;\u0026lt;/body\u0026gt;\u0026lt;/html\u0026gt; Now we can see the content! Can we actually retrieve it? Let\u0026rsquo;s try.\nThe most common selenium functions to get elements of a page, have a very intuitive syntax and are: find_element_by_id\nfind_element_by_name find_element_by_xpath find_element_by_link_text find_element_by_partial_link_text find_element_by_tag_name find_element_by_class_name find_element_by_css_selector We will not try to recover all elements with tag \u0026lt;li\u0026gt; (element of list \u0026lt;ul\u0026gt;).\n# Scrape content quotes = [li.text for li in driver.find_elements_by_tag_name('li')] quotes /var/folders/xy/szm3b7211cj16ldwcywbvfzdqv7yhd/T/ipykernel_92159/157107938.py:2: DeprecationWarning: find_elements_by_* commands are deprecated. Please use find_elements() instead quotes = [li.text for li in driver.find_elements_by_tag_name('li')] [] Yes! It worked! But why?\n# Headless option headless_option = webdriver.ChromeOptions() headless_option.add_argument('--headless') # Scraping driver = webdriver.Chrome(path+chromedriver_name, options=headless_option) driver.get(url) quotes = [li.text for li in driver.find_elements_by_tag_name('li')] quotes /var/folders/xy/szm3b7211cj16ldwcywbvfzdqv7yhd/T/ipykernel_92159/2173692453.py:6: DeprecationWarning: executable_path has been deprecated, please pass in a Service object driver = webdriver.Chrome(path+chromedriver_name, options=headless_option) /var/folders/xy/szm3b7211cj16ldwcywbvfzdqv7yhd/T/ipykernel_92159/2173692453.py:8: DeprecationWarning: find_elements_by_* commands are deprecated. Please use find_elements() instead quotes = [li.text for li in driver.find_elements_by_tag_name('li')] [] Mmm, it (probably) didn\u0026rsquo;t work. Why?\nThe problem is that we are trying to retrieve the content of the page too fast. The page hasn\u0026rsquo;t loaded yet. This is a common issue with selenium. Where are two ways to solve it:\nwaiting waiting for the element to load The second way is the best way but we will first try the first and simpler one: we will just ask the browser to wait for 1 second before searching for \u0026lt;li\u0026gt; tags\n# Scraping driver = webdriver.Chrome(path+chromedriver_name, options=headless_option) driver.get(url) time.sleep(1) quotes = [li.text for li in driver.find_elements_by_tag_name('li')] quotes /var/folders/xy/szm3b7211cj16ldwcywbvfzdqv7yhd/T/ipykernel_92159/2964398594.py:2: DeprecationWarning: executable_path has been deprecated, please pass in a Service object driver = webdriver.Chrome(path+chromedriver_name, options=headless_option) /var/folders/xy/szm3b7211cj16ldwcywbvfzdqv7yhd/T/ipykernel_92159/2964398594.py:5: DeprecationWarning: find_elements_by_* commands are deprecated. Please use find_elements() instead quotes = [li.text for li in driver.find_elements_by_tag_name('li')] ['The best time to plant a tree was 20 years ago. The second best time is now. –Chinese Proverb', 'The most common way people give up their power is by thinking they don’t have any. –Alice Walker', 'I am not a product of my circumstances. I am a product of my decisions. –Stephen Covey'] Nice! Now you should have obtained the list that we could not scrape with requests. If it didn\u0026rsquo;t work, just increase the waiting time and it should work.\nWe can now have a look at the \u0026ldquo;better\u0026rdquo; way to use a series of built-in functions:\nWebDriverWait: the waiting function. We will call the until method expected_conditions: the condition function. We will call the visibility_of_all_elements_located method By: the selector function. Some of the options are: By.ID By.XPATH By.NAME By.TAG_NAME By.CLASS_NAME By.CSS_SELECTOR By.LINK_TEXT By.PARTIAL_LINK_TEXT from selenium.webdriver.common.by import By from selenium.webdriver.support.ui import WebDriverWait from selenium.webdriver.support import expected_conditions as EC # Scraping driver = webdriver.Chrome(path+chromedriver_name, options=headless_option) driver.get(url) quotes = WebDriverWait(driver, 10).until(EC.visibility_of_all_elements_located((By.TAG_NAME, 'li'))) quotes = [quote.text for quote in quotes] quotes /var/folders/xy/szm3b7211cj16ldwcywbvfzdqv7yhd/T/ipykernel_92159/152412441.py:6: DeprecationWarning: executable_path has been deprecated, please pass in a Service object driver = webdriver.Chrome(path+chromedriver_name, options=headless_option) ['The most common way people give up their power is by thinking they don’t have any. –Alice Walker', 'The best time to plant a tree was 20 years ago. The second best time is now. –Chinese Proverb', 'An unexamined life is not worth living. –Socrates'] In this case, we have told the browser to wait until either all elements with tag \u0026lt;li\u0026gt; are visible or 10 seconds have passed. After one condition is realized, the WebDriverWait function also automatically retrieves all the elements which the expected_condition function is conditioning on. There are many different conditions we can use. A list can be found here: https://selenium-python.readthedocs.io/waits.html.\nWe can easily generalize the function above as follows.\n# Find element function def find_elements(driver, function, identifier): element = WebDriverWait(driver, 10).until(EC.visibility_of_all_elements_located((function, identifier))) return element quotes = [quote.text for quote in find_elements(driver, By.TAG_NAME, 'li')] quotes ['The most common way people give up their power is by thinking they don’t have any. –Alice Walker', 'The best time to plant a tree was 20 years ago. The second best time is now. –Chinese Proverb', 'An unexamined life is not worth living. –Socrates'] ","date":1651363200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1651363200,"objectID":"bfe25f3c8fd85b78b79bafee570b4721","permalink":"https://matteocourthoud.github.io/course/data-science/07_web_scraping/","publishdate":"2022-05-01T00:00:00Z","relpermalink":"/course/data-science/07_web_scraping/","section":"course","summary":"import os import re import time import requests import pandas as pd from bs4 import BeautifulSoup from pprint import pprint from selenium import webdriver There is no silver bullet to getting info from the internet.","tags":null,"title":"Web Scraping","type":"book"},{"authors":null,"categories":null,"content":"%matplotlib inline from utils.lecture07 import * Decision Trees Decision trees involve segmenting the predictor space into a number of simple regions. In order to make a prediction for a given observation, we typically use the mean or the mode of the training observations in the region to which it belongs. Since the set of splitting rules used to segment the predictor space can be summarized in a tree, these types of approaches are known as decision tree methods.\nRegression Trees For this session we will consider the Hitters dataset. It consists in individual level data of baseball players. In our applications, we are interested in predicting the players Salary.\n# Load the data hitters = pd.read_csv('data/Hitters.csv').dropna() hitters.head() Unnamed: 0 AtBat Hits HmRun Runs RBI Walks Years CAtBat CHits ... CRuns CRBI CWalks League Division PutOuts Assists Errors Salary NewLeague 1 -Alan Ashby 315 81 7 24 38 39 14 3449 835 ... 321 414 375 N W 632 43 10 475.0 N 2 -Alvin Davis 479 130 18 66 72 76 3 1624 457 ... 224 266 263 A W 880 82 14 480.0 A 3 -Andre Dawson 496 141 20 65 78 37 11 5628 1575 ... 828 838 354 N E 200 11 3 500.0 N 4 -Andres Galarraga 321 87 10 39 42 30 2 396 101 ... 48 46 33 N E 805 40 4 91.5 N 5 -Alfredo Griffin 594 169 4 74 51 35 11 4408 1133 ... 501 336 194 A W 282 421 25 750.0 A 5 rows × 21 columns\nIn particular, we are interested in looking how the number of Hits and the Years of experience predict the Salary.\n# Get Features features = ['Years', 'Hits'] X = hitters[features].values y = np.log(hitters.Salary.values) We are actually going to use log(salary) since it has a more gaussian distribution.\nfig, (ax1, ax2) = plt.subplots(1,2, figsize=(11,4)) # Plot salary distribution ax1.hist(hitters.Salary.values) ax1.set_xlabel('Salary') ax2.hist(y) ax2.set_xlabel('Log(Salary)'); In order to understand what is a tree, let\u0026rsquo;s first have a look at one. We fit a regression three with 3 leaves or, equivalently put, 2 nodes.\n# Fit regression tree tree = DecisionTreeRegressor(max_leaf_nodes=3) tree.fit(X, y) DecisionTreeRegressor(max_leaf_nodes=3) We are now going to plot the results visually. The biggest avantage of trees is interpretability.\n# Figure 8.1 fig, ax = plt.subplots(1,1) ax.set_title('Figure 8.1'); # Plot tree plot_tree(tree, filled=True, feature_names=features, fontsize=14, ax=ax); The tree consists of a series of splitting rules, starting at the top of the tree. The top split assigns observations having Years\u0026lt;4.5 to the left branch.1 The predicted salary for these players is given by the mean response value for the players in the data set with Years\u0026lt;4.5. For such players, the mean log salary is 5.107, and so we make a prediction of 5.107 thousands of dollars, i.e. $165,174, for these players. Players with Years\u0026gt;=4.5 are assigned to the right branch, and then that group is further subdivided by Hits.\nOverall, the tree stratifies or segments the players into three regions of predictor space:\nplayers who have played for four or fewer years players who have played for five or more years and who made fewer than 118 hits last year, and players who have played for five or more years and who made at least 118 hits last year. These three regions can be written as\nR1 = {X | Years\u0026lt;4.5} R2 = {X | Years\u0026gt;=4.5, Hits\u0026lt;117.5}, and R3 = {X | Years\u0026gt;=4.5, Hits\u0026gt;=117.5}. Since the dimension of $X$ is 2, we can visualize the space and the regions in a 2-dimensional graph.\n# Figure 8.2 def make_figure_8_2(): # Init hitters.plot('Years', 'Hits', kind='scatter', color='orange', figsize=(7,6)) plt.title('Figure 8.2') plt.xlim(0,25); plt.ylim(ymin=-5); plt.xticks([1, 4.5, 24]); plt.yticks([1, 117.5, 238]); # Split lines plt.vlines(4.5, ymin=-5, ymax=250, color='g') plt.hlines(117.5, xmin=4.5, xmax=25, color='g') # Regions plt.annotate('R1', xy=(2,117.5), fontsize='xx-large') plt.annotate('R2', xy=(11,60), fontsize='xx-large') plt.annotate('R3', xy=(11,170), fontsize='xx-large'); make_figure_8_2() We might interpret the above regression tree as follows: Years is the most important factor in determining Salary, and players with less experience earn lower salaries than more experienced players. Given that a player is less experienced, the number of hits that he made in the previous year seems to play little role in his salary. But among players who have been in the major leagues for five or more years, the number of hits made in the previous year does affect salary, and players who made more hits last year tend to have higher salaries.\nBuilding a Tree There are two main steps in the construction of a tree:\nWe divide the predictor space—that is, the set of possible values for $X_1, X_2, \u0026hellip; , X_p$ into $J$ distinct and non-overlapping regions, $R_1,R_2,\u0026hellip;,R_J$. For every observation that falls into the region $R_j$ , we make the same prediction, which is simply the mean of the response values for the training observations in $R_j$. The second step is easy. But how does one construct the regions? Our purpose is to minimize the Sum of Squared Residuals, across the different regions:\n$$ \\sum_{j=1}^{J} \\sum_{i \\in R_{j}}\\left(y_{i}-\\hat{y}{R{j}}\\right)^{2} $$\nUnfortunately, it is computationally infeasible to consider every possible partition of the feature space into $J$ boxes.\nFor this reason, we take a top-down, greedy approach that is known as recursive binary splitting. The approach is top-down because it begins at the top of the tree (at which point all observations belong to a single region) and then successively splits the predictor space; each split is indicated via two new branches further down on the tree. It is greedy because at each step of the tree-building process, the best split is made at that particular step, rather than looking ahead and picking a split that will lead to a better tree in some future step.\nIn practice, the method is the following:\nwe select the predictor $X_j$ we select the cutpoint $s$ such that splitting the predictor space into the regions ${X|X_j \u0026lt; s}$ and ${X|X_j \\geq s}$ leads to the greatest possible reduction in RSS we repeat (1)-(2) for all predictors $X_1, \u0026hellip; , X_p$, i.e. we solve $$ \\arg \\min_{j,s} \\ \\sum_{i: x_{i} \\in {X|X_j \u0026lt; s}}\\left(y_{i}-\\hat{y}i\\right)^{2}+\\sum{i: x_{i} \\in {X|X_j \\geq s}}\\left(y_{i}-\\hat{y}_i\\right)^{2} $$\nwe choose the predictor and cutpoint such that the resulting tree has the lowest RSS we keep repeating (1)-(4) until a certain condition is met. However, after the first iteration we also have to pick which region to split which adds a further dimension to optimize over. Let\u0026rsquo;s build our own Node class to play around with trees.\nclass Node: \u0026quot;\u0026quot;\u0026quot; Class used to represent nodes in a Regression Tree Attributes ---------- x : np.array independent variables y : np.array dependent variables idxs : np.array indexes fo x and y for current node depth : int depth of the sub-tree (default 5) Methods ------- find_next_nodes(self) Keep growing the tree find_best_split(self) Find the best split split(self) Split the tree \u0026quot;\u0026quot;\u0026quot; def __init__(self, x, y, idxs, depth=5): \u0026quot;\u0026quot;\u0026quot;Initialize node\u0026quot;\u0026quot;\u0026quot; self.x = x self.y = y self.idxs = idxs self.depth = depth self.get_next_nodes() def get_next_nodes(self): \u0026quot;\u0026quot;\u0026quot;If the node is not terminal, get further splits\u0026quot;\u0026quot;\u0026quot; if self.is_last_leaf: return self.find_best_split() self.split() def find_best_split(self): \u0026quot;\u0026quot;\u0026quot;Loop over variables and their values to find the best split\u0026quot;\u0026quot;\u0026quot; best_score = float('inf') # Loop over variables for col in range(self.x.shape[1]): x = self.x[self.idxs, col] # Loop over all splits for s in np.unique(x): lhs = x \u0026lt;= s rhs = x \u0026gt; s curr_score = self.get_score(lhs, rhs) # If best score, save it if curr_score \u0026lt; best_score: best_score = curr_score self.split_col = col self.split_val = s return self def get_score(self, lhs, rhs): \u0026quot;\u0026quot;\u0026quot;Get score of a given split\u0026quot;\u0026quot;\u0026quot; y = self.y[self.idxs] lhs_mse = self.get_mse(y[lhs]) rhs_mse = self.get_mse(y[rhs]) return lhs_mse * lhs.sum() + rhs_mse * rhs.sum() def get_mse(self, y): return np.mean((y-np.mean(y))**2) def split(self): \u0026quot;\u0026quot;\u0026quot;Split a node into 2 sub-nodes (recursive)\u0026quot;\u0026quot;\u0026quot; x = self.x[self.idxs, self.split_col] lhs = x \u0026lt;= self.split_val rhs = x \u0026gt; self.split_val self.lhs = Node(self.x, self.y, self.idxs[lhs], self.depth-1) self.rhs = Node(self.x, self.y, self.idxs[rhs], self.depth-1) to_print = (self.depth, self.split_col, self.split_val, sum(lhs), sum(rhs)) print('Split on layer %.0f: var%1.0f = %.4f (%.0f/%.0f)' % to_print) return self @property def is_last_leaf(self): return self.depth\u0026lt;=1 What does a Node look like?\n# Init first node tree1 = Node(X, y, np.arange(len(y)), 1) # Documentation (always comment and document your code!) print(tree1.__doc__) Class used to represent nodes in a Regression Tree Attributes ---------- x : np.array independent variables y : np.array dependent variables idxs : np.array indexes fo x and y for current node depth : int depth of the sub-tree (default 5) Methods ------- find_next_nodes(self) Keep growing the tree find_best_split(self) Find the best split split(self) Split the tree Which properties does it have?\n# Inspect the class dir(tree1) ['__class__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__le__', '__lt__', '__module__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__sizeof__', '__str__', '__subclasshook__', '__weakref__', 'depth', 'find_best_split', 'get_mse', 'get_next_nodes', 'get_score', 'idxs', 'is_last_leaf', 'split', 'x', 'y'] What is the depth? How many observations are there?\n# Get info print('Tree of depth %.0f with %.0f observations' % (tree1.depth, len(tree1.idxs))) Tree of depth 1 with 263 observations Fair enough, the tree is just a single leaf.\n# Check if terminal tree1.is_last_leaf True Let\u0026rsquo;s find the first split.\n# Find best split tree1.find_best_split() print('Split at var%1.0f = %.4f' % (tree1.split_col, tree1.split_val)) Split at var0 = 4.0000 If has selected the first variable, at the value $4$.\nIf we call the split function, it will also tell us how many observations per leaf the split generates.\n# Split tree tree1.split(); Split on layer 1: var0 = 4.0000 (90/173) Now we are ready to compute even deeper trees\n# Check depth-3 tree tree3 = Node(X, y, np.arange(len(y)), 3) Split on layer 2: var1 = 4.0000 (2/88) Split on layer 2: var1 = 117.0000 (90/83) Split on layer 3: var0 = 4.0000 (90/173) Pruning The process described above may produce good predictions on the training set, but is likely to overfit the data, leading to poor test set performance. This is because the resulting tree might be too complex. A smaller tree with fewer splits might lead to lower variance and better interpretation at the cost of a little bias.\nWe can see it happening if we build the same tree as above, but with 5 leaves.\n# Compute tree overfit_tree = DecisionTreeRegressor(max_leaf_nodes=5).fit(X, y) We plot the 5-leaf tree.\n# Plot tree fig, ax = plt.subplots(1,1) plot_tree(overfit_tree, filled=True, feature_names=features, fontsize=14, ax=ax); The split on the far left is predicting a very high Salary (7.243) for players with few Years of experience and few Hits. Indeed this prediction is based on an extremely tiny subsample (2). They are probably outliers and our tree is most likely overfitting.\nOne possible alternative is to insert a minimum number of observation per leaf.\n# Compute tree no_overfit_tree = DecisionTreeRegressor(max_leaf_nodes=5, min_samples_leaf=10).fit(X, y) # Plot tree fig, ax = plt.subplots(1,1) plot_tree(no_overfit_tree, filled=True, feature_names=features, fontsize=14, ax=ax); Now the tree makes much more sense: the lower the Years and the Hits, the lower the predicted Salary as we can see from the shades getting darker and darker as we move left to right\nAnother possible alternative to the process described above is to build the tree only so long as the decrease in the RSS due to each split exceeds some (high) threshold.\nThis strategy will result in smaller trees, but is too short-sighted since a seemingly worthless split early on in the tree might be followed by a very good split—that is, a split that leads to a large reduction in RSS later on.\nWe can use cross-validation to pick the optimal tree length.\n# Import original split features = ['Years', 'Hits', 'RBI', 'PutOuts', 'Walks', 'Runs', 'AtBat', 'HmRun'] X_train = pd.read_csv('data/Hitters_X_train.csv').dropna()[features] X_test = pd.read_csv('data/Hitters_X_test.csv').dropna()[features] y_train = pd.read_csv('data/Hitters_y_train.csv').dropna() y_test = pd.read_csv('data/Hitters_y_test.csv').dropna() # Init params = range(2,11) reg_scores = np.zeros((len(params),3)) best_score = 10**6 # Loop over all parameters for i,k in enumerate(params): # Model tree = DecisionTreeRegressor(max_leaf_nodes=k) # Loop over splits tree.fit(X_train, y_train) reg_scores[i,0] = mean_squared_error(tree.predict(X_train), y_train) reg_scores[i,1] = mean_squared_error(tree.predict(X_test), y_test) # Get CV score kf6 = KFold(n_splits=6) reg_scores[i,2] = -cross_val_score(tree, X_train, y_train, cv=kf6, scoring='neg_mean_squared_error').mean() # Save best model if reg_scores[i,2]\u0026lt;best_score: best_model = tree best_score = reg_scores[i,2] Let\u0026rsquo;s plot the optimal tree depth, using 6-fold cv.\n# Figure 8.5 def make_figure_8_5(): # Init fig, (ax1,ax2) = plt.subplots(1,2,figsize=(16,6)) fig.suptitle('Figure 8.5') # Plot scores ax1.plot(params, reg_scores); ax1.axvline(params[np.argmin(reg_scores[:,2])], c='k', ls='--') ax1.legend(['Train','Test','6-fold CV']); ax1.set_title('Cross-Validation Scores'); # Plot best tree plot_tree(best_model, filled=True, impurity=False, feature_names=features, fontsize=12, ax=ax2); ax2.set_title('Best Model'); make_figure_8_5() The optimal tree has 4 leaves.\nClassification Trees A classification tree is very similar to a regression tree, except that it is used to predict a qualitative response rather than a quantitative one.\nFor a classification tree, we predict that each observation belongs to the most commonly occurring class of training observations in the region to which it belongs.\nBuilding a Classification Tree The task of growing a classification tree is similar to the task of growing a regression tree. However, in the classification setting, RSS cannot be used as a criterion for making the binary splits.\nWe define $\\hat p_{mk}$ as the proportion of training observations in the $m^{th}$ region that are from the $k^{th}$ class. Possible loss functions to decide the splits are:\nClassification error rate $$ E = 1 - \\max {k}\\left(\\hat{p}{m k}\\right) $$\nGini index $$ G=\\sum_{k=1}^{K} \\hat{p}{m k}\\left(1-\\hat{p}{m k}\\right) $$\nEntropy $$ D=-\\sum_{k=1}^{K} \\hat{p}{m k} \\log \\hat{p}{m k} $$\nIn 2-class classification problems, this is what the different scores look like, for different proportions of class 2 ($p$), when the true proportion is $p_0 =0.5$.\nWhen building a classification tree, either the Gini index or the entropy are typically used to evaluate the quality of a particular split, since these two approaches are more sensitive to node purity than is the classification error rate.\nFor this section we will work with the Heart dataset on individual heart failures. We will try to use individual characteristics in order to predict heart deseases (HD). The varaible is binary: Yes, No.\n# Load heart dataset heart = pd.read_csv('data/Heart.csv').drop('Unnamed: 0', axis=1).dropna() heart.head() Age Sex ChestPain RestBP Chol Fbs RestECG MaxHR ExAng Oldpeak Slope Ca Thal AHD 0 63 1 typical 145 233 1 2 150 0 2.3 3 0.0 fixed No 1 67 1 asymptomatic 160 286 0 2 108 1 1.5 2 3.0 normal Yes 2 67 1 asymptomatic 120 229 0 2 129 1 2.6 2 2.0 reversable Yes 3 37 1 nonanginal 130 250 0 0 187 0 3.5 3 0.0 normal No 4 41 0 nontypical 130 204 0 2 172 0 1.4 1 0.0 normal No # Fastorize variables heart.ChestPain = pd.factorize(heart.ChestPain)[0] heart.Thal = pd.factorize(heart.Thal)[0] # Set features features = [col for col in heart.columns if col!='AHD'] X2 = heart[features] y2 = pd.factorize(heart.AHD)[0] We now fit our classifier.\n# Fit classification tree clf = DecisionTreeClassifier(max_depth=None, max_leaf_nodes=11) clf.fit(X2,y2) DecisionTreeClassifier(max_leaf_nodes=11) What is the score?\n# Final score clf.score(X2,y2) 0.8686868686868687 Let\u0026rsquo;s have a look at the whole tree.\n# Figure 8.6 a def make_fig_8_6a(): # Init fig, ax = plt.subplots(1,1, figsize=(16,12)) ax.set_title('Figure 8.6'); # Plot tree plot_tree(clf, filled=True, feature_names=features, class_names=['No','Yes'], fontsize=12, ax=ax); make_fig_8_6a() This figure has a surprising characteristic: some of the splits yield two terminal nodes that have the same predicted value.\nFor instance, consider the split Age\u0026lt;=57.5 near the bottom left of the unpruned tree. Regardless of the value of Age, a response value of No is predicted for those observations. Why, then, is the split performed at all?\nThe split is performed because it leads to increased node purity. That is, 2/81 of the observations corresponding to the left-hand leaf have a response value of Yes, whereas 9/36 of those corresponding to the right-hand leaf have a response value of Yes. Why is node purity important? Suppose that we have a test observation that belongs to the region given by that left-hand leaf. Then we can be pretty certain that its response value is No. In contrast, if a test observation belongs to the region given by the right-hand leaf, then its response value is probably No, but we are much less certain. Even though the split Age\u0026lt;=57.5 does not reduce the classification error, it improves the Gini index and the entropy, which are more sensitive to node purity.\nPruning for Classification We can repeat the pruning exercise also for the classification task.\n# Figure 8.6 b def make_figure_8_6b(): # Init fig, (ax1, ax2) = plt.subplots(1,2,figsize=(14,6)) fig.suptitle('Figure 8.6') # Plot scores ax1.plot(params, clf_scores); ax1.legend(['Train','Test','6-fold CV']); # Plot best tree plot_tree(best_model, filled=True, impurity=False, feature_names=features, fontsize=12, ax=ax2); # Init J = 10 params = range(2,11) clf_scores = np.zeros((len(params),3)) best_score = 100 # Loop over all parameters for i,k in enumerate(params): # Model tree = DecisionTreeClassifier(max_leaf_nodes=k) # Loop J times temp_scores = np.zeros((J,3)) for j in range (J): # Loop over splits X2_train, X2_test, y2_train, y2_test = train_test_split(X2, y2, test_size=0.5, random_state=j) m = tree.fit(X2_train, y2_train) temp_scores[j,0] = mean_squared_error(m.predict(X2_train), y2_train) temp_scores[j,1] = mean_squared_error(m.predict(X2_test), y2_test) # Get CV score kf6 = KFold(n_splits=6) temp_scores[j,2] = -cross_val_score(tree, X2_train, y2_train, cv=kf6, scoring='neg_mean_squared_error').mean() # Save best model if temp_scores[j,2]\u0026lt;best_score: best_model = m best_score = temp_scores[j,2] # Average clf_scores[i,:] = np.mean(temp_scores, axis=0) make_figure_8_6b() Other Issues Missing Predictor Values There are usually 2 main ways to deal with missing values:\ndiscard the observations fill the missing values with predictions using the other observations (e.g. mean) With trees we can do better:\ncode them as a separate class (e.g. \u0026lsquo;missing\u0026rsquo;) generate splits using non-missing data and use non-missing variables on missing data to mimic the splits with missing data Categorical Predictors When splitting a predictor having q possible unordered values, there are $2^{q−1} − 1$ possible partitions of the q values into two groups, and the computations become prohibitive for large $q$. However, with a $0 − 1$ outcome, this computation simplifies.\nLinear Combination Splits Rather than restricting splits to be of the form $X_j \\leq s$, one can allow splits along linear combinations of the form $a_j X_j \\leq s$. The weights $a_j$ become part of the optimization procedure.\nOther Tree-Building Procedures The procedure we have seen for building trees is called CART (Classification and Regression Tree). There are other procedures.\nThe Loss Matrix With respect to other methods, the choice of the loss functions plays a much more important role.\nBinary Splits You can do non-binary splits but in the end they are just weaker versions of binary splits.\nInstability Trees have very high variance.\nDifficulty in Capturing Additive Structure Trees are quite bad at modeling additive structures.\nLack of Smoothness Trees are not smooth.\nTrees vs Regression Advantages\nTrees are very easy to explain to people. In fact, they are even easier to explain than linear regression! Some people believe that decision trees more closely mirror human decision-making than do the regression and classification approaches seen in previous chapters. Trees can be displayed graphically, and are easily interpreted even by a non-expert (especially if they are small). Trees can easily handle qualitative predictors without the need to create dummy variables. Disadvantages\ntrees generally do not have the same level of predictive accuracy as some of the other regression and classification approaches seen in this book. trees can be very non-robust. In other words, a small change in the data can cause a large change in the final estimated tree. 7.2 Bagging, Random Forests, Boosting Bagging, random forests, and boosting use trees as building blocks to construct more powerful prediction models.\nBagging The main problem of decision trees is that they suffer from high variance. Bootstrap aggregation, or bagging, is a general-purpose procedure for reducing the variance of a statistical learning method.\nThe main idea behind bagging is that, given a set of n independent observations $Z_1,\u0026hellip;,Z_n$, each with variance $\\sigma^2$, the variance of the mean $\\bar Z$ of the observations is given by $\\sigma^2/n$. In other words, averaging a set of observations reduces variance.\nIndeed bagging consists in taking many training sets from the population, build a separate prediction model using each training set, and average the resulting predictions. Since we do not have access to many training sets, we resort to bootstrapping.\nOut-of-Bag Error Estimation It turns out that there is a very straightforward way to estimate the test error of a bagged model, without the need to perform cross-validation or the validation set approach. Recall that the key to bagging is that trees are repeatedly fit to bootstrapped subsets of the observations. One can show that on average, each bagged tree makes use of around two-thirds of the observations. The remaining one-third of the observations not used to fit a given bagged tree are referred to as the out-of-bag (OOB) observations. We can predict the response for the ith observation using each of the trees in which that observation was OOB.\nWe are now going to compute the Gini index for the Heart dataset using different numbers of trees.\n# Init (takes a lot of time with J=30) params = range(2,50) bagging_scores = np.zeros((len(params),2)) J = 30; # Loop over parameters for i, k in enumerate(params): print(\u0026quot;Computing k=%1.0f\u0026quot; % k, end =\u0026quot;\u0026quot;) # Repeat J temp_scores = np.zeros((J,2)) for j in range(J): X2_train, X2_test, y2_train, y2_test = train_test_split(X2, y2, test_size=0.5, random_state=j) bagging = BaggingClassifier(DecisionTreeClassifier(), max_samples=k, oob_score=True) bagging.fit(X2_train,y2_train) temp_scores[j,0] = bagging.score(X2_test, y2_test) temp_scores[j,1] = bagging.oob_score_ # Average bagging_scores[i,:] = np.mean(temp_scores, axis=0) print(\u0026quot;\u0026quot;, end=\u0026quot;\\r\u0026quot;) Computing k=49 Let\u0026rsquo;s plot the Out-of-Bag error computed while generating the bagged estimator.\n# Make new figure 1 def make_new_figure_1(): # Init fig, ax = plt.subplots(1,1,figsize=(10,6)) fig.suptitle(\u0026quot;Estimated $R^2$\u0026quot;) # Plot scores ax.plot(params, bagging_scores); ax.legend(['Test','OOB']); ax.set_xlabel('Number of Trees'); ax.set_ylabel('R^2'); make_new_figure_1() It can be shown that with B sufficiently large, OOB error is virtually equivalent to leave-one-out cross-validation error. The OOB approach for estimating the test error is particularly convenient when performing bagging on large data sets for which cross-validation would be computationally onerous.\nVariable Importance Measures As we have discussed, the main advantage of bagging is to reduce prediction variance. However, with bagging it can be difficult to interpret the resulting model. In fact we cannot draw trees anymore given we have too many of them.\nHowever, one can obtain an overall summary of the importance of each predictor using the RSS (for bagging regression trees) or the Gini index (for bagging classification trees). In the case of bagging regression trees, we can record the total amount that the RSS is decreased due to splits over a given predictor, averaged over all trees. A large value indicates an important predictor. Similarly, in the context of bagging classification trees, we can add up the total amount that the Gini index is decreased by splits over a given predictor, averaged over all trees.\n# Compute feature importance feature_importances = np.mean([tree.feature_importances_ for tree in bagging.estimators_], axis=0) We can have a look at the importance of each feature.\n# Figure 8.9 def make_figure_8_9(): # Init fig, ax = plt.subplots(1,1,figsize=(8,8)) ax.set_title('Figure 8.9: Feature Importance'); # Plot feature importance h1 = pd.DataFrame({'Importance':feature_importances*100}, index=features) h1 = h1.sort_values(by='Importance', axis=0, ascending=False) h1.plot(kind='barh', color='r', ax=ax) ax.set_xlabel('Variable Importance'); plt.yticks(fontsize=14); plt.gca().legend_ = None; make_figure_8_9() Random Forests Random forests provide an improvement over bagged trees by way of a small tweak that decorrelates the trees. As in bagging, we build a number of decision trees on bootstrapped training samples. But when building these decision trees, each time a split in a tree is considered, a random sample of $m$ predictors is chosen as split candidates from the full set of $p$ predictors. The split is allowed to use only one of those m predictors. A fresh sample of $m$ predictors is taken at each split, and typically we choose $m \\sim \\sqrt{p}$ — that is, the number of predictors considered at each split is approximately equal to the square root of the total number of predictors\nIn other words, in building a random forest, at each split in the tree, the algorithm is not even allowed to consider a majority of the available predictors. This may sound crazy, but it has a clever rationale. Suppose that there is one very strong predictor in the data set, along with a number of other moderately strong predictors. Then in the collection of bagged trees, most or all of the trees will use this strong predictor in the top split. Consequently, all of the bagged trees will look quite similar to each other. Hence the predictions from the bagged trees will be highly correlated. Unfortunately, averaging many highly correlated quantities does not lead to as large of a reduction in variance as averaging many uncorrelated quantities. In particular, this means that bagging will not lead to a substantial reduction in variance over a single tree in this setting.\nRandom forests overcome this problem by forcing each split to consider only a subset of the predictors.\nLet\u0026rsquo;s split the data in 2 and compute test and estimated $R^2$, for both forest and trees.\nimport warnings warnings.simplefilter('ignore') # Init (takes a lot of time with J=30) params = range(2,50) forest_scores = np.zeros((len(params),2)) J = 30 # Loop over parameters for i, k in enumerate(params): print(\u0026quot;Computing k=%1.0f\u0026quot; % k, end =\u0026quot;\u0026quot;) # Repeat J temp_scores = np.zeros((J,2)) for j in range(J): X2_train, X2_test, y2_train, y2_test = train_test_split(X2, y2, test_size=0.5, random_state=j) forest = RandomForestClassifier(n_estimators=k, oob_score=True, max_features=\u0026quot;sqrt\u0026quot;) forest.fit(X2_train,y2_train) temp_scores[j,0] = forest.score(X2_test, y2_test) temp_scores[j,1] = forest.oob_score_ # Average forest_scores[i,:] = np.mean(temp_scores, axis=0) print(\u0026quot;\u0026quot;, end=\u0026quot;\\r\u0026quot;) Computing k=49 # Figure 8.8 def make_figure_8_8(): # Init fig, ax = plt.subplots(1,1,figsize=(10,6)) ax.set_title('Figure 8.8'); # Plot scores ax.plot(params, bagging_scores); ax.plot(params, forest_scores); ax.legend(['Test - Bagging','OOB - Bagging', 'Test - Forest','OOB - Forest']); ax.set_xlabel('Number of Trees'); ax.set_ylabel('R^2'); make_figure_8_8() As for bagging, we can plot feature importance.\n# Make new figure 2 def make_new_figure_2(): # Init fig, (ax1,ax2) = plt.subplots(1,2,figsize=(15,7)) # Plot feature importance - Bagging h1 = pd.DataFrame({'Importance':feature_importances*100}, index=features) h1 = h1.sort_values(by='Importance', axis=0, ascending=False) h1.plot(kind='barh', color='r', ax=ax1) ax1.set_xlabel('Variable Importance'); ax1.set_title('Tree Bagging') # Plot feature importance h2 = pd.DataFrame({'Importance':forest.feature_importances_*100}, index=features) h2 = h2.sort_values(by='Importance', axis=0, ascending=False) h2.plot(kind='barh', color='r', ax=ax2) ax2.set_title('Random Forest') # All plots for ax in fig.axes: ax.set_xlabel('Variable Importance'); ax.legend([]) make_new_figure_2() From the figure we observe that varaible importance ranking is similar with bagging and random forests, but there are significant differences.\nWe are now going to look at the importance of random forests using the Khan gene dataset. This dataset has the peculiarity of having a large number of features and very few observations.\n# Load data gene = pd.read_csv('data/Khan.csv') print(len(gene)) gene.head() 83 x V1 V2 V3 V4 V5 V6 V7 V8 V9 ... V2299 V2300 V2301 V2302 V2303 V2304 V2305 V2306 V2307 V2308 0 2 0.773344 -2.438405 -0.482562 -2.721135 -1.217058 0.827809 1.342604 0.057042 0.133569 ... -0.238511 -0.027474 -1.660205 0.588231 -0.463624 -3.952845 -5.496768 -1.414282 -0.647600 -1.763172 1 2 -0.078178 -2.415754 0.412772 -2.825146 -0.626236 0.054488 1.429498 -0.120249 0.456792 ... -0.657394 -0.246284 -0.836325 -0.571284 0.034788 -2.478130 -3.661264 -1.093923 -1.209320 -0.824395 2 2 -0.084469 -1.649739 -0.241308 -2.875286 -0.889405 -0.027474 1.159300 0.015676 0.191942 ... -0.696352 0.024985 -1.059872 -0.403767 -0.678653 -2.939352 -2.736450 -1.965399 -0.805868 -1.139434 3 2 0.965614 -2.380547 0.625297 -1.741256 -0.845366 0.949687 1.093801 0.819736 -0.284620 ... 0.259746 0.357115 -1.893128 0.255107 0.163309 -1.021929 -2.077843 -1.127629 0.331531 -2.179483 4 2 0.075664 -1.728785 0.852626 0.272695 -1.841370 0.327936 1.251219 0.771450 0.030917 ... -0.200404 0.061753 -2.273998 -0.039365 0.368801 -2.566551 -1.675044 -1.082050 -0.965218 -1.836966 5 rows × 2309 columns\nThe dataset has 83 rows and 2309 columns.\nSince it\u0026rsquo;s a very wide dataset, selecting the right features is crucial.\nAlso note that we cannot run linear regression on this dataset.\n# Reduce dataset size gene_small = gene.iloc[:,0:202] X = gene_small.iloc[:,1:] y = gene_small.iloc[:,0] Let\u0026rsquo;s now cross-validate over number of trees and maximum number of features considered.\n# Init (takes a lot of time with J=30) params = range(50,150,10) m_scores = np.zeros((len(params),3)) p = np.shape(X)[1] J = 30; # Loop over parameters for i, k in enumerate(params): # Array of features ms = [round(p/2), round(np.sqrt(p)), round(np.log(p))] # Repeat L times temp_scores = np.zeros((J,3)) for j in range(J): print(\u0026quot;Computing k=%1.0f (iter=%1.0f)\u0026quot; % (k,j+1), end =\u0026quot;\u0026quot;) # Loop over values of m for index, m in enumerate(ms): forest = RandomForestClassifier(n_estimators=k, max_features=m, oob_score=True) forest.fit(X, y) temp_scores[j,index] = forest.oob_score_ print(\u0026quot;\u0026quot;, end=\u0026quot;\\r\u0026quot;) # Average m_scores[i,:] = np.mean(temp_scores, axis=0) Computing k=140 (iter=30) # Figure 8.10 def make_figure_8_10(): # Init fig, ax = plt.subplots(1,1,figsize=(10,6)) ax.set_title('Figure 8.10'); # Plot scores ax.plot(params, m_scores); ax.legend(['m=p/2','m=sqrt(p)','m=log(p)']); ax.set_xlabel('Number of Trees'); ax.set_ylabel('Test Classification Accuracy'); make_figure_8_10() It seems that the best scores are achieved with few features and many trees.\nBoosting Like bagging, boosting is a general approach that can be applied to many statistical learning methods for regression or classification. Here we restrict our discussion of boosting to the context of decision trees.\nBoosting works similarly to bagging, except that the trees are grown sequentially: each tree is grown using information from previously grown trees. Boosting does not involve bootstrap sampling; instead each tree is fit on a modified version of the original data set.\nWhat is the idea behind this procedure? Given the current model, we fit a decision tree to the residuals from the model. That is, we fit a tree using the current residuals, rather than the outcome $y$, as the response. We then add this new decision tree into the fitted function in order to update the residuals. Each of these trees can be rather small, with just a few terminal nodes, determined by the parameter $d$ in the algorithm. By fitting small trees to the residuals, we slowly improve $\\hat f$ in areas where it does not perform well. The shrinkage parameter λ slows the process down even further, allowing more and different shaped trees to attack the resid- uals. In general, statistical learning approaches that learn slowly tend to perform well.\nAlgorithm The boosting algorithm works as follows:\nSet $\\hat f(x)=0$ and $r_i=y_i$ for all $i$ in the training set.\nFor $b=1,2,\u0026hellip;,B$ repeat:\na. Fit a tree $\\hat f^b $ with $d$ splits ($d+1$ terminal nodes) to the training data $(X,r)$.\nb. Update $\\hat f$ by adding in a shrunken version of the new tree: $$ \\hat f(x) \\leftarrow \\hat f(x) + \\lambda \\hat f^b(x) $$\nc. Update the residuals $$ r_i = r_i - \\lambda \\hat f^b(x_i) $$\nOutput the boosted model $$ \\hat{f}(x)=\\sum_{b=1}^{B} \\lambda \\hat{f}^{b}(x) $$\nBoosting has three tuning parameters:\nThe number of trees $B$ The shrinkage parameter $\\lambda$. This controls the rate at which boosting learns. The number of splits in each tree $d$ , which controls the complexity of the boosted ensemble. Often d = 1 works well, in which case each tree is a stump, consisting of a single split. # Init , oob_score=True params = range(50,150,10) boost_scores = np.zeros((len(params),3)) p = np.shape(X)[1] J = 30 # Loop over parameters for i, k in enumerate(params): # Repeat L times temp_scores = np.zeros((J,3)) for j in range(J): print(\u0026quot;Computing k=%1.0f (iter=%1.0f)\u0026quot; % (k,j+1), end =\u0026quot;\u0026quot;) X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=20, random_state=j) # First score: random forest forest = RandomForestClassifier(n_estimators=k, max_features=\u0026quot;sqrt\u0026quot;) forest.fit(X_train, y_train) temp_scores[j,0] = forest.score(X_test, y_test) # Second score: boosting with 1-split trees boost1 = GradientBoostingClassifier(learning_rate=0.01, max_depth=1, n_estimators=k, max_features=\u0026quot;sqrt\u0026quot;) boost1.fit(X_train, y_train) temp_scores[j,1] = boost1.score(X_test, y_test) # Third score: boosting with 1-split trees boost2 = GradientBoostingClassifier(learning_rate=0.01, max_depth=2, n_estimators=k, max_features=\u0026quot;sqrt\u0026quot;) boost2.fit(X_train, y_train) temp_scores[j,2] = boost2.score(X_test, y_test) print(\u0026quot;\u0026quot;, end=\u0026quot;\\r\u0026quot;) # Average boost_scores[i,:] = np.mean(temp_scores, axis=0) Computing k=140 (iter=30) Let\u0026rsquo;s compare boosting and forest.\n# Figure 8.11 def make_figure_8_11(): # Init fig, ax = plt.subplots(1,1,figsize=(10,6)) ax.set_title('Figure 8.11'); # Plot scores ax.plot(params, m_scores); ax.legend(['forest','boosting with d=1','boosting with d=2']); ax.set_xlabel('Number of Trees'); ax.set_ylabel('Test Classification Accuracy'); make_figure_8_11() ","date":1646784000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1646784000,"objectID":"54aa48269b53b49cab7798a7b9e67a1b","permalink":"https://matteocourthoud.github.io/course/ml-econ/07_trees/","publishdate":"2022-03-09T00:00:00Z","relpermalink":"/course/ml-econ/07_trees/","section":"course","summary":"%matplotlib inline from utils.lecture07 import * Decision Trees Decision trees involve segmenting the predictor space into a number of simple regions. In order to make a prediction for a given observation, we typically use the mean or the mode of the training observations in the region to which it belongs.","tags":null,"title":"Tree-based Methods","type":"book"},{"authors":null,"categories":null,"content":"Instrumental Variables Endogeneity We say that there is endogeneity in the linear regression model if $\\mathbb E[x_i \\varepsilon_i] \\neq 0$.\nThe random vector $z_i$ is an instrumental variable in the linear regression model if the following conditions are met.\nExclusion restriction: the instruments are uncorrelated with the regression error $$ \\mathbb E_n[z_i \\varepsilon_i] = 0 $$ almost surely, i.e. with probability $p \\to 1$. Rank condition: no linearly redundant instruments $$ \\mathbb E_n[z_i z_i\u0026rsquo;] \\neq 0 $$ almost surely, i.e. with probability $p \\to 1$. Relevance condition (need $L \u0026gt; K$): $$ rank \\ (\\mathbb E_n[z_i x_i\u0026rsquo;]) = K $$ almost surely, i.e. with probability $p \\to 1$. IV and 2SLS Let $K = dim(x_i)$ and $L = dim(z_i)$. We say that the model is just-identified if $L = K$ (method: IV) and over-identified if $L \u0026gt; K$ (method: 2SLS).\nAssume $z_i$ satisfies the instrumental variable assumptions above and $dim(z_i) = dim(x_i)$, then the instrumental variables (IV) estimator $\\hat{\\beta} _ {IV}$ is given by $$ \\begin{aligned} \\hat{\\beta} _ {IV} \u0026amp;= \\mathbb E_n[z_i x_i\u0026rsquo;]^{-1} \\mathbb E_n[z_i y_i] = \\newline \u0026amp;= \\left( \\frac{1}{n} \\sum _ {i=1}^n z_i x_i\\right)^{-1} \\left( \\frac{1}{n} \\sum _ {i=1}^n z_i y_i\\right) = \\newline \u0026amp;= (Z\u0026rsquo;X)^{-1} (Z\u0026rsquo;y) \\end{aligned} $$\nAssume $z_i$ satisfies the instrumental variable assumptions above and $dim(z_i) \u0026gt; dim(x_i)$, then the two-stage-least squares (2SLS) estimator $\\hat{\\beta} _ {2SLS}$ is given by $$ \\hat{\\beta} _ {2SLS} = \\Big( X\u0026rsquo;Z (Z\u0026rsquo;Z)^{-1} Z\u0026rsquo;X \\Big)^{-1} \\Big( X\u0026rsquo;Z (Z\u0026rsquo;Z)^{-1} Z\u0026rsquo;y \\Big) $$ Where $\\hat{x}_i$ is the predicted $x_i$ from the first stage regression of $x_i$ on $z_i$. This is equivalent to the IV estimator using $\\hat{x}_i$ as an instrument for $x_i$.\n2SLS Algebra The estimator is called two-stage-least squares since it can be rewritten as an IV estimator that uses $\\hat{X}$ as instrument: $$ \\begin{aligned} \\hat{\\beta} _ {\\text{2SLS}} \u0026amp;= \\Big( X\u0026rsquo;Z (Z\u0026rsquo;Z)^{-1} Z\u0026rsquo;X \\Big)^{-1} \\Big( X\u0026rsquo;Z (Z\u0026rsquo;Z)^{-1} Z\u0026rsquo;y \\Big) = \\newline \u0026amp;= (\\hat{X}\u0026rsquo; X)^{-1} \\hat{X}\u0026rsquo; y = \\newline \u0026amp;= \\mathbb E_n[\\hat{x}_i x_i\u0026rsquo;]^{-1} \\mathbb E_n[\\hat{x}_i y_i] \\end{aligned} $$\nMoreover it can be rewritten as $$ \\begin{aligned} \\hat{\\beta} _ {\\text{2SLS}} \u0026amp;= (\\hat{X}\u0026rsquo; X)^{-1} \\hat{X}\u0026rsquo; y = \\newline \u0026amp;= (X\u0026rsquo; P_Z X)^{-1} X\u0026rsquo; P_Z y = \\newline \u0026amp;= (X\u0026rsquo; P_Z P_Z X)^{-1} X\u0026rsquo; P_Z y = \\newline \u0026amp;= (\\hat{X}\u0026rsquo; \\hat{X})^{-1} \\hat{X}\u0026rsquo; y = \\newline \u0026amp;= \\mathbb E_n [\\hat{x}_i \\hat{x}_i]^{-1} \\mathbb E_n[\\hat{x}_i y_i] \\end{aligned} $$\nRule of Thumb How to the test the relevance condition? Rule of thumb: $F$-test in the first stage $\u0026gt;10$ (joint test on $z_i$).\nProblem: as $n \\to \\infty$, with finite $L$, $F \\to \\infty$ (bad rule of thumb).\nEquivalence Theorem\nIf $K=L$, $\\hat{\\beta} _ {\\text{2SLS}} = \\hat{\\beta} _ {\\text{IV}}$.\nProof\nIf $K=L$, $X\u0026rsquo;Z$ and $Z\u0026rsquo;X$ are squared matrices and, by the relevance condition, non-singular (invertible). $$ \\begin{aligned} \\hat{\\beta} _ {\\text{2SLS}} \u0026amp;= \\Big( X\u0026rsquo;Z (Z\u0026rsquo;Z)^{-1} Z\u0026rsquo;X \\Big)^{-1} \\Big( X\u0026rsquo;Z (Z\u0026rsquo;Z)^{-1} Z\u0026rsquo;y \\Big) = \\newline \u0026amp;= (Z\u0026rsquo;X)^{-1} (Z\u0026rsquo;Z) (X\u0026rsquo;Z)^{-1} X\u0026rsquo;Z (Z\u0026rsquo;Z)^{-1} Z\u0026rsquo;y = \\newline \u0026amp;= (Z\u0026rsquo;X)^{-1} (Z\u0026rsquo;Z) (Z\u0026rsquo;Z)^{-1} Z\u0026rsquo;y = \\newline \u0026amp;= (Z\u0026rsquo;X)^{-1} (Z\u0026rsquo;y) = \\newline \u0026amp;= \\hat{\\beta} _ {\\text{IV}} \\end{aligned} $$ $$\\tag*{$\\blacksquare$}$$\nDemand Example Example from Hayiashi (2000) page 187: demand and supply simultaneous equations. $$ \\begin{aligned} \u0026amp; q_i^D(p_i) = \\alpha_0 + \\alpha_1 p_i + u_i \\newline \u0026amp; q_i^S(p_i) = \\beta_0 + \\beta_1 p_i + v_i \\end{aligned} $$\nWe have an endogeneity problem. To see why, we solve the system of equations for $(p_i, q_i)$: $$ \\begin{aligned} \u0026amp; p_i = \\frac{\\beta_0 - \\alpha_0}{\\alpha_1 - \\beta_1} + \\frac{v_i - u_i}{\\alpha_1 - \\beta_1 } \\newline \u0026amp; q_i = \\frac{\\alpha_1\\beta_0 - \\alpha_0 \\beta_1}{\\alpha_1 - \\beta_1} + \\frac{\\alpha_1 v_i - \\beta_1 u_i}{\\alpha_1 - \\beta_1 } \\end{aligned} $$\nDemand Example (2) Then the price variable is not independent from the error term in neither equation: $$ \\begin{aligned} \u0026amp; Cov(p_i, u_i) = - \\frac{Var(u_i)}{\\alpha_1 - \\beta_1 } \\newline \u0026amp; Cov(p_i, v_i) = \\frac{Var(v_i)}{\\alpha_1 - \\beta_1 } \\end{aligned} $$\nAs a consequence, the OLS estimators are not consistent: $$ \\begin{aligned} \u0026amp; \\hat{\\alpha} _ {1, OLS} \\overset{p}{\\to} \\alpha_1 + \\frac{Cov(p_i, u_i)}{Var(p_i)} \\newline \u0026amp; \\hat{\\beta} _ {1, OLS} \\overset{p}{\\to} \\beta_1 + \\frac{Cov(p_i, v_i)}{Var(p_i)} \\end{aligned} $$\nDemand Example (3) In general, running regressing $q$ on $p$ you estimate $$ \\begin{aligned} \\hat{\\gamma} _ {OLS} \u0026amp;\\overset{p}{\\to} \\frac{Cov(p_i, q_i)}{Var(p_i)} = \\newline \u0026amp;= \\frac{\\alpha_1 Var(v_i) + \\beta_1 Var(u_i)}{(\\alpha_1 - \\beta_1)^2} \\left( \\frac{Var(v_i) + Var(u_i)}{(\\alpha_1 - \\beta_1)^2} \\right)^{-1} = \\newline \u0026amp;= \\frac{\\alpha_1 Var(v_i) + \\beta_1 Var(u_i)}{Var(v_i) + Var(u_i)} \\end{aligned} $$ Which is neither $\\alpha_1$ nor $\\beta_1$ but a variance weighted average of the two.\nDemand Example (4) Suppose we have a supply shifter $z_i$ such that\n$\\mathbb E[z_i v_i] \\neq 0$ $\\mathbb E[z_i u_i] = 0$. We combine the second condition and $\\mathbb E[u_i] = 0$ to get a system of 2 equations in 2 unknowns: $\\alpha_0$ and $\\alpha_1$. $$ \\begin{aligned} \u0026amp; \\mathbb E[z_i u_i] = \\mathbb E[ z_i (q_i^D(p_i) - \\alpha_0 - \\alpha_1 p_i) ] = 0 \\newline \u0026amp; \\mathbb E[u_i] = \\mathbb E[q_i^D(p_i) - \\alpha_0 - \\alpha_1 p_i] = 0\n\\end{aligned} $$\nWe could try to solve for the vector $\\alpha$ that solves $$ \\begin{aligned} \u0026amp; \\mathbb E_n[z_i (q_i^D - x_i\\alpha)] = 0 \\newline \u0026amp; \\mathbb E_n[z_i q_i^D] - \\mathbb E_n[z_ix_i\\alpha] = 0 \\end{aligned} $$\nIf $\\mathbb E_n[z_ix_i]$ is invertible, we get $\\hat{\\alpha} = \\mathbb E_n[z_ix_i]^{-1} \\mathbb E_n[z_i q^D_i]$ which is indeed the IV estimator of $\\alpha$ using $z_i$ as an instrument for the endogenous variable $p_i$.\nCode - DGP This code draws 100 observations from the model $y = 2 x_1 - x_2 + \\varepsilon$ where $x_1, x_2 \\sim U[0,1]$ and $\\varepsilon \\sim N(0,1)$.\n# Set seed Random.seed!(123); # Set the number of observations n = 100; # Set the dimension of Z l = 3; # Draw instruments Z = rand(Uniform(0,1), n, l); # Correlation matrix for error terms S = [1 0.8; 0.8 1]; # Endogenous X γ = [2 0; 0 -1; -1 3]; ε = rand(Normal(0,1), n, 2) * cholesky(S).U; X = Z*γ .+ ε[:,1]; # Calculate y y = X*β .+ ε[:,2]; Code - IV # Estimate beta OLS β_OLS = (X'*X)\\(X'*y) ## 2-element Array{Float64,1}: ## 2.335699233358403 ## -0.8576266209987325 # IV: l=k=2 instruments Z_IV = Z[:,1:k]; β_IV = (Z_IV'*X)\\(Z_IV'*y) ## 2-element Array{Float64,1}: ## 1.6133344277861439 ## -0.6678537395714547 # Calculate standard errors ε_hat = y - X*β_IV; V_NHC_IV = var(ε_hat) * inv(Z_IV'*X)*Z_IV'*Z_IV*inv(Z_IV'*X); V_HC0_IV = inv(Z_IV'*X)*Z_IV' * (I(n) .* ε_hat.^2) * Z_IV*inv(Z_IV'*X); Code - 2SLS # 2SLS: l=3 instruments Pz = Z*inv(Z'*Z)*Z'; β_2SLS = (X'*Pz*X)\\(X'*Pz*y) ## 2-element Array{Float64,1}: ## 1.904553638377971 ## -0.8810907510370429 # Calculate standard errors ε_hat = y - X*β_2SLS; V_NCH_2SLS = var(ε_hat) * inv(X'*Pz*X); V_HC0_2SLS = inv(X'*Pz*X)*X'*Pz * (I(n) .* ε_hat.^2) *Pz*X*inv(X'*Pz*X); GMM Setting We have a system of $L$ moment conditions $$ \\begin{aligned} \u0026amp; \\mathbb E[g_1(\\omega_i, \\delta_0)] = 0 \\newline \u0026amp; \\vdots \\newline \u0026amp; \\mathbb E[g_L(\\omega_i, \\delta_0)] = 0 \\end{aligned} $$\nIf $L = \\dim (\\delta_0)$, no problem. If $L \u0026gt; \\dim (\\delta_0)$, there may be no solution to the system of equations.\nOptions There are two possibilities.\nFirst Solution: add moment conditions until the system is identified $$ \\mathbb E[ a\u0026rsquo; g(\\omega_i, \\delta_0)] = 0 $$ Solve $\\mathbb E[Ag(\\omega_i, \\delta)] = 0$ for $\\hat{\\delta}$. How to choose $A$? Such that it minimizes $Var(\\hat{\\delta})$. Second Solution: generalized method of moments (GMM) $$ \\begin{aligned} \\hat{\\delta} _ {GMM} \u0026amp;= \\arg \\min _ \\delta \\quad \\Big| \\Big| \\mathbb E_n [ g(\\omega_i, \\delta) ] \\Big| \\Big| = \\newline \u0026amp;= \\arg \\min _ \\delta \\quad n \\mathbb E_n[g(\\omega_i, \\delta)]\u0026rsquo; W \\mathbb E_n [g(\\omega_i, \\delta)] \\end{aligned} $$ The choice of $A$ and $W$ are closely related to each other.\n1-step GMM Since $J(\\delta,W)$ is a quadratic form, a closed form solution exists: $$ \\hat{\\delta}(W) = \\Big(\\mathbb E_n[z_i x_i\u0026rsquo;] W \\mathbb E_n[z_i x_i\u0026rsquo;] \\Big)^{-1}\\mathbb E_n[z_i x_i\u0026rsquo;] W \\mathbb E_n[z_i y_i] $$\nAssumptions for consistency of the GMM estimator given data $\\mathcal D = \\lbrace y_i, x_i, z_i \\rbrace _ {i=1}^n$:\nLinearity: $y_i = x_i\\gamma_0 + \\varepsilon_i$ IID: $(y_i, x_i, z_i)$ iid Orthogonality: $\\mathbb E [z_i(y_i - x_i\\gamma_0)] = \\mathbb E[z_i \\varepsilon_i] = 0$ Rank identification: $\\Sigma_{xz} = \\mathbb E[z_i x_i\u0026rsquo;]$ has full rank Convergence Theorem\nUnder linearity, independence, orthogonality and rank conditions, if $\\hat{W} \\overset{p}{\\to} W$ positive definite, then $$ \\hat{\\delta}(\\hat{W}) \\to \\delta(W) $$ If in addition to the above assumption, $\\sqrt{n} \\mathbb E_n [g(\\omega_i, \\delta_0)] \\overset{d}{\\to} N(0,S)$ for a fixed positive definite $S$, then $$ \\sqrt{n} (\\hat{\\delta} (\\hat{W}) - \\delta(W)) \\overset{d}{\\to} N(0,V) $$ where $V = (\\Sigma\u0026rsquo; _ {xz} W \\Sigma _ {xz})^{-1} \\Sigma _ {xz} W S W \\Sigma _ {xz}(\\Sigma\u0026rsquo; _ {xz} W \\Sigma _ {xz})^{-1}$.\nFinally, if a consistent estimator $\\hat{S}$ of $S$ is available, then using sample analogues $\\hat{\\Sigma}_{xz}$ it follows that $$ \\hat{V} \\overset{p}{\\to} V $$\nIf $W = S^{-1}$ then $V$ reduces to $V = (\\Sigma\u0026rsquo; _ {xz} W \\Sigma _ {xz})^{-1}$. Moreover, $(\\Sigma\u0026rsquo; _ {xz} W \\Sigma _ {xz})^{-1}$ is the smallest possible form of $V$, in a positive definite sense.\nTherefore, to have an efficient estimator, you want to construct $\\hat{W}$ such that $\\hat{W} \\overset{p}{\\to} S^{-1}$.\n2-step GMM Estimation steps:\nChoose an arbitrary weighting matrix $\\hat{W}_{init}$ (usually the identity matrix $I_K$) Estimate $\\hat{\\delta} _ {init}(\\hat{W} _ {init})$ Estimate $\\hat{S}$ (asymptotic variance of the moment condition) Estimate $\\hat{\\delta}(\\hat{S}^{-1})$ On the procedure:\nThis estimator achieves the semiparametric efficiency bound. This strategy works only if $\\hat{S} \\overset{p}{\\to} S$ exists. For iid cases: we can use $\\hat{\\delta} = \\mathbb E_n[(\\hat{\\varepsilon}_i z_i)(\\hat{\\varepsilon}_i z_i) \u0026rsquo; ]$ where $\\hat{\\varepsilon}_i = y_i - x_i \\hat{\\delta}(\\hat{W} _ {init})$. Code - 1-step GMM # GMM 1-step: inefficient weighting matrix W_1 = I(l); # Objective function gmm_1(b) = ( y - X*b )' * Z * W_1 * Z' * ( y - X*b ); # Estimate GMM β_gmm_1 = optimize(gmm_1, β_OLS).minimizer ## 2-element Array{Float64,1}: ## 1.91556882526808 ## -0.8769689391885799 # Standard errors GMM ε_hat = y - X*β_gmm_1; S_hat = Z' * (I(n) .* ε_hat.^2) * Z; d_hat = -X'*Z; V_gmm_1 = inv(d_hat * inv(S_hat) * d_hat') ## 2×2 Array{Float64,2}: ## 0.0158497 -0.00346601 ## -0.00346601 0.00616531 Code - 2-step GMM # GMM 2-step: efficient weighting matrix W_2 = inv(S_hat); # Objective function gmm_2(b) = ( y - X*b )' * Z * W_2 * Z' * ( y - X*b ); # Estimate GMM β_gmm_2 = optimize(gmm_2, β_OLS).minimizer ## 2-element Array{Float64,1}: ## 1.905326742963115 ## -0.881808949213345 # Standard errors GMM ε_hat = y - X*β_gmm_2; S_hat = Z' * (I(n) .* ε_hat.^2) * Z; d_hat = -X'*Z; V_gmm_2 = inv(d_hat * inv(S_hat) * d_hat') ## 2×2 Array{Float64,2}: ## 0.0162603 -0.00357632 ## -0.00357632 0.00631259 Testing Overidentifying Restrictions If the equations are exactly identified, then it is possible to choose $\\delta$ so that all the elements of the sample moments $\\mathbb E_n[g(\\omega_i; \\delta)]$ are zero and thus that the distance $$ J(\\delta, \\hat{W}) = n \\mathbb E_n[g(\\omega_i, \\delta)]\u0026rsquo; \\hat{W} \\mathbb E_n[g(\\omega_i, \\delta)] $$ is zero. (The $\\delta$ that does it is the IV estimator.)\nIf the equations are overidentified, i.e. $L$ (number of instruments) $\u0026gt; K$ (number of equations), then the distance cannot be zero exactly in general, but we would expect the minimized distance to be close to zero.\nNaive Test Suppose your model is overidentified ($L \u0026gt; K$) and you use the following naive testing procedure:\nEstimate $\\hat{\\delta}$ using a subset of dimension $K$ of instruments $\\lbrace z_1 , .. , z_K\\rbrace$ for $\\lbrace x_1 , \u0026hellip; , x_K\\rbrace$ Set $\\hat{\\varepsilon}_i = y_i - x_i \\hat{\\delta} _ {\\text{GMM}}$ Infer the size of the remaining $L-K$ moment conditions $\\mathbb E[z _{i, K+1} \\varepsilon_i], \u0026hellip;, \\mathbb E[z _{i, L} \\varepsilon_i]$ looking at their empirical counterparts $\\mathbb E_n[z _{i, K+1} \\hat{\\varepsilon}_i], \u0026hellip;, \\mathbb E_n[z _{i, L} \\hat{\\varepsilon}_i]$ Reject exogeneity if the empirical expectations are high. How high? Calculate p-values. Example If you have two invalid instruments and you use one to test the validity of the other, it might happen by chance that you don’t reject it.\nModel: $y_i = x_i + \\varepsilon_i$ and $x_i = \\frac{1}{2} z _{i1} - \\frac{1}{2} z _{i2} + u_i$\nHave $$ Cov (z _{i1}, z _{i2}, \\varepsilon_i, u_i) = \\begin{bmatrix} 1 \u0026amp; 0 \u0026amp; 0 \u0026amp; 0 \\newline 0 \u0026amp; 1 \u0026amp; 0 \u0026amp; 0 \\newline 0 \u0026amp; 0 \u0026amp; 1 \u0026amp; 0.5 \\newline 0 \u0026amp; 0 \u0026amp; 0.5 \u0026amp; 1 \\end{bmatrix} $$\nYou want to test whether the second instrument is valid (is not since $\\mathbb E[z_2 \\varepsilon] \\neq 0$). You use $z_1$ and estimate $\\hat{\\beta} \\to$ the estimator is consistent.\nYou obtain $\\mathbb E_n[z _{i2} \\hat{\\varepsilon}_i] \\simeq 0$ even if $z_2$ is invalid\nProblem: you are using an invalid instrument in the first place.\nHansen’s Test Theorem: We are interested in testing $H_0: \\mathbb E[z_i \\varepsilon_i] = 0$ against $H_1: \\mathbb E[z_i \\varepsilon_i] \\neq 0$. Suppose $\\hat{S} \\overset{p}{\\to} S$. Then $$ J(\\hat{\\delta}(\\hat{S}^{-1}) , \\hat{S}^{-1}) \\overset{d}{\\to} \\chi^2 _ {L-K} $$ For $c$ satisfying $\\alpha = 1- G_{L - K} ( c )$, $\\Pr(J\u0026gt;c | H_0) \\to \\alpha$ so the test reject $H_0$ if $J \u0026gt; c$ has asymptotic size $\\alpha$.\nComments The degrees of freedom of the asymptotic distribution are the number of overidentifying restrictions. This is a specification test, testing whether all model assumptions are true jointly. Only when we are confident that about the other assumptions, can we interpret a large $J$ statistic as evidence for the endogeneity of some of the $L$ instruments included in $x$. Unlike the tests we have encountered so far, the test is not consistent against some failures of the orthogonality conditions (that is, it is not consistent against some fixed elements of the alternative). Several papers in the July 1996 issue of JBES report that the finite-sample null rejection probability of the test can far exceed the nominal significance level $\\alpha$. Special Case: Conditional Homoskedasticity The main implication of conditional homoskedasticity is that efficient GMM becomes 2SLS. With efficient GMM estimation, the weighting matrix is $\\hat{S}^{-1} = \\mathbb En [z_i z_i\u0026rsquo; \\varepsilon_i^2]^{-1}$. With conditional homoskedasticity, the efficient weighting matrix is $\\mathbb E_n[z_iz_i\u0026rsquo;]^{-1} \\sigma^{-2}$, or equivalently $\\mathbb E_n[z_iz_i\u0026rsquo;]^{-1}$. Then, the GMM estimator becomes $$ \\hat{\\delta}(\\hat{S}^{-1}) = \\Big(\\mathbb E_n[z_i x_i\u0026rsquo;]\u0026rsquo; \\underbrace{\\mathbb E_n[z_iz_i\u0026rsquo;]^{-1} \\mathbb E[z_i x_i\u0026rsquo;]} _ {\\text{ols of } x_i \\text{ on }z_i} \\Big)^{-1}\\mathbb E_n[z_i x_i\u0026rsquo;]\u0026rsquo; \\underbrace{\\mathbb E_n[z_iz_i\u0026rsquo;]^{-1} \\mathbb E[z_i y_i\u0026rsquo;]} _ {\\text{ols of } y_i \\text{ on }z_i}= \\hat{\\delta} _ {2SLS} $$\nProof: Consider the matrix notation. $$ \\begin{aligned} \\hat{\\delta} \\left( \\frac{Z\u0026rsquo;Z}{n}\\right) \u0026amp;= \\left( \\frac{X\u0026rsquo;Z}{n} \\left( \\frac{Z\u0026rsquo;Z}{n}\\right)^{-1} \\frac{Z\u0026rsquo;X}{n} \\right)^{-1} \\frac{X\u0026rsquo;Z}{n} \\left( \\frac{Z\u0026rsquo;Z}{n}\\right)^{-1} \\frac{Z\u0026rsquo;Y}{n} = \\newline \u0026amp;= \\left( X\u0026rsquo;Z(Z\u0026rsquo;Z)^{-1} Z\u0026rsquo;X \\right)^{-1} X\u0026rsquo;Z(Z\u0026rsquo;Z)^{-1} Z\u0026rsquo;Y = \\newline \u0026amp;= \\left(X\u0026rsquo;P_ZX\\right)^{-1} X\u0026rsquo;P_ZY = \\newline \u0026amp;= \\left(X\u0026rsquo;P_ZP_ZX\\right)^{-1} X\u0026rsquo;P_ZY = \\newline \u0026amp;= \\left(\\hat{X}\u0026rsquo;_Z \\hat{X}_Z\\right)^{-1} \\hat{X}\u0026rsquo;_ZY = \\newline \u0026amp;= \\hat{\\delta} _ {2SLS} \\end{aligned} $$ $$\\tag*{$\\blacksquare$}$$\nSmall-Sample Properties of 2SLS Theorem: When the number of instruments is equal to the sample size ($L = n$), then $\\hat{\\delta} _ {2SLS} = \\hat{\\delta} _ {OLS}$\nProof: We have a perfect prediction problem. The first stage estimated coefficient $\\hat{\\gamma}$ is such that it solves the normal equations: $\\hat{\\gamma} = z_i^{-1} x_i$. Then $$ \\begin{aligned} \\hat{\\delta} _ {2SLS} \u0026amp;= \\mathbb E_n[\\hat{x}_i x\u0026rsquo;_i]^{-1} \\mathbb E_n[\\hat{x}_i y_i] = \\newline \u0026amp;= \\mathbb E_n[z_i z_i^{-1} x_i x\u0026rsquo;_i]^{-1} \\mathbb E_n[z_i z_i^{-1} x_i y_i] = \\newline \u0026amp;= \\mathbb E_n[x_i x\u0026rsquo;_i]^{-1} \\mathbb E_n[x_i y_i] = \\newline \u0026amp;= \\hat{\\delta} _ {OLS} \\end{aligned} $$ $$\\tag*{$\\blacksquare$}$$\nYou have this overfitting problem in general when the number of instruments is large relative to the sample size. This problem arises even if the instruments are valid.\nExample from Angrist (1992) They regress wages on years of schooling. Problem: endogeneity: both variables are correlated with skills which are unobserved. Solution: instrument years of schooling with the quarter of birth. Idea: if born in the first three quarters, can attend school from the year of your sixth birthday. Otherwise, you have to wait one more year. Problem: quarters of birth are three dummies. In order to ``improve the first stage fit” they interact them with year of birth (180 effective instruments) and also with the state (1527 effective instruments). This mechanically increases the $R^2$ but also increases the bias of the 2SLS estimator. Solutions: LIML, JIVE, RJIVE (Hansen et al., 2014), Post-Lasso (Belloni et al., 2012). Example from Angrist (1992) Many Instrument Robust Estimation Issue Why having too many instruments is problematic? As the number of instruments increases, the estimated coefficient gets closer to OLS which is biased. As seen in the theorem above, for $L=n$, the two estimators coincide.\nLIML An alternative method to estimate the parameters of the structural equation is by maximum likelihood. Anderson and Rubin (1949) derived the maximum likelihood estimator for the joint distribution of $(y_i, x_i)$. The estimator is known as limited information maximum likelihood, or LIML.\nThis estimator is called “limited information” because it is based on the structural equation for $(y_i, x_i)$ combined with the reduced form equation for $x_i$. If maximum likelihood is derived based on a structural equation for $x_i$ as well, then this leads to what is known as full information maximum likelihood (FIML). The advantage of the LIML approach relative to FIML is that the former does not require a structural model for $x_i$, and thus allows the researcher to focus on the structural equation of interest - that for $y_i$.\nK-class Estimators The k-class estimators have the form $$ \\hat{\\delta}(\\alpha) = (X\u0026rsquo; P_Z X - \\alpha X\u0026rsquo; X)^{-1} (X\u0026rsquo; P_Z Y - \\alpha X\u0026rsquo; Y) $$\nThe limited information maximum likelihood estimator LIML is the k-class estimator $\\hat{\\delta}(\\alpha)$ where $$ \\alpha = \\lambda_{min} \\Big( ([X\u0026rsquo; , Y]^{-1} [X\u0026rsquo; , Y])^{-1} [X\u0026rsquo; , Y]^{-1} P_Z [X\u0026rsquo; , Y] \\Big) $$\nIf $\\alpha = 0$ then $\\hat{\\delta} _ {\\text{LIML}} = \\hat{\\delta} _ {\\text{2SLS}}$ while for $\\alpha \\to \\infty$, $\\hat{\\delta} _ {\\text{LIML}} \\to \\hat{\\delta} _ {\\text{OLS}}$.\nComments on LIML The particular choice of $\\alpha$ gives a many instruments robust estimate The LIML estimator has no finite sample moments. $\\mathbb E[\\delta(\\alpha_{LIML})]$ does not exist in general In simulation studies performs well Has good asymptotic properties Asymptotically the LIML estimator has the same distribution as 2SLS. However, they can have quite different behaviors in finite samples. There is considerable evidence that the LIML estimator has superior finite sample performance to 2SLS when there are many instruments or the reduced form is weak. However, on the other hand there is worry that since the LIML estimator is derived under normality it may not be robust in non-normal settings.\nJIVE The Jacknife IV procedure is the following\nRegress $\\lbrace x_j \\rbrace _ {j \\neq i}$ on $\\lbrace z_j \\rbrace _ {j \\neq i}$ and estimate $\\pi_{-i}$ (leave the $i^{th}$ observation out). Form $\\hat{x}_i = \\hat{\\pi} _ {-i} z_i$. Run IV using $\\hat{x}_i$ as instruments. $$ \\hat{\\delta} _ {JIVE} = \\mathbb E_n[\\hat{x}_i x_i\u0026rsquo;]^{-1} \\mathbb E_n[\\hat{x}_i y_i\u0026rsquo;] $$ Comments on JIVE: Prevents overfitting. With many instruments you get bad out of sample prediction which implies low correlation between $\\hat{x}_i$ and $x_i$: $\\mathbb E_n[\\hat{x}_i x_i\u0026rsquo;] \\simeq 0$. Use lasso/ridge regression in the first stage in case of too many instruments. Hausman Test Here we consider testing the validity of OLS. OLS is generally preferred to IV in terms of precision. Many researchers only doubt the (joint) validity of the regressor $z_i$ instead of being certain that it is invalid (in the sense of not being predetermined). So then they wish to choose between OLS and 2SLS, assuming that they have an instrument vector $x_i$ whose validity is not in question. Further, assume for simplicity that $L = K$ so that the efficient GMM estimator is the IV estimator.\nThe Hausman test statistic $$ H \\equiv n (\\hat{\\delta} _ {IV} - \\hat{\\delta} _ {OLS})\u0026rsquo; [\\hat{Avar} (\\hat{\\delta} _ {IV} - \\hat{\\delta} _ {OLS})]^{-1} (\\hat{\\delta} _ {IV} - \\hat{\\delta} _ {OLS}) $$ is asymptotically distributed as a $\\chi^2_{L-s}$ under the null where $s = | z_i \\cup x_i |$: the number of regressors that are retained as instruments in $x_i$.\nComments In general, the idea of the Hausman test is the following. If you have two estimators, one which is efficient under $H_0$ but inconsistent under $H_1$ (in this case, OLS), and another which is consistent under $H_1$ (in this case, IV), then construct a test as a quadratic form in the differences of the estimators. Another classic example arises in panel data with the hypothesis $H_0$ of unconditional strict exogeneity. In that case, under $H_0$ Random Effects estimators are efficient but under $H_1$ they are inconsistent. Fixed Effects estimators instead are consistent under $H_1$.\nThe Hausman test statistic can be used as a pretest procedure: select either OLS or IV according to the outcome of the test. Although widely used, this pretest procedure is not advisable. When the null is false, it is still possible that the test accepts the null (committing a Type 2 error). In particular, this can happen with a high probability when the sample size is small and/or when the regressor $z_i$ is almost valid. In such an instance, estimation and also inference will be based on incorrect methods. Therefore, the overall properties of the Hausman pretest procedure are undesirable.\nThe Hausman test is an example of a specification test. There are many other specification tests. One could for example test for conditional homoskedasticity. Unlike for the OLS case, there does not exist a convenient test for conditional homoskedasticity for the GMM case. A test statistic that is asymptotically chi-squared under the null is available but is extremely cumbersome; see White (1982, note 2). If in doubt, it is better to use the more generally valid inference methods that allow for conditional heteroskedasticity. Similarly, there does not exist a convenient test for serial correlation for the GMM case. If in doubt, it is better to use the more generally valid inference methods that allow for serial correlation; for example, when data are collected over time (that is, time-series data).\n","date":1635465600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1635465600,"objectID":"ce8d70a7023a1cf2ff6e68938b4ca4c9","permalink":"https://matteocourthoud.github.io/course/metrics/07_endogeneity/","publishdate":"2021-10-29T00:00:00Z","relpermalink":"/course/metrics/07_endogeneity/","section":"course","summary":"Instrumental Variables Endogeneity We say that there is endogeneity in the linear regression model if $\\mathbb E[x_i \\varepsilon_i] \\neq 0$.\nThe random vector $z_i$ is an instrumental variable in the linear regression model if the following conditions are met.","tags":null,"title":"Endogeneity","type":"book"},{"authors":null,"categories":null,"content":"Introduction Motivation IO: role of market structure on equilibrium outcomes.\nDynamics: study the endogenous evolution of market structure.\nSupply side dynamics Irreversible investment Entry sunk costs Product repositioning costs Price adjustment costs Learning by doing Demand side dynamics Switching costs Durable or storable products Bonus motivation: AI literature studies essentially the same set of problems with similar tools (Igami 2020)\nIrony: niche topic in IO (super niche in econ), but at the core of the frontier in computer science Why? Computation is hard, estimation harder, but extremely powerful prediction tool The world is intrinsecally dynamic Examples (1) Some examples in empirical IO\nInvestment Rust (1987): bus engine replacement decision Durable goods Gowrisankaran and Rysman (2012): consumer demand in the digital camcorder industry Stockpiling Erdem, Imai, and Keane (2003): promotions and stockpiling of ketchup Hendel and Nevo (2006): stockpiling of laundry detergents Learning Erdem and Keane (1996): brand learning in the laundry detergent industry Crawford and Shum (2005): demand learning of anti‐ulcer drug prescriptions Switching costs Handel (2013): inertia in demand for health insurance Examples (2) But also in other applied micro fields:\nLabor economics Should you go to college? (Keane and Wolpin 1997) Health economics Which health insurance to pick given there are switching costs? (Handel 2013)\nAddiction (Becker and Murphy 1988)\nPublic finance How should you set optimal taxes in a dynamic environment? (Golosov et al. 2006) Do we really need dynamics? In some cases, we can reduce a dynamic problem to a:\nStatic problem Reduced-form problem E.g., Investment decision\nDynamic problem, as gains are realized after costs\n“Static” solution: invest if $\\mathbb E (NPV ) \u0026gt; TC$\nAction today ($a_t=0$ or $1$) does not affect the amount of future payoffs (NPV)\nBut many cases where it’s hard to evaluate dynamic questions in a static/reduced-form setting.\nTypically, cases where decision today would affect payoffs tomorrow And you care about those payoffs ($\\neq$ myopia) “A dynamic model can do anything a static model can.”\nNew Empirical IO So-called New Empirical IO (summary in Bresnahan (1989))\nSome decisions today might affect payoffs tomorrow But the decision today depends on the state today And the state today might have been the result of a decision yesterday Etc… Need dynamics to study these questions Where does it all start? Pakes (1986) Berry (1992) Pros and Cons Advantages\nWe can adress intertemporal trade-offs\nFlow vs stock stocks and benefits We can examine transitions and not only steady states\nWe are able to address policy questions that cannot be addressed with reduced-form methods\nStandard advantage of structural estimation But in a context with relevant intertemporal trade-offs / decisions Disadvantages\nWe typically need more assumptions\nRobustness testing will therefore be important Identification in dynamic models is less transparent\nThus time should be spent articulating what variation in the data identifies our parameters of interest) It is often computationally intensive (i.e., slow / unfeasible)\nFrom Statics to Dynamics Typical steps\nSpecify the primitives of the model Static: single period agents’ payoff functions (utility or profit) Dynamic: static payoffs + evolution of state variables Can be exogenous … or endogenous: decision today has an effect on the state tomorrow Solve for optimal behavior Static: tipically agents maximize current utility or profit Dynamic: agents maximize present discounted value of future utilities or profits Search for parameter values that result in the “best match” between our model predictions and observed behavior 1st year Macro Recap Markov Decision Processes Formally, a discrete-time MDP consists of the following objects\nA discrete time index $t \\in \\lbrace 0,1,2,\u0026hellip;,T \\rbrace$, for $T \\leq \\infty$\nA state space $\\mathcal S$\nAn action space $\\mathcal A$\nand a family of constraint sets $\\lbrace \\mathcal a_t(s_t) \\subseteq \\mathcal A \\rbrace$ A family of transition probabilities $\\lbrace \\Pr_{t}(s_{t+1}|s_t,a_t) \\rbrace$\nA discount factor, $\\beta$\nA family of single-period reward functions $\\lbrace (u_t(s_t,a_t) \\rbrace$\nso that the utility functional $U$ has an additively separable decomposition $$ U(\\boldsymbol s, \\boldsymbol a) = \\sum_{t=0}^{T} \\beta^{t} u_{t}\\left(s_t, a_{t}\\right) $$ MDP (2) In words\nThe state space $\\mathcal S$ contains all the information needed to\ncompute static utilities $u_t (s_t, a_t)$ compute transition probabilities $\\lbrace \\Pr_{t} (s_{t+1}|s_t,a_t) \\rbrace$ The (conditional) action space $\\mathcal A (s_t)$ contains all the actions available in state $s_t$\nHow can it be different by state? E.g. entry/exit decision if you’re in/out of the market The transition probabilities $\\lbrace \\Pr_{t+1}(s_{t+1}|s_t,a_t) \\rbrace$ define the probabilities of future states $s_{t+1}$ conditional on\nPresent state $s_t$ Present decision $a_t$ The discount factor $\\beta$ together with the static reward functions $\\lbrace (u_t(s_t,a_t) \\rbrace$ determines the objective function $$ \\mathbb E_{\\boldsymbol s\u0026rsquo;} \\Bigg[ \\sum_{t=0}^{T} \\beta^{t} u_{t}\\left(s_t, a_{t}\\right) \\Bigg] $$\nNotation Brief parenthesis on notation\nI have seen states denoted as\n$s$ (for state) $x$ $\\omega$ others, depending on the specific context, e.g. $e$ for experience I will try to stick to $s$ all the time\nI have seen decisions denoted as\n$a$ (for action) $d$ (for decision) $x$ others, depending on the specific context, e.g. $i$ for investment I will try to stick to $a$ all the time\nMaximization Problem The objective is to pick the decision rule (or policy function) $P = \\boldsymbol a^* = \\lbrace a_1^*, \u0026hellip;, a_t ^ * \\rbrace$ that solves $$ \\max_{\\boldsymbol a} \\ \\mathbb E_{\\boldsymbol s\u0026rsquo;} \\Bigg[ \\sum_{t=0}^{T} \\beta^{t} u_{t} \\left(s_{t}, a_{t} \\right) \\Bigg] $$ Where the expectation is taken over transition probabilities generated by the decision rule $\\boldsymbol a$.\nStationarity In many applications, we assume stationarity\nThe transition probabilities and utility functions do not directly depend on $t$\ni.e., are the same for all $t$ $\\Pr_{{\\color{red}{t}}} (s_{t+1}|s_t,a_t) \\ \\to \\ \\Pr(s_{t+1}|s_t,a_t)$ $u_{{\\color{red}{t}}} (s_t,a_t) \\ \\to \\ u(s_t,a_t)$ Uncomfortable assumption?\nYou think there is some reason (variable) why today’s probabilities should be different from tomorrow’s?\nIf observable, include that variable in the state space If unobservable, integrate it out Stationarity (2) In the finite horizon case ($T \\leq \\infty$), stationarity does not help much\n$\\sum_{t=0}^{T} \\beta^{t} u(s_t, a_{t})$ still depends on $t$, conditional on $s_t$ Why? Difference between $t$ and $T$ matters in the sum In infinite-horizon problems, stationarity helps a lot\nNow the difference between $t$ and $T$ is always the same, i.e. $\\infty$\n$\\sum_{t=0}^{\\infty} \\beta^{t} u(s_t, a_{t})$ does not depend on $t$, conditional on $s_t$\nThe future looks the same whether the agent is in state $s_t$ at time $t$ or in state $s_{t+\\tau} = s_t$ at time $t + \\tau$\nValue Function Consider a stationary infinite-horizon problem\nThe only variable which affects the agent’s view about the future is the current value of the state, $s_t$\nWe can rewrite the agent’s problem as $$ V_0(s_0) = \\max_{\\boldsymbol a} \\ \\mathbb E_{\\boldsymbol s\u0026rsquo;} \\Bigg[ \\sum_{t=0}^{\\infty} \\beta^{t} u\\left(s_t, a_{t}\\right) \\Bigg] $$ where\n$a_t \\in \\mathcal A(s_t) \\ \\forall t$ The expectation is taken over future states $\\boldsymbol s'$ that evolve according to $\\lbrace \\Pr(s_{t+1}|s_t,a_t) \\rbrace$ $V(\\cdot)$ is called the value function How to solve? One could try to solve it by brute force i.e. try to solve for the structure of all of the optimal decisions, $\\boldsymbol a^*$ Indeed, for finite-horizon problems, that might be necessary For stationary infinite-horizon problems, the value and policy function should be time invariant $V_{\\color{red}{t}} (s_t) = V(s_t)$ $P_{\\color{red}{t}} (s_t) = P(s_t)$ What do we gain? Bellman Equation $$ \\begin{align} V(s_0) \u0026amp;= \\max_{\\boldsymbol a} \\ \\mathbb E_{\\boldsymbol s\u0026rsquo;} \\Bigg[ \\sum_{t=0}^{\\infty} \\beta^{t} u(s_t, a_{t}) \\Bigg] = \\newline \u0026amp;= \\max_{\\boldsymbol a} \\ \\mathbb E_{\\boldsymbol s\u0026rsquo;} \\Bigg[ {\\color{red}{u(s_{0}, a_{0})}} + \\sum_{{\\color{red}{t=1}}}^{\\infty} \\beta^{t} u(s_t, a_{t}) \\Bigg] = \\newline \u0026amp;= \\max_{\\boldsymbol a} \\ \\Bigg\\lbrace u(s_{0}, a_{0}) + {\\color{red}{\\mathbb E_{\\boldsymbol s\u0026rsquo;}}} \\Bigg[ \\sum_{t=1}^{\\infty} \\beta^{t} u(s_t, a_{t}) \\Bigg] \\Bigg\\rbrace = \\newline \u0026amp;= \\max_{\\boldsymbol a} \\ \\Bigg\\lbrace u(s_{0}, a_{0}) + {\\color{red}{\\beta}} \\ \\mathbb E_{\\boldsymbol s\u0026rsquo;} \\Bigg[ \\sum_{t=1}^{\\infty} \\beta^{{\\color{red}{t-1}}} u(s_t, a_{t}) \\Bigg] \\Bigg\\rbrace = \\newline \u0026amp;= \\max_{{\\color{red}{a_0}}} \\ \\Bigg\\lbrace u(s_{0}, a_{0}) + \\beta \\ {\\color{red}{\\max_{\\boldsymbol a}}}\\ \\mathbb E_{\\boldsymbol s\u0026rsquo;} \\Bigg[ \\sum_{t=1}^{\\infty} \\beta^{t-1} u(s_t, a_{t}) \\Bigg] \\Bigg\\rbrace = \\newline \u0026amp;= \\max_{a_0} \\ \\Bigg\\lbrace u(s_{0}, a_{0}) + \\beta \\ {\\color{red}{\\int V(s_1) \\Pr(s_1 | s_0, a_0)}} \\Bigg\\rbrace \\end{align} $$\nBellman Equation (2) We have now a recursive formulation of the value function: the Bellman Equation $$ {\\color{red}{V(s_0)}} = \\max_{a_0} \\ \\Bigg\\lbrace u(s_{0}, a_{0}) + \\beta \\ \\int {\\color{red}{V(s_1)}} \\Pr(s_1 | s_0, a_0) \\Bigg\\rbrace $$ Intuition\nThe Bellman Equation is a functional equation Has to be satisfied in every state Can be written as ${\\color{red}{V}} = T({\\color{red}{V}})$ We are actually looking for a fixed point of $T$ The decision rule that satisfies the Bellman Equation is called the policy function $$ a(s_0) = \\arg \\max_{a_0} \\ \\Bigg\\lbrace u(s_{0}, a_{0}) + \\beta \\ \\int V(s_1) \\Pr(s_1 | s_0, a_0) \\Bigg\\rbrace $$\nContractions Under regularity conditions\n$u(s, a)$ is jointly continuous and bounded in $(s, a)$ $\\mathcal A (s)$ is a continuous correspondence It is possible to show that $$ T(W)(s) = \\max_{a \\in \\mathcal A(s)} \\ \\Bigg\\lbrace u(s, a) + \\beta \\ \\int W(s\u0026rsquo;) \\Pr(s\u0026rsquo; | s, a) \\Bigg\\rbrace $$ is a contraction mapping of modulus $\\beta$.\nContraction Mapping Theorem: then $T$ has a unique fixed point! Solving for the Value Function How do we actually do it in practice?\nFor finite horizon MDPs: backward induction Start from the last period: static maximization problem Move backwards taking the future value as given For infinite horizon MDPs: different options value function iteration most common policy function iteration successive approximations Difference with 1st year Macro So what’s going to be new here?\nEstimation: retrieve model primitives from observed behavior And related: uncertainty Strategic interaction: multiple agents taking dynamic decisions Next lecture Rust (1987) Setting Rust (1987): An Empirical Model of Harold Zurcher\nHarold Zurcher (HZ) is the city bus superintendant in Madison, WI\nAs bus engines get older, the probability of malfunctions increases\nHZ decides when to replace old bus engines with new ones\nOptimal stopping / investment problem Tradeoff\nCost of a new engine (fixed, stock) Repair costs, because of engine failures (continuous, flow) Do we care about Harold Zurcher?\nObviously not (and neither did Rust), it’s a method paper But referee asked for an application Data Units of observation\nRust observes 162 buses over time Observables: for each bus, he sees\nmonthly mileage (RHS, state variable) and whether the engine was replaced (LHS, choice variable), in a given month Variation\non average, bus engines were replaced every 5 years with over 200,000 elapsed miles considerable variation in the time and mileage at which replacement occurs Idea Construct a (parametric) model which predicts the time and mileage at which engine replacement occurs Use the model predictions (conditional on parameter values) to estimate parameters that “fit” the data predicted replacements, given mileage VS observed replacements, given mileage Ideally use the estimates to learn something new e.g. the correct dynamic demand curve for bus engine replacement Static Alternative What would you do otherwise?\nYou observe replacement decisions … and replacement costs $\\to$ Regress replacement decision on replacement costs Problem\nReplacement benefits are a flow (lower maintenance costs) … while the cost is a stock Outcome\nWe expect the overestimate demand elasticity. Why? Overpredict substitutions at low costs and underpredict substitution at high cost Model Assumptions of the structural model\nState: $s_t \\in \\lbrace 0, \u0026hellip; , s_{max} \\rbrace$ engine accumulated mileage at time $t$ Note: “continuous” in the data but has to be discretized into bins Action: $a_t \\in \\lbrace 0, 1 \\rbrace$ replace engine at time $t$ State transitions: $\\Pr ( s_{t+1} | s_{0}, \u0026hellip; , s_t ; \\theta)= \\Pr (s_{t+1} | s_t ; \\theta )$ mileage $s_t$ evolves exogenously according to a 1st-order Markov process The transition function is the same for every bus. If HZ replaces in period $t$ ($a_t = 1$), then $s_t = 0$ Model (2) HZ static utility function (for a single bus) $$ u\\left(s_t, a_{t} ; \\theta\\right)= \\begin{cases}-c\\left(s_t ; \\theta\\right) \u0026amp; \\text { if } a_{t}=0 \\text { (not replace) } \\newline -R-c(0 ; \\theta) \u0026amp; \\text { if } a_{t}=1 \\text { (replace) }\\end{cases} $$ where\n$c(s_t ; \\theta)$: expected costs of operating a bus with mileage $s_t$ ​ including maintenance costs \u0026amp; social costs of breakdown We would expect $\\frac{\\partial c}{\\partial s}\u0026gt;0$ $R$ is the cost of replacement (i.e., a new engine) Note that replacement occurs immediately $u(s_t , a_t ; \\theta)$: expected current utility from operating a bus with mileage $s_t$ and making replacement decision $a_t$ Model (3) HZ objective function is to maximize the expected present discounted sum of future utilities $$ V(s_t ; \\theta) = \\max_{\\boldsymbol a} \\mathbb E_{s_{t+1}} \\left[\\sum_{\\tau=t}^{\\infty} \\beta^{\\tau-t} u\\left(s_{\\tau}, a_{\\tau} ; \\theta\\right) \\ \\Bigg| \\ s_t, \\boldsymbol a ; \\theta\\right] $$ where\nThe expectation $\\mathbb E$ is over future $x$, which evolve according to Markov process $\\max$ is over future choices $a_{t+1}, \u0026hellip; ,a_{\\infty}$, because HZ will observe future states $s_{\\tau}$ before choosing future actions $a_\\tau$, this is a functional Notes\nThis is for one bus (but multiple engines). HZ has an infinite horizon for his decision making $s_t$ summarizes state at time $t$, i.e., the expected value of future utilities only depends on $s_t$ Bellman Equation This (sequential) representation of HZ’s problem is very cumbersome to work with.\nWe can rewrite $V (s_t; \\theta)$ with the following Bellman equation $$ V\\left(s_t ; \\theta\\right) = \\max_{a_{t}} \\Bigg\\lbrace u\\left(s_t, a_{t} ; \\theta\\right)+\\beta \\mathbb E_{s_{t+1}} \\Big[V\\left(s_{t+1} ; \\theta\\right) \\Big| s_t, a_{t} ; \\theta\\Big] \\Bigg\\rbrace $$ Basically we are dividing the infinite sum (in the sequential form) into a present component and a future component.\nNotes:\nSame $V$ on both sides of equation because of infinite horizon - the future looks the same as the present for a given $s$ (i.e., it doesn’t matter where you are in time). The expectation $\\mathbb E$ is over the state-transition probabilities, $\\Pr (s_{t+1} | s_t, a_t ; \\theta)$ Order of Markow Process Suppose for a moment that $s_t$ follows a second-order markov process $$ s_{t+1}=f\\left(s_t, {\\color{red}{s_{t-1}}}, \\varepsilon ; \\theta\\right) $$ Now $s_t$ is not sufficient to describe current $V$\nWe need both $s_t$ and $s_{t-1}$ in the state space (i.e., $V (s_t , {\\color{red}{s_{t-1}}}; \\theta)$ contains $s_{t-1}$, too), and the expectation is over the transition probability $\\Pr (s_{t+1} | s_t, {\\color{red}{s_{t-1}}}, a_t ; \\theta)$ Parenthesis: State Variables Which variables should be state variables? I.e. should be included in the state space?\nGeneral rule for 1st order markow processes: variables need to\ndefine expected current payoff, and define expectations over next period state (i.e., distribution of $s_{t+1}$) What do you do otherwise? Integrate them out! Examples\nWeather affects static utitilies but not transition probabilities More annoying to replace the engine if it rains Integration means: “compute expected utility of Harold Zurcher before he opens the window” Month of the year affects transition probabilities but not utilities Buses are used more in the winter Integration means: “compute average transition probabilities over months” Note: you can always get the non-expected value function if you know the probability of raining or the transition probabilities by month\nPolicy Function Along with this value function comes a corresponding policy (or choice) function mapping the state $s_t$ into HZ’s optimal replacement choice $a_t$ $$ P \\left(s_t ; \\theta\\right) = \\max_{a_{t}} \\Bigg\\lbrace u\\left(s_t, a_{t} ; \\theta\\right) + \\beta \\mathbb E_{s_{t+1}} \\Big[ V \\left(s_{t+1} ; \\theta\\right) \\Big| s_t, a_{t} ; \\theta\\Big] \\Bigg\\rbrace $$ Given $\\frac{\\partial c}{\\partial s}\u0026gt;0$, the policy function has the form $$ P \\left(s_t ; \\theta\\right) = \\begin{cases}1 \u0026amp; \\text { if } s_t \\geq \\gamma(\\theta) \\newline 0 \u0026amp; \\text { if } s_t\u0026lt;\\gamma(\\theta)\\end{cases} $$ where $\\gamma$ is the replacement mileage.\nHow would this compare with the optimal replacement mileage if HZ was myopic?\nAnswer: HZ would wait until $R \\leq c(s)$ for the replacement action Solving the Model Why do we want to solve for the value and policy functions?\nWe want to know the agent’s optimal behavior and the equilibrium outcomes and be able to conduct comparative statics/dynamics (a.k.a. counterfactual simulations) We have the Bellman Equation $$ V\\left(s_t ; \\theta\\right) = \\max_{a_{t}} \\Bigg\\lbrace u\\left(s_t, a_{t} ; \\theta\\right)+\\beta \\mathbb E_{s_{t+1}} \\Big[V\\left(s_{t+1} ; \\theta\\right) \\ \\Big| \\ s_t, a_{t} ; \\theta\\Big] \\Bigg\\rbrace $$ Which we can compactly write as $$ V\\left(s_t ; \\theta\\right) = T \\Big( V\\left(s_{t+1} ; \\theta\\right) \\Big) $$ Blackwell’s Theorem: under regularity conditions, $T$ is a contraction mapping with modulus $\\beta$.\nContraction Mapping Theorem: $T$ has a fixed point and we can find it by iterating $T$ from any starting value $V^{(0)}$.\nValue Function Iteration What does Blackwell’s Theorem allow us to do?\nStart with any arbitrary function $V^{(0)}(\u0001\\cdot)$ Apply the mapping $T$ to get $V^{(1)}(\u0001\\cdot) = T (V^{(0)}(\u0001\\cdot))$ Apply again $V^{(2)}(\\cdot\u0001) = T (V^{(1)}(\\cdot\u0001))$ Continue applying $T$ , and $V^{(k)}$ will converge to the unique fixed point of $T$ i.e., the true value function $V(s_t; \\theta)$ Once we have $V(s_t; \\theta)$, it’s fairly trivial to compute the policy function $P(s_t; \\theta)$ Static optimization problem (given $V$) This process is called value function iteration\nHow to Reconcile Model and Data? Ideal Estimation Routine\nPick a parameter value $\\theta$ Solve value and policy function (inner loop) Match predicted choices with observed choices Find the parameter value $\\hat \\theta$ that best fits the data (outer loop) Makes the observed choices “closest” to the predicted choices (or maximizes the likelihood of the observed choices) Issue: model easily rejected by the data\nThe policy function takes the the form: replace iff $s_t \\geq \\gamma(\\theta)$\nCan’t explain the coexistence of e.g. “a bus without replacement at 22K miles” and “another bus being replaced at 17K miles” in the data\nWe need some unobservables in the model to explain why observed choices do not exactly match predicted choices\nRust (1987) - Estimation Uncertainty How can we explain different replacement actions at different mileages in the data?\nAdd other observables Add some stochastic element But where? Two options\nRandomness in decisions I.e. “Harold Zurcher sometimes would like to replace the bus engine but he forgets” Probably still falsifiable Also need “Harold Zurcher sometimes would like not to replace but replacement happens” 🤔🤔🤔 Randomness in the state Harold Zurcher knows something that we don’t He always makes the optimal decision but based on somethig we don’t observe Unobservables Rust uses the following utility specification: $$ u\\left(s_t, a_{t}, {\\color{red}{\\epsilon_{t}}} ; \\theta\\right) = u\\left(s_t, a_{t} ; \\theta\\right) + {\\color{red}{\\epsilon_{a_{t} t}}} = \\begin{cases} - c\\left(s_t ; \\theta\\right) + {\\color{red}{\\epsilon_{0 t}}} \u0026amp; \\text { if } \\ a_{t}=0 \\newline \\newline -R-c(0 ; \\theta) + {\\color{red}{\\epsilon_{1 t}}} \u0026amp; \\text { if } \\ a_{t}=1 \\end{cases} $$\nThe $\\epsilon_{it}$ are components of utility of alternative $a$ that are observed by HZ but not by us, the econometrician. E.g., the fact that an engine is running unusually smoothly given its mileage, or the fact that HZ is sick and doesn’t feel like replacing the engine this month Note: we have assumed addictive separability of $\\epsilon$ The $\\epsilon_a$s also affect HZ’s replacement decision $\\epsilon_{it}$ are both observed and relevant $\\to$ part of the state space Can we still solve the model? Can we estimate it?\nUnobservables (2) The Bellman Equation becomes $$ V \\Big( {\\color{red}{ \\lbrace s_\\tau \\rbrace_{\\tau=1}^t , \\lbrace \\epsilon_\\tau \\rbrace_{\\tau=1}^t }} ; \\theta \\Big) = \\max_{a_{t}} \\Bigg\\lbrace u\\left(s_t, a_{t} ; \\theta\\right) + {\\color{red}{\\epsilon_{it}}} + \\beta \\mathbb E_{s_{t+1}, {\\color{red}{\\epsilon_{t+1}}}} \\Big[V\\left(s_{t+1}, {\\color{red}{\\epsilon_{it+1}}} ; \\theta\\right) \\ \\Big| \\ {\\color{red}{ \\lbrace s_\\tau \\rbrace_{\\tau=1}^t , \\lbrace \\epsilon_\\tau \\rbrace_{\\tau=1}^t }}, a_{t} ; \\theta\\Big] \\Bigg\\rbrace $$ Issues\nThe problem is not Markow anymore Is $\\epsilon_t$ correlated with $\\epsilon_{t-\\tau}$? How? Is $\\epsilon_t$ correlated with $s_t$? And $s_{t-\\tau}$? How? Dimension of the state space has increased From $k = (k \\text{ points})^{1 \\text{ variable} \\times 1 \\text{ period}}$ points, to $\\infty = (k \\text{ points})^{3 \\text{ variables} \\times \\infty \\text{ periods}}$ 🤯🤯 Assuming all variables assume $k$ values Number of variables to integrate over to compute expectation $\\mathbb E$ has increased From one variable, $s$, to three, $(s, \\epsilon_{0}, \\epsilon_{1})$ Assumptions Rust makes 4 assumptions to make the problem tractable:\nFirst order Markow process of $\\epsilon$ Conditional independence of $\\epsilon_t | s_t$ from $\\epsilon_{t-1}$ and $s_{t-1}$ Independence of $\\epsilon_t$ from $s_t$ Logit distribution of $\\epsilon$ Assumption 1 A1: first-order markov process of $\\epsilon$ $$ \\Pr \\Big(s_{t+1}, \\epsilon_{t+1} \\Big| s_{1}, \u0026hellip;, s_t, \\epsilon_{1}, \u0026hellip;, \\epsilon_{t}, a_{t} ; \\theta\\Big) = \\Pr \\Big(s_{t+1}, \\epsilon_{t+1} \\Big| s_t, \\epsilon_{t}, a_{t} ; \\theta \\Big) $$\nWhat it buys\n$s$ and $\\epsilon$ prior to current period are irrelevant What it still allows:\nallows $s_t$ to be correlated with $\\epsilon_t$ What are we assuming away\nAny sort of longer run dependence Does it matter? If yes, just re-consider what is one time period Or make the state space larger (as usual in Markow processes) Assumption 1 - Implications The Bellman Equation becomes $$ V\\left(s_t, {\\color{red}{\\epsilon_{t}}} ; \\theta\\right) = \\max_{a_{t}} \\Bigg\\lbrace u\\left(s_t, a_{t} ; \\theta\\right) + {\\color{red}{\\epsilon_{a_{t} t}}} + \\beta \\mathbb E_{s_{t+1}, {\\color{red}{\\epsilon_{t+1}}}} \\Big[V(s_{t+1}, {\\color{red}{\\epsilon_{t+1}}} ; \\theta) \\ \\Big| \\ s_t, a_{t}, {\\color{red}{\\epsilon_{t}}} ; \\theta \\Big] \\Bigg\\rbrace $$\nNow the state is $(s_t, \\epsilon_t)$ sufficient, because defines both current utility and (the expectation of) next-period state, under the first-order Markov assumption $\\epsilon_t$ is now analogous to $s_t$ State space now is $k^3 = (k \\text{ points})^{3 \\text{ variables} \\times 1 \\text{ period}}$ From $\\infty = (k \\text{ points})^{3 \\text{ variables} \\times \\infty \\text{ periods}}$ Now we could use value function iteration to solve the problem If $\\epsilon_t$ is continuous, it has to be discretised Assumption 1 - Issues Open issues\nCurse of dimensionality in the state space: ($s_t, \\epsilon_{0t}, \\epsilon_{1t}$)\nBefore, there were $k$ points in state space (discrete values of $x$) Now there are $k^3$ : $k$ each for $s$, $\\epsilon_0$, $\\epsilon_1$ (Assuming we discretize all state variables into $k$ values) Generally, number of points in state space (and thus computational time) increases exponentially in the number of variables Curse of dimensionality in the expected value: $\\mathbb E_{s_{t+1}, \\epsilon_{0,t+1}, \\epsilon_{1,t+1}}$\nFor each point in state space (at each iteration of the contraction mapping), need to compute $$ \\mathbb E_{s_{t+1}, \\epsilon_{t+1}} \\Big[V (s_{t+1}, \\epsilon_{t+1} ; \\theta) \\ \\Big| \\ s_t, a_{t}, \\epsilon_{t} ; \\theta \\Big] $$\nBefore, this was a 1-dimensional integral (or sum), now it’s 3-dimensional Initial conditions\nAssumption 2 A2: conditional independence of $\\epsilon_t | s_t$ from $\\epsilon_{t-1}$ and $s_{t-1}$ $$ \\Pr \\Big(s_{t+1}, \\epsilon_{t+1} \\Big| s_t, \\epsilon_{t}, a_{t} ; \\theta \\Big) = \\Pr \\Big( \\epsilon_{t+1} \\Big| s_{t+1} ; \\theta \\Big) \\Pr \\Big( s_{t+1} \\Big| s_t, a_{t} ; \\theta \\Big) $$\nWhat it buys\n$s_{t+1}$ is independent of $\\epsilon_t$ $\\epsilon_{t+1}$ is independent of $\\epsilon_t$ and $s_t$, conditional on $s_{t+1}$ What it still allows:\n$\\epsilon$ can be correlated across time, but only through the $s$ process What are we assuming away\nAny time of persistent heterogeneity\nDoes it matter? Easily yes\nThere are tons of applications where the unobservables are either fixed or correlated over time\nIf fixed, there are methods to handle unobserved heterogeneity (i.e. bus “types”) Assumption 2 - Implications The Bellman Equation is $$ V\\left(s_t, {\\color{red}{\\epsilon_{t}}} ; \\theta\\right) = \\max_{a_{t}} \\Bigg\\lbrace u\\left(s_t, a_{t} ; \\theta\\right) + {\\color{red}{\\epsilon_{a_{t} t}}} + \\beta \\mathbb E_{s_{t+1}, {\\color{red}{\\epsilon_{t+1}}}} \\Big[V (s_{t+1}, {\\color{red}{\\epsilon_{t+1}}} ; \\theta) \\Big| s_t, a_{t} ; \\theta \\Big] \\Bigg\\rbrace $$\nNow $\\epsilon_{t}$ is noise that doesn’t affect the future That is, conditional on $s_{t+1}$, $\\epsilon_{t+1}$ is uncorrelated with $\\epsilon_{t}$ Remeber: if $\\epsilon$ does not affect the future, it should’t be in the state space!\nHow? Integrate it out.\nRust Shortcut: ASV Rust: define the alternative-specific value function $$ \\begin{align} \u0026amp;\\bar V_0 \\left(s_t ; \\theta\\right) = u\\left(s_t, 0 ; \\theta\\right) + \\beta \\mathbb E_{s_{t+1}, {\\color{red}{\\epsilon_{t+1}}}} \\Big[V\\left(s_{t+1}, {\\color{red}{\\epsilon_{t+1}} }; \\theta\\right) | s_t, a_{t}=0 ; \\theta\\Big] \\newline \u0026amp;\\bar V_1 \\left(s_t ; \\theta\\right) = u\\left(s_t, 1 ; \\theta\\right) + \\beta \\mathbb E_{s_{t+1}, {\\color{red}{\\epsilon_{t+1}}}} \\Big[V\\left(s_{t+1}, {\\color{red}{\\epsilon_{t+1}}} ; \\theta\\right) | s_t, a_{t}=1 ; \\theta\\Big] \\end{align} $$\n$\\bar V_0 (s_t)$ is the present discounted value of not replacing, net of $\\epsilon_{0t}$\nThe state does not depend on $\\epsilon_{t}$!\nWhat is the relationship with the value function? $$ V\\left(s_t, \\epsilon_{t} ; \\theta\\right) = \\max_{a_{t}} \\Bigg\\lbrace \\begin{array}{l} \\bar V_0 \\left(s_t ; \\theta\\right)+\\epsilon_{0 t} \\ ; \\newline \\bar V_1 \\left(s_t ; \\theta\\right)+\\epsilon_{1 t} \\end{array} \\Bigg\\rbrace $$\nWe have a 1-to-1 mapping between $V\\left(s_t, \\epsilon_{t} ; \\theta\\right)$ and $\\bar V_a \\left(s_t ; \\theta\\right)$ !\nIf we have one, we can get the other Rust Shortcut Can we solve for $\\bar V$?\nYes! They have a recursive formulation $$ \\begin{aligned} \u0026amp; \\bar V_0 \\left(s_t ; \\theta\\right) = u\\left(s_t, 0 ; \\theta\\right) + \\beta \\mathbb E_{s_{t+1}, {\\color{red}{\\epsilon_{t+1}}}} \\Bigg[ \\max_{a_{t+1}} \\Bigg\\lbrace \\begin{array}{l} \\bar V_0 \\left(s_{t+1} ; \\theta\\right) + {\\color{red}{\\epsilon_{0 t+1}}} \\ ; \\newline \\bar V_1 \\left(s_{t+1} ; \\theta\\right) + {\\color{red}{\\epsilon_{1 t+1}}} \\end{array} \\Bigg\\rbrace \\ \\Bigg| \\ s_t, a_{t}=0 ; \\theta \\Bigg] \\newline \u0026amp; \\bar V_1 \\left(s_t ; \\theta\\right) = u\\left(s_t, 1 ; \\theta\\right) + \\beta \\mathbb E_{s_{t+1}, {\\color{red}{\\epsilon_{t+1}}}} \\Bigg[ \\max_{a_{t+1}} \\Bigg\\lbrace \\begin{array}{l} \\bar V_0 \\left(s_{t+1} ; \\theta\\right) + {\\color{red}{\\epsilon_{0 t+1}}} \\ ; \\newline \\bar V_1 \\left(s_{t+1} ; \\theta\\right) + {\\color{red}{\\epsilon_{1 t+1}}} \\end{array} \\Bigg\\rbrace \\ \\Bigg| \\ s_t, a_{t}=1 ; \\theta \\Bigg] \\newline \\end{aligned} $$\nRust (1988) shows that it’s a joint contraction mapping Memo: the state space now is $2k = (2 \\text{ actions}) \\times (k \\text{ points})^{1 \\text{ variables} \\times 1 \\text{ period}}$ instead of $3^k = (k \\text{ points})^{3 \\text{ variables} \\times 1 \\text{ period}}$ Much smaller! Lesson: any state variable that does not affect continuation values (the future) does not have to be in the “actual” state space Assumption 2 - Implications We can also split the expectation in the alternative-specific value function $$ \\begin{aligned} \u0026amp; \\bar V_0 \\left(s_t ; \\theta\\right) = u\\left(s_t, 0 ; \\theta\\right) + \\beta \\mathbb E_{s_{t+1}} \\Bigg[ \\mathbb E_{{\\color{red}{\\epsilon_{t+1}}}} \\Bigg[ \\max_{a_{t+1}} \\Bigg\\lbrace \\begin{array}{l} \\bar V_0 \\left(s_{t+1} ; \\theta\\right) + {\\color{red}{\\epsilon_{0 t+1}}} \\ ; \\newline \\bar V_1 \\left(s_{t+1} ; \\theta\\right) + {\\color{red}{\\epsilon_{1 t+1}}} \\end{array} \\Bigg\\rbrace \\ \\Bigg| \\ s_t \\Bigg] \\ \\Bigg| \\ s_t, a_{t}=0 ; \\theta \\Bigg] \\newline \u0026amp; \\bar V_1 \\left(s_t ; \\theta\\right) = u\\left(s_t, 1 ; \\theta\\right) + \\beta \\mathbb E_{s_{t+1}} \\Bigg[ \\mathbb E_{{\\color{red}{\\epsilon_{t+1}}}} \\Bigg[ \\max_{a_{t+1}} \\Bigg\\lbrace \\begin{array}{l} \\bar V_0 \\left(s_{t+1} ; \\theta\\right) + {\\color{red}{\\epsilon_{0 t+1}}} \\ ; \\newline \\bar V_1 \\left(s_{t+1} ; \\theta\\right) + {\\color{red}{\\epsilon_{1 t+1}}} \\end{array} \\Bigg\\rbrace \\ \\Bigg| \\ s_t \\Bigg] \\ \\Bigg| \\ s_t, a_{t}=1 ; \\theta \\Bigg] \\newline \\end{aligned} $$ This allows us to concentrate on one single term $$ \\mathbb E_{{\\color{red}{\\epsilon_{t+1}}}} \\Bigg[ \\max_{a_{t+1}} \\Bigg\\lbrace \\begin{array}{l} \\bar V_0 \\left(s_{t+1} ; \\theta\\right) + {\\color{red}{\\epsilon_{0 t+1}}} \\ ; \\newline \\bar V_1 \\left(s_{t+1} ; \\theta\\right) + {\\color{red}{\\epsilon_{1 t+1}}} \\end{array} \\Bigg\\rbrace \\ \\Bigg| \\ s_t \\Bigg] $$ Open issues\nDistribution of $\\epsilon_{t+1}$ has to be simulated Distribution of $\\epsilon_{t+1}$ depends on $s_t$ Assumption 3 A3: independence of $\\epsilon_t$ from $s_t$ $$ \\Pr \\Big( \\epsilon_{t+1} \\Big| s_{t+1} ; \\theta \\Big) \\Pr \\Big( s_{t+1} \\Big| s_t, a_{t} ; \\theta \\Big) = \\Pr \\big( \\epsilon_{t+1} \\big| \\theta \\big) \\Pr \\Big( s_{t+1} \\Big| s_t, a_{t} ; \\theta \\Big) $$\nWhat it buys\n$\\epsilon$ not correlated with anything $$ \\mathbb E_{{\\color{red}{\\epsilon_{t+1}}}} \\Bigg[ \\max_{a_{t+1} \\in \\lbrace 0, 1 \\rbrace } \\Bigg\\lbrace \\begin{array}{l} \\bar V_0 \\left(s_{t+1} ; \\theta\\right) + {\\color{red}{\\epsilon_{0 t+1}}} \\ ; \\newline \\bar V_1 \\left(s_{t+1} ; \\theta\\right) + {\\color{red}{\\epsilon_{1 t+1}}} \\end{array} \\Bigg\\rbrace \\ \\Bigg] $$ What are we assuming away\nSome state-specific noise… probably irrelevant Open Issues\nDistribution of $\\epsilon_{t+1}$ has to be simulated Assumption 4 A4: $\\epsilon$ is type 1 extreme value distributed (logit)\nWhat it buys\nClosed form solution for $\\mathbb E_{\\epsilon_{t+1}}$ What are we assuming away\nDifferent substitution patterns\nRelevant? Maybe, if there are at least three options (here binary choice)\nAs logit assumption in demand estimation Logit magic 🧙🪄 $$ \\mathbb E_{\\epsilon} \\Bigg[ \\max_n \\bigg( \\Big\\lbrace \\delta_n + \\epsilon_n \\Big\\rbrace_{n=1}^N \\bigg) \\Bigg] = 0.5772 + \\ln \\bigg( \\sum_{n=1}^N e^{\\delta_n} \\bigg) $$\nwhere $0.5772$ is Euler’s constant\nAssumption 4 - Implications The Bellman equation becomes $$ \\begin{aligned} \u0026amp; \\bar V_0 \\left(s_t ; \\theta\\right) = u\\left(s_t, 0 ; \\theta\\right) + \\beta \\mathbb E_{s_{t+1}} \\Bigg[ 0.5772 + \\ln \\Bigg( \\sum_{a\u0026rsquo; \\in \\lbrace 0, 1 \\rbrace} e^{\\bar V_{a\u0026rsquo;} (s_{t+1} ; \\theta)} \\Bigg) \\ \\Bigg| \\ s_t, a_{t}=0 ; \\theta \\Bigg] \\newline \u0026amp; \\bar V_1 \\left(s_t ; \\theta\\right) = u\\left(s_t, 1 ; \\theta\\right) + \\beta \\mathbb E_{s_{t+1}} \\Bigg[ 0.5772 + \\ln \\Bigg( \\sum_{a\u0026rsquo; \\in \\lbrace 0, 1 \\rbrace} e^{\\bar V_{a\u0026rsquo;} (s_{t+1} ; \\theta)} \\Bigg) \\ \\Bigg| \\ s_t, a_{t}=1 ; \\theta \\Bigg] \\newline \\end{aligned} $$\nWe got fully rid of $\\epsilon$! How? With a lot of assumptions Estimation So far we have analysized how the 4 assumptions help solving the model.\nWhat about estimation? Maximum Likelihood\nFor a single bus, the likelihood function is $$ \\mathcal L = \\Pr \\Big(s_{1}, \u0026hellip; , s_T, a_{0}, \u0026hellip; , a_{T} \\ \\Big| \\ s_{0} ; \\theta\\Big) $$\ni.e. probability of observed decisions $\\lbrace a_{0}, \u0026hellip; , a_{T} \\rbrace$ and sequence of states $\\lbrace s_{1}, \u0026hellip; , s_T \\rbrace$ conditional on the initial state $s_0$ and the parameter values $\\theta$ What is the impact of the 4 assumptions on the likelihood function?\nLikelihood Function (A1) A1: First order Markow process of $\\epsilon$\nWe gain independence across time We can decompose the joint distribution in marginals across time $$ \\begin{align} \\mathcal L(\\theta) \u0026amp;= \\Pr \\Big(s_{1}, \u0026hellip; , s_T, a_{0}, \u0026hellip; , a_{T} \\Big| s_{0} ; \\theta\\Big)\\newline \u0026amp;= \\prod_{t=1}^T \\Pr \\Big(a_{t+1} , s_{t+1} \\Big| s_t, a_t ; \\theta\\Big) \\end{align} $$\nLikelihood Function (A2) A2: independence of $\\epsilon_t$ from $\\epsilon_{t-1}$ and $s_{t-1}$ on $s_t$\nWe can decompose the joint distribution of $a_t$ and $s_{t+1}$ into marginals $$ \\begin{align} \\mathcal L(\\theta) \u0026amp;= \\prod_{t=1}^T \\Pr \\Big(a_{t+1} , s_{t+1} \\Big| s_t, a_t ; \\theta\\Big) = \\newline \u0026amp;= \\prod_{t=1}^T \\Pr \\big(a_t \\big| s_t ; \\theta\\big) \\Pr \\Big(s_{t+1} \\Big| s_t, a_t ; \\theta\\Big) \\end{align} $$\n$\\Pr \\big(s_{t+1} \\big| s_t, a_t ; \\theta\\big)$ can be estimated from the data\nwe’ll come back to it for $\\Pr \\big(a_t \\big| s_t ; \\theta\\big)$ we need the two remaining assumptions\nLikelihood Function (A3) A3: Independence of $\\epsilon_t$ from $s_t$\nNo need to condition on $s_t$ E.g. probability of replacement $$ \\begin{align} \\Pr \\big(a_t=1 \\big| s_t ; \\theta \\big) \u0026amp;= \\Pr \\Big( \\bar V_1 (s_{t+1} ; \\theta) + \\epsilon_{1 t+1} \\geq \\bar V_0 (s_{t+1} ; \\theta) + \\epsilon_{0 t+1} \\ \\Big| \\ s_t ; \\theta \\Big) = \\newline \u0026amp;= \\Pr \\Big( \\bar V_1 (s_{t+1} ; \\theta) + \\epsilon_{1 t+1} \\geq \\bar V_0 (s_{t+1} ; \\theta) + \\epsilon_{0 t+1} \\ \\Big| \\ \\theta \\Big) \\end{align} $$\nIn words: same distribution of shocks in every state Likelihood Function (A4) A4: Logit distribution of $\\epsilon$\nE.g. probability of replacement becomes $$ \\begin{align} \\Pr \\big(a_t=1 \\big| s_t ; \\theta \\big) \u0026amp;= \\Pr \\Big( \\bar V_1 (s_{t+1} ; \\theta) + \\epsilon_{1 t+1} \\geq \\bar V_0 (s_{t+1} ; \\theta) + \\epsilon_{0 t+1} \\ \\Big| \\ \\theta \\Big) = \\newline \u0026amp;= \\frac{e^{\\bar V_1 (s_{t+1} ; \\theta)}}{e^{\\bar V_0 (s_{t+1} ; \\theta)} + e^{\\bar V_1 (s_{t+1} ; \\theta)}} \\end{align} $$\nWe have a closed form expression! Likelihood Function The final form of the likelihood function for one bus is $$ \\mathcal L(\\theta) = \\prod_{t=1}^T \\Pr\\big(a_t \\big| s_t ; \\theta \\big) \\Pr \\Big(s_{t+1} \\ \\Big| \\ s_t, a_t ; \\theta\\Big) $$ where\n$\\Pr \\Big(s_{t+1} \\ \\Big| \\ s_t, a_t ; \\theta\\Big)$ can be estimated from the data given mileage $x$ and investment decision $a$, what are the observed frequencies of future states $x\u0026rsquo;$? does not have to depend on $\\theta$ $\\Pr\\big(a_t \\big| s_t ; \\theta \\big)$ depends on $\\bar V_a (s ; \\theta)$ $\\bar V_a (s ; \\theta)$ we know how to compute given a value of $\\theta$ solve by value function iteration Likelihood Function (2) Since we have may buses, $j$, the likelihood of the data is $$ \\mathcal L(\\theta) = \\prod_{j} \\mathcal L_j (\\theta) = \\prod_{j} \\prod_{t=1}^T \\Pr\\big(a_{jt} \\big| s_{jt} ; \\theta \\big) \\Pr \\Big(s_{j,t+1} \\ \\Big| \\ s_{jt}, a_{jt} ; \\theta\\Big) $$ And, as usual, we prefer to work with log-likelihoods $$ \\log \\mathcal L(\\theta) = \\sum_{j} \\sum_{t=1}^T \\Bigg( \\log \\Pr\\big(a_{jt} \\big| s_{jt} ; \\theta \\big) + \\log\\Pr \\Big(s_{j,t+1} \\ \\Big| \\ s_{jt}, a_{jt} ; \\theta\\Big) \\Bigg) $$\nEstimation Now we have all the pieces to estimate $\\theta$!\nProcedure\nEstimate the state transition probabilities $\\Pr \\big(s_{t+1} \\big| s_t, a_t ; \\theta\\big)$ Select a value of $\\theta$ Init a choice-specific value function $\\bar V_a^{(0)} (s_{t+1} ; \\theta)$ Apply the Bellman operator to compute $\\bar V_a^{(1)} (s_{t+1} ; \\theta)$ Iterate until convergence to $\\bar V_d^{(k \\to \\infty)} (s_{t+1} ; \\theta)$ (inner loop) Compute the choice probabilities $\\Pr \\big(a_t\\big| s_t ; \\theta \\big)$ Compute the likelihood $\\mathcal L = \\prod_j \\prod_{t=1}^T \\Pr \\big(a_t \\big| s_t ; \\theta\\big) \\Pr \\Big(s_{t+1} \\Big| s_t, a_t ; \\theta\\Big)$ Iterate (2-5) until you are have found a (possibly global) minimum (outer loop) Results What do dynamics add?\nStatic demand curve ($\\beta =0$) is much more sensitive to the price of engine replacement. Why? Compares present price with present savings If you compare present price with flow of future benefits, you are less price sensitive More realistic Extensions Main limitation of Rust (1987): value function iteration\nCostly: has to be done for each parameter explored during optimization Particularly costly if the state space is large Solutions\nSolve the model without solving a fixed point problem Hotz and Miller (1993) Solve the model and estimate the parameters at the same time Inner and outer loop in parallel Imai, Jain, and Ching (2009) Treat the estimation as a constrained optimization problem MPEC, as for demand Use off-the-shelf optimization algorithms Su and Judd (2012) We’ll cover Hotz and Miller (1993) since at the core of the estimation of dynamic games.\nHotz \u0026amp; Miller (1993) Motivation Setting: Harold Zurcher problem\nsame model same assumptions same notation same objective Problem: computationally intense to do value function iteration\nCan we solve the model without solving a fixed point problem?\nEstimation in Rust How did we estimate the model in Rust? Two main equations\nSolve the Bellman equation of the alternative-specific value function $$ {\\color{green}{\\bar V(s; \\theta)}} = \\tilde f( {\\color{green}{\\bar V(s; \\theta)}}) $$\nCompute the expected policy function $$ {\\color{blue}{P( \\cdot | s; \\theta)}} = \\tilde g( {\\color{green}{\\bar V(s; \\theta)}} ; \\theta) $$\nMaximize the likelihood function\n$$ \\mathcal L(\\theta) = \\prod_{j} \\prod_{t=1}^T {\\color{blue}{ \\Pr\\big(a_{jt} \\big| s_{jt} ; \\theta \\big)}} \\Pr \\Big(s_{j,t+1} \\ \\Big| \\ s_{jt}, a_{jt} ; \\theta\\Big) $$\nCan we remove step 1?\nHotz \u0026amp; Miller Idea(s) Idea 1: it would be great if we could start from something like $$ {\\color{blue}{P(\\cdot|s; \\theta)}} = T( {\\color{blue}{P(\\cdot|s; \\theta)}} ; \\theta) $$\nNo need to solve for the value function But we would still need a to solve a fixed point problem Back from the start? No Idea 2: could replace the RHS element with a consistent estimate $$ {\\color{blue}{P(\\cdot|s; \\theta)}} = T( {\\color{red}{\\hat P(\\cdot|s; \\theta)}} ; \\theta) $$ And this could give us an estimating equation!\nUnclear? No problem, let’s go slowly step by step\nTwo Main Equations Bellman equation $$ {\\color{green}{\\bar V_a \\left(s_t ; \\theta\\right)}} = u\\left(s_t, a ; \\theta\\right) + \\beta \\mathbb E_{s_{t+1}, \\epsilon_{t+1}} \\Bigg[ \\max_{a\u0026rsquo;} \\Big\\lbrace {\\color{green}{\\bar V_{a\u0026rsquo;}}} \\left(s_{t+1}; \\theta\\right) + \\epsilon_{a\u0026rsquo;,t+1} \\Big\\rbrace \\ \\Big| \\ s_t, a_t=a ; \\theta \\Bigg] $$\nExpected policy function\n$$ {\\color{blue}{\\Pr \\big(a_t=a \\big| s_t ; \\theta \\big)}} = \\Pr \\Big( {\\color{green}{\\bar V_a (s_{t+1} ; \\theta)}} + \\epsilon_{a, t+1} \\geq {\\color{green}{\\bar V_{a\u0026rsquo;} (s_{t+1} ; \\theta)}} + \\epsilon_{a\u0026rsquo;, t+1} , \\ \\forall a\u0026rsquo; \\ \\Big| \\ \\theta \\Big) $$\nExpected decision before the shocks $\\epsilon_t$ are realized\nNot the policy function The policy function maps $s_t \\times \\epsilon \\to \\lbrace 0 , 1 \\rbrace$ The expected policy function maps $s_t \\to [ 0 , 1 ]$ Easier to work with: does not depend on the shocks Not a deterministic policy, but a stochastic one Hotz \u0026amp; Miller - Idea 1 How do we get from the two equations $$ \\begin{aligned} {\\color{green}{\\bar V(s; \\theta)}} \u0026amp;= \\tilde f( {\\color{green}{\\bar V(s; \\theta)}}) \\newline {\\color{blue}{P(\\cdot|s; \\theta)}} \u0026amp;= \\tilde g( {\\color{green}{\\bar V(s; \\theta)}} ; \\theta) \\end{aligned} $$ To one? $$ {\\color{blue}{P(\\cdot|s; \\theta)}} = T ({\\color{blue}{P(\\cdot|s; \\theta)}}; \\theta) $$ If we could express $\\bar V$ in terms of $P$, … $$ \\begin{aligned} {\\color{green}{\\bar V(s; \\theta)}} \u0026amp; = \\tilde h( {\\color{blue}{P(\\cdot|s; \\theta)}}) \\newline {\\color{blue}{P(\\cdot|s; \\theta)}} \u0026amp;= \\tilde g( {\\color{green}{\\bar V(s; \\theta)}} ; \\theta) \\end{aligned} $$\n…. we could then substitute the first equation into the second …\nBut, easier to work with a different representation of the value function.\nExpected Value Function Recall Rust value function (not the alternative-specific $\\bar V$) $$ V\\left(s_t, \\epsilon_t ; \\theta\\right) = \\max_{a_{t}} \\Bigg\\lbrace u \\left( s_t, a_{t} ; \\theta \\right) + \\epsilon_{a_{t} t} + \\beta \\mathbb E_{s_{t+1}, \\epsilon_{t+1}} \\Big[V\\left(s_{t+1}, \\epsilon_{t+1} ; \\theta\\right) \\Big| s_t, a_{t} ; \\theta\\Big] \\Bigg\\rbrace $$ We can express it in terms of expected value function\n$$ V\\left(s_t ; \\theta\\right) = \\mathbb E_{\\epsilon_t} \\Bigg[ \\max_{a_{t}} \\Bigg\\lbrace u\\left(s_t, a_t ; \\theta\\right) + \\epsilon_{a_{t} t}+ \\beta \\mathbb E_{s_{t+1}} \\Big[V\\left(s_{t+1}; \\theta\\right) \\Big| s_t, a_{t} ; \\theta\\Big] \\Bigg\\rbrace \\Bigg] $$\nValue of being in state $s_t$ without knowing the realization of the shock $\\epsilon_t$\n“Value of Harold Zurcher before opening the window and seeing if it’s raining or not” Analogous to the relationship between policy funciton and expected policy function\nNote\nexpectation of future value now is only over $s_{t+1}$ $V\\left(s_t ; \\theta\\right)$ can be solved via value function iteration as the operator on the RHS is a contraction Representation Equivalence Recall the alternative-specific value function of Rust\n$$ \\begin{align} {\\color{green}{\\bar V_a \\left( s_t ; \\theta\\right)}} \u0026amp;= u\\left(s_t, d ; \\theta\\right) + \\beta \\mathbb E_{s_{t+1}, \\epsilon_{t+1}} \\Bigg[ \\max_{a\u0026rsquo;} \\Big\\lbrace {\\color{green}{\\bar V_{a\u0026rsquo;} \\left(s_{t+1}; \\theta\\right)}} + \\epsilon_{a\u0026rsquo;,t+1} \\Big\\rbrace \\ \\Big| \\ s_t, a_t=a ; \\theta \\Bigg] \\newline \u0026amp;=u\\left(s_t, a ; \\theta\\right)+\\beta \\mathbb E_{s_{t+1}, \\epsilon_{t+1}} \\Big[ {\\color{orange}{V \\left( s_{t+1}, \\epsilon_{t+1} ; \\theta \\right)}} \\Big| s_t, a_t=a ; \\theta \\Big] \\newline \u0026amp;= u \\left( s_t, a ; \\theta \\right) + \\beta \\mathbb E_{s_{t+1}} \\Big[ {\\color{red}{V \\left( s_{t+1} ; \\theta \\right)}} \\Big| s_t, a_t=a; \\theta \\Big] \\end{align} $$\nRelationship with the value function\n$$ {\\color{orange}{V \\left(s_t, \\epsilon_{t} ; \\theta \\right)}} = \\max_{a_{t}} \\Big\\lbrace {\\color{green}{ \\bar V_0 \\left( s_t ; \\theta \\right)}} + \\epsilon_{0t}, {\\color{green}{\\bar V_1 \\left( s_t ; \\theta \\right)}} + \\epsilon_{1t} \\Big\\rbrace $$\nRelationship with the expected value function $$ {\\color{red}{V\\left(s_t ; \\theta\\right)}} = \\mathbb E_{\\epsilon_t} \\Big[ {\\color{orange}{V\\left(s_t, \\epsilon_{t} ; \\theta\\right)}} \\ \\Big| \\ s_t \\Big] $$\nGoal We switched from alternative-specific value function ${\\color{green}{\\bar V (s_t ; \\theta)}}$ to expected value function ${\\color{red}{V(s_t ; \\theta)}}$\nBut the goal is the same Go from this representation $$ \\begin{align} {\\color{red}{V(s ; \\theta)}} \u0026amp; = f( {\\color{red}{V(s ; \\theta)}}) \\newline {\\color{blue}{P(\\cdot | s ; \\theta)}} \u0026amp; = g( {\\color{red}{V(s ; \\theta)}}; \\theta) \\end{align} $$ To this $$ \\begin{align} {\\color{red}{V(s ; \\theta)}} \u0026amp; = h( {\\color{blue}{P(\\cdot|s ; \\theta)}} ; \\theta) \\newline {\\color{blue}{P(\\cdot|s ; \\theta)}} \u0026amp; = g({\\color{red}{V(s ; \\theta)}}; \\theta) \\end{align} $$ I.e. we want to express the expected value function (EV) in terms of the expected policy function (EP).\n**Note **: the $f$, $g$ and $h$ functions are different functions now.\nExpress EV in terms of EP (1) First, let’s ged rid of one operator: the max operator $$ V\\left(s_t ; \\theta\\right) = \\sum_a \\Pr \\Big(a_t=a | s_t ; \\theta \\Big) * \\left[\\begin{array}{c} u\\left(s_t, a ; \\theta\\right) + \\mathbb E_{\\epsilon_t} \\Big[\\epsilon_{at}\\Big| a_t=a, s_t\\Big] \\newline \\qquad + \\beta \\mathbb E_{s_{t+1}} \\Big[V\\left(s_{t+1} ; \\theta\\right) \\Big| s_t, a_t=a ; \\theta\\Big] \\end{array}\\right] $$\nWe are just substituting the $\\max$ with the policy $\\Pr\\left(a_t=a| s_t ; \\theta\\right)$\nImportant: we got rid of the $\\max$ operator\nBut we are still taking the expectation over\nFuture states $s_{t+1}$ Shocks $\\epsilon_t$ Express EV in terms of EP (2) Now we get rid of another operator: the expectation over $s_{t+1}$ $$ \\mathbb E_{s_{t+1}} \\Big[V\\left(s_{t+1} ; \\theta\\right) \\Big| s_t, a_t=a ; \\theta\\Big] \\qquad \\to \\qquad \\sum_{s_{t+1}} V\\left(s_{t+1} ; \\theta\\right) \\Pr \\Big(s_{t+1} \\Big| s_t, a_t=a ; \\theta \\Big) $$ where\n$\\sum_{s_{t+1}}$ is the summation over the next states $\\Pr (s_{t+1} | s_t, a_t=a ; \\theta )$ is the transition probability (conditional on a particular choice) so that the expected value function becomes $$ V\\left(s_t ; \\theta\\right) = \\sum_a \\Pr \\Big(a_t=a | s_t ; \\theta \\Big) * \\left[\\begin{array}{c} u\\left(s_t, a ; \\theta\\right) + \\mathbb E_{\\epsilon_t} \\Big[\\epsilon_{at}\\Big| a_t=a, s_t\\Big] \\newline + \\beta \\sum_{s_{t+1}} V\\left(s_{t+1} ; \\theta\\right) \\Pr \\Big(s_{t+1} \\Big| s_t, a_t=a ; \\theta \\Big) \\end{array}\\right] $$\nExpress EV in terms of EP (3) The previous equation, was defined at the state level $s_t$\nsystem of $k$ equations, 1 for each state (value of $x$) If we stack them, we can write them as $$ V\\left(s ; \\theta\\right) = \\sum_a \\Pr \\Big(a \\ \\Big| \\ s ; \\theta \\Big) .* \\Bigg[ u\\left(s, a ; \\theta\\right) + \\mathbb E_{\\epsilon} \\Big[\\epsilon_{a} \\ \\Big| \\ a, s \\Big] + \\beta \\ T(a ; \\theta) \\ V(s ; \\theta) \\Bigg] $$ where\n$T(a)$: $k \\times k$ matrix of transition probabilities from state $s_t$ to $s_{t+1}$, given decision $a$ $.*$ is the dot product operator (or element-wise matrix multiplication) Express EV in terms of EP (4) Now we have a system of $k$ equations in $k$ unknowns that we can solve.\nTearing down notation to the bare minimum, we have $$ V = \\sum_a P_a .* \\bigg[ u_a + \\mathbb E [\\epsilon_a ] + \\beta \\ T_a \\ V \\bigg] $$ which we can rewrite as $$ V - \\beta \\ \\left( \\sum_a P_a .* T_a \\right) V = \\sum_a P_a .* \\bigg[ u_a + \\mathbb E [\\epsilon_a ] \\bigg] $$\nand finally we can solve for $V$ through the famous Hotz and Miller inversion $$ V = \\left[I - \\beta \\ \\sum_a P_a .* T_a \\right]^{-1} \\ * \\ \\left( \\sum_a P_a \\ .* \\ \\bigg[ u_a + \\mathbb E [\\epsilon_a] \\bigg] \\right) $$ Solved? No. We still need to do something about $\\mathbb E [\\epsilon_a]$.\nExpress EV in terms of EP (5) What is $\\mathbb E [\\epsilon_a]$?\nLet’s consider for example the expected value of the shock, conditional on investment $$ \\begin{aligned} \\mathbb E \\Big[ \\epsilon_{1 t} \\ \\Big| \\ a_t = 1, \\cdot \\Big] \u0026amp;= \\mathbb E \\Big[ \\epsilon_{t} \\ \\Big| \\ \\bar V_1 \\left( s_t ; \\theta \\right) + \\epsilon_{1 t} \u0026gt; \\bar V_0 \\left( s_t ; \\theta \\right) + \\epsilon_{0 t} \\Big] \\newline \u0026amp; = \\mathbb E \\Big[ \\epsilon_{1 t} \\ \\Big| \\ \\bar V_1 \\left( s_t ; \\theta \\right) - \\bar V_0 \\left( s_t ; \\theta \\right) \u0026gt; \\epsilon_{0 t} - \\epsilon_{1 t} \\Big] \\end{aligned} $$ with logit magic 🧙🪄 is $$ \\mathbb E\\left[\\epsilon_{1 t} | a_{t}=1, s_t\\right] = 0.5772 - \\ln \\left(P\\left(s_t ; \\theta\\right)\\right) $$\nwhere $0.5772$ is Euler’s constant. We again got rid of another $\\max$ operator!\nExpress EV in terms of EP (6) Now we can substitute it back and we have an equation which is just a function of primitives $$ \\begin{aligned} V(\\cdot ; \\theta) =\u0026amp; \\Big[I-(1-P(\\cdot ; \\theta)) \\beta T(0 ; \\theta)-P(\\cdot ; \\theta) \\beta T(1 ; \\theta)\\Big]^{-1} \\newline * \u0026amp; \\left[ \\begin{array}{c} (1-P(\\cdot ; \\theta))\\Big[u(\\cdot, 0 ; \\theta)+0.5772-\\ln (1-P(\\cdot ; \\theta))\\Big] \\newline + P(\\cdot ; \\theta)\\Big[u(\\cdot, 1 ; \\theta) + 0.5772 - \\ln (P(\\cdot ; \\theta))\\Big] \\end{array} \\right] \\end{aligned} $$\nOr more compactly $$ V = \\left[I - \\beta \\ \\sum_a P_a .* T_a \\right]^{-1} \\ * \\ \\left( \\sum_a P_a \\ .* \\ \\bigg[ u_a + 0.5772 - \\ln(P_d) \\bigg] \\right) $$\nFirst Equation What is the first equation? $$ V = \\left[I - \\beta \\ \\sum_a P_a .* T_a \\right]^{-1} \\ * \\ \\left( \\sum_a P_a \\ .* \\ \\bigg[ u_a + 0.5772 - \\ln(P_a) \\bigg] \\right) $$ Expected static payoff: $\\sum_a P_a \\ .* \\ \\bigg[ u_a + 0.5772 + \\ln(P_a) \\bigg]$\nIs the expected static payoff of choice $a$ in each state, $u_a + 0.5772 + \\ln(P_a)$ … integrated over the choice probabilities, $P_a$ It’s a $k \\times 1$ vector Unconditional transition probabilities: $\\sum_a P_a .* T_a$\nAre the transition probabilities conditional on a choice $a$ for every present and future state, $T_a$ … integrated over the choice probabilities, $P_a$ It’s a $k \\times k$ matrix Recap We got our first equation $$ {\\color{red}{V}} = \\left[I - \\beta \\ \\sum_a {\\color{blue}{P_a}} .* T_a \\right]^{-1} \\ * \\ \\left( \\sum_a {\\color{blue}{P_a}} \\ .* \\ \\bigg[ u_a + 0.5772 - \\ln({\\color{blue}{P_a}}) \\bigg] \\right) $$\nI.e. $$ \\begin{align} {\\color{red}{V(s ; \\theta)}} \u0026amp; = h( {\\color{blue}{P(s ; \\theta)}} ; \\theta) \\newline \\end{align} $$\nWhat about the second equation ${\\color{blue}{P(\\cdot|s ; \\theta)}} = g({\\color{red}{V(s ; \\theta)}}; \\theta)$?\nFrom V to P In general, the expected probability of investment is $$ P(a=1; \\theta)= \\Pr \\left[\\begin{array}{c} u(\\cdot, 1 ; \\theta)+\\epsilon_{1 t}+\\beta \\mathbb E \\Big[V(\\cdot ; \\theta) \\Big| \\cdot, a_{t}=1 ; \\theta \\Big]\u0026gt; \\newline \\qquad u(\\cdot, 0 ; \\theta) + \\epsilon_{0 t}+\\beta \\mathbb E \\Big[V(\\cdot ; \\theta) \\Big| \\cdot, a_{t}=0 ; \\theta \\Big] \\end{array}\\right] $$\nWith the logit assumption, simplifies to $$ {\\color{blue}{P(a=1 ; \\theta)}} = \\frac{\\exp \\Big(u(\\cdot, 1 ; \\theta)+\\beta T(1 ; \\theta) V(\\cdot ; \\theta) \\Big)}{\\sum_{a\u0026rsquo;} \\exp \\Big(u(\\cdot, a\u0026rsquo; ; \\theta)+\\beta T(a\u0026rsquo; ; \\theta) V(\\cdot ; \\theta) \\Big)} = \\frac{\\exp (u_1 +\\beta T_1 {\\color{red}{V}} )}{\\sum_{a\u0026rsquo;} \\exp (u_{a\u0026rsquo;} +\\beta T_{a\u0026rsquo;} {\\color{red}{V}} )} $$\nNow we have also the second equation! $$ \\begin{align} {\\color{blue}{P(s ; \\theta)}} \u0026amp; = g({\\color{red}{V(s ; \\theta)}}; \\theta) \\end{align} $$\nHotz \u0026amp; Miller - Idea 2 Idea 2: Replace ${\\color{blue}{P} (\\cdot)}$ on the RHS with a consistent estimator ${\\color{Turquoise}{\\hat P (\\cdot)}}$ $$ {\\color{cyan}{\\bar P(\\cdot ; \\theta)}} = g(h({\\color{Turquoise}{\\hat P(\\cdot)}} ; \\theta); \\theta) $$\n${\\color{cyan}{\\bar P(\\cdot ; \\theta_0)}}$ will converge to the true ${\\color{blue}{P(\\cdot ; \\theta_0)}}$, because ${\\color{Turquoise}{\\hat P (\\cdot)}}$ is converging to ${\\color{blue}{P(\\cdot ; \\theta_0)}}$ asymptotically.\nNote: pay attention to $\\theta_0$ vs $\\theta$ here: ${\\color{cyan}{\\bar P(\\cdot ; \\theta)}}$ does not generally converge to ${\\color{blue}{P(\\cdot ; \\theta)}}$for arbitrary $\\theta$, because ${\\color{Turquoise}{\\hat P(\\cdot)}}$ is converging to ${\\color{blue}{P(\\cdot ; \\theta_0)}}$ but not ${\\color{blue}{P(\\cdot ; \\theta)}}$ with any $\\theta$. How to compute ${\\color{Turquoise}{\\hat P(\\cdot)}}$?\nFrom the data, you observe states and decisions\nYou can compute frequency of decisions given states\nIn Rust: frequency of engine replacement, given a mileage (discretized) Assumption: you have enough data\nWhat if a state is not realised? Use frequencies in observed states to extrapolate frequencies in unobserved states Recap Steps so far\nEstimate the conditional choice probabilities ${\\color{Turquoise}{\\hat P}}$ from the data\nNonparametrically: frequency of each decision in each state Solve for the expected value function with the inverstion step $$ {\\color{orange}{\\hat V}} = \\left[I - \\beta \\ \\sum_a {\\color{Turquoise}{\\hat P_a}} .* T_a \\right]^{-1} \\ * \\ \\left( \\sum_a {\\color{Turquoise}{\\hat P_a}} \\ .* \\ \\bigg[ u_a + 0.5772 - \\ln({\\color{Turquoise}{\\hat P_a}}) \\bigg] \\right) $$\nCompute the predicted CCP, given $V$ $$ {\\color{cyan}{\\bar P(a=1 ; \\theta)}} = \\frac{\\exp (u_1 +\\beta T_1 {\\color{orange}{\\hat V}} )}{\\sum_{a\u0026rsquo;} \\exp (u_{a\u0026rsquo;} +\\beta T_{a\u0026rsquo;} {\\color{orange}{\\hat V}} )} $$\nWhat now? Use the estimated CCP to build an objective function.\nObjective Function We have (at least) 2 options\nHotz and Miller (1993) use GMM $$ \\mathbb E \\Big[a_t - \\bar P(s_t, \\theta) \\ \\Big| \\ s_t \\Big] = 0 \\quad \\text{ at } \\quad \\theta = \\theta_0 $$\nAguirregabiria and Mira (2002) use MLE by putting $\\bar P(s_t, \\theta)$ in the likelihood function instead of $P(s_t, \\theta)$ We will follow the second approach\nPseudo-Likelihood The likelihood function for one bus is $$ \\mathcal{L}(\\theta) = \\prod_{t=1}^{T}\\left(\\hat{\\operatorname{Pr}}\\left(a=1 \\mid s_{t}; \\theta\\right) \\mathbb{1}\\left(a_{t}=1\\right)+\\left(1-\\hat{\\operatorname{Pr}}\\left(a=0 \\mid s_{t}; \\theta\\right)\\right) \\mathbb{1}\\left(a_{t}=0\\right)\\right) $$ where $\\hat \\Pr\\big(a_{t} \\big| s_{t} ; \\theta \\big)$ is a function of\nCCPs $\\hat P$: estimated from data transition matrix $T$: estimated from the data, given $\\theta$ static payoffs $u$: known, given $\\theta$ discount factor $\\beta$ : assumed Why pseudo-likelihood? We have inputed something that is not a primitive but a consistent estimate of an equilibrium object, $\\hat P$\nComments Now a few comments on Hotz and Miller (1993)\nComputational bottleneck Aguirregabiria and Mira (2002) Importance of the T1EV assumption Data requirements Unobserved heterogeneity Identification Computational Bottleneck There is still 1 computational bottleneck in HM: the inversion step $$ V = \\left[I - \\beta \\ \\sum_a P_a .* T_a \\right]^{-1} \\ * \\ \\left( \\sum_a P_a \\ .* \\ \\bigg[ u_a + 0.5772 - \\ln(P_a) \\bigg] \\right) $$ The $\\left[I - \\beta \\ \\sum_a P_a .* T_a \\right]$ matrix has dimension $k \\times k$\nWith large state space, hard to invert Even with modern computational power Hotz et al. (1994): forward simulation of the value function You have the policy, the transitions and the utilities Just compute discounted flow of payoffs Core idea behind the estimation of dynamic games Aguirregabiria, Mira (2002) Hotz and Miller (1993) inversion gets us a recursive equation in probability space\ninstead of the Bellman Equation in the value space $$ \\bar P(\\cdot ; \\theta) = g(h(\\hat P(\\cdot) ; \\theta); \\theta) $$\nIdea\nDo you gain something by iterating $K$ imes? $K=1$: Hotz and Miller (1993) $K \\to \\infty$: Rust (1987) Monte Carlo simulations: finite sample properties of K−stage estimators improve monotonically with K But especially for $K=2$! Really worth iterating once Type 1 EV errors Crucial assumption\nWithout logit errors, we need to simulate their distribution True also for Rust But it’s generally accepted doesn’t imply it’s innocuous Data Requirements For both Hotz et al. (1994) and Rust (1987), we need to discretize the state space\nCan be complicated with continuous variables Problem also in Rust But particularly problematic in Hotz et al. (1994) Relies crucially on consistency of CCP estimates Need sufficient variation in actions for each state Unobserved Heterogeneity Hotz et al. (1994) cannot handle unobserved heterogeneity or “unobserved state variables” that are persistent over time.\nExample\nSuppose there are 2 bus types $\\tau$: high and low quality\nWe don’t know the share of types in the data\nWith Rust\nParametrize the effect of the difference in qualities\nE.g. high quality engines break less often Parametrize the proportion of high quality buses\nSolve the value function by type $V(s_t, \\tau ; \\theta)$\nIntegrate over types when computing choice probabilities $$ P(a|s) = \\int P(a|s,\\tau) P(\\tau) = \\Pr(a|s, \\tau=0) * \\Pr(\\tau=0) + \\Pr(a|s, \\tau=1) * \\Pr(\\tau=1) $$\nUnobserved Heterogeneity (2) What is the problem with Hotz et al. (1994)?\nThe unobserved heterogeneity generates persistency in choices\nI don’t replace today because it’s high quality, but I also probably don’t replace tomorrow either Decisions independent across time only conditional on type Likelihood of decisions must be integrated over types $$ \\mathcal L (\\theta) = \\sum_{\\tau_a} \\prod_{t=1}^{T} \\Pr (a_{jt}| s_{jt}, \\tau_a) \\Pr(\\tau_a) $$\nHotz \u0026amp; Miller needs consistent estimates of $P(a, s, \\tau)$\nDifficult when $\\tau$ is not observed!\nIdentification Work on identification\nRust (1994) and Magnac and Thesmar (2002) Rust (1987) is non-paramentrically underidentified $\\to$ parametric assumptions are essential Aguirregabiria and Suzuki (2014) Kalouptsidi, Scott, and Souza-Rodrigues (2017) Abbring and Daljord (2020) Can identify discount factor with some “instrument” that shifts future utilities but not current payoff Kalouptsidi et al. (2020) Appendix References [references] Abbring, Jaap H, and Øystein Daljord. 2020. “Identifying the Discount Factor in Dynamic Discrete Choice Models.” Quantitative Economics 11 (2): 471–501.\nAguirregabiria, Victor, and Pedro Mira. 2002. “Swapping the Nested Fixed Point Algorithm: A Class of Estimators for Discrete Markov Decision Models.” Econometrica 70 (4): 1519–43.\nAguirregabiria, Victor, and Junichi Suzuki. 2014. “Identification and Counterfactuals in Dynamic Models of Market Entry and Exit.” Quantitative Marketing and Economics 12 (3): 267–304.\nBecker, Gary S, and Kevin M Murphy. 1988. “A Theory of Rational Addiction.” Journal of Political Economy 96 (4): 675–700.\nBerry, Steven T. 1992. “Estimation of a Model of Entry in the Airline Industry.” Econometrica: Journal of the Econometric Society, 889–917.\nBresnahan, Timothy F. 1989. “Empirical Studies of Industries with Market Power.” Handbook of Industrial Organization 2: 1011–57.\nCrawford, Gregory S, and Matthew Shum. 2005. “Uncertainty and Learning in Pharmaceutical Demand.” Econometrica 73 (4): 1137–73.\nErdem, Tülin, Susumu Imai, and Michael P Keane. 2003. “Brand and Quantity Choice Dynamics Under Price Uncertainty.” Quantitative Marketing and Economics 1 (1): 5–64.\nErdem, Tülin, and Michael P Keane. 1996. “Decision-Making Under Uncertainty: Capturing Dynamic Brand Choice Processes in Turbulent Consumer Goods Markets.” Marketing Science 15 (1): 1–20.\nGolosov, Mikhail, Aleh Tsyvinski, Ivan Werning, Peter Diamond, and Kenneth L Judd. 2006. “New Dynamic Public Finance: A User’s Guide [with Comments and Discussion].” NBER Macroeconomics Annual 21: 317–87.\nGowrisankaran, Gautam, and Marc Rysman. 2012. “Dynamics of Consumer Demand for New Durable Goods.” Journal of Political Economy 120 (6): 1173–1219.\nHandel, Benjamin R. 2013. “Adverse Selection and Inertia in Health Insurance Markets: When Nudging Hurts.” American Economic Review 103 (7): 2643–82.\nHendel, Igal, and Aviv Nevo. 2006. “Measuring the Implications of Sales and Consumer Inventory Behavior.” Econometrica 74 (6): 1637–73.\nHotz, V Joseph, and Robert A Miller. 1993. “Conditional Choice Probabilities and the Estimation of Dynamic Models.” The Review of Economic Studies 60 (3): 497–529.\nHotz, V Joseph, Robert A Miller, Seth Sanders, and Jeffrey Smith. 1994. “A Simulation Estimator for Dynamic Models of Discrete Choice.” The Review of Economic Studies 61 (2): 265–89.\nIgami, Mitsuru. 2020. “Artificial Intelligence as Structural Estimation: Deep Blue, Bonanza, and AlphaGo.” The Econometrics Journal 23 (3): S1–24.\nImai, Susumu, Neelam Jain, and Andrew Ching. 2009. “Bayesian Estimation of Dynamic Discrete Choice Models.” Econometrica 77 (6): 1865–99.\nKalouptsidi, Myrto, Yuichi Kitamura, Lucas Lima, and Eduardo A Souza-Rodrigues. 2020. “Partial Identification and Inference for Dynamic Models and Counterfactuals.” National Bureau of Economic Research.\nKalouptsidi, Myrto, Paul T Scott, and Eduardo Souza-Rodrigues. 2017. “On the Non-Identification of Counterfactuals in Dynamic Discrete Games.” International Journal of Industrial Organization 50: 362–71.\nKeane, Michael P, and Kenneth I Wolpin. 1997. “The Career Decisions of Young Men.” Journal of Political Economy 105 (3): 473–522.\nMagnac, Thierry, and David Thesmar. 2002. “Identifying Dynamic Discrete Decision Processes.” Econometrica 70 (2): 801–16.\nPakes, Ariel. 1986. “Patents as Options: Some Estimates of the Value of Holding European Patent Stocks.” Econometrica 54 (4): 755–84.\nRust, John. 1987. “Optimal Replacement of GMC Bus Engines: An Empirical Model of Harold Zurcher.” Econometrica: Journal of the Econometric Society, 999–1033.\n———. 1988. “Maximum Likelihood Estimation of Discrete Control Processes.” SIAM Journal on Control and Optimization 26 (5): 1006–24.\n———. 1994. “Structural Estimation of Markov Decision Processes.” Handbook of Econometrics 4: 3081–3143.\nSu, Che-Lin, and Kenneth L Judd. 2012. “Constrained Optimization Approaches to Estimation of Structural Models.” Econometrica 80 (5): 2213–30.\n","date":1635465600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1635465600,"objectID":"33374ffd438bd550113e66846a8b147d","permalink":"https://matteocourthoud.github.io/course/empirical-io/07_dynamics_singleagent/","publishdate":"2021-10-29T00:00:00Z","relpermalink":"/course/empirical-io/07_dynamics_singleagent/","section":"course","summary":"Introduction Motivation IO: role of market structure on equilibrium outcomes.\nDynamics: study the endogenous evolution of market structure.\nSupply side dynamics Irreversible investment Entry sunk costs Product repositioning costs Price adjustment costs Learning by doing Demand side dynamics Switching costs Durable or storable products Bonus motivation: AI literature studies essentially the same set of problems with similar tools (Igami 2020)","tags":null,"title":"Single Agent Dynamics","type":"book"},{"authors":null,"categories":null,"content":"# Remove warnings import warnings warnings.filterwarnings('ignore') # Import everything import pandas as pd import numpy as np import seaborn as sns import statsmodels.api as sm import copy import torch import torch.nn as nn import torch.utils.data as Data from torch.autograd import Variable from sklearn.linear_model import LinearRegression from torchviz import make_dot # Import matplotlib for graphs import matplotlib.pyplot as plt from mpl_toolkits.mplot3d import axes3d from IPython.display import clear_output # Set global parameters %matplotlib inline plt.style.use('seaborn-white') plt.rcParams['lines.linewidth'] = 3 plt.rcParams['figure.figsize'] = (10,6) plt.rcParams['figure.titlesize'] = 20 plt.rcParams['axes.titlesize'] = 18 plt.rcParams['axes.labelsize'] = 14 plt.rcParams['legend.fontsize'] = 14 While sklearn has a library for neural networks, it is very basic and not the standard in the industry. The most commonly used libraries as of 2020 are Tensorflow and Pytorch.\nTensorFlow is developed by Google Brain and actively used at Google both for research and production needs. Its closed-source predecessor is called DistBelief.\nPyTorch is a cousin of lua-based Torch framework which was developed and used at Facebook. However, PyTorch is not a simple set of wrappers to support popular language, it was rewritten and tailored to be fast and feel native.\nHere is an article that explains very well the difference between the two libraries: pytorch-vs-tensorflow. In short, pytorch is much more intuitive for a python programmer and more user friendly. It also has a superior development and debugging experience. However, if you want more control on the fundamentals, a better community support and you need to train large models, Tensorflow is better.\n8.1 Introduction The term neural network has evolved to encompass a large class of models and learning methods. Here I describe the most widely used “vanilla” neural net, sometimes called the single hidden layer back-propagation network, or single layer perceptron.\nRegression Imagine a setting with two inputs available (let’s denote these inputs $i_1$ and $i_2$), and no special knowledge about the relationship between these inputs and the output that we want to predict (denoted by $o$) except that this relationship is, a priori, pretty complex and non-linear.\nSo we want to learn the function $f$ such that f($i_1$, $i_2$) is a good estimator of $o$. We could then suggest the following first model:\n$$ o = w_{11} i_1 + w_{12} i_2 $$\nwhere $w_{11}$ and $w_{12}$ are just weights/coefficients (do not take care about the indices for now). Before going any further, we should notice that, here, there is no constant term in the model. However, we could have introduced such term by setting $f(i_1, i_2) = w_{11} i_1 + w_{12} i_2 + c$. The constant is often called bias.\nWe can represent the setting as follows.\nIn this case, the model is easy to understand and to fit but has a big drawback : there is no non-linearity! This obviously do not respect our non-linear assumption.\nActivation Functions In order to introduce a non-linearity, let us make a little modification in the previous model and suggest the following one.\n$$ o = a ( w_{11} i_1 + w_{12} i_2) $$\nwhere $a$ is a function called activation function which is non-linear.\nOne activation function that is well known in economics (and other disciplines) is the sigmoid function or logit function\n$$ a (w_{11} i_1 + w_{12} i_2) = \\frac{1}{1 + e^{w_{11} i_1 + w_{12} i_2}} $$\nLayers However, even if better than multilinear model, this model is still too simple and can’t handle the assumed underlying complexity of the relationship between inputs and output. We can make a step further and enrich the model the following way.\nFirst we could consider that the quantity $a ( w_{11} i_1 + w_{12} i_2)$ is no longer the final output but instead a new intermediate feature of our function, called $l_1$, which stands for layer. $$ l_1 = a ( w_{11} i_1 + w_{12} i_2) $$\nSecond we could consider that we build several (3 in our example) such features in the same way, but possibly with different weights and different activation functions $$ l_1 = a ( w_{11} i_1 + w_{12} i_2) \\ l_2 = a ( w_{21} i_1 + w_{22} i_2) \\ l_3 = a ( w_{31} i_1 + w_{32} i_2) $$\nwhere the $a$’s are just activation functions and the $w$’s are weights.\nFinally, we can consider that our final output is build based on these intermediate features with the same “template” $$ a_2 ( v_1 l_1 + v_2 l_2 + v_3 * l_3 ) $$\nIf we aggregate all the pieces, we then get our prediction $p$\n$$ \\begin{aligned} p = f_{3}\\left(i_{1}, i_{2}\\right) \u0026amp;=a_{2}\\left(v_{1} l_{1}+v_{2} l_{2}+v_{3} l_{3}\\right) \\ \u0026amp;=a_{2}\\left(v_{1} \\times a_{11}\\left(w_{11} i_{1}+w_{12} i_{2}\\right)+v_{2} \\times a_{12}\\left(w_{21} i_{1}+w_{22} i_{2}\\right)+v_{3} \\times a_{13}\\left(w_{31} i_{1}+w_{32} i_{2}\\right)\\right) \\end{aligned} $$\nwhere we should mainly keep in mind that $a$’s are non-linear activation functions and $w$’s and $v$’s are weights.\nGraphically:\nThis last model is a basic feedforward neural network with:\n2 entries ($i_1$ and $i_2$) 1 hidden layer with 3 hidden neurones (whose outputs are $l_1$, $l_2$ and $l_3$) 1 final output ($p$) Pytorch Tensors We can express the data as a numpy array.\nx_np = np.arange(6).reshape((3, 2)) x_np array([[0, 1], [2, 3], [4, 5]]) Or equivalently as a pytorch tensor.\nx_tensor = torch.from_numpy(x_np) x_tensor tensor([[0, 1], [2, 3], [4, 5]]) We can also translate tensors back to arrays.\ntensor2array = x_tensor.numpy() tensor2array array([[0, 1], [2, 3], [4, 5]]) We can make operations over this data. For example we can take the mean\ntry: torch.mean(x_tensor) except Exception as e: print(e) mean(): input dtype should be either floating point or complex dtypes. Got Long instead. We first have to convert the data in float\nx_tensor = torch.FloatTensor(x_np) x_tensor tensor([[0., 1.], [2., 3.], [4., 5.]]) print(np.mean(x_np), '\\n\\n', torch.mean(x_tensor)) 2.5 tensor(2.5000) We can also apply compontent-wise functions\nprint(np.sin(x_np), '\\n\\n', torch.sin(x_tensor)) [[ 0. 0.84147098] [ 0.90929743 0.14112001] [-0.7568025 -0.95892427]] tensor([[ 0.0000, 0.8415], [ 0.9093, 0.1411], [-0.7568, -0.9589]]) We can multiply tensors as we multiply matrices\nprint(np.matmul(x_np.T, x_np), '\\n\\n', torch.mm(x_tensor.T, x_tensor)) [[20 26] [26 35]] tensor([[20., 26.], [26., 35.]]) But the element-wise multiplication does not work\ntry: x_tensor.dot(x_tensor) except Exception as e: print(e) 1D tensors expected, but got 2D and 2D tensors Variables Variable in torch is to build a computational graph, but this graph is dynamic compared with a static graph in Tensorflow or Theano. So torch does not have placeholder, torch can just pass variable to the computational graph.\n# build a variable, usually for compute gradients x_variable = Variable(x_tensor, requires_grad=True) x_variable tensor([[0., 1.], [2., 3.], [4., 5.]], requires_grad=True) Until now the tensor and variable seem the same. However, the variable is a part of the graph, it\u0026rsquo;s a part of the auto-gradient.\nSuppose we are interested in:\n$$ y = \\text{mean} (x_1^2) = \\frac{1}{6} x^2 $$\ny = torch.mean(x_variable*x_variable) print(y) tensor(9.1667, grad_fn=\u0026lt;MeanBackward0\u0026gt;) We can compute the gradient by backpropagation\n$$ \\nabla y(x) = \\frac{2}{3} x $$\ni.e. if we call the backward method on our outcome y, we see that the gradient of our variable x gets updated.\nprint(x_variable.grad) None y.backward() print(x_variable.grad) tensor([[0.0000, 0.3333], [0.6667, 1.0000], [1.3333, 1.6667]]) However, its value has not changed.\nprint(x_variable) tensor([[0., 1.], [2., 3.], [4., 5.]], requires_grad=True) We can also access the tensor part of the variable alone by calling the data method.\nprint(x_variable.data) tensor([[0., 1.], [2., 3.], [4., 5.]]) Activation Function The main advantage of neural networks is that they introduce non-linearities among the layers. The standard non-linear function\nReLu Sigmoid TanH Softmax # X grid x_grid = torch.linspace(-5, 5, 200) # x data (tensor), shape=(100, 1) x_grid = Variable(x_grid) x_grid_np = x_grid.data.numpy() # numpy array for plotting # Activation functions y_relu = torch.relu(x_grid).data.numpy() y_sigmoid = torch.sigmoid(x_grid).data.numpy() y_tanh = torch.tanh(x_grid).data.numpy() y_softmax = torch.softmax(x_grid, dim=0).data.numpy() # New figure 1 def make_new_figure_1(): # Init figure fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2,2, figsize=(8,6)) fig.suptitle('Activation Functions') # Relu ax1.plot(x_grid_np, y_relu, c='red', label='relu') ax1.set_ylim((-1, 6)); ax1.legend() # Sigmoid ax2.plot(x_grid_np, y_sigmoid, c='red', label='sigmoid') ax2.set_ylim((-0.2, 1.2)); ax2.legend() # Tanh ax3.plot(x_grid_np, y_tanh, c='red', label='tanh') ax3.set_ylim((-1.2, 1.2)); ax3.legend() # Softmax ax4.plot(x_grid_np, y_softmax, c='red', label='softmax') ax4.set_ylim((-0.01, 0.06)); ax4.legend(); Let\u0026rsquo;s compare the different activation functions.\nmake_new_figure_1() ReLu is very popular since it\u0026rsquo;s non-linear.\n8.3 Optimization and Gradient Descent Gradient Descent Gradient descent works as follows:\nInitialize the parameters Compute the Loss Compute the Gradients Update the Parameters Repeat (1)-(3) until convergence Gradient Descent in Linear Regression In order to understand how are NN optimized, we start with a linear regression example. Remember that linear regression can be interpreted as the simplest possible NN.\nWe generate the following data:\n$$ y = 1 + 2 x - 3 x^2 + \\varepsilon $$\nwith $x \\sim N(0,1)$ and $\\varepsilon \\sim N(0,0.1)$\n# Data Generation np.random.seed(42) N = 100 x = np.sort(np.random.rand(N, 1), axis=0) e = .1*np.random.randn(N, 1) y_true = 1 + 2*x - 3*x**2 y = y_true + e Let\u0026rsquo;s plot the data.\n# New figure 2 def make_new_figure_2(): # Init fig, ax = plt.subplots(figsize=(8,6)) fig.suptitle('Activation Functions') # Scatter ax.scatter(x,y); ax.plot(x,y_true,color='orange'); ax.set_xlabel('X'); ax.set_ylabel('Y'); ax.legend(['y true','y']); make_new_figure_2() Suppose we try to fit the data with a linear model\n$$ y = a + b x $$\nWe proceed iteratively by gradient descent. Our objective function is the Mean Squared Error.\nAlgorithm\nTake an initial guess of the parameters $$ a = a_0 \\ b = b_0 $$\nCompute the Mean Squared Error $$ \\begin{array} \\text{MSE} \u0026amp;= \\frac{1}{N} \\sum_{i=1}^{N}\\left(y_{i}-\\hat{y}{i}\\right)^{2} \\ \u0026amp;= \\frac{1}{N} \\sum{i=1}^{N}\\left(y_{i}-a-b x_{i}\\right)^{2} \\end{array} $$\nCompute its derivative $$ \\begin{array}{l} \\frac{\\partial M S E}{\\partial a}=\\frac{\\partial M S E}{\\partial \\hat{y}{i}} \\cdot \\frac{\\partial \\hat{y}{i}}{\\partial a}=\\frac{1}{N} \\sum_{i=1}^{N} 2\\left(y_{i}-a-b x_{i}\\right) \\cdot(-1)=-2 \\frac{1}{N} \\sum_{i=1}^{N}\\left(y_{i}-\\hat{y}{i}\\right) \\ \\frac{\\partial M S E}{\\partial b}=\\frac{\\partial M S E}{\\partial \\hat{y}{i}} \\cdot \\frac{\\partial \\hat{y}{i}}{\\partial b}=\\frac{1}{N} \\sum{i=1}^{N} 2\\left(y_{i}-a-b x_{i}\\right) \\cdot\\left(-x_{i}\\right)=-2 \\frac{1}{N} \\sum_{i=1}^{N} x_{i}\\left(y_{i}-\\hat{y}_{i}\\right) \\end{array} $$\nUpdate the parameters $$ \\begin{array}{l} a=a-\\eta \\frac{\\partial M S E}{\\partial a} \\ b=b-\\eta \\frac{\\partial M S E}{\\partial b} \\end{array} $$\nWhere $\\eta$ is the learning rate. A lower learning rate makes learning more stable but slower.\nRepeat (1)-(3) $T$ times, where the number of total iterations $T$ is called epochs.\nWe start by taking a random guess of $\\alpha$ and $\\beta$.\n# Initializes parameters \u0026quot;a\u0026quot; and \u0026quot;b\u0026quot; randomly np.random.seed(42) a = np.random.randn(1) b = np.random.randn(1) print(a, b) [0.49671415] [-0.1382643] # Plot gradient def gradient_plot(x, y, y_hat, y_true, EPOCHS, losses): clear_output(wait=True) fig, (ax1, ax2) = plt.subplots(1,2, figsize=(12,5)) # First figure ax1.clear() ax1.scatter(x, y) ax1.plot(x, y_true, 'orange') ax1.plot(x, y_hat, 'r-') ax1.set_title('Data and Fit') ax1.legend(['True', 'Predicted']) # Second figure ax2.clear() ax2.plot(range(len(losses)), losses, color='g') ax2.set_xlim(0,EPOCHS); ax2.set_ylim(0,1.1*np.max(losses)) ax2.set_title('True MSE = %.4f' % losses[-1]) # Plot plt.show(); We set the learning rate $\\eta = 0.1$ and the number of epochs $T=200$\n# parameters LR = 0.1 # learning rate EPOCHS = 200 # number of epochs We can now plot the training and the result.\n# New figure 3 def make_new_figure_3(a, b): # Init losses = [] # train for t in range(EPOCHS): # compute loss y_hat = a + b * x error = (y - y_hat) loss = (error**2).mean() # compute gradient a_grad = -2 * error.mean() b_grad = -2 * (x * error).mean() # update parameters a -= LR * a_grad b -= LR * b_grad # plot losses += [loss] if (t+1) % (EPOCHS/25) == 0: # print 25 times gradient_plot(x, y, y_hat, y_true, EPOCHS, losses) print(a, b) return a, b a_fit, b_fit = make_new_figure_3(a, b) [1.40589939] [-0.83739496] Sanity Check: do we get the same results as our gradient descent?\n# OLS estimates ols = LinearRegression() ols.fit(x, y) print(ols.intercept_, ols.coef_[0]) [1.4345303] [-0.89397853] Close enough!\nLet\u0026rsquo;s plot both lines in the graph.\n# New figure 4 def make_new_figure_4(): # Init fig, ax = plt.subplots(figsize=(8,6)) # Scatter ax.plot(x,y_true,color='orange'); ax.plot(x,a_fit + b_fit*x,color='red'); ax.plot(x,ols.predict(x),color='green'); ax.legend(['y true','y gd', 'y ols']) ax.scatter(x,y); ax.set_xlabel('X'); ax.set_ylabel('Y'); ax.set_title(\u0026quot;Data\u0026quot;); make_new_figure_4() Now we are going to do exactly the same but with pytorch.\nAutograd Autograd is PyTorch’s automatic differentiation package. Thanks to it, we don’t need to worry about partial derivatives, chain rule or anything like it.\nSo, how do we tell PyTorch to do its thing and compute all gradients? That’s what backward() is good for. § Do you remember the starting point for computing the gradients? It was the loss, as we computed its partial derivatives w.r.t. our parameters. Hence, we need to invoke the backward() method from the corresponding Python variable, like, loss.backward().\nWhat about the actual values of the gradients? We can inspect them by looking at the grad attribute of a tensor.\nIf you check the method’s documentation, it clearly states that gradients are accumulated. So, every time we use the gradients to update the parameters, we need to zero the gradients afterwards. And that’s what zero_() is good for.\nWhat does the underscore (_) at the end of the method name mean? Do you remember? If not, scroll back to the previous section and find out.\nSo, let’s ditch the manual computation of gradients and use both backward() and zero_() methods instead.\nFirst, we convert our variables to tensors.\n# Convert data to tensors x_tensor = torch.from_numpy(x).float().to('cpu') y_tensor = torch.from_numpy(y).float().to('cpu') print(type(x), type(x_tensor)) \u0026lt;class 'numpy.ndarray'\u0026gt; \u0026lt;class 'torch.Tensor'\u0026gt; We take the initial parameters guess\n# initial parameter guess torch.manual_seed(42) a = torch.randn(1, requires_grad=True, dtype=torch.float, device='cpu') b = torch.randn(1, requires_grad=True, dtype=torch.float, device='cpu') Now we are ready to fit the model.\n# New figure 5 def make_new_figure_5(a, b): # Init losses = [] # parameters LR = 0.1 EPOCHS = 200 # train for t in range(EPOCHS): # compute loss y_hat = a + b * x_tensor error = y_tensor - y_hat loss = (error ** 2).mean() # compute gradient loss.backward() # update parameters with torch.no_grad(): a -= LR * a.grad b -= LR * b.grad # clear gradients a.grad.zero_() b.grad.zero_() # Plot losses += [((y_true - y_hat.detach().numpy())**2).mean()] if (t+1) % (EPOCHS/25) == 0: # print 25 times gradient_plot(x, y, y_hat.data.numpy(), y_true, EPOCHS, losses) print(a, b) make_new_figure_5(a, b) tensor([1.3978], requires_grad=True) tensor([-0.8214], requires_grad=True) Optimizer So far, we’ve been manually updating the parameters using the computed gradients. That’s probably fine for two parameters… but what if we had a whole lot of them?! We use one of PyTorch’s optimizers.\nAn optimizer takes the following arguments:\nthe parameters we want to update he learning rate we want to use (possibly many other hyper-parameters) Moreover, we can now call the function zero_grad() to automatically update the parameters. In particular, we will need to perform the following steps at each iteration:\nClear the parameters: optimizer.zero_grad() Compute the gradient: loss.backward() Update the parameters: optimizer.step() In the code below, we create a Stochastic Gradient Descent (SGD) optimizer to update our parameters $a$ and $b$.\n# Init parameters torch.manual_seed(42) a = torch.randn(1, requires_grad=True, dtype=torch.float, device='cpu') b = torch.randn(1, requires_grad=True, dtype=torch.float, device='cpu') # Defines a SGD optimizer to update the parameters optimizer = torch.optim.SGD([a, b], lr=LR) print(optimizer) SGD ( Parameter Group 0 dampening: 0 lr: 0.1 momentum: 0 nesterov: False weight_decay: 0 ) We can also define a default loss function so that we don\u0026rsquo;t have to compute it by hand. We are going to use the MSE loss function.\n# Define a loss function loss_func = torch.nn.MSELoss() print(loss_func) MSELoss() Let\u0026rsquo;s plot the estimator and the MSE.\n# New figure 6 def make_new_figure_6(a, b): # parameters EPOCHS = 200 # init losses = [] # train for t in range(EPOCHS): # compute loss y_hat = a + b * x_tensor error = y_tensor - y_hat loss = (error ** 2).mean() # update parameters optimizer.zero_grad() # clear gradients for next train loss.backward() # backpropagation, compute gradients optimizer.step() # apply gradients, update parameters # Plot losses += [((y_true - y_hat.detach().numpy())**2).mean()] if (t+1) % (EPOCHS/25) == 0: gradient_plot(x, y, y_hat.data.numpy(), y_true, EPOCHS, losses) print(a, b) make_new_figure_6(a, b) tensor([1.3978], requires_grad=True) tensor([-0.8214], requires_grad=True) Building a NN In our model, we manually created two parameters to perform a linear regression. Let’s use PyTorch’s Sequential module to create our neural network.\nWe first want to build the linear regression framework\n$$ y = a + b x $$\nWhich essentially is a network with\n1 input no hidden layer no activation function 1 output Let\u0026rsquo;s build the simplest possible neural network with PyTorch.\n# Simplest possible neural network linear_net = torch.nn.Sequential( torch.nn.Linear(1, 1) ) print(linear_net) Sequential( (0): Linear(in_features=1, out_features=1, bias=True) ) Now, if we call the parameters() method of this model, PyTorch will figure the parameters of its attributes in a recursive way.\n[*linear_net.parameters()] [Parameter containing: tensor([[-0.2191]], requires_grad=True), Parameter containing: tensor([0.2018], requires_grad=True)] We can now define the definitive training function.\ndef train_NN(x, y, y_true, net, optimizer, loss_func, EPOCHS): # transform variables x_tensor = torch.from_numpy(x).float().to('cpu') y_tensor = torch.from_numpy(y).float().to('cpu') # init losses = [] # train for t in range(EPOCHS): # compute loss y_hat = net(x_tensor) loss = loss_func(y_hat, y_tensor) # update parameters optimizer.zero_grad() # clear gradients for next train loss.backward() # backpropagation, compute gradients optimizer.step() # apply gradients, update parameters # plot losses += [((y_true - y_hat.detach().numpy())**2).mean()] if (t+1) % (EPOCHS/25) == 0: # print 25 times gradient_plot(x, y, y_hat.data.numpy(), y_true, EPOCHS, losses) Now we are ready to train our neural network.\noptimizer = torch.optim.SGD(linear_net.parameters(), lr=LR) # train train_NN(x, y, y_true, linear_net, optimizer, loss_func, EPOCHS) We now define a more complicated NN. In particular we, build a neural network with\n1 input 1 hidden layer with 10 neurons and Relu activation function 1 output layer # Relu Net relu_net = torch.nn.Sequential( torch.nn.Linear(1, 10), torch.nn.ReLU(), torch.nn.Linear(10, 1) ) print(relu_net) Sequential( (0): Linear(in_features=1, out_features=10, bias=True) (1): ReLU() (2): Linear(in_features=10, out_features=1, bias=True) ) This network has much more parameters.\n[*relu_net.parameters()] [Parameter containing: tensor([[-0.4869], [ 0.5873], [ 0.8815], [-0.7336], [ 0.8692], [ 0.1872], [ 0.7388], [ 0.1354], [ 0.4822], [-0.1412]], requires_grad=True), Parameter containing: tensor([ 0.7709, 0.1478, -0.4668, 0.2549, -0.4607, -0.1173, -0.4062, 0.6634, -0.7894, -0.4610], requires_grad=True), Parameter containing: tensor([[-0.0893, -0.1901, 0.0298, -0.3123, 0.2856, -0.2686, 0.2441, 0.0526, -0.1027, 0.1954]], requires_grad=True), Parameter containing: tensor([0.0493], requires_grad=True)] We are again using Stochastic Gradient Descent (SGD) as optimization algorithm and Mean Squared Error (MSELoss) as objective function.\n# parameters LR = 0.1 EPOCHS = 1000 # optimizer and loss function optimizer = torch.optim.SGD(relu_net.parameters(), lr=LR) loss_func = torch.nn.MSELoss() # Train train_NN(x, y, y_true, relu_net, optimizer, loss_func, EPOCHS) It seems that we can use fewer nodes to get the same result.\nLet\u0026rsquo;s make a smallet network.\n# Relu Net relu_net2 = torch.nn.Sequential( torch.nn.Linear(1, 4), torch.nn.ReLU(), torch.nn.Linear(4, 1) ) And train it.\n# parameters LR = 0.1 EPOCHS = 1000 # optimizer and loss function optimizer = torch.optim.SGD(relu_net2.parameters(), lr=LR) loss_func = torch.nn.MSELoss() # Train train_NN(x, y, y_true, relu_net2, optimizer, loss_func, EPOCHS) We can try different activation functions.\nFor example the tangent.\n# TanH Net tanh_net = torch.nn.Sequential( torch.nn.Linear(1, 10), torch.nn.Tanh(), torch.nn.Linear(10, 1) ) # parameters LR = 0.2 EPOCHS = 1000 # optimizer and loss function optimizer = torch.optim.SGD(tanh_net.parameters(), lr=LR) loss_func = torch.nn.MSELoss() # train train_NN(x, y, y_true, tanh_net, optimizer, loss_func, EPOCHS) Loss functions So far we have used the Stochastic as loss function.\nNotice that nn.MSELoss actually creates a loss function for us — it is NOT the loss function itself. Moreover, you can specify a reduction method to be applied, that is, how do you want to aggregate the results for individual points — you can average them (reduction=mean) or simply sum them up (reduction=sum).\nWe are now going to use different ones.\n# parameters LR = 0.1 EPOCHS = 25 # nets n = torch.nn.Sequential(torch.nn.Linear(1, 10), torch.nn.ReLU(), torch.nn.Linear(10, 1)) nets = [n,n,n,n] # optimizers optimizers = [torch.optim.SGD(n.parameters(), lr=LR) for n in nets] # different loss functions loss_MSE = torch.nn.MSELoss() loss_L1 = torch.nn.L1Loss() loss_NLL = torch.nn.NLLLoss() loss_KLD = torch.nn.KLDivLoss() loss_funcs = [loss_MSE, loss_L1, loss_NLL, loss_KLD] This is the description of the loss functions:\nMSELoss: Creates a criterion that measures the mean squared error (squared L2 norm) between each element in the input $x$ and target $y$.\nL1Loss: Creates a criterion that measures the mean absolute error (MAE) between each element in the input $x$ and target $y$.\nNLLLoss: The negative log likelihood loss.\nKLDivLoss: The Kullback-Leibler divergence loss measure\n# Train multiple nets def train_nets(x, y, y_true, nets, optimizers, loss_funcs, labels, EPOCHS): # Put dateset into torch dataset x_tensor = torch.from_numpy(x).float().to('cpu') y_tensor = torch.from_numpy(y).float().to('cpu') torch_dataset = Data.TensorDataset(x_tensor, y_tensor) # Init losses = np.zeros((0,4)) # Train for epoch in range(EPOCHS): # for each epoch losses = np.vstack((losses, np.zeros((1,4)))) for k, net, opt, lf in zip(range(4), nets, optimizers, loss_funcs): y_hat = net(x_tensor) # get output for every net loss = loss_func(y_hat, y_tensor) # compute loss for every net opt.zero_grad() # clear gradients for next train loss.backward() # backpropagation, compute gradients opt.step() # apply gradients losses[-1,k] = ((y_true - y_hat.detach().numpy())**2).mean() plot_losses(losses, labels, EPOCHS) # Plot losses def plot_losses(losses, labels, EPOCHS): clear_output(wait=True) fig, ax = plt.subplots(1,1, figsize=(10,6)) # Plot ax.clear() ax.plot(range(len(losses)), losses) ax.set_xlim(0,EPOCHS-1); ax.set_ylim(0,1.1*np.max(losses)) ax.set_title('Compare Losses'); ax.set_ylabel('True MSE') legend_txt = ['%s=%.4f' % (label, loss) for label,loss in zip(labels, losses[-1,:])] ax.legend(legend_txt) # Shot plt.show(); Let\u0026rsquo;s compare them.\n# Train labels = ['MSE', 'L1', 'LogL', 'KLdiv'] train_nets(x, y, y_true, nets, optimizers, loss_funcs, labels, EPOCHS) In this very simple case, all loss functions are very similar.\nOptimizers So far we have used the Stochastic Gradient Descent to fit the neural network. We are now going to use different ones.\nThis is the description of the optimizers:\nSGD: Implements stochastic gradient descent (optionally with momentum).\nMomentum: Nesterov momentum is based on the formula from On the importance of initialization and momentum in deep learning.\nRMSprop: Proposed by G. Hinton in his course. The centered version first appears in Generating Sequences With Recurrent Neural Networks. The implementation here takes the square root of the gradient average before adding epsilon (note that TensorFlow interchanges these two operations). The effective learning rate is thus $\\frac{\\alpha}{\\sqrt{v} + \\epsilon}$ where $\\alpha$ is the scheduled learning rate and $v$ is the weighted moving average of the squared gradient.\nAdam: Proposed in Adam: A Method for Stochastic Optimization. The implementation of the L2 penalty follows changes proposed in Decoupled Weight Decay Regularization.\n# parameters LR = 0.1 EPOCHS = 25 # nets n = torch.nn.Sequential(torch.nn.Linear(1, 10), torch.nn.ReLU(), torch.nn.Linear(10, 1)) nets = [n,n,n,n] # different optimizers opt_SGD = torch.optim.SGD(nets[0].parameters(), lr=LR) opt_Momentum = torch.optim.SGD(nets[1].parameters(), lr=LR, momentum=0.8) opt_RMSprop = torch.optim.RMSprop(nets[2].parameters(), lr=LR, alpha=0.9) opt_Adam = torch.optim.Adam(nets[3].parameters(), lr=LR, betas=(0.9, 0.99)) optimizers = [opt_SGD, opt_Momentum, opt_RMSprop, opt_Adam] # loss functions l = torch.nn.MSELoss() loss_funcs = [l,l,l,l] Let\u0026rsquo;s prot the loss functions over training, for different optimizers.\n# train labels = ['SGD', 'Momentum', 'RMSprop', 'Adam'] train_nets(x, y, y_true, nets, optimizers, loss_funcs, labels, EPOCHS) Training on batch Until now, we have used the whole training data at every training step. It has been batch gradient descent all along.\nThis is fine for our ridiculously small dataset, sure, but if we want to go serious about all this, we must use mini-batch gradient descent. Thus, we need mini-batches. Thus, we need to slice our dataset accordingly. Do you want to do it manually?! Me neither!\nSo we use PyTorch’s DataLoader class for this job. We tell it which dataset to use (the one we just built in the previous section), the desired mini-batch size and if we’d like to shuffle it or not. That’s it!\nOur loader will behave like an iterator, so we can loop over it and fetch a different mini-batch every time.\n# Init data x_tensor = torch.from_numpy(x).float().to('cpu') y_tensor = torch.from_numpy(y).float().to('cpu') torch_dataset = Data.TensorDataset(x_tensor, y_tensor) # Build DataLoader BATCH_SIZE = 25 loader = Data.DataLoader( dataset=torch_dataset, # torch TensorDataset format batch_size=BATCH_SIZE, # mini batch size shuffle=True, # random shuffle for training ) Let\u0026rsquo;s try using sub-samples of dimension BATCH_SIZE = 25.\ndef train_NN_batch(loader, y_true, net, optimizer, loss_func, EPOCHS): # init losses = [] # train for t in range(EPOCHS): # train entire dataset 3 times for step, (batch_x, batch_y) in enumerate(loader): # compute loss y_hat = net(batch_x) loss = loss_func(y_hat, batch_y) # update parameters optimizer.zero_grad() # clear gradients for next train loss.backward() # backpropagation, compute gradients optimizer.step() # apply gradients # plt every epoch y_hat = net(x_tensor) losses += [((y_true - y_hat.detach().numpy())**2).mean()] if (t+1) % (EPOCHS/25) == 0: gradient_plot(x, y, y_hat.data.numpy(), y_true, EPOCHS, losses) # parameters LR = 0.1 EPOCHS = 1000 net = torch.nn.Sequential(torch.nn.Linear(1, 10), torch.nn.ReLU(), torch.nn.Linear(10, 1)) optimizer = torch.optim.SGD(net.parameters(), lr=LR) loss_func = torch.nn.MSELoss() # Train train_NN_batch(loader, y_true, net, optimizer, loss_func, EPOCHS) Two things are different now: not only we have an inner loop to load each and every mini-batch from our DataLoader but, more importantly, we are now sending only one mini-batch to the device.\nFor bigger datasets, loading data sample by sample (into a CPU tensor) using Dataset’s __get_item__ and then sending all samples that belong to the same mini-batch at once to your GPU (device) is the way to go in order to make the best use of your graphics card’s RAM.\nMoreover, if you have many GPUs to train your model on, it is best to keep your dataset “agnostic” and assign the batches to different GPUs during training.\n8.4 Advanced Topics Issues Starting Values Usually starting values for weights are chosen to be random values near zero. Hence the model starts out nearly linear, and becomes nonlinear as the weights increase.\nOverfitting In early developments of neural networks, either by design or by accident, an early stopping rule was used to avoid overfitting.\nA more explicit method for regularization is weight decay.\nScaling of the Inputs Since the scaling of the inputs determines the effective scaling of the weights in the bottom layer, it can have a large effect on the quality of the final solution. At the outset it is best to standardize all inputs to have mean zero and standard deviation one.\nNumber of Hidden Units and Layers Generally speaking it is better to have too many hidden units than too few. With too few hidden units, the model might not have enough flexibility to capture the nonlinearities in the data; with too many hidden units, the extra weights can be shrunk toward zero if appropriate regularization is used.\nChoice of the number of hidden layers is guided by background knowledge and experimentation. Each layer extracts features of the input for regression or classification. Use of multiple hidden layers allows construction of hierarchical features at different levels of resolution.\nYou can get an intuition on the role of hidden layers here: https://playground.tensorflow.org/\nMultiple Minima The error function R(θ) is nonconvex, possessing many local minima. One approach is to use the average predictions over the collection of networks as the final prediction. Another approach is via bagging.\nDeep Neural Networks and Deep Learning Deep Neural Networks are just Neural Networks with more than one hidden layer.\nConvolutional Neural Nets Convolutional Neural Nets are often applied when dealing with image/video data. They are usually coded with each feature being a pixel and its value is the pixel color (3 dimensional RGB array).\nVideos and images have 2 main characteristics:\nhave lots of features \u0026ldquo;close\u0026rdquo; features are often similar Convolutional Neural Nets exploit the second characteristic to alleviate the computational problems arising from the first. They do it by constructing a first layer that does not build on evey feature but only on adjacent ones.\nIn this way, most of the information is preserved, on a lower dimensional representation.\nRecurrent Neural Nets Recurrent Neural Networks are often applied in contexts in which the data generating process is dynamic. The most important example is Natural Language Processing. The idea is that you want to make predictions \u0026ldquo;live\u0026rdquo; as data comes in. Moreover, the order of the data is relevant, so that you also what to keep track of what the model has learned so far.\nWhile RNNs learn similarly while training, in addition, they remember things learnt from prior input(s) while generating output(s). It’s part of the network. RNNs can take one or more input vectors and produce one or more output vectors and the output(s) are influenced not just by weights applied on inputs like a regular NN, but also by a “hidden” state vector representing the context based on prior input(s)/output(s). So, the same input could produce a different output depending on previous inputs in the series.\nGrafically:\nIn summary, in a vanilla neural network, a fixed size input vector is transformed into a fixed size output vector. Such a network becomes “recurrent” when you repeatedly apply the transformations to a series of given input and produce a series of output vectors.\nBidirectional RNN Sometimes it’s not just about learning from the past to predict the future, but we also need to look into the future to fix the past. In speech recognition and handwriting recognition tasks, where there could be considerable ambiguity given just one part of the input, we often need to know what’s coming next to better understand the context and detect the present.\nThis does introduce the obvious challenge of how much into the future we need to look into, because if we have to wait to see all inputs then the entire operation will become costly.\nRecursive Neural Netw A recurrent neural network parses the inputs in a sequential fashion. A recursive neural network is similar to the extent that the transitions are repeatedly applied to inputs, but not necessarily in a sequential fashion. Recursive Neural Networks are a more general form of Recurrent Neural Networks. It can operate on any hierarchical tree structure. Parsing through input nodes, combining child nodes into parent nodes and combining them with other child/parent nodes to create a tree like structure. Recurrent Neural Networks do the same, but the structure there is strictly linear. i.e. weights are applied on the first input node, then the second, third and so on.\nBut this raises questions pertaining to the structure. How do we decide that? If the structure is fixed like in Recurrent Neural Networks then the process of training, backprop etc makes sense in that they are similar to a regular neural network. But if the structure isn’t fixed, is that learnt as well?\n","date":1646784000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1646784000,"objectID":"9d0e7e97b8f20fca1686acda2d5705cf","permalink":"https://matteocourthoud.github.io/course/ml-econ/08_neuralnets/","publishdate":"2022-03-09T00:00:00Z","relpermalink":"/course/ml-econ/08_neuralnets/","section":"course","summary":"# Remove warnings import warnings warnings.filterwarnings('ignore') # Import everything import pandas as pd import numpy as np import seaborn as sns import statsmodels.api as sm import copy import torch import torch.","tags":null,"title":"Neural Networks","type":"book"},{"authors":null,"categories":null,"content":"Introduction Intro Setting: agents making strategic decisions (new) in dynamic environments.\nEntry and exit: Collard-Wexler (2013) Sunk costs: Ryan (2012) Innovation: Goettler and Gordon (2011) (or whatever changes in response to investment) Exploitation of natural resources: Huang and Smith (2014) Durable goods: Esteban and Shum (2007) Lit review: forthcoming IO Handbook chapter Aguirregabiria, Collard-Wexler, and Ryan (2021)\nSingle- vs Multi-Agent Typically in IO we study agents in strategic environments. Complicated in dynamic environments.\nCurse of dimensionality Single agent: need to track what the agent sees ($k$ states) Multi-agent: need to keep track what every agent sees ($k^J$states) Difference exponential in the number of agents Expectations Need not only to keep track of how the environment evolves … but also of how other players act Equilibrium Because of the strategic interaction, the Bellman equation is not a contraction anymore Equilibrium existence? Equilibrium uniqueness? Plan We will cover first the estimation and then the computation of dynamic games\nWeird… Standard estimation method: Bajari, Benkard, and Levin (2007) Does not require to solve the model Indeed, that’s the advantage of the method Disadvantages: still need to solve the model for counterfactuals So we’ll cover computation afterwards Last: bridge between Structural IO and Artificial Intelligence\nDifferent objectives but similar methods Dynamic tools niche in IO but at the core of AI Bajari, Benkard, Levin (2008) Model Stylized version of Ericson and Pakes (1995) (no entry/exit)\n$J$ firms (products) indexed by $j \\in \\lbrace 1, \u0026hellip;, J \\rbrace$\nTime $t$ is dicrete, horizon is infinite\nStates $s_{jt} \\in \\lbrace 1, \u0026hellip; \\bar s \\rbrace$: quality of product $j$ in period $t$\nActions $a_{jt} \\in \\mathbb R^+$: investment decision of firm $j$ in period $t$\nStatic payoffs $$ \\pi_j (s_{jt}, \\boldsymbol s_{-jt}, a_{jt}; \\theta^\\pi) $$ where\n$\\boldsymbol s_{-it}$: state vector of all other firms in period $t$ $\\theta^\\pi$: parameters that govern static profits Note: if we micro-fund $\\pi(\\cdot)$ , e.g. with some demand and supply model, we have 2 strategic decisions: prices (static) and investment (dynamic).\nModel (2) State transitions $$ \\boldsymbol s_{t+1} = f(\\boldsymbol s_t, \\boldsymbol a_t, \\boldsymbol \\epsilon_t; \\theta^f) $$ where\n$\\boldsymbol a_t$: vector of actions of all firm $\\boldsymbol \\epsilon_t$: vector of idiosyncratic shocks $\\theta^f$: parameters that govern state transitions Objective function: firms maximize expected discounted future profits $$ \\max_{\\boldsymbol a} \\ \\mathbb E_t \\left[ \\sum_{\\tau=0}^\\infty \\beta^{\\tau} \\pi_{j, t+\\tau} (\\theta^\\pi) \\right] $$\nValue Function The value function of firm $j$ at time $t$ in state $\\boldsymbol s_{t}$, under a set of strategy functions $\\boldsymbol P$ (one for each firm) is $$ V^{\\boldsymbol P_{-j}}{j} (\\mathbf{s}{t}) = \\max_{a_{jt} \\in \\mathcal{A}j \\left(\\mathbf{s}{t}\\right)} \\Bigg\\lbrace \\pi_{j}^{\\boldsymbol P_{-j}} (a_{jt}, \\mathbf{s}{t} ; \\theta^\\pi ) + \\beta \\mathbb E{\\boldsymbol s_{t+1}} \\Big[ V_{j}^{\\boldsymbol P_{-j}} \\left(\\mathbf{s}{t+1}\\right) \\ \\Big| \\ a{jt}, \\boldsymbol s_{t} ; \\theta^f \\Big] \\Bigg\\rbrace $$ where\n$\\pi_{j}^{\\boldsymbol P_{-j}} (a_{jt}, \\mathbf{s}{t} ; \\theta^\\pi )$ are the static profits of firm $j$ given action $a{jt}$ and policy functions $\\boldsymbol P_{-j}$ for all firms a part from $j$\nThe expecation $\\mathbb E$ is taken with respect to the conditional transition probabilities $f^{\\boldsymbol P_{-j}} (\\mathbf{s}{t+1} | \\mathbf{s}{t}, a_{jt} ; \\theta^f)$\nEquilibrium Equillibrium notion: Markow Perfect Equilibrium (Maskin and Tirole 1988)\nAssumption: players’ strategies at period $t$ are functions only of payoff-relevant state variables at the same period Definition: a set of $J$ value and policy functions, $\\boldsymbol V$ and $\\boldsymbol P$ such that each firm maximizes its value function $V_j$ given the policy function of every other firm $\\boldsymbol P_{-j}$ What is it basically?\nNash Equilibrium in the policy functions What are we ruling out? Strategies that depend on longer histories E.g. “has anyone ever cheated in a cartel?” Estimation We want to estimate 2 sets of parameters:\n$\\theta^\\pi$: parameterizes period profit function $\\pi(\\cdot)$ $\\theta^f$: parameterizes state transition function $f(\\cdot)$ Generally 2 approaches\nFull solution Impractical (we’ll see more details later) Rely on some sort of Hotz and Miller (1993) CCP inversion Aguirregabiria and Mira (2007) Bajari, Benkard, and Levin (2007) Pakes, Ostrovsky, and Berry (2007) Pesendorfer and Schmidt-Dengler (2008) BBL Overview Bajari, Benkard, and Levin (2007) plan\nEstimate transition probabilities and conditional choice probabilities from the data Use them to simulate the expected value function, given a set of parameters Use optimality of estimated choices to pin down static profit parameters I.e. repeat (2) for alternative strategies By definition suboptimal Estimating equation: values implied by observed strategies should be higher than values implied by alternative strategies BBL: First Stage Estimate the transition probabilities $f ( \\cdot | a_{jt}, \\boldsymbol s_t; \\hat \\theta^f )$ I.e. what is the observed frequency of any state-to-state transition? For any given action of firm $j$ … and conditional choice probabilities $\\hat P_j(\\cdot | \\boldsymbol s_t)$ I.e. what is the probability of each action, for each firm $j$ in each state $\\boldsymbol s$ Can be done non-parametrically i.e. just observe frequencies Conditional on having enough data Note: need to estimate transitions, conditional on each state and action Problem with many states and actions, but especially with many players Curse of dimensionality Number of states increases exponentially in number of players Important: parametric assumptions would contradict the model for the estimation of value/policy functions\nBBL: Second Stage First step: from transitions $f(\\hat \\theta^f)$ and CCPs $\\boldsymbol{\\hat P}$ to values\nWe can use transitions and CCPs to simulate histories (of length $\\tilde T$)\nof states $\\lbrace \\boldsymbol{\\tilde{s}{\\tau}} \\rbrace{\\tau = 1}^{\\tilde T}$ and actions $\\lbrace \\boldsymbol{\\tilde{a}{\\tau}} \\rbrace{\\tau = 1}^{\\tilde T}$ Given a parameter value $\\tilde \\theta^\\pi$, we can compute static payoffs: $\\pi_{j}^{\\boldsymbol {\\hat{P}{-j}}} \\left( \\tilde a{j\\tau}, \\boldsymbol{\\tilde s}_{\\tau} ; \\tilde \\theta^\\pi \\right)$\nSimulated history + static payoffs = simulated value function $$ {V}{j}^{\\boldsymbol {\\hat{P}}} \\left(\\boldsymbol{s}{t} ; \\tilde \\theta^\\pi \\right) = \\sum_{\\tau=0}^{\\tilde T} \\beta^{\\tau} \\pi_{j}^{\\boldsymbol {\\hat{P}{-j}}} \\left( \\tilde a{j\\tau}, \\boldsymbol{\\tilde s}_{\\tau} ; \\tilde \\theta^\\pi \\right) $$\nWe can average over many, e.g. $R$, simulated value functions to get an expected value function $$ {V}{j}^{\\boldsymbol {\\hat{P}}, R} \\left( \\boldsymbol{s}{t} ; \\tilde \\theta^\\pi \\right) = \\frac{1}{R} \\sum_{r=0}^{R}\\Bigg( \\sum_{\\tau=0}^{\\tilde T} \\beta^{\\tau} \\pi_{j}^{\\boldsymbol {\\hat{P}{-j}}} \\left(\\tilde a^{(r)}{j\\tau}, \\boldsymbol{\\tilde s}^{(r)}_{\\tau} ; \\tilde \\theta^\\pi \\right) \\Bigg) $$\nIn practice, for a parameter value $\\tilde \\theta^\\pi$ For $r = 1, \u0026hellip;, R$ simulations do:\nInitialize firms value to zero Fot $\\tau=0, \u0026hellip;, \\tilde T$ do For each state in $\\boldsymbol{\\tilde s}^{(r)}_{\\tau}$ do: Use $\\boldsymbol{\\hat P}$ to draw a vector of firm actions $\\boldsymbol{\\tilde a}^{(r)}_{\\tau}$ For each firm $j = 1, \u0026hellip;, J$ do: Compute static profits $\\pi_{j}^{\\boldsymbol {\\hat{P}{-j}}} \\left(\\tilde a^{(r)}{j\\tau}, \\boldsymbol{\\tilde s}^{(r)}_{\\tau} ; \\tilde \\theta^\\pi \\right)$ Add discounted profits $\\beta^{\\tau} \\pi_{j}^{\\boldsymbol {\\hat{P}{-j}}} \\left(\\tilde a^{(r)}{j\\tau}, \\boldsymbol{\\tilde s}^{(r)}_{\\tau} ; \\tilde \\theta^\\pi \\right)$ to the value function Use $f ( \\cdot | \\boldsymbol {a_{t}}, \\boldsymbol s_t; \\hat \\theta^f )$ to draw the next state $\\boldsymbol{\\tilde s}^{(r)}_{\\tau + 1}$ Use the next state, $\\boldsymbol{\\tilde s}^{(r)}_{\\tau + 1}$ as current state for the next iteration Then average all the value functions together to obtain an expected value function $V_{j}^{\\boldsymbol {\\hat{P}}, R} \\left(\\boldsymbol{s}_{t} ; \\tilde \\theta^\\pi \\right)$\nNote: advantage of simulations: can be parallelized\nObjective Function What have we done so far?\nGiven some parameters $\\theta^\\pi$, we computed the expected value function How do we pick the $\\theta^\\pi$ that best rationalizes the data?\nI.e. what is the objective function? Potentially many options BBL idea\nthe expected value function has to be optimal, given the CCPs I.e. any other policy function should give a lower expected value “Best” $\\theta^\\pi$: those for which the implied expected value function under the estimated CCPs is greater than the one implied by any other CCP Note: it’s an inequality statement\nObjective Function (2) Idea\nIf the observed policy ${\\color{green}{\\boldsymbol{\\hat P}}}$ is optimal,\nAll other policies ${\\color{red}{\\boldsymbol{\\tilde P}}}$\n… at the true parameters $\\theta^f$\n… should give a lower expected value $$ V_{j}^{{\\color{red}{\\boldsymbol{\\tilde P}}}, R} \\left( \\boldsymbol{s}{t} ; \\tilde \\theta^\\pi \\right) \\leq V{j}^{{\\color{green}{\\boldsymbol{\\hat P}}}, R} \\left( \\boldsymbol{s}_{t} ; \\tilde \\theta^\\pi \\right) $$\nSo which are the true parameters?\nThose for which any deviation from the observed policy ${\\color{green}{\\boldsymbol{\\hat P}}}$ yields a lower value\nObjective function to minimize: violations under alternative policies ${\\color{red}{\\boldsymbol{\\tilde P}}}$ $$ \\min_{\\tilde \\theta^\\pi} \\sum_{\\boldsymbol s_{t}} \\sum_{{\\color{red}{\\boldsymbol{\\tilde P}}}} \\Bigg[\\min \\bigg\\lbrace V_{j}^{{\\color{green}{\\boldsymbol{\\hat P}}}, R} \\left( \\boldsymbol{s}{t} ; \\tilde \\theta^\\pi \\right) - V{j}^{{\\color{red}{\\boldsymbol{\\tilde P}}}, R} \\left( \\boldsymbol{s}_{t} ; \\tilde \\theta^\\pi \\right) \\ , \\ 0 \\bigg\\rbrace \\Bigg]^{2} $$\nEstimator Estimator: $\\theta^\\pi$ that minimizes the average (squared) magnitude of violations for any alternative policy ${\\color{red}{\\boldsymbol{\\tilde P}}}$ $$ \\hat{\\theta}^\\pi= \\arg \\min_{\\tilde \\theta^\\pi} \\sum_{\\boldsymbol s_{t}} \\sum_{{\\color{red}{\\boldsymbol{\\tilde P}}}} \\Bigg[\\min \\bigg\\lbrace V_{j}^{{\\color{green}{\\boldsymbol{\\hat P}}}, R} \\left( \\boldsymbol{s}{t} ; \\tilde \\theta^\\pi \\right) - V{j}^{{\\color{red}{\\boldsymbol{\\tilde P}}}, R} \\left( \\boldsymbol{s}_{t} ; \\tilde \\theta^\\pi \\right) \\ , \\ 0 \\bigg\\rbrace \\Bigg]^{2} $$\n$\\min \\Big\\lbrace V_{j}^{{\\color{green}{\\boldsymbol{\\hat P}}}, R} - V_j^{{\\color{red}{\\boldsymbol{\\tilde P}}}, R} \\ , \\ 0 \\Big\\rbrace$ to pick only the violations If ${\\color{green}{\\boldsymbol{\\hat P}}}$ implies higher value, we can ignore Doesn’t matter by how much you respect the inequality Which alternative policies ${\\color{red}{\\boldsymbol{\\tilde P}}}$ should we use? In principle, any perturbation is ok But in practice, if we perturbe it too much, we can go too far off Tip 1: start with very small perturbations Tip 2: use perturbation that sensibly affect the dynamics E.g. exiting in a state in which a firm is not a competitive threat Tip 3: use perturbations on dimensions that are relevant for the research question E.g. they affect dimensions where you want to make counterfactual predictions Advantages We have seen that there are competing methods.\nWhat are the advantages of Bajari, Benkard, and Levin (2007) over those?\nContinuous actions BBL does not require actions to be discretised You can just sample actions from the data! Choice of alternative CCPs The researcher is free to choose the alternative CCPs ${\\color{red}{\\boldsymbol{\\tilde P}}}$ Pros: can make source of variation more transparent allows the researcher to focus on those predictions of the model that are key for the specific research questions Cons: it’s a very high dimensional space There are very very many alternative policy functions Problems Computational curse of dimensionality is gone (in the state space) But we have a curse of dimensionality in data Need a lot of markets because now 1 market is 1 observation Multiple equilibria?? We are basically assuming it away Estimating the CCPs in the first stage we assume that is the equilibrium that is played in all markets at all times To run counterfactuals, we still need to solve the model Unobserved heterogeneity Kasahara and Shimotsu (2009): how to identify the (minimum) number of unobserved types Arcidiacono and Miller (2011): how to use an EM algorithm for the 1st stage estimation with unobserved types, conditional on the number of types Berry and Compiani (2021): instrumental variables approach, relying on observed states in the distant past Non-stationarity If we have a long time period, something fundamentally might have changed Ericson Pakes (1995) Introduction Ericson and Pakes (1995) and companion paper Pakes and McGuire (1994) for the computation\n$J$ firms indexed by $j \\in \\lbrace 1, \u0026hellip;, J \\rbrace$\nTime $t$ is dicrete $t$, horizon is infinite\nState $s_{jt}$: quality of firm $j$ in period $t$\nPer period profits $$ \\pi (s_{jt}, \\boldsymbol s_{-jt}, ; \\theta^\\pi) $$ where\n$\\boldsymbol s_{-it}$: state vector of all other firms in period $t$ $\\theta^\\pi$: parameters that govern static profits We can micro-fund profits with some demand and supply functions\nThere can be some underlying static strategic interaction E.g. logit demand and bertrand competition in prices $p_{it}$ State Transitions Investment: firms can invest an dollar amount $x$ to increase their future quality\nContinuous decision variable ($\\neq$ Rust)\nProbability that investment is successful $$ \\Pr \\big(i_{jt} \\ \\big| \\ a_{it} = x \\big) = \\frac{\\alpha x}{1 + \\alpha x} $$\nHigher investment, higher success probability\n$\\alpha$ parametrizes the returns on investment\nQuality depreciation\nWith probability $\\delta$, quality decreases by one level Law of motion $$ s_{j,t+1} = s_{jt} + i_{jt} - \\delta $$\nDecision Variables Note that in Ericson and Pakes (1995) we have two separate decision variables\nStatic decision variable: price $p_{jt}$ Dynamic decision variable: investment $i_{jt}$ Does not have to be the case!\nExample: Besanko et al. (2010)\nModel of learning-by-doing: firms decrease their marginal cost through sales State variable: firm stock of know how $e$ The higher the stock of know-how, the lower the marginal cost Increases when a firm manages to make a sale $q \\in [0,1]$ now is both static quantity and transition probability Single decision variable: price $p$ Usual static effects on profits $\\pi_{jt} = (p_{jt} - c(e_{jt})) \\cdot q_j(\\boldsymbol p_t)$ But also dynamic effect through transition probabilities Probability of increasing $e_t$: $q_j(\\boldsymbol p_t)$ Equilibrium Firms maximize the expected flow of discounted profits $$ \\max_{\\boldsymbol a} \\ \\mathbb E_t \\left[ \\sum_{\\tau=0}^\\infty \\beta^{\\tau} \\pi_{j, t+\\tau} (\\theta^\\pi) \\right] $$ Markow Perfect Equilibrium\nEquillibrium notion: Markow Perfect Equilibrium (Maskin and Tirole 1988)\nA set of $J$ value and policy functions, $\\boldsymbol V$ and $\\boldsymbol P$ such that each firm maximizes its value function $V_j$ given the policy function of every other firm $\\boldsymbol P_{-j}$ Exit One important extension is exit.\nIn each time period, incuments decide whether to stay … or exit and get a scrap value $\\phi^{exit}$ The Belman Equation of incumbent $j$ at time $t$ is $$ V^{\\boldsymbol P_{-j}}{j} (\\mathbf{s}{t}) = \\max_{d^{exit}{jt} \\in \\lbrace 0, 1 \\rbrace} \\Bigg\\lbrace \\begin{array}{c} \\beta \\phi^{exit} \\ , \\newline \\max{a_{jt} \\in \\mathcal{A}j \\left(\\mathbf{s}{t}\\right)} \\Big\\lbrace \\pi_{j}^{\\boldsymbol P_{-j}} (a_{jt}, \\mathbf{s}{t} ; \\theta^\\pi ) + \\beta \\mathbb E{\\boldsymbol s_{t+1}} \\Big[ V_{j}^{\\boldsymbol P_{-j}} \\left(\\mathbf{s}{t+1}\\right) \\ \\Big| \\ a{jt}, \\boldsymbol s_{t} ; \\theta^f \\Big] \\Big\\rbrace \\end{array} \\Bigg\\rbrace $$ where\n$\\phi^{exit}$: exit scrap value $d^{exit}_{jt} \\in \\lbrace 0,1 \\rbrace$: exit decision Entry We can also incorporate endogenous entry.\nOne or more potential entrants exist outside the market They can pay an entry cost $\\phi^{entry}$ and enter the market at a quality state $\\bar s$ … or remain outside at no cost Value function $$ V_{j}^{\\boldsymbol P_{-j}} (e, \\boldsymbol x_{-jt} ; \\theta) = \\max_{d^{entry} \\in \\lbrace 0,1 \\rbrace } \\Bigg\\lbrace \\begin{array}{c} 0 \\ ; \\newline\n\\phi^{entry} + \\beta \\mathbb E_{\\boldsymbol s_{t+1}} \\Big[ V_{j}^{\\boldsymbol P_{-j}} (\\bar s, \\boldsymbol s_{-j, t+1} ; \\theta) \\ \\Big| \\ \\boldsymbol s_{t} ; \\theta^f \\Big] \\end{array} \\Bigg\\rbrace $$ where\n$d^{entry} \\in \\lbrace 0,1 \\rbrace$: entry decision\n$\\phi^{entry}$: entry cost\n$\\bar s$: state in which entrants enters (could be random)\nDo we observe potential entrants?\nIgami (2017): tech industry announce their entry Critique: not really potential entrants, they are half-way inside Equilibrium Existence Doraszelski and Satterthwaite (2010): a MPE might not exist in Ericson and Pakes (1995) model.\nSolution\nReplace fixed entry costs $\\phi^{entry}$ and exit scrap values $\\phi^{exit}$ with random ones It becomes a game of incomplete information First explored in Rust (1994) New equilibrium concept Markov Perfect Bayesian Nash Equilibrium (MPBNE)\nBasically the same, with rational beliefs on random variables Solving the Model Solving the model is very similar to Rust\nGiven parameter values $\\theta$ Start with a guess for the value and policy functions Until convergence, do: For each firm $j = 1, \u0026hellip;, J$, do: Take the policy functions of all other firms Compute the implied transition probabilities Use them to compute the new policy function for firm $j$ Compute the implied value function Where do things get complicated / tricky? Policy function update\nPolicy Update Example: exit game Imagine a stylized exit game with 2 firms\nEasy to get an update rule of the form: “exit if opponent stays, stay if opponent exits” Computationally\nInitialize policy functions to $(exit, exit)$ Iteration 1: Each firm takes opponent policy as given: $exit$ Update own optimal policy: $stay$ New policy: $(stay, stay)$ Iteration 2: $(stay, stay) \\to (exit, exit)$ Iteration 2: $(exit, exit) \\to (stay, stay)$ Etc… Issues: value function iteration might not converge and equilibrium multeplicity.\nConvergence Tips Try different starting values Often it’s what makes the biggest difference Ideally, start as close as possible to true values Approximation methods can help (we’ll see more later) I.e. get a fast approximation to use as starting vlaue for solution algorithm Partial/stochastic value function update rule Instead of $V\u0026rsquo; = T(V)$, use $V\u0026rsquo; = \\alpha T(V) + (1-\\alpha)V$ Very good to break loops, especially if $\\alpha$ is stochastic, e.g. $\\alpha \\sim U(0,1)$ How large is the support of the entry/exit costs? If support is too small, you end up back in the entry/exit loop Try alternative non-parallel updating schemes E.g. update value one state at the time (in random order?) Last but not least: change the model In particular, from simultaneous to alternating moves or continuous time Multiple Equilibria How to find them?\nBesanko et al. (2010) and Borkovsky, Doraszelski, and Kryukov (2010): homotopy method can find some equilibria, but not all complicated to implement: need to compute first order conditions $H(\\boldsymbol V, \\theta) = 0$ and their Jacobian $\\Delta H(\\boldsymbol V, \\theta)$ Idea: trace the equilibrium correspondence $H^{-1} = \\lbrace (\\boldsymbol V, \\theta) : H(\\boldsymbol V, \\theta) = 0 \\rbrace$ in the value-parameter space Eibelshäuser and Poensgen (2019) Markov Quantal Response Equilibrium approact dynamic games from a evolutionary game theory perspective actions played at random and those bringing highest payoffs survive $\\to$ homothopy method guaranteed to find one equilibrium Pesendorfer and Schmidt-Dengler (2010): some equilibria are not Lyapunov-stable BR iteration cannot find them unless you start exactly at the solution Su and Judd (2012) and Egesdal, Lai, and Su (2015): same point, but numerically using MPEC approach Multiple Equilibria (2) Can we assume them away?\nIgami (2017) Finite horizon Homogenous firms (in profit functions and state transitions) One dynamic move per period (overall, not per-firm) Abbring and Campbell (2010) Entry/exit game Homogeneous firms Entry and exit decisions are follow a last-in first-out (LIFO) structure “An entrant expects to produce no longer than any incumbent” Iskhakov, Rust, and Schjerning (2016) can find all equilibria, but for very specific class of dynamic games must always proceed “forward” e.g. either entry or exit but not both Idea: can solve by backward induction even if horizon is infinite Curse of Dimensionality What are the computational bottlenecks? $$ V^{\\boldsymbol P_{-j}}{j} ({\\color{red}{\\mathbf{s}{t}}}) = \\max_{a_{jt} \\in \\mathcal{A}j \\left(\\mathbf{s}{t}\\right)} \\Bigg\\lbrace \\pi_{j}^{\\boldsymbol P_{-j}} (a_{jt}, \\mathbf{s}{t} ; \\theta^\\pi ) + \\beta \\mathbb E{{\\color{red}{\\mathbf{s}{t+1}}}} \\Big[ V{j}^{\\boldsymbol P_{-j}} \\left(\\mathbf{s}{t+1}\\right) \\ \\Big| \\ a{jt}, \\boldsymbol s_{t} ; \\theta^f \\Big] \\Bigg\\rbrace $$\nDimension of the state space In single agent problems, we have as many states as many values of $s_{jt}$ ($k$) In dynamics games, the state space goes from $k$ to $k^J$ symmetry helps: state $[1,2,3]$ and $[1,3,2]$ become the same for firm 1 How much do we gain? From $k^J$ to $k \\cdot {k + J - 2 \\choose k - 1}$ Dimension of the integrand If in single agent problems, we have to integrate over $\\kappa$ outcomes, 4 in Rust: engine replaced (yes|no) $\\times$ mileage increases (yes|no) … in dynamic games, we have to consider $\\kappa^J$ outcomes Note: bottlenecks are not addittive but multiplicative: have to solve the expectation for each point in the state space. Improving on any of the two helps a lot.\nCurse of Dimensionality (2) Two and a half classes of solutions:\nComputational: approximate the equilibrium Doraszelski (2003): use Chebyshev polynomials for a basis function Farias, Saure, and Weintraub (2012): combine approximations with a MPEC-like approach Conceptual: define another game Weintraub, Benkard, and Van Roy (2008): oblivious equilibrium Ifrach and Weintraub (2017): moment based equilibrium Doraszelski and Judd (2012): games in continuous time Doraszelski and Judd (2019): games with random moves Kind of both: Pakes and McGuire (2001) experience-based equilibrium (Fershtman and Pakes 2012) Note: useful also to get good starting values for a full solution method!\nOblivious Equilibrium Weintraub, Benkard, and Van Roy (2008): what if firms had no idea about the state of other firms?\nor atomistic firms The value function becomes $$ V_{j} ({\\color{red}{s_{t}}}) = \\max_{a_{jt} \\in \\mathcal{A}j \\left({\\color{red}{s{t}}}\\right)} \\Bigg\\lbrace {\\color{red}{\\mathbb E_{\\boldsymbol s_t}}} \\Big[ \\pi_{j} (a_{jt}, \\mathbf{s}{t} ; \\theta^\\pi ) \\Big| P \\Big] + \\beta \\mathbb E{{\\color{red}{s_{t+1}}}} \\Big[ V_{j} \\left({\\color{red}{s_{t+1}}}\\right) \\ \\Big| \\ a_{jt}, {\\color{red}{s_{t}}} ; \\theta^f \\Big] \\Bigg\\rbrace $$\nNow the state is just $s_t$ instead of $\\boldsymbol s_t$ Huge computational gain: from $k^J$ points to $k$ Also the expectation of future states is taken over $3$ instead of $3^J$ points (3 because quality can go up, down or stay the same) But need to compute static profits as the expected value given the current policy function Need to keep track of the asymptotic state distribution as you iterate the value Games with Random Moves Doraszelski and Judd (2019): what if instead of simultaneously, firms would move one at the time at random?\nImportant: to have the same frequency of play, a period now is $J$ times shorter The value function becomes $$ V^{\\boldsymbol P_{-j}}{j} (\\mathbf{s}{t}, {\\color{red}{n=j}}) = \\max_{a_{jt} \\in \\mathcal{A}j \\left(\\mathbf{s}{t}\\right)} \\Bigg\\lbrace {\\color{red}{\\frac{1}{J}}}\\pi_{j}^{\\boldsymbol P_{-j}} (a_{jt}, \\mathbf{s}{t} ; \\theta^\\pi ) + {\\color{red}{\\sqrt[J]{\\beta}}} \\mathbb E{{\\color{red}{n, s_{j, t+1}}}} \\Big[ V_{j}^{\\boldsymbol P_{-j}} \\left(\\mathbf{s}{t+1}, {\\color{red}{n}} \\right) \\ \\Big| \\ a{jt}, \\boldsymbol s_{t} ; \\theta^f \\Big] \\Bigg\\rbrace $$\n$n$: indicates whose turn is to play since a turn is $J$ times shorter, profits are $\\frac{1}{J} \\pi$ and discount factor is $\\sqrt[J]{\\beta}$ Computational gain\nExpectation now taken over $n, s_{j, t+1}$ instead of $\\boldsymbol s_{t+1}$ I.e. $Jk$ points instead of $3^k$ (3 because quality can go up, down or stay the same) Huge computational difference! Games in Continuous Time Doraszelski and Judd (2012): what’s the advantage of continuous time?\nProbability that two firms take a decision simultaneously is zero With continuous time, the value function becomes $$ V^{\\boldsymbol P_{-j}}{j} (\\mathbf{s}{t}) = \\max_{a_{jt} \\in \\mathcal{A}j \\left(\\mathbf{s}{t}\\right)} \\Bigg\\lbrace \\frac{1}{\\lambda(a_{jt}) - \\log(\\beta)} \\Bigg( \\pi_{j}^{\\boldsymbol P_{-j}} (a_{jt}, \\mathbf{s}{t} ; \\theta^\\pi ) + \\lambda(a{jt}) \\mathbb E_{\\boldsymbol s_{t+1}} \\Big[ V_{j}^{\\boldsymbol P_{-j}} \\left(\\mathbf{s}{t+1}\\right) \\ \\Big| \\ a{jt}, \\boldsymbol s_{t} ; \\theta^f \\Big] \\Bigg) \\Bigg\\rbrace $$\n$\\lambda(a_{jt}) = \\delta + \\frac{\\alpha a_{jt}}{1 + \\alpha a_{jt}}$ is the hazard rate for firm $j$ that something happens i.e. either an increase in quality, with probability $\\frac{\\alpha a_{jt}}{1 + \\alpha a_{jt}}$ … or a decrease in quality with probability $\\delta$ Computational gain\nNow the expectation over future states $\\mathbb E_{\\boldsymbol s_{t+1}}$ is over $2J$ points instead of $3^J$ 3 because quality can go up, down or stay the same 2 because in continuous time we don’t care if the state does not change (investment fails) Comparison Which method is best?\nI compare them in Courthoud (2020)\nFastest: Weintraub, Benkard, and Van Roy (2008) Effectively transforms the game into single-agent dynamics Best trade-off: Doraszelski and Judd (2019) Simple, practical and also helps in terms of equilibrium multeplicity Also in Courthoud (2020): games with random order Better approximation than Doraszelski and Judd (2019) And similar similar time Applications Some applications of these methods include\nApproximation methods Sweeting (2013): product repositioning among radio stations Barwick and Pathak (2015): entry and exit in the real estate brokerage industry Oblivious equilibrium Xu and Chen (2020): R\u0026amp;D investment in the Korean electric motor industry Moment based equilibrium Jeon (2020): demand learning in the container shipping industry Caoui (2019): technology adoption with network effects in the movie industry Vreugdenhil (2020): search and matching in the oil drilling industry Games in continuous time Arcidiacono et al. (2016): entry, exit and scale decisions in retail competition Games with random moves Igami (2017): innovation, entry, exit in the hard drive industry From IO to AI Bridging two Literatures There is one method to approximate the equilibrium in dynamic games that is a bit different from the others: Pakes and McGuire (2001)\nIdea: approximate the value function by Monte-Carlo simulation Firms start with a guess for the alternative-specific value function Act according to it Observe realized payoffs and state transitions And update the alternative-specific value function according to the realized outcomes Experience-Based Equilibrium\nDefined in Fershtman and Pakes (2012) Def: policy is optimal given beliefs of state transitions and observed transitions are consistent with the beliefs Note: definition silent on off-equilibrium path beliefs Pakes and McGuire (2001) Players start with alternative-specific value function\nyes, the ASV from Rust (1994) $\\bar V_{j,a}^{(0)} (\\boldsymbol s ; \\theta)$: initial value of player $j$ for action $a$ in state $\\boldsymbol s$ Until convergence, do:\nCompute optimal action, given $\\bar V_{j, a}^{(t)} (\\boldsymbol s ; \\theta)$ $$ a^* = \\arg \\max_a \\bar V_{j, a}^{(t)} (\\boldsymbol s ; \\theta) $$\nObserve the realized payoff $\\pi_{j, a^}(\\boldsymbol s ; \\theta)$ and the realized next state $\\boldsymbol {s\u0026rsquo;}(\\boldsymbol s, a^; \\theta)$\nUpdate the alternative-specific value function of the chosen action $k^$ $$ \\bar V_{j, a^}^{(t+1)} (\\boldsymbol s ; \\theta) = (1-\\alpha_{\\boldsymbol s, t}) \\bar V_{j, a^}^{(t)} (\\boldsymbol s ; \\theta) + \\alpha_{\\boldsymbol s, t} \\Big[\\pi_{j, a^}(\\boldsymbol s ; \\theta) + \\arg \\max_a \\bar V_{j, a}^{(t)} (\\boldsymbol s\u0026rsquo; ; \\theta) \\Big] $$ where\n$\\alpha_{\\boldsymbol s, t} = \\frac{1}{\\text{number of times state } \\boldsymbol s \\text{ has been visited}}$ Comments Where is the strategic interaction?\nFirm always take “best action so far” in each state Start to take a new action only when the previous best has performed badly for many periods Remindful of literature of evolutionary game theory Importance of starting values\nImagine, all payoffs are positive but value initialized to zero First action in each state $\\to$ only action ever taken in that state Loophole. Why? Firms always take $\\arg \\max_a \\bar V_a$ and never explore the alternatives Convergence by desing\nAs $\\lim_{t \\to \\infty} \\alpha_{\\boldsymbol s, t} = 1$ Firms stop updating the value by design Q-Learning Computer Science reinforcement learning literature (AI): Q-learning\nDifferences\n$\\bar V_a( \\boldsymbol s)$ called $Q_a(\\boldsymbol s)$, hence the name Firms don’t always take the optimal action At the beginning of the algorithm: exploration Firms take actions at random Just to explore what happens taking different actions Gradually shift towards exploitation I.e. take the optimal action, given $\\bar V^{(t)}( \\boldsymbol s)$ at iteration $t$ I.e. shift towards Pakes and McGuire (2001) Applications Doraszelski, Lewis, and Pakes (2018) Firm do actually learn by trial and error Setting: demand learning in the UK frequency response market (electricity) Asker et al. (2020) Uses Pakes and McGuire (2001) for estimation Setting: dynamic timber auctions with information sharing Calvano et al. (2020) Study Q-learning pricing algorithms In repeated price competition with differentiated products (Computational) lab experiment: what do these algorithms converge to? Finding: algorithms learn reward-punishment collusive strategies Appendix References [references] Abbring, Jaap H, and Jeffrey R Campbell. 2010. “Last-in First-Out Oligopoly Dynamics.” Econometrica 78 (5): 1491–1527.\nAguirregabiria, Victor, Allan Collard-Wexler, and Stephen P Ryan. 2021. “Dynamic Games in Empirical Industrial Organization.” National Bureau of Economic Research.\nAguirregabiria, Victor, and Pedro Mira. 2007. “Sequential Estimation of Dynamic Discrete Games.” Econometrica 75 (1): 1–53.\nArcidiacono, Peter, Patrick Bayer, Jason R Blevins, and Paul B Ellickson. 2016. “Estimation of Dynamic Discrete Choice Models in Continuous Time with an Application to Retail Competition.” The Review of Economic Studies 83 (3): 889–931.\nArcidiacono, Peter, and Robert A Miller. 2011. “Conditional Choice Probability Estimation of Dynamic Discrete Choice Models with Unobserved Heterogeneity.” Econometrica 79 (6): 1823–67.\nAsker, John, Chaim Fershtman, Jihye Jeon, and Ariel Pakes. 2020. “A Computational Framework for Analyzing Dynamic Auctions: The Market Impact of Information Sharing.” The RAND Journal of Economics 51 (3): 805–39.\nBajari, Patrick, C Lanier Benkard, and Jonathan Levin. 2007. “Estimating Dynamic Models of Imperfect Competition.” Econometrica 75 (5): 1331–70.\nBarwick, Panle Jia, and Parag A Pathak. 2015. “The Costs of Free Entry: An Empirical Study of Real Estate Agents in Greater Boston.” The RAND Journal of Economics 46 (1): 103–45.\nBerry, Steven T, and Giovanni Compiani. 2021. “Empirical Models of Industry Dynamics with Endogenous Market Structure.” Annual Review of Economics 13.\nBesanko, David, Ulrich Doraszelski, Yaroslav Kryukov, and Mark Satterthwaite. 2010. “Learning-by-Doing, Organizational Forgetting, and Industry Dynamics.” Econometrica 78 (2): 453–508.\nBorkovsky, Ron N, Ulrich Doraszelski, and Yaroslav Kryukov. 2010. “A User’s Guide to Solving Dynamic Stochastic Games Using the Homotopy Method.” Operations Research 58 (4-part-2): 1116–32.\nCalvano, Emilio, Giacomo Calzolari, Vincenzo Denicolo, and Sergio Pastorello. 2020. “Artificial Intelligence, Algorithmic Pricing, and Collusion.” American Economic Review 110 (10): 3267–97.\nCaoui, El Hadi. 2019. “Estimating the Costs of Standardization: Evidence from the Movie Industry.” R\u0026amp;R, Review of Economic Studies.\nCollard-Wexler, Allan. 2013. “Demand Fluctuations in the Ready-Mix Concrete Industry.” Econometrica 81 (3): 1003–37.\nCourthoud, Matteo. 2020. “Approximation Methods for Large Dynamic Stochastic Games.” Working Paper.\nDoraszelski, Ulrich. 2003. “An r\u0026amp;d Race with Knowledge Accumulation.” Rand Journal of Economics, 20–42.\nDoraszelski, Ulrich, and Kenneth L Judd. 2012. “Avoiding the Curse of Dimensionality in Dynamic Stochastic Games.” Quantitative Economics 3 (1): 53–93.\n———. 2019. “Dynamic Stochastic Games with Random Moves.” Quantitative Marketing and Economics 17 (1): 59–79.\nDoraszelski, Ulrich, Gregory Lewis, and Ariel Pakes. 2018. “Just Starting Out: Learning and Equilibrium in a New Market.” American Economic Review 108 (3): 565–615.\nDoraszelski, Ulrich, and Mark Satterthwaite. 2010. “Computable Markov-Perfect Industry Dynamics.” The RAND Journal of Economics 41 (2): 215–43.\nEgesdal, Michael, Zhenyu Lai, and Che-Lin Su. 2015. “Estimating Dynamic Discrete-Choice Games of Incomplete Information.” Quantitative Economics 6 (3): 567–97.\nEibelshäuser, Steffen, and David Poensgen. 2019. “Markov Quantal Response Equilibrium and a Homotopy Method for Computing and Selecting Markov Perfect Equilibria of Dynamic Stochastic Games.” Working Paper.\nEricson, Richard, and Ariel Pakes. 1995. “Markov-Perfect Industry Dynamics: A Framework for Empirical Work.” The Review of Economic Studies 62 (1): 53–82.\nEsteban, Susanna, and Matthew Shum. 2007. “Durable-Goods Oligopoly with Secondary Markets: The Case of Automobiles.” The RAND Journal of Economics 38 (2): 332–54.\nFarias, Vivek, Denis Saure, and Gabriel Y Weintraub. 2012. “An Approximate Dynamic Programming Approach to Solving Dynamic Oligopoly Models.” The RAND Journal of Economics 43 (2): 253–82.\nFershtman, Chaim, and Ariel Pakes. 2012. “Dynamic Games with Asymmetric Information: A Framework for Empirical Work.” The Quarterly Journal of Economics 127 (4): 1611–61.\nGoettler, Ronald L, and Brett R Gordon. 2011. “Does AMD Spur Intel to Innovate More?” Journal of Political Economy 119 (6): 1141–1200.\nHotz, V Joseph, and Robert A Miller. 1993. “Conditional Choice Probabilities and the Estimation of Dynamic Models.” The Review of Economic Studies 60 (3): 497–529.\nHuang, Ling, and Martin D Smith. 2014. “The Dynamic Efficiency Costs of Common-Pool Resource Exploitation.” American Economic Review 104 (12): 4071–4103.\nIfrach, Bar, and Gabriel Y Weintraub. 2017. “A Framework for Dynamic Oligopoly in Concentrated Industries.” The Review of Economic Studies 84 (3): 1106–50.\nIgami, Mitsuru. 2017. “Estimating the Innovator’s Dilemma: Structural Analysis of Creative Destruction in the Hard Disk Drive Industry, 1981–1998.” Journal of Political Economy 125 (3): 798–847.\nIskhakov, Fedor, John Rust, and Bertel Schjerning. 2016. “Recursive Lexicographical Search: Finding All Markov Perfect Equilibria of Finite State Directional Dynamic Games.” The Review of Economic Studies 83 (2): 658–703.\nJeon, Jihye. 2020. “Learning and Investment Under Demand Uncertainty in Container Shipping.” The RAND Journal of Economics.\nKasahara, Hiroyuki, and Katsumi Shimotsu. 2009. “Nonparametric Identification of Finite Mixture Models of Dynamic Discrete Choices.” Econometrica 77 (1): 135–75.\nMaskin, Eric, and Jean Tirole. 1988. “A Theory of Dynamic Oligopoly, II: Price Competition, Kinked Demand Curves, and Edgeworth Cycles.” Econometrica: Journal of the Econometric Society, 571–99.\nPakes, Ariel, and Paul McGuire. 1994. “Computing Markov-Perfect Nash Equilibria: Numerical Implications of a Dynamic Differentiated Product Model.” RAND Journal of Economics 25 (4): 555–89.\n———. 2001. “Stochastic Algorithms, Symmetric Markov Perfect Equilibrium, and the ‘Curse’of Dimensionality.” Econometrica 69 (5): 1261–81.\nPakes, Ariel, Michael Ostrovsky, and Steven Berry. 2007. “Simple Estimators for the Parameters of Discrete Dynamic Games (with Entry/Exit Examples).” The RAND Journal of Economics 38 (2): 373–99.\nPesendorfer, Martin, and Philipp Schmidt-Dengler. 2008. “Asymptotic Least Squares Estimators for Dynamic Games.” The Review of Economic Studies 75 (3): 901–28.\n———. 2010. “Sequential Estimation of Dynamic Discrete Games: A Comment.” Econometrica 78 (2): 833–42.\nRust, John. 1994. “Structural Estimation of Markov Decision Processes.” Handbook of Econometrics 4: 3081–3143.\nRyan, Stephen P. 2012. “The Costs of Environmental Regulation in a Concentrated Industry.” Econometrica 80 (3): 1019–61.\nSu, Che-Lin, and Kenneth L Judd. 2012. “Constrained Optimization Approaches to Estimation of Structural Models.” Econometrica 80 (5): 2213–30.\nSweeting, Andrew. 2013. “Dynamic Product Positioning in Differentiated Product Markets: The Effect of Fees for Musical Performance Rights on the Commercial Radio Industry.” Econometrica 81 (5): 1763–803.\nVreugdenhil, Nicholas. 2020. “Booms, Busts, and Mismatch in Capital Markets: Evidence from the Offshore Oil and Gas Industry.” R\u0026amp;R at Journal of Political Economy.\nWeintraub, Gabriel Y, C Lanier Benkard, and Benjamin Van Roy. 2008. “Markov Perfect Industry Dynamics with Many Firms.” Econometrica 76 (6): 1375–1411.\nXu, Daniel Yi, and Yanyou Chen. 2020. “A Structural Empirical Model of r\u0026amp;d, Firm Heterogeneity, and Industry Evolution.” Journal of Industrial Economics.\n","date":1635465600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1635465600,"objectID":"503282b0c5ba446d2962ab39615c5eae","permalink":"https://matteocourthoud.github.io/course/empirical-io/08_dynamics_games/","publishdate":"2021-10-29T00:00:00Z","relpermalink":"/course/empirical-io/08_dynamics_games/","section":"course","summary":"Introduction Intro Setting: agents making strategic decisions (new) in dynamic environments.\nEntry and exit: Collard-Wexler (2013) Sunk costs: Ryan (2012) Innovation: Goettler and Gordon (2011) (or whatever changes in response to investment) Exploitation of natural resources: Huang and Smith (2014) Durable goods: Esteban and Shum (2007) Lit review: forthcoming IO Handbook chapter Aguirregabiria, Collard-Wexler, and Ryan (2021)","tags":null,"title":"Dynamic Games","type":"book"},{"authors":null,"categories":null,"content":"Introduction Non-parametric regression is a flexible estimation procedure for\nregression functions $\\mathbb E [y|x ] = g (x)$ and density functions $f(x)$. You want to let your data to tell you how flexible you can afford to be in terms of estimation procedures. Non-parametric regression is naturally introduced in terms of fitting a curve.\nConsider the problem of estimating the Conditional Expectation Function, defined as $\\mathbb E [y_i |x_i ] = g(x_i)$ given data $D = (x_i, y_i)_{i=1}^n$ under minimal assumption of $g(\\cdot)$, e.g. smoothness. There are two main methods:\nLocal methods: Kernel-based estimation Global methods: Series-based estimation Another way of looking at non-parametrics is to do estimation/inference without specifying functional forms. With no assumptions, informative inference is impossible. Non parametrics tries to work with functional restrictions—continuity, differentiability, etc.—rather than pre-specifying functional form.\nDiscrete x - Cell Estimator Suppose that $x$ can take $R$ distinct values, e.g. gender $R=2$, years of schooling $R=20$, gender $\\times$ years of schooling $R = 2 \\times 20$.\nA simple way for estimating $\\mathbb E \\left[ y |x \\right] = g(x)$ is to split the sample to include observations with $x_i = x$ and calculate the sample mean of $\\bar{y}$ for these observations. Note that this requires no assumptions about how $\\mathbb E [y_i |x_i]$ varies with $x$ since we fit a different value for each value $x$. $$ \\hat{g}(x) = \\frac{1}{| i: x_i = x |} \\sum_{i : x_i = x} y_i $$\nIssues:\nCurse of dimensionality: if $R$ is big compared to $n$, there will be only a small number of observations per $x$ values. If $x_i$ is continuous, $R=n$ with probability 1. Solution: we can borrow information about $g_0(x)$ using neighboring observations of $x$. Averaging for each separate $x_r$ value is only feasible in cases where $x_i$ is coarsely discrete. Local Non-Parametric Estimation Kernels Suppose we believe that $\\mathbb E [y_i |x_i]$ is a smooth function of $x_i$ – e.g. continuous, differentiable, etc. Then it should not change too much across values of $x$ that are close to each other: we can estimate the conditional expectation at $x = \\bar{x}$ by averaging $y$’s over the values of $x$ that are “close”” to $\\bar{x}$. This procedure relies on two (three) arbitrary choices:\nChoice of the kernel function $K (\\cdot)$; it is used to weight “far out”” observations, such that $K: \\mathbb R \\to \\mathbb R$ $K$ is symmetric: $K(\\bar{x} + x_i) = K(\\bar{x} - x_i)$ $\\lim_{x_i \\to \\infty}K(x_i - \\bar{x}) = 0$ Choice of the bandwidth $h$: it measures the size of a ``small’’ window around $\\bar{x}$, e.g. $(\\bar{x} - h, \\bar{x} + h)$. Choice of the local estimation procedure. Examples are locally constant, a.k.a. Nadaraya-Watson (NW), and locally linear (LL). Generally, the choice of $h$ is more important than $K(\\cdot)$ in low dimensional settings.\nOptimal h We need to define what is an “optimal” $h$, depending on the smoothness level of $g_0$, typically unknown. The choice of $h$ relates to the bias-variance trade-off:\nlarge $h$: small variance, higher bias; small $h$: high variance, smaller bias. Note that $K_h (\\cdot) = K (\\cdot / h)$.\nLocally Constant Estimator Nadaraya-Watson estimator, or locally constant estimator. It assumes the CEF locally takes the form $g(x) = \\beta_0(x)$. The local parameter is estimated as: $$ \\hat{\\beta}0 (\\bar{x}) = \\arg\\min{\\beta_0} \\quad \\mathbb E_n \\Big[ K_h (x_i - \\bar{x}) \\cdot \\big(y_i - \\beta_0 \\big)^2 \\Big] $$ CEF The Nadaraya-Watson estimate of the CEF takes the form: $$ \\mathbb E_n \\left[ y | x = \\bar{x}\\right] = \\hat{g}(\\bar{x}) = \\frac{\\sum_{i=1}^n y_i K_h (x_i - \\bar{x})}{\\sum_{i=1}^n K_h (x_i - \\bar{x})} $$\nLocally Linear Estimator Local Linear estimator. It assumes the CEF locally takes the form $g(x) = \\beta_0(x) + \\beta_1(x) x$. The local parameters are estimated as: $$ \\left( \\hat{\\beta}_0 (\\bar{x}), \\hat{\\beta}1 (\\bar{x}) \\right) = \\arg\\min{\\beta_0, \\beta_1} \\quad \\mathbb E_n \\Big[ K_h (x_i - \\bar{x}) \\cdot \\big(y_i - \\beta_0 - (x_i - \\bar{x}) \\beta_1 \\big)^2 \\Big] $$\nCEF In this case, we do LS estimate with $i$’s contribution of residual weighted by the kernel $K_h (x_i - \\bar{x})$. The final estimate at $\\bar{x}$ is given by: $$ \\hat{g} (\\bar{x}) = \\hat{\\beta}_0 (\\bar{x}) + (\\bar{x} - \\bar{x}) \\hat{\\beta}_1 (\\bar{x}) = \\hat{\\beta}_0 (\\bar{x}) $$ since we have centered the $x_s$ at $\\bar{x}$ in the kernel. - It is possible to add linearly higher order polynomials, e.g. do locally quadratic least squares using loss function:\n$$ \\mathbb E_n \\left[ K_h (x_i - \\bar{x}) \\big(y_i - \\beta_0 - (x_i - \\bar{x}) \\beta_1 - (x_i - \\bar{x})^2 \\beta_2 \\big)^2 \\right] $$\nUniform Kernel LS restricted to sample $i$ such that $x_i$ within $h$ of $\\bar{x}$. $$ \\begin{aligned} \u0026amp; K (\\cdot) = \\mathbb I\\lbrace \\cdot \\in [-1, 1] \\rbrace \\newline \u0026amp; K_h (\\cdot) = \\mathbb I\\lbrace \\cdot/h \\in [-1, 1] \\rbrace = \\mathbb I\\lbrace \\cdot \\in [-h, h] \\rbrace \\newline \u0026amp; K_h (x_i - \\bar{x}) = \\mathbb I\\lbrace x_i - \\bar{x} \\in [-h, h] \\rbrace = \\mathbb I\\lbrace x_i \\in [\\bar{x}-h, \\bar{x} + h] \\rbrace \\end{aligned} $$ Employed together with the locally linear estimator, the estimation procedure reduces to **local least squares}. The loss function is: $$ \\mathbb E_n \\Big[ K_n (x_i - \\bar{x}) \\big(y_i -\\beta_0 - \\beta_1 (x_i - \\bar{x}) \\big)^2 \\Big] = \\frac{1}{n} \\sum_{i: x_i \\in [\\bar{x}-h, \\bar{x} +h ]} \\big(y_i -\\beta_0 - \\beta_1 (x_i - \\bar{x}) \\big)^2 $$\nThe more local is the estimation, the more appropriate the linear regression: if $g_0$ is smooth, $g_0(\\bar{x}) + g_0\u0026rsquo;(\\bar{x}) (x_i - \\bar{x})$ is a better approximation for $g_0 (x_i)$.\nHowever, the uniform density is not a good kernel choice as it produces discontinuous CEF estimates. The following are two popular alternative choices that produce continuous CEF estimates.\nOther Kernels Epanechnikov kernel $$ K_h(x_i - \\bar{x}) = \\frac { 3 } { 4 } \\left( 1 - (x_i - \\bar{x}) ^ { 2 } \\right) \\mathbb I\\lbrace x_i \\in [\\bar{x}-h, \\bar{x} + h] \\rbrace $$\nNormal or Gaussian kernel $$ K_\\phi (x_i - \\bar{x}) = \\frac { 1 } { \\sqrt { 2 \\pi } } \\exp \\left( - \\frac { (x_i - \\bar{x}) ^ { 2 } } { 2 } \\right) $$\nK-Nearest Neighbors (KNN): choose bandwidth so that there is a fixed number of observations in each kernel. This kernel is different from the others since it takes a nonparamentric form.\nExample Choice of the optimal bandwidth Practical methods:\nEyeball Method. (i) Choose a bandwidth (ii) Estimate the regression function (iii) Look at the result: if it looks more wiggly than you would like, increase the bandwidth: if it looks more smooth than you would like, decrease the bandwidth. Con: It only works for $\\dim(x_i) = 1$ or $2$.\nRule of Thumb. For example, Silverman’s rule of thumb: $h = \\left( \\frac{4 \\hat{\\sigma}^5}{3n} \\right)^{\\frac{1}{5}}$. Con: It requires too much knowledge about $g_0$ (i.e. normality) which you don’t have.\nCross Validation. Under some assumptions, CV will approximately gives the MSE optimal bandwidth. The basic idea is to evaluate quality of the bandwidth by looking at how well the resulting estimator forecasts in the given sample.\nLeave-one-out CV. For each $h \u0026gt; 0$ and each $i$, $\\hat{g}{-i} (x_i)$ is the estimate of the conditional expectation at $x_i$ using bandwidth $h$ and all observations expect observation $i$. The CV bandwidth is defined as $$ \\hat{h} = \\arg \\min_h CV(h) = \\arg \\min_h \\sum{i=1}^n \\Big( y_i - \\hat{g}_{-i} (x_i) \\Big)^2 $$\nPractical Tips Select a value for $h$. For each observation $i$, calculate $$ \\hat{g}{-i} (x_i) = \\frac{\\sum{j \\ne i} y_j K_h (x_j - x_i) }{\\sum_{i=1}^n K_h (x_j - x_i)}, \\qquad e_{i,h}^2 = \\left(y_i - \\hat{g}_{-i} (x_i) \\right)^2 $$ Calculate $\\text{CV}(h) = \\sum_{i=1}^n e^2_{i,h}$. Repeat for each $h$ and choose the one that minimizes $\\text{CV}(h)$. Inference Theorem: Consider data $\\lbrace y_i, x_i \\rbrace_{i=1}^n$, iid and suppose that $y_i = g(x_i) + \\varepsilon_i$ where $\\mathbb E[\\varepsilon_i|x_i] = 0$. Assume that $x_i \\in Interior(X)$ where $X \\subseteq \\mathbb R$, $g(x)$ and $f(x)$ are three times continuously differentiable, and $f(x) \u0026gt; 0$ on $X$. $f(x)$ is the probability density of $x \\in X$ , and $g(x)$ is the function of interest. Suppose that $K(\\cdot)$ is a kernel function. Suppose $n\\to\\infty$, $h\\to0$ , $nh\\to\\infty$, and $nh^7\\to0$. Then for any fixed $x\\in X$, $$ AMSE = \\sqrt{nh} \\Big( \\hat{g}(x) - g(x) - h^2 B(x)\\Big) \\overset{d}{\\to} N \\left( 0, \\frac{\\kappa \\sigma^2(x)}{f(x)}\\right) $$ for $\\sigma^2(x) = Var(y_i|x_i = x)$, $\\kappa = \\int K^2(v)dv$, and $B(x) = \\frac{\\kappa_2}{2} \\frac{f\u0026rsquo;(x)g\u0026rsquo;(x) + f(x) g\u0026rsquo;\u0026rsquo;(x)}{f(x)}$ where $\\kappa_2 = \\int v^2 K(v)dv$.\nRemarks If the function is smooth enough and the bandwidth small enough, you can ignore the bias relative to sampling variation. To make this plausible, use a smaller bandwidth than would be the “optimal”. All kernel regression estimators can be written as a weighted average $$ \\hat{g}(x) = \\frac{1}{n} \\sum_{i=1}^n w_i (x) y_i, \\quad \\text{ with } \\quad w_i (x) = \\frac{n K_h (x_i - x)}{\\sum_{i=1}^n K_h (x_i - x)} $$ Do inference as if you were estimating a mean $\\mathbb E[z_i]$ with sample mean $\\frac{1}{n} \\sum_{i=1}^n z_i$ using $z_i = w_i (x) y_i$. If you are doing inference at more than one value of $x$, do inference as in the previous point, treating each value of $x$ as a different sample mean and note that even with independent data, these means will be correlated in general because there will generally be some common observations in to each of the averages. If you have a time series, make sure you account for correlation between the observations going in the different averages even if they don’t overlap. Issue when doing inference: the estimation of the bandwidth from the data is generally not accounted for in the distributional approximation (when doing inference). In large-samples, this is unlikely to lead to large changes, but uncertainty is understated in small samples.\nBias-Variance Trade-off Theorem\nFor any estimator mean-square error MSE is decomposable into variance and bias-squared: $$ \\text{MSE} (\\bar{x}, \\hat{g}) = \\mathbb E \\left[ \\left( \\hat{g}(\\bar{x}) - g_0 (\\bar{x}) \\right)^2 \\right] = \\mathbb E \\Big[\\underbrace{ \\hat{g}(\\bar{x}) - g_0 (\\bar{x}) }_{\\text{Bias}} \\Big]^2 + Var (\\hat{g} (\\bar{x})). $$\nProof The theorem follows from the following corollary.\nCorollary\nLet $A$ be a random variable and $\\theta_0$ a fixed parameter. Then, $$ \\mathbb E [ (A - \\theta_0)^2] = Var (A) + \\mathbb E [A-\\theta_0]^2 $$\nProof $$ \\begin{aligned} \\mathbb E [ (A - \\theta_0)^2] \u0026amp; = \\mathbb E[A^2] - 2 \\mathbb E [A \\theta_0] + \\mathbb E [\\theta_0] \\newline \u0026amp; = \\mathbb E[A^2] \\underbrace{- \\mathbb E[A]^2 + E[A]^2}_{\\text{add and subtract}} - 2 \\mathbb E [A \\theta_0] + \\mathbb E [\\theta_0] \\newline \u0026amp; = Var(A) + \\mathbb E [A]^2 - 2 \\theta_0 \\mathbb E [A ] + \\mathbb E [\\theta_0] \\newline \u0026amp; = Var(A) + \\mathbb E [A - \\theta_0]^2 \\end{aligned} $$\nNote that $\\mathbb E [ (A - \\theta_0)^2] = \\mathbb E [A - \\theta_0]^2$. $$\\tag*{$\\blacksquare$}$$\nCriteria Which criteria should we use with non-parametric estimators?\nMean squared error (MSE): $$ \\text{MSE} (\\bar{x}) (\\hat{g}) = \\mathbb E \\left[ \\left( \\hat{g}(\\bar{x}) - g_0 (\\bar{x}) \\right)^2 \\right] $$ NB! This is the criterium we are going to use.\nIntegrated mean squared error (IMSE): $$ \\text{IMSE} ( \\hat{g} ) = \\mathbb E \\left[ \\int | \\hat{g} (x) - g_0 (x) |^2 \\mathrm{d} F(x) \\right] $$\nType I - Type II error.\nComments Hansen (2019): the theorem above implies that we can asymptotically approximate the MSE as $$ \\text{AMSE} = \\Big( h^2 \\sigma_k^2 B(x) \\Big)^2 + \\frac{\\kappa \\sigma^2(x)}{nh f(x)} \\approx \\text{const} \\cdot \\left( h^4 + \\frac{1}{n h} \\right) $$\nWhere\n$Var \\propto \\frac{1}{h n}$, where you can think of $n h$ as the effective sample size. Bias $\\propto h^2$, derived if $g_0$ is twice continuously differentiable using Taylor expansion. Trade-Off The asymptotic MSE is dominated by the larger of $h^4$ and $\\frac{1}{h n}$. Notice that the bias is increasing in $h$ and the variance is decreasing in $h$ (more smoothing means more observations are used for local estimation: this increases the bias but decreases estimation variance). To select $h$ to minimize the asymptotic MSE, these two components should balance each other: $$ \\frac{1}{h n} \\propto h^4 \\quad \\Rightarrow \\quad h \\propto n^{-1/5} $$\nThis result means that the bandwidth should take the form $h = c \\cdot n^{-1/5}$. The optimal constant $c$ depends on the kernel $k$ the bias function $B(x)$ and the marginal density $f_x(x)$. A common misinterpretation is to set $h = n^{-1/5}$ which is equivalent to setting $c = 1$ and is completely arbitrary. Instead, an empirical bandwidth selection rule such as cross-validation should be used in practice.\nGlobal Non-Parametric Estimation Series The goal is to try to globally approximate the CEF with a function $g(x)$. Series methods are based on the Stone-Weierstrass theorem: a real-valued continuous function $g(x)$ defined in a compact set can be approximated with polynomials for any degree of accuracy $$ g_0 (x) = p_1 (x) \\beta _1 + \\dots + p_K (x) \\beta_K + r(x) $$ where $p_1(x), \\dots, p_K(x)$ are called ``a dictionary of approximating series’’ and $r(x)$ is a remainder function. If $p_1(x), \\dots, p_K(x)$ are sufficiently rich, $r(x)$ will be small. If $K \\to \\infty$, then $r \\to 0$.\nExample - Taylor series: if $g(x)$ is infinitely differentiable, then $$ g(x) = \\sum_{k=0}^{\\infty } a_k x^k $$ where $a_k = \\frac{1}{k!} \\frac{\\partial^k g_0}{\\partial x^k}$.\nIn Practice The basic idea is to approximate the infinite sum by chopping it off after $K$ terms and then estimate the coefficients by OLS.\nSeries estimation:\nChoose $K$, i.e. the number of series terms, and an approximating dictionary $p_1(x), \\dots, p_K(x)$ Expand data to $D = \\left( y_i, p_1(x_i), \\dots, p_K(x_i) \\right)_{i=1}^n$ Estimate OLS to get $\\hat{\\beta}_1, \\dots, \\hat{\\beta}_K$ Set $\\hat{g}(x) = p_1 (x)\\hat{\\beta}_1 + \\dots + p_K(x) \\hat{\\beta}_K$ Examples Monomials: $p_1(x) = 1, p_2(x) = x, p_3(x)=x^2, \\dots$\nHermite Polynomials: $p_1(x) = 1$, $p_2(x) = x$, $p_3(x)=x^2 -1$, $p_4(x)= x^3 - 3x, \\dots$. Con: edge effects. The estimated function is particularly volatile at the edges of the sample space (Gibbs effect)\nTrig Polynomials: $p_1(x) = 1$, $p_2(x) = \\cos 2 \\pi x$, $p_3(x)= \\sin 2 \\pi x$, $p_4(x) = \\cos 2 \\pi x \\cdot 2 x \\dots$. Pro: cyclical therefore good for series. Con: edge effects\nB-splines: recursively constructed using knot points $$ B_{i, 0} = \\begin{cases} 1 \u0026amp; \\text{if } t_i \\leq x \u0026lt; t_{i+1} \\newline 0 \u0026amp; \\text{otherwise} \\end{cases} \\qquad B_{i_k} (x) = \\frac{x - t_i}{ t_{i+k} - t_i} B_{i, k-1} (x) + \\frac{t_{i+k+1}-x}{t_{i+k+1} - t_{i+1}} B_{i+1, k-1} (x) $$ where $t_0, \\dots, t_i, \\dots$ are knot points and $k$ is the order of the spline. Pro: faster rate of convergence and lower asymptotic bias.\nHermite Polynomials Estimation Given $K$, inference proceeds exactly as if one had run an OLS of $y$ on $(p_k)_{k=1}^K$. The idea is that you ignore that you are doing non-parametric regression as long as you believe you have put enough terms (high $K$). Then the function is smooth enough so that the bias of the approximation is small relative to the variance (see Newey, 1997). Note that his approximation does not account for data-dependent estimation of the bandwidth.\nConsistency Newey (1997): results about consistency of $\\hat{g}$ and asymptotic normality of $\\hat{g}$.\nOLS: $\\hat{\\beta} \\overset{p}{\\to} \\beta_0$ Non-parametric: you have a sequence $\\lbrace\\beta_k\\rbrace_{k=1}^K$ with $\\hat{\\beta}_k \\overset{p}{\\to} \\beta_k$ as $n \\to \\infty$ (as $k \\to \\infty$). However, this does not make sense because $\\lbrace\\beta_k\\rbrace$ is not constant. Moreover, $\\beta_k$ is not the quantity of interest. We want to make inference on $\\hat{g}(x)$. Theorem\nUnder regularity conditions, including $| | \\hat{\\beta} - \\beta_0 | | \\overset{p}{\\to} 0$,\nUniform Consistency: $\\sup_x | \\hat{g}(x) - g_0(x)| \\overset{p}{\\to} 0$ Mean-square Consistency: $\\int | \\hat{g}(x) - g_0(x)|^2 \\mathrm{d} F(x) \\overset{p}{\\to} 0$ IMSE Theorem\nUnder the following assumptions:\n$(x_i, y_i)$ are iid and $Var(y_i|x_i)$ is bounded; For all $K$, there exists a non-singular matrix $B$ such that $A = \\left[ (B p(x)) (B p(x))\u0026rsquo; \\right]$ where $p(x) = \\left( p_1(x), \\dots, p_K (x) \\right)$ has the properties that $\\lambda_{\\min} (A)^{-1} = O(1)$. In addition, $\\sup_x | | B p(x) | | = o(\\sqrt{K/n})$. There exists $\\alpha$ and $\\beta_K$ for all $K$ such that $$ \\sup_x | g_0 (x) - p(x) \\beta_K | = O_p(K^{-\\alpha}) $$ Then, it holds that\n$$ \\text{IMSE = }\\int \\left( g_0 (x) - \\hat{g} (x) \\right)^2 \\mathrm{d} F(x) = O_p \\left( \\frac{K}{n} + K^{-2\\alpha}\\right) $$\nChoice of the optimal $K$ The bias-variance trade-off for series comes in through the choice of $K$:\nHigher $K$: smaller bias, since we are leaving out less terms form the infinite sum. Smaller $K$: smaller variance, since we are estimating less regression coefficients from the same amount of data. Cross-validation for series: For each $K \\geq 0$ and for each $i=1, \\dots, n$, consider\n$$ D_{-i} = \\lbrace (x_1, y_1), \\dots, (x_{i-1}, y_{i-1}),(x_{i+1}, y_{i+1}), \\dots (x_n, y_n) \\rbrace $$ and calculate $\\hat{g}^{(K)}_{-i} (x)$ using series estimate with $p_1(x), \\dots, p_K (x)$ in order to get $e^{(K)}i = y_i - \\hat{g}^{(K)}{-i} (x_i)$. Choose $\\hat{K}$ such that\n$$ \\hat{K} = \\arg \\min_K \\mathbb E_n \\left[ {e^{(K)}_i}^2 \\right] $$\nInference Consider the data $D = \\lbrace (x_i, y_i) \\rbrace_{i=1}^n$ such that $y_i = g_0 (x_i) + \\varepsilon_i$. You may want to form confidence intervals for quantities that depends on $g_0$.\nExample: $\\theta_0$ functional forms of interests:\nPoint estimate: $\\theta_0 = g_0 (\\bar{x} )$ for fixed $\\bar{x}$ Interval estimate: $\\theta_0 = g_0 (\\bar{x}_2) - g_0 (\\bar{x}_1)$ Point derivative estimate: $\\theta_0 = g_0 \u0026rsquo; (\\bar{x})$ at $\\bar{x}$ Average derivative $\\theta_0 = \\mathbb E [g_0 \u0026rsquo; (x) ]$ Consumer surplus: $\\theta_0 = \\int_a^b g_0(x)dx \\quad$ when $g_0$ is a demand function. Those estimates are functionals: maps from a function to a real number. We are doing inference on a function now, not on a point estimate.\nInference In order to form a confidence interval for $\\theta_0$, with series you can\nUndersmooth: in order to apply a \\textit{central limit theorem}{=tex}, you need deviations around the function to be approximately gaussian. Undersmoothing makes the function oscillate much more than the curve you are estimating in order to obtain such guassian deviations. Use the delta method. It would usually require more series terms than a criterion like cross-validation would suggest. Undersmoothing If on the contrary you oversmooth (e.g. $g_0$ linear), errors are going to constantly be on either one or the other side of the curve $\\to$ not gaussian!\nDelta Method Theorem: Under the assumptions of the consistency theorem $$ \\frac{\\sqrt{n} \\Big(\\hat{\\theta} - \\theta_0 + B(r_K) \\Big)}{\\sqrt{v_K}} \\overset{d}{\\to} N (0,1) $$\nTheorem: Under the assumptions of the consistency theorem and $\\sqrt{n} K^{-\\alpha} = o(1)$ (or equivalently $n K^{-2\\alpha} = O(1)$ in Hansen), $$ \\frac{\\sqrt{n} \\Big(\\hat{\\theta} - \\theta_0 \\Big)}{\\sqrt{v_K}} \\overset{d}{\\to} N (0,1) $$\nRemark The rate of convergence of splines is faster than for power series (Newey 1997). We have undersmoothing if $\\sqrt{n} K^{\\alpha} = o(1)$ (see comment below) Usually, in order to prove asymptotic normality, we first prove unbiasedness. However here we have a biased estimator but we make the bias converge to zero faster than the variance. Hansen (2019): The critical condition is the assumption that $\\sqrt{n} K^{\\alpha} = o(1)$ This requires that $K \\to \\infty$ at a rate faster than $n^{\\frac{1}{2\\alpha}}$ This is a troubling condition. The optimal rate for estimation of $g(x)$ is $K = O(n^{\\frac{1}{1+ 2\\alpha}})$. If we set $K = n^{\\frac{1}{1+ 2\\alpha}}$ by this rule then $n K^{-2\\alpha} = n^{\\frac{1}{1+ 2\\alpha}} \\to \\infty$ not zero. Thus this assumption is equivalent to assuming that $K$ is much larger than optimal. The reason why this trick works (that is, why the bias is negligible) is that by increasing $K$ the asymptotic bias decreases and the asymptotic variance increases and thus the variance dominates. Because $K$ is larger than optimal, we typically say that $\\hat{g}(x)$ is undersmoothed relative to the optimal series estimator.\nMore Remarks Many authors like to focus their asymptotic theory on the assumptions in the theorem, as the distribution of $\\theta$ appears cleaner. However, it is a poor use of asymptotic theory. There are three problems with the assumption $\\sqrt{n} K^{-\\alpha} = o(1)$ and the approximation of the theorem.\nFirst, it says that if we intentionally pick $K$ to be larger than optimal, we can increase the estimation variance relative to the bias so the variance will dominate the bias. But why would we want to intentionally use an estimator which is sub-optimal? Second, the assumption $\\sqrt{n} K^{-\\alpha} = o(1)$ does not eliminate the asymptotic bias, it only makes it of lower order than the variance. So the approximation of the theorem is technically valid, but the missing asymptotic bias term is just slightly smaller in asymptotic order, and thus still relevant in finite samples. Third, the condition $\\sqrt{n} K^{\\alpha} = o(1)$ is just an assumption, it has nothing to do with actual empirical practice. Thus the difference between the two theorems is in the assumptions, not in the actual reality or in the actual empirical practice. Eliminating a nuisance (the asymptotic bias) through an assumption is a trick, not a substantive use of theory. My strong view is that the result (1) is more informative than (2). It shows that the asymptotic distribution is normal but has a non-trivial finite sample bias. Kernel vs Series Hansen (2019): in this and the previous chapter we have presented two distinct methods of nonparametric regression based on kernel methods and series methods. Which should be used in practice? Both methods have advantages and disadvantages and there is no clear overall winner.\nFirst, while the asymptotic theory of the two estimators appear quite different, they are actually rather closely related. When the regression function $g(x)$ is twice differentiable $(s = 2)$ then the rate of convergence of both the MSE of the kernel regression estimator with optimal bandwidth $h$ and the series estimator with optimal $K$ is $n^{-\\frac{2}{k+4}}$ (where $k = \\dim(x)$). There is no difference. If the regression function is smoother than twice differentiable ($s \u0026gt; 2$) then the rate of the convergence of the series estimator improves. This may appear to be an advantage for series methods, but kernel regression can also take advantage of the higher smoothness by using so-called higher-order kernels or local polynomial regression, so perhaps this advantage is not too large.\nBoth estimators are asymptotically normal and have straightforward asymptotic standard error formulae. The series estimators are a bit more convenient for this purpose, as classic parametric standard error formula work without amendment.\nAdvantages of Kernels An advantage of kernel methods is that their distributional theory is easier to derive. The theory is all based on local averages which is relatively straightforward. In contrast, series theory is more challenging, dealing with increasing parameter spaces. An important difference in the theory is that for kernel estimators we have explicit representations for the bias while we only have rates for series methods. This means that plug-in methods can be used for bandwidth selection in kernel regression. However, typically we rely on cross-validation, which is equally applicable in both kernel and series regression.\nKernel methods are also relatively easy to implement when the dimension of $x$, $k$, is large. There is not a major change in the methodology as $k$ increases. In contrast, series methods become quite cumbersome as $k$ increases as the number of cross-terms increases exponentially. E.g ($K=2$) with $k=1$ you have only $\\lbrace x_1, x_1^2\\rbrace$; with $k=2$ you have to add $\\lbrace x_2, x_2^2, x_1 x_2 \\rbrace$; with $k=3$ you have to add $\\lbrace x_3, x_3^2, x_1 x_3, x_2 x_3\\rbrace$, etc..\nAdvantages of Series A major advantage of series methods is that it has inherently a high degree of flexibility, and the user is able to implement shape restrictions quite easily. For example, in series estimation it is relatively simple to implement a partial linear CEF, an additively separable CEF, monotonicity, concavity or convexity. These restrictions are harder to implement in kernel regression.\n","date":1635465600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1635465600,"objectID":"486b7a0f83ad926dab18785e2fbd49e6","permalink":"https://matteocourthoud.github.io/course/metrics/08_nonparametric/","publishdate":"2021-10-29T00:00:00Z","relpermalink":"/course/metrics/08_nonparametric/","section":"course","summary":"Introduction Non-parametric regression is a flexible estimation procedure for\nregression functions $\\mathbb E [y|x ] = g (x)$ and density functions $f(x)$. You want to let your data to tell you how flexible you can afford to be in terms of estimation procedures.","tags":null,"title":"Non-Parametric Estimation","type":"book"},{"authors":null,"categories":null,"content":"# Remove warnings import warnings warnings.filterwarnings('ignore') # Import everything import pandas as pd import numpy as np import seaborn as sns import statsmodels.api as sm from numpy.linalg import inv from statsmodels.iolib.summary2 import summary_col from sklearn.model_selection import train_test_split from sklearn.linear_model import LinearRegression, Lasso, Ridge from sklearn.tree import DecisionTreeRegressor from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor from sklearn.preprocessing import PolynomialFeatures, StandardScaler # Import matplotlib for graphs import matplotlib.pyplot as plt from mpl_toolkits.mplot3d import axes3d # Set global parameters %matplotlib inline plt.style.use('seaborn-white') plt.rcParams['lines.linewidth'] = 3 plt.rcParams['figure.figsize'] = (10,6) plt.rcParams['figure.titlesize'] = 20 plt.rcParams['axes.titlesize'] = 18 plt.rcParams['axes.labelsize'] = 14 plt.rcParams['legend.fontsize'] = 14 9.1 Frisch-Waugh theorem Consider the data $D = { x_i, y_i, z_i }_{i=1}^n$ with DGP:\n$$ y_i = x_i\u0026rsquo; \\alpha_0+ z_i\u0026rsquo; \\beta_0 + \\varepsilon_i $$\n. The following estimators of $\\alpha$ are numerically equivalent (if $[x, z]$ has full rank):\nOLS: $\\hat{\\alpha}$ from regressing $y$ on $x, z$ Partialling out: $\\tilde{\\alpha}$ from regressing $y$ on $\\tilde{x}$ \u0026ldquo;Double\u0026rdquo; partialling out: $\\bar{\\alpha}$ from regressing $\\tilde{y}$ on $\\tilde{x}$ where the operation of passing to $y, x$ to $\\tilde{y}, \\tilde{x}$ is called projection out $z$, e.g. $\\tilde{x}$ are the residuals from regressing $x$ on $z$.\n$$ \\tilde{x} = x - \\hat \\gamma z = (I - z (z\u0026rsquo; z)^{-1} z\u0026rsquo; ) x = (I-P_z) x = M_z x $$\nI.e we have done the following:\nregress $x$ on $z$ compute $\\hat x$ compute the residuals $\\tilde x = x - \\hat x$ We now explore the theorem through simulation. In particular, we generate a sample from the following model:\n$$ y_i = x_i - 0.3 z_i + \\varepsilon_i $$\nwhere $x_i,z_i,\\varepsilon_i \\sim N(0,1)$ and $n=1000$.\nnp.random.seed(1) # Init n = 1000 a = 1 b = -.3 # Generate data x = np.random.uniform(0,1,n).reshape(-1,1) z = np.random.uniform(0,1,n).reshape(-1,1) e = np.random.normal(0,1,n).reshape(-1,1) y = a*x + b*z + e Let\u0026rsquo;s compute the value of the OLS estimator.\n# Estimate alpha by OLS xz = np.concatenate([x,z], axis=1) ols_coeff = inv(xz.T @ xz) @ xz.T @ y alpha_ols = ols_coeff[0][0] print('alpha OLS: %.4f (true=%1.0f)' % (alpha_ols, a)) alpha OLS: 1.0928 (true=1) The partialling out estimator.\n# Partialling out x_tilde = (np.eye(n) - z @ inv(z.T @ z) @ z.T ) @ x alpha_po = inv(x_tilde.T @ x_tilde) @ x_tilde.T @ y print('alpha partialling out: %.4f (true=%1.0f)' % (alpha_po, a)) alpha partialling out: 1.0928 (true=1) And lastly, the double-partialling out estimator.\n# \u0026quot;Double\u0026quot; partialling out y_tilde = (np.eye(n) - z @ inv(z.T @ z) @ z.T ) @ y alpha_po2 = inv(x_tilde.T @ x_tilde) @ x_tilde.T @ y_tilde print('alpha double partialling out: %.4f (true=%1.0f)' % (alpha_po2, a)) alpha double partialling out: 1.0928 (true=1) 9.2 Omitted Variable Bias Consider two separate statistical models. Assume the following long regression of interest:\n$$ y_i = x_i\u0026rsquo; \\alpha_0+ z_i\u0026rsquo; \\beta_0 + \\varepsilon_i $$\nDefine the corresponding short regression as\n$$ y_i = x_i\u0026rsquo; \\alpha_0 + v_i \\quad \\text{ with } \\quad x_i = z_i\u0026rsquo; \\gamma_0 + u_i $$\nOVB Theorem Suppose that the DGP for the long regression corresponds to $\\alpha_0$, $\\beta_0$. Suppose further that $\\mathbb E[x_i] = 0$, $\\mathbb E[z_i] = 0$, $\\mathbb E[\\varepsilon_i |x_i,z_i] = 0$. Then, unless $\\beta_0 = 0$ or $z_i$ is orthogonal to $x_i$, the (sole) stochastic regressor $x_i$ is correlated with the error term in the short regression which implies that the OLS estimator of the short regression is inconsistent for $\\alpha_0$ due to the omitted variable bias. In particular, one can show that the plim of the OLS estimator of $\\hat{\\alpha}_{SHORT}$ from the short regression is\n$$ \\hat{\\alpha}_{SHORT} \\overset{p}{\\to} \\frac{Cov(y_i, x_i)}{Var(x_i)} = \\alpha_0 + \\beta_0 \\frac{Cov(z_i, x_i)}{Var(x_i)} $$\nConsider data $D= (y_i, x_i, z_i)_{i=1}^n$, where the true model is:\n$$ \\begin{aligned} \u0026amp; y_i = x_i\u0026rsquo; \\alpha_0 + z_i\u0026rsquo; \\beta_0 + \\varepsilon_i \\ \u0026amp; x_i = z_i\u0026rsquo; \\gamma_0 + u_i \\end{aligned} $$\nLet\u0026rsquo;s investigate the Omitted Variable Bias by simulation. In particular, we generate a sample from the following model:\n$$ \\begin{aligned} \u0026amp; y_i = x_i - 0.3 z_i + \\varepsilon_i \\ \u0026amp; x_i = 3 z_i + u_i \\ \\end{aligned} $$\nwhere $z_i,\\varepsilon_i,u_i \\sim N(0,1)$ and $n=1000$.\ndef generate_data(a, b, c, n): # Generate data z = np.random.normal(0,1,n).reshape(-1,1) u = np.random.normal(0,1,n).reshape(-1,1) x = c*z + u e = np.random.normal(0,1,n).reshape(-1,1) y = a*x + b*z + e return x, y, z First let\u0026rsquo;s compute the value of the OLS estimator.\n# Init n = 1000 a = 1 b = -.3 c = 3 x, y, z = generate_data(a, b, c, n) # Estimate alpha by OLS ols_coeff = inv(x.T @ x) @ x.T @ y alpha_short = ols_coeff[0][0] print('alpha OLS: %.4f (true=%1.0f)' % (alpha_short, a)) alpha OLS: 0.9115 (true=1) In our case the expected bias is:\n$$ \\begin{aligned} Bias \u0026amp; = \\beta_0 \\frac{Cov(z_i, x_i)}{Var(x_i)} = \\ \u0026amp; = \\beta_0 \\frac{Cov(z_i\u0026rsquo; \\gamma_0 + u_i, x_i)}{Var(z_i\u0026rsquo; \\gamma_0 + u_i)} = \\ \u0026amp; = \\beta_0 \\frac{\\gamma_0 Var(z_i)}{\\gamma_0^2 Var(z_i) + Var(u_i)} \\end{aligned} $$\nwhich in our case is $b \\frac{c}{c^2 + 1}$.\n# Expected bias bias = alpha_short - a exp_bias = b * c / (c**2 + 1) print('Empirical bias: %.4f \\nExpected bias: %.4f' % (bias, exp_bias)) Empirical bias: -0.0885 Expected bias: -0.0900 9.3 Pre-Test Bias Consider data $D= (y_i, x_i, z_i)_{i=1}^n$, where the true model is:\n$$ \\begin{aligned} \u0026amp; y_i = x_i\u0026rsquo; \\alpha_0 + z_i\u0026rsquo; \\beta_0 + \\varepsilon_i \\ \u0026amp; x_i = z_i\u0026rsquo; \\gamma_0 + u_i \\end{aligned} $$\nWhere $x_i$ is the variable of interest (we want to make inference on $\\alpha_0$) and $z_i$ is a high dimensional set of control variables.\nFrom now on, we will work under the following assumptions:\n$\\dim(x_i)=1$ for all $n$ $\\beta_0$ uniformely bounded in $n$ Strict exogeneity: $\\mathbb E[\\varepsilon_i | x_i, z_i] = 0$ and $\\mathbb E[u_i | z_i] = 0$ $\\beta_0$ and $\\gamma_0$ have dimension (and hence value) that depend on $n$ Pre-Testing procedure:\nRegress $y_i$ on $x_i$ and $z_i$ For each $j = 1, \u0026hellip;, p = \\dim(z_i)$ calculate a test statistic $t_j$ Let $\\hat{T} = { j: |t_j| \u0026gt; C \u0026gt; 0 }$ for some constant $C$ (set of statistically significant coefficients). Re-run the new \u0026ldquo;model\u0026rdquo; using $(x_i, z_{\\hat{T},i})$ (i.e. using the selected covariates with statistically significant coefficients). Perform statistical inference (i.e. confidence intervals and hypothesis tests) as if no model selection had been done. Pre-testing leads to incorrect inference. Why? Because of test errors in the first stage.\n# T-test def t_test(y, x, k): beta_hat = inv(x.T @ x) @ x.T @ y residuals = y - x @ beta_hat sigma2_hat = np.var(residuals) beta_std = np.sqrt(np.diag(inv(x.T @ x)) * sigma2_hat ) return beta_hat[k,0]/beta_std[k] First of all the t-test for $H_0: \\beta_0 = 0$:\n$$ t = \\frac{\\hat \\beta_k}{\\hat \\sigma_{\\beta_k}} $$\nwhere the standard deviation of the ols coefficient is given by\n$$ \\hat \\sigma_{\\beta_k} = \\sqrt{ \\hat \\sigma^2 \\cdot (X\u0026rsquo;X)^{-1}_{[k,k]} } $$\nwhere we estimate the variance of the error term with the variance of the residuals\n$$ \\hat \\sigma^2 = Var \\big( y - \\hat y \\big) = Var \\big( y - X (X\u0026rsquo;X)^{-1}X\u0026rsquo;y \\big) $$\n# Pre-testing def pre_testing(a, b, c, n, simulations=1000): np.random.seed(1) # Init alpha = {'Long': np.zeros((simulations,1)), 'Short': np.zeros((simulations,1)), 'Pre-test': np.zeros((simulations,1))} # Loop over simulations for i in range(simulations): # Generate data x, y, z = generate_data(a, b, c, n) xz = np.concatenate([x,z], axis=1) # Compute coefficients alpha['Long'][i] = (inv(xz.T @ xz) @ xz.T @ y)[0][0] alpha['Short'][i] = inv(x.T @ x) @ x.T @ y # Compute significance of z on y t = t_test(y, xz, 1) # Select specification based on test if np.abs(t)\u0026gt;1.96: alpha['Pre-test'][i] = alpha['Long'][i] else: alpha['Pre-test'][i] = alpha['Short'][i] return alpha Let\u0026rsquo;s compare the different estimates.\n# Get pre_test alpha alpha = pre_testing(a, b, c, n) for key, value in alpha.items(): print('Mean alpha %s = %.4f' % (key, np.mean(value))) Mean alpha Long = 0.9994 Mean alpha Short = 0.9095 Mean alpha Pre-test = 0.9925 The pre-testing coefficient is very close to the true coefficient.\nHowever, the main effect of pre-testing is on inference. With pre-testing, the distribution of the estimator is not gaussian anymore.\ndef plot_alpha(alpha, a): fig = plt.figure(figsize=(17,6)) # Plot distributions x_max = np.max([np.max(np.abs(x-a)) for x in alpha.values()]) # All axes for i, key in enumerate(alpha.keys()): # Reshape exisiting subplots k = len(fig.axes) for i in range(k): fig.axes[i].change_geometry(1, k+1, i+1) # Add new plot ax = fig.add_subplot(1, k+1, k+1) ax.hist(alpha[key], bins=30) ax.set_title(key) ax.set_xlim([a-x_max, a+x_max]) ax.axvline(a, c='r', ls='--') legend_text = [r'$\\alpha_0=%.0f$' % a, r'$\\hat \\alpha=%.4f$' % np.mean(alpha[key])] ax.legend(legend_text, prop={'size': 10}) Let\u0026rsquo;s compare the long, short and pre-test estimators.\n# Plot plot_alpha(alpha, a) As we can see, the main problem of pre-testing is inference.\nBecause of the testing procedure, the distribution of the estimator is a combination of tho different distributions: the one resulting from the long regression and the one resulting from the short regression. Pre-testing is not a problem in 3 cases:\nwhen $\\beta_0$ is very large: in this case the test always rejects the null hypothesis $H_0 : \\beta_0=0$ and we always run the correct specification, i.e. the long regression\nwhen $\\beta_0$ is very small: in this case the test has very little power. However, as we saw from the Omitted Variable Bias formula, the bias is small.\nwhen $\\gamma_0$ is very small: also in this case the test has very little power. However, as we saw from the Omitted Variable Bias formula, the bias is small.\nLet\u0026rsquo;s compare the pre-test estimates for different values of the true parameter $\\beta$.\n# Case 1: different betas and same sample size b_sequence = b*np.array([0.1,0.3,1,3]) alpha = {} # Get sequence for k, b_ in enumerate(b_sequence): label = 'beta = %.2f' % b_ alpha[label] = pre_testing(a, b_, c, n)['Pre-test'] print('Mean alpha with beta=%.2f: %.4f' % (b_, np.mean(alpha[label]))) Mean alpha with beta=-0.03: 0.9926 Mean alpha with beta=-0.09: 0.9826 Mean alpha with beta=-0.30: 0.9925 Mean alpha with beta=-0.90: 0.9994 The means are similar, but let\u0026rsquo;s look at the distributions.\n# Plot plot_alpha(alpha, a) When $\\beta_0$ is \u0026ldquo;small\u0026rdquo;, the distribution of the pre-testing estimator for $\\alpha$ is not normal.\nHowever, the magnitue of $\\beta_0$ is a relative concept. For an infinite sample size, $\\beta_0$ is always going to be \u0026ldquo;big enough\u0026rdquo;, in the sense that with an infinite sample size the probability fo false positives in testing $H_0: \\beta_0 = 0$ is going to zero. I.e. we always select the correct model specification, the long regression.\nLet\u0026rsquo;s have a look at the distibution of $\\hat \\alpha_{\\text{PRE-TEST}}$ when the sample size increaes.\n# Case 2: same beta and different sample sizes n_sequence = [100,300,1000,3000] alpha = {} # Get sequence for k, n_ in enumerate(n_sequence): label = 'n = %.0f' % n_ alpha[label] = pre_testing(a, b, c, n_)['Pre-test'] print('Mean alpha with n=%.0f: %.4f' % (n_, np.mean(alpha[label]))) Mean alpha with n=100: 0.9442 Mean alpha with n=300: 0.9635 Mean alpha with n=1000: 0.9925 Mean alpha with n=3000: 0.9989 # Plot plot_alpha(alpha, a) As we can see, for large samples, $\\beta_0$ is never \u0026ldquo;small\u0026rdquo;. In the limit, when $n \\to \\infty$, the probability of false positives while testing $H_0: \\beta_0 = 0$ goes to zero.\nWe face a dilemma:\npre-testing is clearlly a problem in finite samples all our econometric results are based on the assumption that $n \\to \\infty$ The problem is solved by assuming that the value of $\\beta_0$ depends on the sample size. This might seems like a weird assumption but is just to have an asymptotically meaningful concept of \u0026ldquo;big\u0026rdquo; and \u0026ldquo;small\u0026rdquo;.\nWe now look at what happens in the simulations when $\\beta_0$ is proportional to $\\frac{1}{\\sqrt{n}}$.\n# Case 3: beta proportional to 1/sqrt(n) and different sample sizes beta = b * 30 / np.sqrt(n_sequence) # Get sequence alpha = {} for k, n_ in enumerate(n_sequence): label = 'n = %.0f' % n_ alpha[label] = pre_testing(a, beta[k], c, n_)['Pre-test'] print('Mean alpha with n=%.0f: %.4f' % (n_, np.mean(alpha[label]))) Mean alpha with n=100: 0.9703 Mean alpha with n=300: 0.9838 Mean alpha with n=1000: 0.9914 Mean alpha with n=3000: 0.9947 # Plot plot_alpha(alpha, a) Now the distribution of $\\hat \\alpha$ does not converge to a normal when the sample size increases.\nPre-Testing and Machine Learning How are machine learning and pre-testing related? The best example is Lasso. Suppose you have a dataset with many variables. This means that you have very few degrees of freedom and your OLS estimates are going to be very imprecise. At the extreme, you have more variables than observations so that your OLS coefficient is undefined since you cannot invert the design matrix $X\u0026rsquo;X$.\nIn this case, you might want to do variable selection. One way of doing variable selection is pre-testing. Another way is Lasso. A third alternative is to use machine learning methods that do not suffer this curse of dimensionality.\nThe purpose and outcome of pre-testing and Lasso are the same:\nyou have too many variables you exclude some of them from the regression / set their coefficients to zero As a consequence, also the problems are the same, i.e. pre-test bias.\n9.4 Post-Double Selection Consider again data $D= (y_i, x_i, z_i)_{i=1}^n$, where the true model is:\n$$ \\begin{aligned} \u0026amp; y_i = x_i\u0026rsquo; \\alpha_0 + z_i\u0026rsquo; \\beta_0 + \\varepsilon_i \\ \u0026amp; x_i = z_i\u0026rsquo; \\gamma_0 + u_i \\end{aligned} $$\nWe would like to guard against pretest bias if possible, in order to handle high dimensional models. A good pathway towards motivating procedures which guard against pretest bias is a discussion of classical partitioned regression.\nConsider a regression $y_i$ on $x_i$ and $z_i$. $x_i$ is the 1-dimensional variable of interest, $z_i$ is a high-dimensional set of control variables. We have the following procedure:\nFirst Stage selection: regress $x_i$ on $z_i$. Select the statistically significant variables in the set $S_{FS} \\subseteq z_i$ Reduced Form selection: lasso $y_i$ on $z_i$. Select the statistically significant variables in the set $S_{RF} \\subseteq z_i$ Regress $y_i$ on $x_i$ and $S_{FS} \\cup S_{RF}$ Theorem: Let ${P^n}$ be a sequence of data-generating processes for $D_n = (y_i, x_i, z_i)^n_{i=1} \\in (\\mathbb R \\times \\mathbb R \\times \\mathbb R^p) ^n$ where $p$ depends on $n$. For each $n$, the data are iid with $yi = x_i\u0026rsquo;\\alpha_0^{(n)} + z_i\u0026rsquo; \\beta_0^{(n)} + \\varepsilon_i$ and $x_i = z_i\u0026rsquo; \\gamma_0^{(n)} + u_i$ where $\\mathbb E[\\varepsilon_i | x_i,z_i] = 0$ and $\\mathbb E[u_i|z_i] = 0$. The sparsity of the vectors $\\beta_0^{(n)}$, $\\gamma_0^{(n)}$ is controlled by $|| \\beta_0^{(n)} ||_0 \\leq s$ with $s^2 (\\log p)^2/n \\to 0$. Suppose that additional regularity conditions on the model selection procedures and moments of the random variables $y_i$ , $x_i$ , $z_i$ as documented in Belloni et al. (2014). Then the confidence intervals, CI, from the post double selection procedure are uniformly valid. That is, for any confidence level $\\xi \\in (0, 1)$ $$ \\Pr(\\alpha_0 \\in CI) \\to 1- \\xi $$\nIn order to have valid confidence intervals you want their bias to be negligibly. Since $$ CI = \\left[ \\hat{\\alpha} \\pm \\frac{1.96 \\cdot \\hat{\\sigma}}{\\sqrt{n}} \\right] $$\nIf the bias is $o \\left( \\frac{1}{\\sqrt{n}} \\right)$ then there is no problem since it is asymptotically negligible w.r.t. the magnitude of the confidence interval. If however the the bias is $O \\left( \\frac{1}{\\sqrt{n}} \\right)$ then it has the same magnitude of the confidence interval and it does not asymptotically vanish.\nThe idea of the proof is to use partitioned regression. An alternative way to think about the argument is: bound the omitted variables bias. Omitted variable bias comes from the product of 2 quantities related to the omitted variable:\nIts partial correlation with the outcome, and Its partial correlation with the variable of interest. If both those partial correlations are $O( \\sqrt{\\log p/n})$, then the omitted variables bias is $(s \\times O( \\sqrt{\\log p/n})^2 = o \\left( \\frac{1}{\\sqrt{n}} \\right)$, provided $s^2 (\\log p)^2/n \\to 0$. Relative to the $ \\frac{1}{\\sqrt{n}} $ convergence rate, the omitted variables bias is negligible.\nIn our omitted variable bias case, we want $| \\beta_0 \\gamma_0 | = o \\left( \\frac{1}{\\sqrt{n}} \\right)$. Post-double selection guarantees that\nReduced form selection (pre-testing): any \u0026ldquo;missing\u0026rdquo; variable has $|\\beta_{0j}| \\leq \\frac{c}{\\sqrt{n}}$ First stage selection (additional): any \u0026ldquo;missing\u0026rdquo; variable has $|\\gamma_{0j}| \\leq \\frac{c}{\\sqrt{n}}$ As a consequence, as long as the number of omitted variables is finite, the omitted variable bias is $$ OVB(\\alpha) = |\\beta_{0j}| \\cdot|\\gamma_{0j}| \\leq \\frac{c}{\\sqrt{n}} \\cdot \\frac{c}{\\sqrt{n}} = \\frac{c^2}{n} = o \\left(\\frac{1}{\\sqrt{n}}\\right) $$\n# Pre-testing code def post_double_selection(a, b, c, n, simulations=1000): np.random.seed(1) # Init alpha = {'Long': np.zeros((simulations,1)), 'Short': np.zeros((simulations,1)), 'Pre-test': np.zeros((simulations,1)), 'Post-double': np.zeros((simulations,1))} # Loop over simulations for i in range(simulations): # Generate data x, y, z = generate_data(a, b, c, n) # Compute coefficients xz = np.concatenate([x,z], axis=1) alpha['Long'][i] = (inv(xz.T @ xz) @ xz.T @ y)[0][0] alpha['Short'][i] = inv(x.T @ x) @ x.T @ y # Compute significance of z on y (beta hat) t1 = t_test(y, xz, 1) # Compute significance of z on x (gamma hat) t2 = t_test(x, z, 0) # Select specification based on first test if np.abs(t1)\u0026gt;1.96: alpha['Pre-test'][i] = alpha['Long'][i] else: alpha['Pre-test'][i] = alpha['Short'][i] # Select specification based on both tests if np.abs(t1)\u0026gt;1.96 or np.abs(t2)\u0026gt;1.96: alpha['Post-double'][i] = alpha['Long'][i] else: alpha['Post-double'][i] = alpha['Short'][i] return alpha Let\u0026rsquo;s now repeat the same exercise as above, but with also post-double selection\n# Get pre_test alpha alpha = post_double_selection(a, b, c, n) for key, value in alpha.items(): print('Mean alpha %s = %.4f' % (key, np.mean(value))) Mean alpha Long = 0.9994 Mean alpha Short = 0.9095 Mean alpha Pre-test = 0.9925 Mean alpha Post-double = 0.9994 # Plot plot_alpha(alpha, a) As we can see, post-double selection has solved the pre-testing problem. Does it work for any magnitude of $\\beta$ (relative to the sample size)?\nWe first have a look at the case in which the sample size is fixed and $\\beta_0$ changes.\n# Case 1: different betas and same sample size b_sequence = b*np.array([0.1,0.3,1,3]) alpha = {} # Get sequence for k, b_ in enumerate(b_sequence): label = 'beta = %.2f' % b_ alpha[label] = post_double_selection(a, b_, c, n)['Post-double'] print('Mean alpha with beta=%.2f: %.4f' % (b_, np.mean(alpha[label]))) Mean alpha with beta=-0.03: 0.9994 Mean alpha with beta=-0.09: 0.9994 Mean alpha with beta=-0.30: 0.9994 Mean alpha with beta=-0.90: 0.9994 # Plot plot_alpha(alpha, a) Post-double selection always selects the correct specification, the long regression, even when $\\beta$ is very small.\nNow we check the same but for fixed $\\beta_0$ and different sample sizes.\n# Case 2: same beta and different sample sizes n_sequence = [100,300,1000,3000] alpha = {} # Get sequence for k, n_ in enumerate(n_sequence): label = 'N = %.0f' % n_ alpha[label] = post_double_selection(a, b, c, n_)['Post-double'] print('Mean alpha with n=%.0f: %.4f' % (n_, np.mean(alpha[label]))) Mean alpha with n=100: 0.9964 Mean alpha with n=300: 0.9985 Mean alpha with n=1000: 0.9994 Mean alpha with n=3000: 0.9990 # Plot plot_alpha(alpha, a) Post-double selection always selects the correct specification, the long regression, even when the sample size is very small.\nLast, we check the case of $\\beta_0$ proportional to $\\frac{1}{\\sqrt{n}}$.\n# Case 3: beta proportional to 1/sqrt(n) and different sample sizes beta = b * 30 / np.sqrt(n_sequence) # Get sequence alpha = {} for k, n_ in enumerate(n_sequence): label = 'N = %.0f' % n_ alpha[label] = post_double_selection(a, beta[k], c, n_)['Post-double'] print('Mean alpha with n=%.0f: %.4f' % (n_, np.mean(alpha[label]))) Mean alpha with n=100: 0.9964 Mean alpha with n=300: 0.9985 Mean alpha with n=1000: 0.9994 Mean alpha with n=3000: 0.9990 # Plot plot_alpha(alpha, a) Once again post-double selection always selects the correct specification, the long regression.\nPost-double Selection and Machine Learning As we have seen at the end of the previous section, Lasso can be used to perform variable selection in high dimensional settings. Therefore, post-double selection solves the pre-test bias problem in those settings. The post-double selection procedure with Lasso is:\nFirst Stage selection: lasso $x_i$ on $z_i$. Let the selected variables be collected in the set $S_{FS} \\subseteq z_i$ Reduced Form selection: lasso $y_i$ on $z_i$. Let the selected variables be collected in the set $S_{RF} \\subseteq z_i$ Regress $y_i$ on $x_i$ and $S_{FS} \\cup S_{RF}$ 9.5 Double/debiased Machine Learning This section is taken from Chernozhukov, V., Chetverikov, D., Demirer, M., Duflo, E., Hansen, C., Newey, W., \u0026amp; Robins, J. (2018). \u0026ldquo;Double/debiased machine learning for treatment and structural parameters\u0026rdquo;.\nConsider the following partially linear model\n$$ y = \\beta_0 D + g_0(X) + u \\ D = m_0(X) + v $$\nwhere $y$ is the outcome variable, $D$ is the treatment to interest and $X$ is a potentially high-dimensional set of controls.\nNaive approach A naive approach to estimation of $\\beta_0$ using ML methods would be, for example, to construct a sophisticated ML estimator $\\beta_0 D + g_0(X)$ for learning the regression function $\\beta_0 D$ + $g_0(X)$.\nSplit the sample in two: main sample and auxiliary sample Use the auxiliary sample to estimate $\\hat g_0(X)$ Use the main sample to compute the orthogonalized component of $Y$ on $X$: $\\hat u = \\left(Y_{i}-\\hat{g}{0}\\left(X{i}\\right)\\right)$ Use the main sample to estimate the residualized OLS estimator $$ \\hat{\\beta}{0}=\\left(\\frac{1}{n} \\sum{i \\in I} D_{i}^{2}\\right)^{-1} \\frac{1}{n} \\sum_{i \\in I} D_{i} \\hat u_i $$\nThis estimator is going to have two problems:\nSlow rate of convergence, i.e. slower than $\\sqrt(n)$ It will be biased because we are employing highdimensional regularized estimators (e.g. we are doing variable selection) Orthogonalization Now consider a second construction that employs an orthogonalized formulation obtained by directly partialling out the effect of $X$ from $D$ to obtain the orthogonalized regressor $v = D − m_0(X)$.\nSplit the sample in two: main sample and auxiliary sample\nUse the auxiliary sample to estimate $\\hat g_0(X)$ from\n$$ y = \\beta_0 D + g_0(X) + u \\ $$\nUse the auxiliary sample to estimate $\\hat m_0(X)$ from\n$$ D = m_0(X) + v $$\nUse the main sample to compute the orthogonalized component of $D$ on $X$ as\n$$ \\hat v = D - \\hat m_0(X) $$\nUse the main sample to estimate the double-residualized OLS estimator as\n$$ \\hat{\\beta}{0}=\\left(\\frac{1}{n} \\sum{i \\in I} \\hat v_i D_{i} \\right)^{-1} \\frac{1}{n} \\sum_{i \\in I} \\hat v_i \\left( Y - \\hat g_0(X) \\right) $$\nThe estimator is unbiased but still has a lower rate of convergence because of sample splitting. The problem is solved by inverting the split sample, re-estimating the coefficient and averaging the two estimates. Note that this procedure is valid since the two estimates are independent by the sample splitting procedure.\nApplication to AJR02 In this section we are going to replicate 6.3 of the \u0026ldquo;Double/debiased machine learning\u0026rdquo; paper based on Acemoglu, Johnson, Robinson (2002), \u0026ldquo;The Colonial Origins of Comparative Development\u0026rdquo;.\nWe first load the dataset\n# Load Acemoglu Johnson Robinson Dataset df = pd.read_csv('data/AJR02.csv',index_col=0) df.head() GDP Exprop Mort Latitude Neo Africa Asia Namer Samer logMort Latitude2 1 8.39 6.50 78.20 0.3111 0 1 0 0 0 4.359270 0.096783 2 7.77 5.36 280.00 0.1367 0 1 0 0 0 5.634790 0.018687 3 9.13 6.39 68.90 0.3778 0 0 0 0 1 4.232656 0.142733 4 9.90 9.32 8.55 0.3000 1 0 0 0 0 2.145931 0.090000 5 9.29 7.50 85.00 0.2683 0 0 0 1 0 4.442651 0.071985 df.info() \u0026lt;class 'pandas.core.frame.DataFrame'\u0026gt; Int64Index: 64 entries, 1 to 64 Data columns (total 11 columns): # Column Non-Null Count Dtype --- ------ -------------- ----- 0 GDP 64 non-null float64 1 Exprop 64 non-null float64 2 Mort 64 non-null float64 3 Latitude 64 non-null float64 4 Neo 64 non-null int64 5 Africa 64 non-null int64 6 Asia 64 non-null int64 7 Namer 64 non-null int64 8 Samer 64 non-null int64 9 logMort 64 non-null float64 10 Latitude2 64 non-null float64 dtypes: float64(6), int64(5) memory usage: 6.0 KB In their paper, AJR note that their IV strategy will be invalidated if other factors are also highly persistent and related to the development of institutions within a country and to the country’s GDP. A leading candidate for such a factor, as they discuss, is geography. AJR address this by assuming that the confounding effect of geography is adequately captured by a linear term in distance from the equator and a set of continent dummy variables.\nThey inclue their results in table 2.\n# Add constant term to dataset df['const'] = 1 # Create lists of variables to be used in each regression X1 = df[['const', 'Exprop']] X2 = df[['const', 'Exprop', 'Latitude', 'Latitude2']] X3 = df[['const', 'Exprop', 'Latitude', 'Latitude2', 'Asia', 'Africa', 'Namer', 'Samer']] y = df['GDP'] # Estimate an OLS regression for each set of variables reg1 = sm.OLS(y, X1, missing='drop').fit() reg2 = sm.OLS(y, X2, missing='drop').fit() reg3 = sm.OLS(y, X3, missing='drop').fit() Let\u0026rsquo;s replicate Table 2 in AJR.\n# Make table 2 def make_table_2(): info_dict={'No. observations' : lambda x: f\u0026quot;{int(x.nobs):d}\u0026quot;} results_table = summary_col(results=[reg1,reg2,reg3], float_format='%0.2f', stars = True, model_names=['Model 1','Model 2','Model 3'], info_dict=info_dict, regressor_order=['const','Exprop','Latitude','Latitude2']) return results_table table_2 = make_table_2() table_2 Model 1 Model 2 Model 3 const 4.66*** 4.55*** 5.95*** (0.41) (0.45) (0.68) Exprop 0.52*** 0.49*** 0.40*** (0.06) (0.07) (0.06) Latitude 2.16 0.42 (1.68) (1.47) Latitude2 -2.12 0.44 (2.86) (2.48) Africa -1.06** (0.41) Asia -0.74* (0.42) Namer -0.17 (0.40) Samer -0.12 (0.42) No. observations 64 64 64 Using DML allows us to relax this assumption and to replace it by a weaker assumption that geography can be sufficiently controlled by an unknown function of distance from the equator and continent dummies, which can be learned by ML methods.\nIn particular, our framework is\n$$ {GDP} = \\beta_0 \\times {Exprop} + g_0({geography}) + u \\ {Exprop} = m_0({geography}) + u $$\nSo that the double/debiased machine learning procedure is\nSplit the sample in two: main sample and auxiliary sample\nUse the auxiliary sample to estimate $\\hat g_0({geography})$ from\n$$ {GDP} = \\beta_0 \\times {Exprop} + g_0({geography}) + u $$\nUse the auxiliary sample to estimate $\\hat m_0({geography})$ from\n$$ {Exprop} = m_0({geography}) + v $$\nUse the main sample to compute the orthogonalized component of ${Exprop}$ on ${geography}$ as\n$$ \\hat v = {Exprop} - \\hat m_0({geography}) $$\nUse the main sample to estimate the double-residualized OLS estimator as\n$$ \\hat{\\beta}{0}=\\left(\\frac{1}{n} \\sum{i \\in I} \\hat v_i \\times {Exprop}{i} \\right)^{-1} \\frac{1}{n} \\sum{i \\in I} \\hat v_i \\times \\left( {GDP} - \\hat g_0({geography}) \\right) $$\nSince we employ an intrumental variable strategy, we replace $m_0({geography})$ with $m_0({geography},{logMort})$ in the first stage.\n# Generate variables D = df['Exprop'].values.reshape(-1,1) X = df[['const', 'Latitude', 'Latitude2', 'Asia', 'Africa', 'Namer', 'Samer']].values y = df['GDP'].values.reshape(-1,1) Z = df[['const', 'Latitude', 'Latitude2', 'Asia', 'Africa', 'Namer', 'Samer','logMort']].values def estimate_beta(algorithm, alg_name, D, X, y, Z, sample): # Split sample D_main, D_aux = (D[sample==1], D[sample==0]) X_main, X_aux = (X[sample==1], X[sample==0]) y_main, y_aux = (y[sample==1], y[sample==0]) Z_main, Z_aux = (Z[sample==1], Z[sample==0]) # Residualize y on D b_hat = inv(D_aux.T @ D_aux) @ D_aux.T @ y_aux y_resid_aux = y_aux - D_aux @ b_hat # Estimate g0 alg_fitted = algorithm.fit(X=X_aux, y=y_resid_aux.ravel()) g0 = alg_fitted.predict(X_main).reshape(-1,1) # Compute v_hat u_hat = y_main - g0 # Estimate m0 alg_fitted = algorithm.fit(X=Z_aux, y=D_aux.ravel()) m0 = algorithm.predict(Z_main).reshape(-1,1) # Compute u_hat v_hat = D_main - m0 # Estimate beta beta = inv(v_hat.T @ D_main) @ v_hat.T @ u_hat return beta def ddml(algorithm, alg_name, D, X, y, Z, p=0.5, verbose=False): # Expand X if Lasso or Ridge if alg_name in ['Lasso ','Ridge ']: X = PolynomialFeatures(degree=2).fit_transform(X) # Generate split (fixed proportions) split = np.array([i in train_test_split(range(len(D)), test_size=p)[0] for i in range(len(D))]) # Compute beta beta = [estimate_beta(algorithm, alg_name, D, X, y, Z, split==k) for k in range(2)] beta = np.mean(beta) # Print and return if verbose: print('%s : %.4f' % (alg_name, beta)) return beta # Generate sample split p = 0.5 split = np.random.binomial(1, p, len(D)) We inspect different algorithms. In particular, we consider:\nLasso Regression Ridge Regression Regression Trees Random Forest Boosted Forests # List all algorithms algorithms = {'Ridge ': Ridge(alpha=.1), 'Lasso ': Lasso(alpha=.01), 'Tree ': DecisionTreeRegressor(), 'Forest ': RandomForestRegressor(n_estimators=30), 'Boosting': GradientBoostingRegressor(n_estimators=30)} Let\u0026rsquo;s compare the results.\n# Loop over algorithms for alg_name, algorithm in algorithms.items(): ddml(algorithm, alg_name, D, X, y, Z, verbose=True) Ridge : 0.1289 Lasso : -8.7963 Tree : 1.2879 Forest : 2.4938 Boosting : 0.5977 The results are extremely volatile.\n# Repeat K times def estimate_beta_median(algorithms, D, X, y, Z, K): # Loop over algorithms for alg_name, algorithm in algorithms.items(): betas = [] # Iterate n times for k in range(K): beta = ddml(algorithm, alg_name, D, X, y, Z) betas = np.append(betas, beta) print('%s : %.4f' % (alg_name, np.median(betas))) Let\u0026rsquo;s try using the median to have a more stable estimator.\nnp.random.seed(123) # Repeat 100 times and take median estimate_beta_median(algorithms, D, X, y, Z, 100) Ridge : 0.6670 Lasso : 1.2511 Tree : 0.9605 Forest : 0.5327 Boosting : 1.0327 The results differ slightly from the ones in the paper, but they are at least closer.\n","date":1646784000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1646784000,"objectID":"18f7a71852cf7865ca2bcf5562ea90ac","permalink":"https://matteocourthoud.github.io/course/ml-econ/09_postdoubleselection/","publishdate":"2022-03-09T00:00:00Z","relpermalink":"/course/ml-econ/09_postdoubleselection/","section":"course","summary":"# Remove warnings import warnings warnings.filterwarnings('ignore') # Import everything import pandas as pd import numpy as np import seaborn as sns import statsmodels.api as sm from numpy.linalg import inv from statsmodels.","tags":null,"title":"Post-Double Selection","type":"book"},{"authors":null,"categories":null,"content":"Lasso Issue Lasso (Least Absolute Shrinkage and Selection Operator) is a popular method for high dimensional regression. It does variable selection and estimation simultaneously. It is a non-parametric (series) estimation technique part of a general class of estimators called penalized estimators. It allows the number of regressors, $p$, to be larger than the sample size, $n$.\nConsider data $D = \\lbrace x_i, y_i \\rbrace_{i=1}^n$ with $\\dim (x_i) = p$. Assume that $p$ is large relative to $n$. Two possible reasons:\nwe have an intrinsic problem of high dimensionality $p$ indicates the number of expansion terms of small number of underlying important variables (e.g. series estimation) Assumption: $y_i = x_i\u0026rsquo; \\beta_0 + r_i + \\varepsilon_i$ where $\\beta_0$ depends on $p$, $r_i$ is a remainder term.\nNote that in classic non-parametrics, we have $x_i\u0026rsquo;\\beta_0$ as $p_1(x_i) \\beta_{1,K} + \\dots + p_K(x_i) \\beta_{K,K}$. For simplicity, we assume $r_i = 0$, as if we had extreme undersmoothing. Hence the model becomes: $$ y_i = x_i\u0026rsquo; \\beta_0 + \\varepsilon_i, \\qquad p \\geq n $$ We cannot run OLS because $p \\geq n$, thus the rank condition is violated.\nDefinition We define the Lasso estimator as $$ \\hat{\\beta}_L = \\arg \\min \\quad \\underbrace{\\mathbb E_n \\Big[ (y_i - x_i\u0026rsquo; \\beta)^2 \\Big]} _ {\\text{SSR term}} + \\underbrace{\\frac{\\lambda}{n} \\sum _ {j=1}^{P} | \\beta_j |} _ {\\text{Penalty term}} $$ where $\\lambda$ is called penalty parameter.\nThe penalty term discourages large values of $| \\beta_j |$. The choice of $\\lambda$ is analogous to the choice of $K$ in series estimation and $h$ in kernel estimation.\nPenalties The shrinkage to zero of the coefficients directly follows from the $|| \\cdot ||_1$ norm. On the contrary, another famous penalized estimator, ridge regression, uses the $|| \\cdot ||_2$ norm and does not have this property.\nMinimizing SSR + penalty is equivalent to minimize SSR $s.t.$ pen $\\leq c$ (clear from the picture).\nSparsity Let $S_0 = \\lbrace j: \\beta_{0,j} \\ne 0 \\rbrace$, we define $s_0 = |S_0|$ as the sparsity of $\\beta_0$. If $s_0/n \\to 0$, we are dealing with a sparse regression (analogous of smooth regression).\nRemark on sparsity:\nIn words, sparsity means that even if we have a lot of variables, only a small number of them (relative to $n$) have an effect on the dependent variable. Approximate sparsity imposes a restriction that only $s_0$ variables among all of $x_{ij}$, where $s_0$ is much smaller than $n$, have associated coefficients $\\beta_{0j}$ that are different from zero, while permitting a nonzero approximation error. Thus, estimators for this kind of model attempt to learn the identities of the variables with large nonzero coefficients, while simultaneously estimating these coefficients. (Belloni et al., 2004) Sparsity is an assumption. $\\beta_0$ is said to be $s_0$-sparse with $s_0 \u0026lt; n$ if $$ | \\lbrace j: \\beta_{0j} \\neq 0 \\rbrace | \\leq s_0 $$ Lasso Theorem Theorem\nSuppose that for data $D_n = (y_i, x_i){i=1}^N$ with $y_i = x_i\u0026rsquo; \\beta + \\varepsilon_i$. Let $\\hat{\\beta}L$ be the Lasso estimator. Let $\\mathcal{S} = 2 \\max_j | \\mathbb E[ x{ij} \\varepsilon_i] |$. Suppose $|support(\\beta_0) \\leq s_0$ (sparsity assumption). Let $c_0 = (\\mathcal{S} + \\lambda/n )/(-\\mathcal{S} + \\lambda/n )$. Let $$ \\kappa{c_0, s_0} = \\min_{ d \\in \\mathbb R^p, A \\subseteq \\lbrace 1, \u0026hellip; , p \\rbrace : |A| \\leq s_0 , || d_{A^c}|| \\leq c_0 || d_A ||_1 } \\sqrt{ \\frac{ s_0 d\u0026rsquo; \\mathbb E_n [x_i x_i\u0026rsquo;] d }{|| d_A ||_1^2} } $$ Then\n$$ \\mathbb I_{ \\left\\lbrace \\frac{\\lambda}{n} \u0026gt; \\mathcal{S} \\right\\rbrace} \\mathbb E_n [(x_i \\beta_0 - x_i \\beta_L)^2]^{\\frac{1}{2}} \\leq 2 \\frac{\\lambda}{n} \\frac{\\sqrt{s_0}}{\\kappa_{c_0, s_0}} $$\nIntuition: for a sufficiently high lambda the root mean squared error of Lasso is approximately zero.\n$$ \\text{ RMSE }: \\mathbb E_n [(x_i \\beta_0 - x_i \\beta_L)^2]^{\\frac{1}{2}} \\simeq 0 \\quad \\Leftrightarrow \\quad \\frac{\\lambda}{n} \u0026gt; \\mathcal{S} $$\nRemarks The minimization region is the set of “essentially sparse” vectors $d \\in \\mathbb R^p$, where “essentially sparse” is defined by $\\mathcal{C}, \\mathcal{S}$. In particular the condition $k_{\\mathcal{C}, \\mathcal{S}}\u0026gt;0$ means that no essentially sparse vector $d$ has $\\mathbb E[x_i x_i\u0026rsquo;]d = 0$, i.e. regressors were not added multiple times. Need to dominate the score with the penalty term $\\lambda$. Need no collinearity on a small ($\\leq s_0$) subset of regressors ($\\to k_{c_0, s_0}\u0026gt;0$). When Lasso? For prediction problems in high dimensional environments. NB! Lasso is not good for inference, only for prediction.\nIn particular, in econometrics it’s used for selecting either\ninstruments (predicting $\\hat{x}$ in the first stage) control variables (next section: double prediction problem, in the first stage and in the reduced form) Choosing the Optimal Lambda The choice of $\\lambda$ determines the bias-variance tradeoff:\nif $\\lambda$ is too big: $\\lambda \\approx \\infty \\mathbb \\Rightarrow \\hat{\\beta} \\approx 0$; if $\\lambda$ is too small: $\\lambda \\approx 0 \\mathbb \\Rightarrow$ overfitting. Possible solutions: Bonferroni correction, bootstrapping or $\\frac{\\lambda}{n} \\asymp \\sqrt{\\frac{\\log(p)}{n}}$ (asymptotically equal to), $\\mathcal{S}$ behaves like the maximum of gaussians.\nLasso Path How the estimated $\\hat{\\beta}$ depends on the penalty parameter $\\lambda$?\nPost Lasso: fit OLS without the penalty with all the nonzero coeficients selected by Lasso in the first step.\nRemarks Do not do inference with post-Lasso because standard errors are not uniformely valid. As $n \\to \\infty$ the CV and the score domination bounds converge to a unique bound. What is the problem of cross-validation? In high dimensional settings you can overfit in so many ways that CV doesn’t work and still overfits. Using $\\lambda$ with $\\frac{\\lambda}{n} \u0026gt; \\mathcal{S}$ small coefficients get shrunk to zero with high probability. In this case with small we mean $\\propto \\frac{1}{\\sqrt{n}}$ or $2 \\max_j | \\mathbb E_n[\\varepsilon_i x_{ij}] |$. If $| \\beta_{0j}| \\leq \\frac{c}{\\sqrt{n}}$ for a sufficiently small constant $c$, then $\\hat{\\beta}_{LASSO} \\overset{p}{\\to} 0$. In standard t-tests $c = 1.96$. $\\sqrt{n}$ factor is important since it is the demarcation line for reliable statistical detection. Optimal Lambda What is the criterium that should guide the selection of $\\lambda$? $$ \\frac{\\lambda}{n} \\geq 2 \\mathbb E_n[x_{ij} \\varepsilon_i] \\qquad \\forall j \\quad \\text{ if } Var(x_{ij} \\varepsilon_i) = 1 $$\nHow to choose the optimal $\\lambda$:\nDecide the coverage of the confidence intervals ($1-\\alpha$): $$ \\Pr \\left( \\sqrt{n} \\Big| \\mathbb E_n [x_{ij} \\varepsilon_i] \\Big| \u0026gt; t \\right) = 1- \\alpha $$ Solve for $t$ Get $\\lambda$ such that all scores are dominated by $\\frac{\\lambda}{n}$ with $\\alpha%$ probability. It turns out that the optimal $t \\propto \\sqrt{\\log(p)}$\nPre-Testing Omitted Variable Bias Consider two separate statistical models. Assume the following long regression of interest:\n$$ y_i = x_i\u0026rsquo; \\alpha_0+ z_i\u0026rsquo; \\beta_0 + \\varepsilon_i $$\nDefine the corresponding short regression as\n$$ y_i = x_i\u0026rsquo; \\alpha_0 + v_i \\quad \\text{ with } v_i = z_i\u0026rsquo; \\beta_0 + \\varepsilon_i $$\nTheorem\nSuppose that the DGP for the long regression corresponds to $\\alpha_0$, $\\beta_0$. Suppose further that $\\mathbb E[x_i] = 0$, $\\mathbb E[z_i] = 0$, $\\mathbb E[\\varepsilon_i |x_i,z_i] = 0$. Then, unless $\\beta_0 = 0$ or $z_i$ is orthogonal to $x_i$, the (sole) stochastic regressor $x_i$ is correlated with the error term in the short regression which implies that the OLS estimator of the short regression is inconsistent for $\\alpha_0$ due to the omitted variable bias. In particular, one can show that the plim of the OLS estimator of $\\hat{\\alpha}{SHORT}$ from the short regression is $$ \\hat{\\alpha}{SHORT} \\overset{p}{\\to} \\frac{Cov(y_i, x_i)}{Var(x_i)} = \\alpha_0 + \\beta_0 \\frac{Cov(z_i, x_i)}{Var(x_i)} $$\nPre-test bias Consider data $D= (y_i, x_i, z_i)_{i=1}^n$, where the true model is: $$ \\begin{aligned} \u0026amp; y_i = x_i\u0026rsquo; \\alpha_0 + z_i\u0026rsquo; \\beta_0 + \\varepsilon_i \\newline \u0026amp; x_i = z_i\u0026rsquo; \\gamma_0 + u_i \\end{aligned} $$\nWhere $x_i$ is the variable of interest (we want to make inference on $\\alpha_0$) and $z_i$ is a high dimensional set of control variables.\nFrom now on, we will work under the following assumptions:\n$\\dim(x_i)=1$ for all $n$ $\\beta_0$ uniformely bounded in $n$ Strict exogeneity: $\\mathbb E[\\varepsilon_i | x_i, z_i] = 0$ and $\\mathbb E[u_i | z_i] = 0$ $\\beta_0$ and $\\gamma_0$ have dimension (and hence value) that depend on $n$ Pre-Testing procedure Regress $y_i$ on $x_i$ and $z_i$ For each $j = 1, \u0026hellip;, p = \\dim(z_i)$ calculate a test statistic $t_j$ Let $\\hat{T} = \\lbrace j: |t_j| \u0026gt; C \u0026gt; 0 \\rbrace$ for some constant $C$ (set of statistically significant coefficients). Re-run the new “model” using $(x_i, z_{\\hat{T},i})$ (i.e. using the selected covariates with statistically significant coefficients). Perform statistical inference (i.e. confidence intervals and hypothesis tests) as if no model selection had been done. Bias As we can see from the figure above (code below), running the short regression instead of the long one introduces Omitted Variable Bias (second column). Instead, the Pre-Testing estimator is consistent but not normally distributed (third column).\nIssue Pre-testing is problematic because the post-selection estimator is not asymptotically normal. Moreover, for particular data generating processes, it even fails to be consistent at the rate of $\\sqrt{n}$ (Belloni et al., 2014).\nIntuition: when performing pre-testing, we might have an Omitted Variable Bias problem when $\\beta_0\u0026gt;0$ but we fail to reject the null hypothesis $H_0 : \\beta_0 = 0$ because of lack of statistical power, i.e. $|\\beta_0|$ is small with respect to the sample size. In particular, we fail to reject the null hypothesis for $\\beta_0(n) = O \\left( \\frac{1}{\\sqrt{n}}\\right)$. However, note that the problem vanishes asymptotically, as the resulting estimator is consistent. In fact, if $\\beta_0(n) = O \\left( \\frac{1}{\\sqrt{n}}\\right)$, then $\\alpha_0 - \\hat \\alpha_{PRETEST} \\overset{p}{\\to} \\lim_{n \\to \\infty} \\beta_0 \\gamma_0 = \\lim_{n \\to \\infty} O \\left( \\frac{1}{\\sqrt{n}} \\right) = 0$. We now clarify what it means to have a coefficient depending on the sample size, $\\beta_0(n)$.\nUniformity Concept of uniformity: the DGP varies with $n$. Instead of having a fixed “true” parameter $\\beta_0$, you have a sequence $\\beta_0(n)$. Having a cofficient that depends on the sample size $n$ is useful to preserve the concept of “small with respect to the sample size” in asymptotic theory.\nIn the context of Pre-Testing, all problems vanish asymptotically since we are able to always reject the null hypothesis $H_0 : \\beta_0 = 0$ when $\\beta_0 \\neq 0$. In the figure below, I plot simulation results for $\\hat \\alpha_{PRETESTING}$ for a fixed coefficient $\\beta_0$ (first row) and variable coefficient $\\beta_0(n)$ that depends on the sample size (second row), for different sample sizes (columns). We see that if $\\beta_0$ is independent from the sample size (first row), the distribution of $\\hat \\alpha_{PRETEST}$ is not normal in small samples and it displays the bimodality that characterizes pre-testing. However, it becomes normal in large samples. On the other hand, when $\\beta_0(n)$ depends on the sample size, and in particular $\\beta_0 = O \\left( \\frac{1}{\\sqrt{n}} \\right)$ (second row), the distribution of $\\hat \\alpha_{PRETEST}$ stays bimodal even when the sample size increases.\nNote that the estimator is always consistent!\nWhere is Pre-Testing a Problem? If we were to draw a map of where the gaussianity assumption of $\\beta_0(n)$ holds well and where it fails, it would look like the following figure.\nIntuition The intuition for the three different regions (from bottom to top) is the following.\nWhen $\\beta_0 = o \\left( \\frac{1}{\\sqrt{n}} \\right)$, $z_i$ is excluded with probability $p \\to 1$. But, given that $\\beta_0$ is small enough, failing to control for $z_i$ does not introduce large omitted variables bias (Belloni et al., 2014). If however the coefficient on the control is “moderately close to zero”, $\\beta_0 = O \\left( \\frac{1}{\\sqrt{n}} \\right)$, the t-test set-up above cannot distinguish this coefficient from $0$, and the control $z_i$ is dropped with probability $p \\to 1$. However, in this case the omitted variable bias generated by excluding $z_i$ scaled by $\\sqrt{n}$ does not converge to zero. That is, the standard post-selection estimator is not asymptotically normal and even fails to be consistent at the rate of $\\sqrt{n}$ (Belloni et al., 2014). Lastly, when $\\beta_0$ is large enough, the null pre-testing hypothesis $H_0 : \\beta_0 = 0$ will be rejected sufficiently often so that the bias is negligible. Post-Double Selection The post-double-selection estimator, $\\hat{\\alpha}_{PDS}$ solves this problem by doing variable selection via standard t-tests or Lasso-type selectors with the two “true model” equations (first stage and reduced form) that contain the information from the model and then estimating $\\alpha_0$ by regressing $y_i$ on $x_i$ and the union of the selected controls. By doing so, $z_i$ is omitted only if its coefficient in both equations is small which greatly limits the potential for omitted variables bias (Belloni et al., 2014).\nIntuition: by performing post-double selection, we ensure that both $\\beta_0 = O \\left( \\frac{1}{\\sqrt{n}} \\right)$ and $\\gamma_0 = O \\left( \\frac{1}{\\sqrt{n}} \\right)$ so that $\\sqrt{n} ( \\hat \\alpha _ {PRETEST} - \\alpha _ 0) \\overset{p}{\\to} \\lim_{n \\to \\infty} \\sqrt{n} \\beta_0 \\gamma_0 = \\lim_{n \\to \\infty} \\sqrt{n} O \\left( \\frac{1}{n} \\right) = 0$ and the estimator is gaussian.\nFrisch-Waugh Theorem Theorem\nConsider the data $D = \\lbrace x_i, y_i, z_i \\rbrace_{i=1}^\\infty$ with DGP: $Y = X \\alpha + Z \\beta + \\varepsilon$. The following estimators of $\\alpha$ are numerically equivalent (if $[X, Z]$ has full rank):\n$\\hat{\\alpha}$ from regressing $Y$ on $X, Z$ $\\tilde{\\alpha}$ from regressing $Y$ on $\\tilde{X}$ $\\bar{\\alpha}$ from regressing $\\tilde{Y}$ on $\\tilde{X}$ where the operation of passing to $Y, X$ to $\\tilde{Y}, \\tilde{X}$ is called projection out $Z$, e.g.$\\tilde{X}$ are the residuals from regressing $X$ on $Z$.\nProof (1) We want to show that $\\hat{\\alpha} = \\tilde{\\alpha}$.\nClaim: $\\hat{\\alpha } = \\tilde{\\alpha} \\Leftrightarrow \\tilde{X}\u0026rsquo; \\left[ (X - \\tilde{X})\\hat{\\alpha} + Z \\hat{\\beta} +\\hat{\\varepsilon} \\right] = 0$.\nProof of the claim: if $\\hat{\\alpha} = \\tilde{\\alpha}$, we can write $Y$ as $$ Y = X \\hat{\\alpha} + Z \\hat{\\beta} + \\hat{\\varepsilon} = \\tilde{X} \\hat{\\alpha} + \\underbrace{(X - \\tilde{X}) \\hat{\\alpha } + Z \\hat{\\beta} + \\hat{\\varepsilon}}_\\text{residual of $Y$ on $\\tilde{X} $} = \\tilde{X} \\tilde{\\alpha} + \\nu_i $$\nTherefore, by the orthogonality property of the OLS residual, it must be that $\\tilde{X}\u0026rsquo;\\nu_i= 0$. $$\\tag*{$\\blacksquare$}$$\nProof (1) Having established the claim, we want to show that the normal equation $\\tilde{X}\u0026rsquo; \\left[ (X - \\tilde{X})\\hat{\\alpha} + Z \\hat{\\beta} +\\hat{\\varepsilon} \\right] = 0$ is satisfied. We follow 3 steps:\nFirst we have that $\\tilde{X}\u0026rsquo; (X - \\tilde{X})\\hat{\\alpha} = 0$. This follows from the fact that $\\tilde{X}\u0026rsquo; = X\u0026rsquo; M_Z$ and hence: $$ \\begin{aligned} \\tilde{X}\u0026rsquo; (X - \\tilde{X}) \u0026amp; = X\u0026rsquo; M_Z (X - M_Z) = X\u0026rsquo; M_Z X - X\u0026rsquo; \\overbrace{M_Z M_Z}^{M_Z} X \\newline \u0026amp; = X\u0026rsquo;M_Z X - X\u0026rsquo; M_Z X = 0 \\end{aligned} $$\n$\\tilde{X}\u0026rsquo; Z \\hat{\\beta} = 0$ since $\\tilde{X}$ is the residual from the regression of $X$ on $Z$, by normal equation it holds that $\\tilde{X}\u0026rsquo; Z = 0$.\n$\\tilde{X}\u0026rsquo; \\hat{\\varepsilon} = 0$. This follows from (i) $M_Z \u0026rsquo; M_{X, Z} = M_{X,Z}$ and (ii) $X\u0026rsquo; M_{X, Z} = 0$: $$ \\tilde{X}\u0026rsquo; \\hat{\\varepsilon} = (M_Z X)\u0026rsquo; (M_{X, Z} \\varepsilon) = X\u0026rsquo;M_Z\u0026rsquo; M_{X, Z} \\varepsilon = \\underbrace{X\u0026rsquo; M_{X, Z}}_0 \\varepsilon = 0. $$ $$\\tag*{$\\blacksquare$}$$\nThe coefficient $\\hat{\\alpha}$ is a partial regression coefficient identified from the variation in $X$ that is orthogonal to $Z$. This is often known as residual variation.\nPost Double Selection Setting Consider again data $D= (y_i, x_i, z_i)_{i=1}^n$, where the true model is: $$ \\begin{aligned} \u0026amp; y_i = x_i\u0026rsquo; \\alpha_0 + z_i\u0026rsquo; \\beta_0 + \\varepsilon_i \\newline \u0026amp; x_i = z_i\u0026rsquo; \\gamma_0 + u_i \\end{aligned} $$\nWe would like to guard against pretest bias if possible, in order to handle high dimensional models. A good pathway towards motivating procedures which guard against pretest bias is a discussion of classical partitioned regression.\nConsider a regression $y_i$ on $x_i$ and $z_i$. $x_i$ is the 1-dimensional variable of interest, $z_i$ is a high-dimensional set of control variables. We have the following procedure:\nFirst Stage selection: lasso $x_i$ on $z_i$. Let the selected variables be collected in the set $S_{FS} \\subseteq z_i$ Reduced Form selection: lasso $y_i$ on $z_i$. Let the selected variables be collected in the set $S_{RF} \\subseteq z_i$ Regress $y_i$ on $x_i$ and $S_{FS} \\cup S_{RF}$ PDS Theorem Theorem\nLet $\\lbrace P^n\\rbrace$ be a sequence of data-generating processes for $D_n = (y_i, x_i, z_i)^n_{i=1} \\in (\\mathbb R \\times \\mathbb R \\times \\mathbb R^p) ^n$ where $p$ depends on $n$. For each $n$, the data are iid with $yi = x_i\u0026rsquo;\\alpha_0^{(n)} + z_i\u0026rsquo; \\beta_0^{(n)} + \\varepsilon_i$ and $x_i = z_i\u0026rsquo; \\gamma_0^{(n)} + u_i$ where $\\mathbb E[\\varepsilon_i | x_i,z_i] = 0$ and $\\mathbb E[u_i|z_i] = 0$. The sparsity of the vectors $\\beta_0^{(n)}$, $\\gamma_0^{(n)}$ is controlled by $|| \\beta_0^{(n)} ||_0 \\leq s$ with $s^2 (\\log p)^2/n \\to 0$. Suppose that additional regularity conditions on the model selection procedures and moments of the random variables $y_i$ , $x_i$ , $z_i$ as documented in Belloni et al. (2014). Then the confidence intervals, CI, from the post double selection procedure are uniformly valid. That is, for any confidence level $\\xi \\in (0, 1)$ $$ \\Pr(\\alpha_0 \\in CI) \\to 1- \\xi $$\nIn order to have valid confidence intervals you want their bias to be negligibly. Since $$ CI = \\left[ \\hat{\\alpha} \\pm \\frac{1.96 \\cdot \\hat{\\sigma}}{\\sqrt{n}} \\right] $$\nIf the bias is $o \\left( \\frac{1}{\\sqrt{n}} \\right)$ then there is no problem since it is asymptotically negligible w.r.t. the magnitude of the confidence interval. If however the the bias is $O \\left( \\frac{1}{\\sqrt{n}} \\right)$ then it has the same magnitude of the confidence interval and it does not asymptotically vanish.\nProof (Idea) The idea of the proof is to use partitioned regression. An alternative way to think about the argument is: bound the omitted variables bias. Omitted variable bias comes from the product of 2 quantities related to the omitted variable:\nIts partial correlation with the outcome, and Its partial correlation with the variable of interest. If both those partial correlations are $O( \\sqrt{\\log p/n})$, then the omitted variables bias is $(s \\times O( \\sqrt{\\log p/n})^2 = o \\left( \\frac{1}{\\sqrt{n}} \\right)$, provided $s^2 (\\log p)^2/n \\to 0$. Relative to the $\\frac{1}{\\sqrt{n}}$ convergence rate, the omitted variables bias is negligible.\nIn our omitted variable bias case, we want $| \\beta_0 \\gamma_0 | = o \\left( \\frac{1}{\\sqrt{n}} \\right)$. Post-double selection guarantees that\nReduced form selection (pre-testing): any “missing” variable has $|\\beta_{0j}| \\leq \\frac{c}{\\sqrt{n}}$ First stage selection (additional): any “missing” variable has $|\\gamma_{0j}| \\leq \\frac{c}{\\sqrt{n}}$ As a consequence, as long as the number of omitted variables is finite, the omitted variable bias is $$ OVB(\\alpha) = |\\beta_{0j}| \\cdot|\\gamma_{0j}| \\leq \\frac{c}{\\sqrt{n}} \\cdot \\frac{c}{\\sqrt{n}} = \\frac{c^2}{n} = o \\left(\\frac{1}{\\sqrt{n}}\\right) $$\nDistribution We can plot the distribution of the post-double selection estimator against the pre-testing one.\nRemark: under homoskedasticity, the above estimator achieves the semiparametric efficiency bound.\n","date":1635465600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1635465600,"objectID":"836b9d8d9e375e7fccbc006323494047","permalink":"https://matteocourthoud.github.io/course/metrics/09_selection/","publishdate":"2021-10-29T00:00:00Z","relpermalink":"/course/metrics/09_selection/","section":"course","summary":"Lasso Issue Lasso (Least Absolute Shrinkage and Selection Operator) is a popular method for high dimensional regression. It does variable selection and estimation simultaneously. It is a non-parametric (series) estimation technique part of a general class of estimators called penalized estimators.","tags":null,"title":"Variable Selection","type":"book"},{"authors":null,"categories":null,"content":"# Setup from utils.lecture10 import * %matplotlib inline Supervised vs Unsupervised Learning The difference between supervised learning and unsupervised learning is that in the first case we have a variable $y$ which we want to predict, given a set of variables ${ X_1, . . . , X_p }$.\nIn unsupervised learning are not interested in prediction, because we do not have an associated response variable $y$. Rather, the goal is to discover interesting properties about the measurements on ${ X_1, . . . , X_p }$.\nQuestions that we are usually interested in are\nClustering Dimensionality reduction In general, unsupervised learning can be viewed as an extention of exploratory data analysis.\nDimensionality Reduction Working in high-dimensional spaces can be undesirable for many reasons; raw data are often sparse as a consequence of the curse of dimensionality, and analyzing the data is usually computationally intractable (hard to control or deal with).\nDimensionality reduction can also be useful to plot high-dimensional data.\nClustering Clustering refers to a very broad set of techniques for finding subgroups, or clusters, in a data set. When we cluster the observations of a data set, we seek to partition them into distinct groups so that the observations within each group are quite similar to each other, while observations in different groups are quite different from each other.\nIn this section we focus on the following algorithms:\nK-means clustering Hierarchical clustering Gaussian Mixture Models Principal Component Analysis Suppose that we wish to visualize $n$ observations with measurements on a set of $p$ features, ${X_1, . . . , X_p}$, as part of an exploratory data analysis.\nWe could do this by examining two-dimensional scatterplots of the data, each of which contains the n observations’ measurements on two of the features. However, there are $p(p−1)/2$ such scatterplots; for example, with $p = 10$ there are $45$ plots!\nPCA provides a tool to do just this. It finds a low-dimensional represen- tation of a data set that contains as much as possible of the variation.\nFirst Principal Component The first principal component of a set of features ${X_1, . . . , X_p}$ is the normalized linear combination of the features $Z_1$\n$$ Z_1 = \\phi_{11} X_1 + \\phi_{21} X_2 + \u0026hellip; + \\phi_{p1} X_p $$\nthat has the largest variance.\nBy normalized, we mean that $\\sum_{i=1}^p \\phi^2_{i1} = 1$.\nPCA Computation In other words, the first principal component loading vector solves the optimization problem\n$$ \\underset{\\phi_{11}, \\ldots, \\phi_{p 1}}{\\max} \\ \\Bigg \\lbrace \\frac{1}{n} \\sum _ {i=1}^{n}\\left(\\sum _ {j=1}^{p} \\phi _ {j1} x _ {ij} \\right)^{2} \\Bigg \\rbrace \\quad \\text { subject to } \\quad \\sum _ {j=1}^{p} \\phi _ {j1}^{2}=1 $$\nThe objective that we are maximizing is just the sample variance of the $n$ values of $z_{i1}$.\nAfter the first principal component $Z_1$ of the features has been determined, we can find the second principal component $Z_2$. The second principal component is the linear combination of ${X_1, . . . , X_p}$ that has maximal variance out of all linear combinations that are uncorrelated with $Z_1$.\nExample We illustrate the use of PCA on the USArrests data set.\nFor each of the 50 states in the United States, the data set contains the number of arrests per $100,000$ residents for each of three crimes: Assault, Murder, and Rape. We also record the percent of the population in each state living in urban areas, UrbanPop.\n# Load crime data df = pd.read_csv('data/USArrests.csv', index_col=0) df.head() Murder Assault UrbanPop Rape State Alabama 13.2 236 58 21.2 Alaska 10.0 263 48 44.5 Arizona 8.1 294 80 31.0 Arkansas 8.8 190 50 19.5 California 9.0 276 91 40.6 Data Scaling To make all the features comparable, we first need to scale them. In this case, we use the sklearn.preprocessing.scale() function to normalize each variable to have zero mean and unit variance.\n# Scale data X_scaled = pd.DataFrame(scale(df), index=df.index, columns=df.columns).values We will see later what are the practical implications of (not) scaling.\nFitting Let\u0026rsquo;s fit PCA with 2 components.\n# Fit PCA with 2 components pca2 = PCA(n_components=2).fit(X_scaled) # Get weights weights = pca2.components_.T df_weights = pd.DataFrame(weights, index=df.columns, columns=['PC1', 'PC2']) df_weights PC1 PC2 Murder 0.535899 0.418181 Assault 0.583184 0.187986 UrbanPop 0.278191 -0.872806 Rape 0.543432 -0.167319 Projecting the data What does the trasformed data looks like?\n# Transform X to get the principal components X_dim2 = pca2.transform(X_scaled) df_dim2 = pd.DataFrame(X_dim2, columns=['PC1', 'PC2'], index=df.index) df_dim2.head() PC1 PC2 State Alabama 0.985566 1.133392 Alaska 1.950138 1.073213 Arizona 1.763164 -0.745957 Arkansas -0.141420 1.119797 California 2.523980 -1.542934 Visualization The advantage og PCA is that it allows us to see the variation in lower dimesions.\nmake_figure_10_1a(df_dim2, df_weights) PCA and Spectral Analysis In case you haven\u0026rsquo;t noticed, calculating principal components, is equivalent to calculating the eigenvectors of the design matrix $X\u0026rsquo;X$, i.e. the variance-covariance matrix of $X$. Indeed what we performed above is a decomposition of the variance of $X$ into orthogonal components.\nThe constrained maximization problem above can be re-written in matrix notation as\n$$ \\max \\ \\phi\u0026rsquo; X\u0026rsquo;X \\phi \\quad \\text{ s. t. } \\quad \\phi\u0026rsquo;\\phi = 1 $$\nWhich has the following dual representation\n$$ \\mathcal L (\\phi, \\lambda) = \\phi\u0026rsquo; X\u0026rsquo;X \\phi - \\lambda (\\phi\u0026rsquo;\\phi - 1) $$\nIf we take the first order conditions\n$$ \\begin{align} \u0026amp; \\frac{\\partial \\mathcal L}{\\partial \\lambda} = \\phi\u0026rsquo;\\phi - 1 \\ \u0026amp; \\frac{\\partial \\mathcal L}{\\partial \\phi} = 2 X\u0026rsquo;X \\phi - 2 \\lambda \\phi \\end{align} $$\nSetting the derivatives to zero at the optimum, we get\n$$ \\begin{align} \u0026amp; \\phi\u0026rsquo;\\phi = 1 \\ \u0026amp; X\u0026rsquo;X \\phi = \\lambda \\phi \\end{align} $$\nThus, $\\phi$ is an eigenvector of the covariance matrix $X\u0026rsquo;X$, and the maximizing vector will be the one associated with the largest eigenvalue $\\lambda$.\nEigenvalues and eigenvectors We can now double-check it using numpy linear algebra package.\neigenval, eigenvec = np.linalg.eig(X_scaled.T @ X_scaled) data = np.concatenate((eigenvec,eigenval.reshape(1,-1))) idx = list(df.columns) + ['Eigenvalue'] df_eigen = pd.DataFrame(data, index=idx, columns=['PC1', 'PC2','PC3','PC4']) df_eigen PC1 PC2 PC3 PC4 Murder 0.535899 0.418181 0.649228 -0.341233 Assault 0.583184 0.187986 -0.743407 -0.268148 UrbanPop 0.278191 -0.872806 0.133878 -0.378016 Rape 0.543432 -0.167319 0.089024 0.817778 Eigenvalue 124.012079 49.488258 8.671504 17.828159 The spectral decomposition of the variance of $X$ generates a set of orthogonal vectors (eigenvectors) with different magnitudes (eigenvalues). The eigenvalues tell us the amount of variance of the data in that direction.\nIf we combine the eigenvectors together, we form a projection matrix $P$ that we can use to transform the original variables: $\\tilde X = P X$\nX_transformed = X_scaled @ eigenvec df_transformed = pd.DataFrame(X_transformed, index=df.index, columns=['PC1', 'PC2','PC3','PC4']) df_transformed.head() PC1 PC2 PC3 PC4 State Alabama 0.985566 1.133392 0.156267 -0.444269 Alaska 1.950138 1.073213 -0.438583 2.040003 Arizona 1.763164 -0.745957 -0.834653 0.054781 Arkansas -0.141420 1.119797 -0.182811 0.114574 California 2.523980 -1.542934 -0.341996 0.598557 This is exactly the dataset that we obtained before.\nScaling the Variables The results obtained when we perform PCA will also depend on whether the variables have been individually scaled. In fact, the variance of a variable depends on its magnitude.\n# Variables variance df.var(axis=0) Murder 18.970465 Assault 6945.165714 UrbanPop 209.518776 Rape 87.729159 dtype: float64 Consequently, if we perform PCA on the unscaled variables, then the first principal component loading vector will have a very large loading for Assault, since that variable has by far the highest variance.\n# Fit PCA with unscaled varaibles X = df.values pca2_u = PCA(n_components=2).fit(X) # Get weights weights_u = pca2_u.components_.T df_weights_u = pd.DataFrame(weights_u, index=df.columns, columns=['PC1', 'PC2']) df_weights_u PC1 PC2 Murder 0.041704 0.044822 Assault 0.995221 0.058760 UrbanPop 0.046336 -0.976857 Rape 0.075156 -0.200718 # Transform X to get the principal components X_dim2_u = pca2_u.transform(X) df_dim2_u = pd.DataFrame(X_dim2_u, columns=['PC1', 'PC2'], index=df.index) Plotting We can compare the lower dimensional representations with and without scaling.\nmake_figure_10_1b(df_dim2, df_dim2_u, df_weights, df_weights_u) As predicted, the first principal component loading vector places almost all of its weight on Assault, while the second principal component loading vector places almost all of its weight on UrpanPop. Comparing this to the left-hand plot, we see that scaling does indeed have a substantial effect on the results obtained. However, this result is simply a consequence of the scales on which the variables were measured.\nThe Proportion of Variance Explained We can now ask a natural question: how much of the information in a given data set is lost by projecting the observations onto the first few principal components? That is, how much of the variance in the data is not contained in the first few principal components? More generally, we are interested in knowing the proportion of variance explained (PVE) by each principal component.\n# Four components pca4 = PCA(n_components=4).fit(X_scaled) # Variance of the four principal components pca4.explained_variance_ array([2.53085875, 1.00996444, 0.36383998, 0.17696948]) Interpretation We can compute it in percentage of the total variance.\n# As a percentage of the total variance pca4.explained_variance_ratio_ array([0.62006039, 0.24744129, 0.0891408 , 0.04335752]) In the Arrest dataset, the first principal component explains $62.0%$ of the variance in the data, and the next principal component explains $24.7%$ of the variance. Together, the first two principal components explain almost $87%$ of the variance in the data, and the last two principal components explain only $13%$ of the variance.\nPlotting We can plot in a graph the percentage of the variance explained, relative to the number of components.\nmake_figure_10_2(pca4) How Many Principal Components? In general, a $n \\times p$ data matrix $X$ has $\\min{n − 1, p}$ distinct principal components. However, we usually are not interested in all of them; rather, we would like to use just the first few principal components in order to visualize or interpret the data.\nWe typically decide on the number of principal components required to visualize the data by examining a scree plot.\nHowever, there is no well-accepted objective way to decide how many principal com- ponents are enough.\nK-Means Clustering The idea behind K-means clustering is that a good clustering is one for which the within-cluster variation is as small as possible. Hence we want to solve the problem\n$$ \\underset{C_{1}, \\ldots, C_{K}}{\\operatorname{minimize}} \\Bigg\\lbrace \\sum_{k=1}^{K} W\\left(C_{k}\\right) \\Bigg\\rbrace $$\nwhere $C_k$ is a cluster and $ W(C_k)$ is a measure of the amount by which the observations within a cluster differ from each other.\nThere are many possible ways to define this concept, but by far the most common choice involves squared Euclidean distance. That is, we define\n$$ W\\left(C_{k}\\right)=\\frac{1}{\\left|C_{k}\\right|} \\sum_{i, i^{\\prime} \\in C_{k}} \\sum_{j=1}^{p}\\left(x_{i j}-x_{i^{\\prime} j}\\right)^2 $$\nwhere $|C_k|$ denotes the number of observations in the $k^{th}$ cluster.\nAlgorithm Randomly assign a number, from $1$ to $K$, to each of the observations. These serve as initial cluster assignments for the observations.\nIterate until the cluster assignments stop changing:\na) For each of the $K$ clusters, compute the cluster centroid. The kth cluster centroid is the vector of the $p$ feature means for the observations in the $k^{th}$ cluster.\nb) Assign each observation to the cluster whose centroid is closest (where closest is defined using Euclidean distance).\nGenerate the data We first generate a 2-dimensional dataset.\n# Simulate data np.random.seed(123) X = np.random.randn(50,2) X[0:25, 0] = X[0:25, 0] + 3 X[0:25, 1] = X[0:25, 1] - 4 make_new_figure_1(X) Step 1: random assignement Now let\u0026rsquo;s randomly assign the data to two clusters, at random.\n# Init clusters K = 2 clusters0 = np.random.randint(K,size=(np.size(X,0))) make_new_figure_2(X, clusters0) Step 2: estimate distributions What are the new centroids?\n# Compute new centroids def compute_new_centroids(X, clusters): K = len(np.unique(clusters)) centroids = np.zeros((K,np.size(X,1))) for k in range(K): if sum(clusters==k)\u0026gt;0: centroids[k,:] = np.mean(X[clusters==k,:], axis=0) else: centroids[k,:] = np.mean(X, axis=0) return centroids # Print centroids0 = compute_new_centroids(X, clusters0) print(centroids0) [[ 1.54179703 -1.65922379] [ 1.67917325 -2.36272948]] Plotting the centroids Let\u0026rsquo;s add the centroids to the graph.\n# Plot plot_assignment(X, centroids0, clusters0, 0, 0) Step 3: assign data to clusters Now we can assign the data to the clusters, according to the closest centroid.\n# Assign X to clusters def assign_to_cluster(X, centroids): K = np.size(centroids,0) dist = np.zeros((np.size(X,0),K)) for k in range(K): dist[:,k] = np.mean((X - centroids[k,:])**2, axis=1) clusters = np.argmin(dist, axis=1) # Compute inertia inertia = 0 for k in range(K): if sum(clusters==k)\u0026gt;0: inertia += np.sum((X[clusters==k,:] - centroids[k,:])**2) return clusters, inertia Plotting assigned data # Get cluster assignment [clusters1,d] = assign_to_cluster(X, centroids0) # Plot plot_assignment(X, centroids0, clusters1, d, 1) Full Algorithm We now have all the components to proceed iteratively.\ndef kmeans_manual(X, K): # Init i = 0 d0 = 1e4 d1 = 1e5 clusters = np.random.randint(K,size=(np.size(X,0))) # Iterate until convergence while np.abs(d0-d1) \u0026gt; 1e-10: d1 = d0 centroids = compute_new_centroids(X, clusters) [clusters, d0] = assign_to_cluster(X, centroids) plot_assignment(X, centroids, clusters, d0, i) i+=1 Plotting k-means clustering # Test kmeans_manual(X, K) Here the observations can be easily plotted because they are two-dimensional. If there were more than two variables then we could instead perform PCA and plot the first two principal components score vectors.\nMore clusters In the previous example, we knew that there really were two clusters because we generated the data. However, for real data, in general we do not know the true number of clusters. We could instead have performed K-means clustering on this example with K = 3. If we do this, K-means clustering will split up the two \u0026ldquo;real\u0026rdquo; clusters, since it has no information about them:\n# K=3 kmeans_manual(X, 3) Sklearn package The automated function in sklearn to persorm $K$-means clustering is KMeans.\n# SKlearn algorithm km1 = KMeans(n_clusters=3, n_init=1, random_state=1) km1.fit(X) KMeans(n_clusters=3, n_init=1, random_state=1) Plotting We can plot the asssignment generated by the KMeans function.\n# Plot plot_assignment(X, km1.cluster_centers_, km1.labels_, km1.inertia_, km1.n_iter_) As we can see, the results are different in the two algorithms? Why? $K$-means is susceptible to the initial values. One way to solve this problem is to run the algorithm multiple times and report only the best results\nInitial Assignment To run the Kmeans() function in python with multiple initial cluster assignments, we use the n_init argument (default: 10). If a value of n_init greater than one is used, then K-means clustering will be performed using multiple random assignments, and the Kmeans() function will report only the best results.\n# 30 runs km_30run = KMeans(n_clusters=3, n_init=30, random_state=1).fit(X) plot_assignment(X, km_30run.cluster_centers_, km_30run.labels_, km_30run.inertia_, km_30run.n_iter_) Best Practices It is generally recommended to always run K-means clustering with a large value of n_init, such as 20 or 50 to avoid getting stuck in an undesirable local optimum.\nWhen performing K-means clustering, in addition to using multiple initial cluster assignments, it is also important to set a random seed using the random_state parameter. This way, the initial cluster assignments can be replicated, and the K-means output will be fully reproducible.\nHierarchical Clustering One potential disadvantage of K-means clustering is that it requires us to pre-specify the number of clusters $K$.\nHierarchical clustering is an alternative approach which does not require that we commit to a particular choice of $K$.\nThe Dendogram Hierarchical clustering has an added advantage over K-means clustering in that it results in an attractive tree-based representation of the observations, called a dendrogram.\nd = dendrogram( linkage(X, \u0026quot;complete\u0026quot;), leaf_rotation=90., # rotates the x axis labels leaf_font_size=8., # font size for the x axis labels ) Interpretation Each leaf of the dendrogram represents one observation.\nAs we move up the tree, some leaves begin to fuse into branches. These correspond to observations that are similar to each other. As we move higher up the tree, branches themselves fuse, either with leaves or other branches. The earlier (lower in the tree) fusions occur, the more similar the groups of observations are to each other.\nWe can use de dendogram to understand how similar two observations are: we can look for the point in the tree where branches containing those two obse rvations are first fused. The height of this fusion, as measured on the vertical axis, indicates how different the two observations are. Thus, observations that fuse at the very bottom of the tree are quite similar to each other, whereas observations that fuse close to the top of the tree will tend to be quite different.\nThe term hierarchical refers to the fact that clusters obtained by cutting the dendrogram at a given height are necessarily nested within the clusters obtained by cutting the dendrogram at any greater height.\nThe Hierarchical Clustering Algorithm Begin with $n$ observations and a measure (such as Euclidean distance) of all the $n(n − 1)/2$ pairwise dissimilarities. Treat each 2 observation as its own cluster.\nFor $i=n,n−1,\u0026hellip;,2$\na) Examine all pairwise inter-cluster dissimilarities among the $i$ clusters and identify the pair of clusters that are least dissimilar (that is, most similar). Fuse these two clusters. The dissimilarity between these two clusters indicates the height in the dendrogram at which the fusion should be placed.\nb) Compute the new pairwise inter-cluster dissimilarities among the $i−1$ remaining clusters.\nThe Linkage Function We have a concept of the dissimilarity between pairs of observations, but how do we define the dissimilarity between two clusters if one or both of the clusters contains multiple observations?\nThe concept of dissimilarity between a pair of observations needs to be extended to a pair of groups of observations. This extension is achieved by developing the notion of linkage, which defines the dissimilarity between two groups of observations.\nLinkages The four most common types of linkage are:\nComplete: Maximal intercluster dissimilarity. Compute all pairwise dissimilarities between the observations in cluster A and the observations in cluster B, and record the largest of these dissimilarities. Single: Minimal intercluster dissimilarity. Compute all pairwise dissimilarities between the observations in cluster A and the observations in cluster B, and record the smallest of these dissimilarities. Average: Mean intercluster dissimilarity. Compute all pairwise dissimilarities between the observations in cluster A and the observations in cluster B, and record the average of these dissimilarities. Centroid: Dissimilarity between the centroid for cluster A (a mean vector of length p) and the centroid for cluster B. Centroid linkage can result in undesirable inversions. Average, complete, and single linkage are most popular among statisticians. Average and complete linkage are generally preferred over single linkage, as they tend to yield more balanced dendrograms. Centroid linkage is often used in genomics, but suffers from a major drawback in that an inversion can occur, whereby two clusters are fused at a height below either of the individual clusters in the dendrogram. This can lead to difficulties in visualization as well as in interpretation of the dendrogram.\n# Init linkages = [hierarchy.complete(X), hierarchy.average(X), hierarchy.single(X)] titles = ['Complete Linkage', 'Average Linkage', 'Single Linkage'] Plotting make_new_figure_4(linkages, titles) For this data, both complete and average linkage generally separates the observations into their correct groups.\nGaussian Mixture Models Clustering methods such as hierarchical clustering and K-means are based on heuristics and rely primarily on finding clusters whose members are close to one another, as measured directly with the data (no probability model involved).\nGaussian Mixture Models assume that the data was generated by multiple multivariate gaussian distributions. The objective of the algorithm is to recover these latent distributions.\nThe advantages with respect to K-means are\na structural interpretaion of the parameters automatically generates class probabilities can generate clusters of observations that are not necessarily close to each other Algorithm Randomly assign a number, from $1$ to $K$, to each of the observations. These serve as initial cluster assignments for the observations.\nIterate until the cluster assignments stop changing:\na) For each of the $K$ clusters, compute its mean and variance. The main difference with K-means is that we also compute the variance matrix.\nb) Assign each observation to its most likely cluster.\nDataset Let\u0026rsquo;s use the same data we have used for k-means, for a direct comparison.\nmake_new_figure_1(X) Step 1: random assignement Let\u0026rsquo;s also use the same random assignment of the K-means algorithm.\nmake_new_figure_2(X, clusters0) Step 2: compute distirbutions What are the new distributions?\n# Compute new centroids def compute_distributions(X, clusters): K = len(np.unique(clusters)) distr = [] for k in range(K): if sum(clusters==k)\u0026gt;0: distr += [multivariate_normal(np.mean(X[clusters==k,:], axis=0), np.cov(X[clusters==k,:].T))] else: distr += [multivariate_normal(np.mean(X, axis=0), np.cov(X.T))] return distr # Print distr0 = compute_distributions(X, clusters0) print(\u0026quot;Mean of the first distribution: \\n\u0026quot;, distr0[0].mean) print(\u0026quot;\\nVariance of the first distribution: \\n\u0026quot;, distr0[0].cov) Mean of the first distribution: [ 1.54179703 -1.65922379] Variance of the first distribution: [[ 3.7160256 -2.27290036] [-2.27290036 4.67223237]] Plotting the distributions Let\u0026rsquo;s add the distributions to the graph.\n# Plot plot_assignment_gmm(X, clusters0, distr0, i=0, logL=0.0) Likelihood The main difference with respect with K-means is that we can now compute the probability that each observation belongs to each cluster. This is the probability that each observation was generated by one of the two bi-variate normal distributions. These probabilities are called likelihoods.\n# Print first 5 likelihoods pdfs0 = np.stack([d.pdf(X) for d in distr0], axis=1) pdfs0[:5] array([[0.03700522, 0.05086876], [0.00932081, 0.02117353], [0.04092453, 0.04480732], [0.00717854, 0.00835799], [0.01169199, 0.01847373]]) Step 3: assign data to clusters Now we can assign the data to the clusters, via maximum likelihood.\n# Assign X to clusters def assign_to_cluster_gmm(X, distr): pdfs = np.stack([d.pdf(X) for d in distr], axis=1) clusters = np.argmax(pdfs, axis=1) log_likelihood = 0 for k, pdf in enumerate(pdfs): log_likelihood += np.log(pdf[clusters[k]]) return clusters, log_likelihood # Get cluster assignment clusters1, logL1 = assign_to_cluster_gmm(X, distr0) Plotting assigned data # Compute new distributions distr1 = compute_distributions(X, clusters1) # Plot plot_assignment_gmm(X, clusters1, distr1, 1, logL1); Expectation - Maximization The two steps we have just seen, are part of a broader family of algorithms to maximize likelihoods called expectation-maximization algorithms.\nIn the expectation step, we computed the expectation of the parameters, given the current cluster assignment.\nIn the maximization step, we assigned observations to the cluster that maximized the likelihood of the single observation.\nThe alternative, and more computationally intensive procedure, would have been to specify a global likelihood function and find the mean and variance paramenters of the two normal distributions that maximized those likelihoods.\nFull Algorithm We can now deploy the full algorithm.\ndef gmm_manual(X, K): # Init i = 0 logL0 = 1e4 logL1 = 1e5 clusters = np.random.randint(K,size=(np.size(X,0))) # Iterate until convergence while np.abs(logL0-logL1) \u0026gt; 1e-10: logL1 = logL0 distr = compute_distributions(X, clusters) clusters, logL0 = assign_to_cluster_gmm(X, distr) plot_assignment_gmm(X, clusters, distr, i, logL0) i+=1 Plotting k-means clustering # Test gmm_manual(X, K) In this case, GMM does a very poor job identifying the original clusters.\nOverlapping Clusters Let\u0026rsquo;s now try with a different dataset, where the data is drawn from two overlapping bi-variate gaussian distributions, forming a cross.\n# Simulate data X = np.random.randn(50,2) X[0:25, :] = np.random.multivariate_normal([0,0], [[50,0],[0,1]], size=25) X[25:, :] = np.random.multivariate_normal([0,0], [[1,0],[0,50]], size=25) make_new_figure_1(X) GMM with overlapping distributions # GMM gmm_manual(X, K) As we can see, GMM is able to correctly recover the original clusters.\nK-means with overlapping distributions # K-means kmeans_manual(X, K) K-means generates completely different clusters.\n","date":1646784000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1646784000,"objectID":"7ac490af7d8081e38d0778e24284cbbb","permalink":"https://matteocourthoud.github.io/course/ml-econ/10_unsupervised/","publishdate":"2022-03-09T00:00:00Z","relpermalink":"/course/ml-econ/10_unsupervised/","section":"course","summary":"# Setup from utils.lecture10 import * %matplotlib inline Supervised vs Unsupervised Learning The difference between supervised learning and unsupervised learning is that in the first case we have a variable $y$ which we want to predict, given a set of variables ${ X_1, .","tags":null,"title":"Unsupervised Learning","type":"book"},{"authors":null,"categories":null,"content":"Intro In this session, I am going to cover demand estimation.\nCompute equilibrium outcomes with Logit demand Simulate a dataset Estimate Logit demand Compare different instruments Include supply Model In this first part, we are going to assume that consumer $i \\in \\lbrace1,\u0026hellip;,I\\rbrace$ utility from good $j \\in \\lbrace1,\u0026hellip;,J\\rbrace$ in market $t \\in \\lbrace1,\u0026hellip;,T\\rbrace$ takes the form\n$$ u_{ijt} = \\boldsymbol x_{jt} \\boldsymbol \\beta_{it} - \\alpha p_{jt} + \\xi_{jt} + \\epsilon_{ijt} $$\nwhere\n$\\xi_{jt}$ is type-1 extreme value distributed $\\boldsymbol \\beta$ has dimension $K$ i.e. goods have $K$ characteristics Setup We have $J$ firms and each product has $K$ characteristics\nJ = 3; # 3 firms == products K = 2; # 2 product characteristics c = rand(Uniform(0, 1), J); # Random uniform marginal costs ξ = rand(Normal(0, 1), J+1); # Random normal individual shocks X = rand(Exponential(1), J, K); # Random exponential product characteristics β = [.5, 2, -1]; # Preferences (last one is for prices, i.e. alpha) Code Demand function demand(p::Vector, X::Matrix, β::Vector, ξ::Vector)::Tuple{Vector, Number} \u0026quot;\u0026quot;\u0026quot;Compute demand\u0026quot;\u0026quot;\u0026quot; δ = 1 .+ [X p] * β # Mean value u = [δ; 0] + ξ # Utility e = exp.(u) # Take exponential q = e ./ sum(e) # Compute demand return q[1:end-1], q[end] end; We can try with an example.\np = 2 .* c; demand(p, X, β, ξ) ## ([0.4120077746005573, 0.26650568009936, 0.24027826270165709], 0.08120828259842561) Code Supply function profits(p::Vector, c::Vector, X::Matrix, β::Vector, ξ::Vector)::Vector \u0026quot;\u0026quot;\u0026quot;Compute profits\u0026quot;\u0026quot;\u0026quot; q, _ = demand(p, X, β, ξ) # Compute demand pr = (p - c) .* q # Compute profits return pr end; We can try with an example.\nprofits(p, c, X, β, ξ) ## 3-element Array{Float64,1}: ## 0.20289186252172428 ## 0.1596422305025479 ## 0.1270874470740512 Code Best Reply We first code the best reply of firm $j$\nfunction profits_j(pj::Number, j::Int, p::Vector, c::Vector, X::Matrix, β::Vector, ξ::Vector)::Number \u0026quot;\u0026quot;\u0026quot;Compute profits of firm j\u0026quot;\u0026quot;\u0026quot; p[j] = pj # Insert price of firm j pr = profits(p, c, X, β, ξ) # Compute profits return pr[j] end; Let’s test it.\nj = 1; obj_fun(pj) = - profits_j(pj[1], j, copy(p), c, X, β, ξ); pj = optimize(x -\u0026gt; obj_fun(x), [1.0], LBFGS()).minimizer[1] ## 1.8019637881982011 What are the implied profits now?\nprint(\u0026quot;Profits old: \u0026quot;, round.(profits(p, c, X, β, ξ), digits=4)) ## Profits old: [0.2029, 0.1596, 0.1271] p_new = copy(p); p_new[j] = pj; print(\u0026quot;Profits new: \u0026quot;, round.(profits(p_new, c, X, β, ξ), digits=4)) ## Profits new: [0.3095, 0.2073, 0.1651] Indeed firm 1 has increased its profits.\nCode Equilibrium We can now compute equilibrium prices\nfunction equilibrium(c::Vector, X::Matrix, β::Vector, ξ::Vector)::Vector \u0026quot;\u0026quot;\u0026quot;Compute equilibrium prices and profits\u0026quot;\u0026quot;\u0026quot; p = 2 .* c; dist = 1; iter = 0; # Until convergence while (dist \u0026gt; 1e-8) \u0026amp;\u0026amp; (iter\u0026lt;1000) # Compute best reply for each firm p1 = copy(p); for j=1:length(p) obj_fun(pj) = - profits_j(pj[1], j, p, c, X, β, ξ); optimize(x -\u0026gt; obj_fun(x), [1.0], LBFGS()).minimizer[1]; end # Update distance dist = max(abs.(p - p1)...); iter += 1; end return p end; Code Equilibrium Let’s test it\n# Compute equilibrium prices p_eq = equilibrium(c, X, β, ξ); print(\u0026quot;Equilibrium prices: \u0026quot;, round.(p_eq, digits=4)) ## Equilibrium prices: [1.9764, 1.9602, 1.8366] # And profits pi_eq = profits(p_eq, c, X, β, ξ); print(\u0026quot;Equilibrium profits: \u0026quot;, round.(pi_eq, digits=4)) ## Equilibrium profits: [0.484, 0.3612, 0.3077] As expected the prices of the first 2 firms are lower and their profits are higher.\nDGP Let’s generate our Data Generating Process (DGP).\n$\\boldsymbol x \\sim exp(V_{x})$ $\\xi \\sim N(0, V_{\\xi})$ $w \\sim N(0, 1)$ $\\omega \\sim N(0, 1)$ function draw_data(J::Int, K::Int, rangeJ::Vector, varX::Number, varξ::Number)::Tuple \u0026quot;\u0026quot;\u0026quot;Draw data for one market\u0026quot;\u0026quot;\u0026quot; J_ = rand(rangeJ[1]:rangeJ[2]) # Number of firms (products) X_ = rand(Exponential(varX), J_, K) # Product characteristics ξ_ = rand(Normal(0, varξ), J_+1) # Product-level utility shocks w_ = rand(Uniform(0, 1), J_) # Cost shifters ω_ = rand(Uniform(0, 1), J_) # Cost shocks c_ = w_ + ω_ # Cost j_ = sort(sample(1:J, J_, replace=false)) # Subset of firms return X_, ξ_, w_, c_, j_ end; Equilibrium We first compute the equilibrium in one market.\nfunction compute_mkt_eq(J::Int, b::Vector, rangeJ::Vector, varX::Number, varξ::Number)::DataFrame \u0026quot;\u0026quot;\u0026quot;Compute equilibrium one market\u0026quot;\u0026quot;\u0026quot; # Initialize variables K = size(β, 1) - 1 X_, ξ_, w_, c_, j_ = draw_data(J, K, rangeJ, varX, varξ) # Compute equilibrium p_ = equilibrium(c_, X_, β, ξ_) # Equilibrium prices q_, q0 = demand(p_, X_, β, ξ_) # Demand with shocks pr_ = (p_ - c_) .* q_ # Profits # Save to data q0_ = ones(length(j_)) .* q0 df = DataFrame(j=j_, w=w_, p=p_, q=q_, q0=q0_, pr=pr_) for k=1:K df[!,\u0026quot;x$k\u0026quot;] = X_[:,k] df[!,\u0026quot;z$k\u0026quot;] = sum(X_[:,k]) .- X_[:,k] end return df end; Simulate Dataset We can now write the code to simulate the whole dataset.\nfunction simulate_data(J::Int, b::Vector, T::Int, rangeJ::Vector, varX::Number, varξ::Number) \u0026quot;\u0026quot;\u0026quot;Simulate full dataset\u0026quot;\u0026quot;\u0026quot; df = compute_mkt_eq(J, β, rangeJ, varX, varξ) df[!, \u0026quot;t\u0026quot;] = ones(nrow(df)) * 1 for t=2:T df_temp = compute_mkt_eq(J, β, rangeJ, varX, varξ) df_temp[!, \u0026quot;t\u0026quot;] = ones(nrow(df_temp)) * t append!(df, df_temp) end CSV.write(\u0026quot;../data/logit.csv\u0026quot;, df) end; Simulate Dataset (2) We generate the dataset by simulating many markets that differ by\nnumber of firms (and their identity) their marginal costs their product characteristics # Set parameters J = 10; # Number of firms K = 2; # Product caracteristics T = 500; # Markets β = [.5, 2, -1]; # Preferences rangeJ = [2, 6]; # Min and max firms per market varX = 1; # Variance of X varξ = 2; # Variance of xi # Simulate df = simulate_data(J, β, T, rangeJ, varX, varξ); The Data What does the data look like? Let’s switch to R!\n# Read data df = fread(\u0026quot;../data/logit.csv\u0026quot;) kable(df[1:6,], digits=4) j w p q q0 pr x1 z1 x2 z2 t 1 0.1491 1.9616 0.0932 0.0013 0.1028 2.4517 0.8219 1.6918 3.6779 1 2 0.8352 2.1112 0.0193 0.0013 0.0197 0.1328 3.1408 0.2075 5.1622 1 7 0.2749 2.2789 0.2710 0.0013 0.3717 0.1449 3.1287 1.1493 4.2205 1 10 0.4118 3.6386 0.6151 0.0013 1.5982 0.5442 2.7294 2.3212 3.0485 1 5 0.6071 2.1457 0.0886 0.0003 0.0972 0.9239 2.4182 3.6551 10.0474 2 6 0.1615 1.1626 0.0000 0.0003 0.0000 0.1763 3.1657 0.3629 13.3396 2 Estimation First we need to compute the dependent variable\ndf$y = log(df$q) - log(df$q0) Now we can estimate the logit model. The true values are $alpha=1$.\nols \u0026lt;- lm(y ~ x1 + x2 + p, data=df) kable(tidy(ols), digits=4) term estimate std.error statistic p.value (Intercept) -1.3558 0.1476 -9.1874 0e+00 x1 0.4176 0.0537 7.7782 0e+00 x2 1.1494 0.0719 15.9903 0e+00 p 0.2406 0.0656 3.6664 3e-04 The estimate of $\\alpha = 1$ is biased (positive and significant) since $p$ is endogenous. We need instruments.\nIV 1: Cost Shifters First set of instruments: cost shifters.\nfm_costiv \u0026lt;- ivreg(y ~ x1 + x2 + p | x1 + x2 + w, data=df) kable(tidy(fm_costiv), digits=4) term estimate std.error statistic p.value (Intercept) 0.2698 0.4923 0.5480 0.5837 x1 0.5249 0.0643 8.1679 0.0000 x2 1.8178 0.2064 8.8059 0.0000 p -0.7034 0.2800 -2.5123 0.0121 Now the estimate of $\\alpha$ is negative and significant.\nIV 2: BLP Instruments Second set of instruments: product characteristics of other firms in the same market.\nfm_blpiv \u0026lt;- ivreg(y ~ x1 + x2 + p | x1 + x2 + z1 + z2, data=df) kable(tidy(fm_blpiv), digits=4) term estimate std.error statistic p.value (Intercept) 1.6616 0.5014 3.3139 9e-04 x1 0.6167 0.0698 8.8380 0e+00 x2 2.3901 0.2110 11.3279 0e+00 p -1.5117 0.2840 -5.3221 0e+00 Also the BLP instruments deliver an estimate of $\\alpha$ is negative and significant.\nAppendix References ","date":1636502400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1636502400,"objectID":"289c62cc59ec0e5e817ebc58bc3283ae","permalink":"https://matteocourthoud.github.io/course/empirical-io/11_logit_demand/","publishdate":"2021-11-10T00:00:00Z","relpermalink":"/course/empirical-io/11_logit_demand/","section":"course","summary":"Intro In this session, I am going to cover demand estimation.\nCompute equilibrium outcomes with Logit demand Simulate a dataset Estimate Logit demand Compare different instruments Include supply Model In this first part, we are going to assume that consumer $i \\in \\lbrace1,\u0026hellip;,I\\rbrace$ utility from good $j \\in \\lbrace1,\u0026hellip;,J\\rbrace$ in market $t \\in \\lbrace1,\u0026hellip;,T\\rbrace$ takes the form","tags":null,"title":"Coding: Logit Demand","type":"book"},{"authors":null,"categories":null,"content":"Intro In this session, I am going to cover demand estimation.\nCompute equilibrium outcomes with RCL demand Simulate market-level data Extremely similar to the logit demand simulation Build the BLP estimator from Berry, Levinsohn, and Pakes (1995) Model In this first part, we are going to assume that consumer $i \\in \\lbrace1,\u0026hellip;,I\\rbrace$ utility from good $j \\in \\lbrace1,\u0026hellip;,J\\rbrace$ in market $t \\in \\lbrace1,\u0026hellip;,T\\rbrace$ takes the form\n$$ u_{ijt} = \\boldsymbol x_{jt} \\boldsymbol \\beta_{it} - \\alpha p_{jt} + \\xi_{jt} + \\epsilon_{ijt} $$\nwhere\n$\\xi_{jt}$ is type-1 extreme value distributed $\\boldsymbol \\beta_{it}$: has dimension $K$ $$\\beta_{it}^k = \\beta_0^k + \\sigma_k \\zeta_{it}^k$$ $\\beta_0^k$: fixed taste for characteristic $k$ (the usual $\\beta$) $\\zeta_{it}^k$: random taste, i.i.d. across consumers and markets $t$ Setup We have $J$ firms and each product has $K$ characteristics.\ni = 100; # Number of consumers J = 10; # Number of firms K = 2; # Product characteristics T = 100; # Number of markets β = [.5, 2, -1]; # Preferences varζ = 5; # Variance of the random taste rangeJ = [2, 6]; # Min and max firms per market varX = 1; # Variance of X varξ = 2; # Variance of xi Demand Demand is the main difference w.r.t. the logit model. Now we have individual shocks $\\zeta$ we have to integrate over.\nfunction demand(p::Vector, X::Matrix, β::Vector, ξ::Matrix, ζ::Matrix)::Tuple{Vector, Number} \u0026quot;\u0026quot;\u0026quot;Compute demand\u0026quot;\u0026quot;\u0026quot; δ = [X p] * (β .+ ζ) # Mean value δ0 = zeros(1, size(ζ, 2)) # Mean value of the outside option u = [δ; δ0] + ξ # Utility e = exp.(u) # Take exponential q = mean(e ./ sum(e, dims=1), dims=2) # Compute demand return q[1:end-1], q[end] end; Supply Computing profits is instead exactly the same as before. We just have to save the shocks $\\zeta$ to be sure demand is stable.\nfunction profits(p::Vector, c::Vector, X::Matrix, β::Vector, ξ::Matrix, ζ::Matrix)::Vector \u0026quot;\u0026quot;\u0026quot;Compute profits\u0026quot;\u0026quot;\u0026quot; q, _ = demand(p, X, β, ξ, ζ) # Compute demand pr = (p - c) .* q # Compute profits return pr end; function profits_j(pj::Number, j::Int, p::Vector, c::Vector, X::Matrix, β::Vector, ξ::Matrix, ζ::Matrix)::Number \u0026quot;\u0026quot;\u0026quot;Compute profits of firm j\u0026quot;\u0026quot;\u0026quot; p[j] = pj # Insert price of firm j pr = profits(p, c, X, β, ξ, ζ) # Compute profits return pr[j] end; Equilibrium We can now compute the equilibrium for a specific market, as before.\nfunction equilibrium(c::Vector, X::Matrix, β::Vector, ξ::Matrix, ζ::Matrix)::Vector \u0026quot;\u0026quot;\u0026quot;Compute equilibrium prices and profits\u0026quot;\u0026quot;\u0026quot; p = 2 .* c; dist = 1; iter = 0; # Iterate until convergence while (dist \u0026gt; 1e-8) \u0026amp;\u0026amp; (iter\u0026lt;1000) # Compute best reply for each firm p_old = copy(p); for j=1:length(p) obj_fun(pj) = - profits_j(pj[1], j, p, c, X, β, ξ, ζ); optimize(x -\u0026gt; obj_fun(x), [1.0], LBFGS()); end # Update distance dist = max(abs.(p - p_old)...); iter += 1; end return p end; Simulating Data We are now ready to simulate the data, i.e. equilibrium outcomes across different markets. We first draw all the variables.\nfunction draw_data(I::Int, J::Int, K::Int, rangeJ::Vector, varζ::Number, varX::Number, varξ::Number)::Tuple \u0026quot;\u0026quot;\u0026quot;Draw data for one market\u0026quot;\u0026quot;\u0026quot; J_ = rand(rangeJ[1]:rangeJ[2]) # Number of firms (products) X_ = rand(Exponential(varX), J_, K) # Product characteristics ξ_ = rand(Normal(0, varξ), J_+1, I) # Product-level utility shocks # Consumer-product-level preference shocks ζ_ = [rand(Normal(0,1), 1, I) * varζ; zeros(K,I)] w_ = rand(Uniform(0, 1), J_) # Cost shifters ω_ = rand(Uniform(0, 1), J_) # Cost shocks c_ = w_ + ω_ # Cost j_ = sort(sample(1:J, J_, replace=false)) # Subset of firms return X_, ξ_, ζ_, w_, c_, j_ end; Simulating Data Then we simulate the data for one market.\nfunction compute_mkt_eq(I::Int, J::Int, β::Vector, rangeJ::Vector, varζ::Number, varX::Number, varξ::Number)::DataFrame \u0026quot;\u0026quot;\u0026quot;Compute equilibrium one market\u0026quot;\u0026quot;\u0026quot; # Initialize variables K = size(β, 1) - 1 X_, ξ_, ζ_, w_, c_, j_ = draw_data(I, J, K, rangeJ, varζ, varX, varξ) # Compute equilibrium p_ = equilibrium(c_, X_, β, ξ_, ζ_) # Equilibrium prices q_, q0 = demand(p_, X_, β, ξ_, ζ_) # Demand with shocks pr_ = (p_ - c_) .* q_ # Profits # Save to data q0_ = ones(length(j_)) .* q0 df = DataFrame(j=j_, w=w_, p=p_, q=q_, q0=q0_, pr=pr_) for k=1:K df[!,\u0026quot;x$k\u0026quot;] = X_[:,k] df[!,\u0026quot;z$k\u0026quot;] = sum(X_[:,k]) .- X_[:,k] end return df end; Simultate the Data (2) We repeat for $T$ markets.\nfunction simulate_data(I::Int, J::Int, β::Vector, T::Int, rangeJ::Vector, varζ::Number, varX::Number, varξ::Number) \u0026quot;\u0026quot;\u0026quot;Simulate full dataset\u0026quot;\u0026quot;\u0026quot; df = compute_mkt_eq(I, J, β, rangeJ, varζ, varX, varξ) df[!, \u0026quot;t\u0026quot;] = ones(nrow(df)) * 1 for t=2:T df_temp = compute_mkt_eq(I, J, β, rangeJ, varζ, varX, varξ) df_temp[!, \u0026quot;t\u0026quot;] = ones(nrow(df_temp)) * t append!(df, df_temp) end CSV.write(\u0026quot;../data/blp.csv\u0026quot;, df) return df end; Simulate the Data (3) Now let’s run the code\n# Simulate df = simulate_data(i, J, β, T, rangeJ, varζ, varX, varξ); The Data What does the data look like? Let’s switch to R!\n# Read data df = fread(\u0026quot;../data/blp.csv\u0026quot;) kable(df[1:6,], digits=4) j w p q q0 pr x1 z1 x2 z2 t 4 0.6481 3.5918 0.2929 0.4558 0.8165 0.6531 1.0002 1.7063 1.8207 1 7 0.9997 5.1926 0.2513 0.4558 0.8717 1.0002 0.6531 1.8207 1.7063 1 1 0.5842 2.6999 0.0800 0.3919 0.1572 0.3591 7.1958 0.4217 2.1996 2 4 0.5291 4.5934 0.1404 0.3919 0.4467 2.3801 5.1748 0.1283 2.4929 2 5 0.5012 4.4196 0.1368 0.3919 0.4461 2.2638 5.2911 0.4408 2.1804 2 8 0.9359 3.2923 0.0863 0.3919 0.1477 0.5182 7.0367 1.0271 1.5942 2 Estimation The BLP estimation procedure\nFrom deltas to shares First, we need to compute the shares implied by aspecific vector of $\\delta$s\nfunction implied_shares(Xt_::Matrix, ζt_::Matrix, δt_::Vector, δ0::Matrix)::Vector \u0026quot;\u0026quot;\u0026quot;Compute shares implied by deltas and shocks\u0026quot;\u0026quot;\u0026quot; u = [δt_ .+ (Xt_ * ζt_); δ0] # Utility e = exp.(u) # Take exponential q = mean(e ./ sum(e, dims=1), dims=2) # Compute demand return q[1:end-1] end; Inner Loop We can now compute the inner loop and invert the demand function: from shares $q$ to $\\delta$s\nfunction inner_loop(qt_::Vector, Xt_::Matrix, ζt_::Matrix)::Vector \u0026quot;\u0026quot;\u0026quot;Solve the inner loop: compute delta, given the shares\u0026quot;\u0026quot;\u0026quot; δt_ = ones(size(qt_)) δ0 = zeros(1, size(ζt_, 2)) dist = 1 # Iterate until convergence while (dist \u0026gt; 1e-8) q = implied_shares(Xt_, ζt_, δt_, δ0) δt2_ = δt_ + log.(qt_) - log.(q) dist = max(abs.(δt2_ - δt_)...) δt_ = δt2_ end return δt_ end; Compute Delta We can now repeat the inversion for every market and get the vector of mean utilities $\\delta$s from the observed market shares $q$.\nfunction compute_delta(q_::Vector, X_::Matrix, ζ_::Matrix, T::Vector)::Vector \u0026quot;\u0026quot;\u0026quot;Compute residuals\u0026quot;\u0026quot;\u0026quot; δ_ = zeros(size(T)) # Loop over each market for t in unique(T) qt_ = q_[T.==t] # Quantity in market t Xt_ = X_[T.==t,:] # Characteristics in mkt t δ_[T.==t] = inner_loop(qt_, Xt_, ζ_) # Solve inner loop end return δ_ end; Compute Xi Now that we have $\\delta$, it is pretty straightforward to compute $\\xi$. We just need to perform a linear regression (with instruments) of mean utilities $\\delta$ on prices $p$ and product characteristics $X$ and compute the residuals $\\xi$.\nfunction compute_xi(X_::Matrix, IV_::Matrix, δ_::Vector)::Tuple \u0026quot;\u0026quot;\u0026quot;Compute residual, given delta (IV)\u0026quot;\u0026quot;\u0026quot; β_ = inv(IV_' * X_) * (IV_' * δ_) # Compute coefficients (IV) ξ_ = δ_ - X_ * β_ # Compute errors return ξ_, β_ end; Objective Function We now have all the ingredients to set up the GMM objective function.\nfunction GMM(varζ_::Number)::Tuple \u0026quot;\u0026quot;\u0026quot;Compute GMM objective function\u0026quot;\u0026quot;\u0026quot; δ_ = compute_delta(q_, X_, ζ_ * varζ_, T) # Compute deltas ξ_, β_ = compute_xi(X_, IV_, δ_) # Compute residuals gmm = ξ_' * Z_ * Z_' * ξ_ / length(ξ_)^2 # Compute ortogonality condition return gmm, β_ end; Estimation (1) First, we need to set up our objects\n# Retrieve data T = Int.(df.t) X_ = [df.x1 df.x2 df.p] q_ = df.q q0_ = df.q0 IV_ = [df.x1 df.x2 df.w] Z_ = [df.x1 df.x2 df.z1 df.z2] Estimation (2) What would a logit regression estimate?\n# Compute logit estimate y = log.(df.q) - log.(df.q0); β_logit = inv(IV_' * X_) * (IV_' * y); print(\u0026quot;Estimated logit coefficients: $β_logit\u0026quot;) ## Estimated logit coefficients: [2.063144844221613, 1.2888511782561123, -0.9824308271558686] Estimation (3) We can now run the BLP machinery\n# Draw shocks (less) ζ_ = [rand(Normal(0,1), 1, i); zeros(K, i)]; # Minimize GMM objective function varζ_ = optimize(x -\u0026gt; GMM(x[1])[1], [2.0], LBFGS()).minimizer[1]; β_blp = GMM(varζ_)[2]; print(\u0026quot;Estimated BLP coefficients: $β_blp\u0026quot;) ## Estimated BLP coefficients: [0.549234645269979, 1.1243451127088748, -0.6229637255651461] Appendix References [references] Berry, Steven, James Levinsohn, and Ariel Pakes. 1995. “Automobile Prices in Market Equilibrium.” Econometrica: Journal of the Econometric Society, 841–90.\n","date":1635465600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1635465600,"objectID":"3c20ec0569d51ddc3466ae8054be35bd","permalink":"https://matteocourthoud.github.io/course/empirical-io/12_blp_1995/","publishdate":"2021-10-29T00:00:00Z","relpermalink":"/course/empirical-io/12_blp_1995/","section":"course","summary":"Intro In this session, I am going to cover demand estimation.\nCompute equilibrium outcomes with RCL demand Simulate market-level data Extremely similar to the logit demand simulation Build the BLP estimator from Berry, Levinsohn, and Pakes (1995) Model In this first part, we are going to assume that consumer $i \\in \\lbrace1,\u0026hellip;,I\\rbrace$ utility from good $j \\in \\lbrace1,\u0026hellip;,J\\rbrace$ in market $t \\in \\lbrace1,\u0026hellip;,T\\rbrace$ takes the form","tags":null,"title":"Coding: BLP (1995)","type":"book"},{"authors":null,"categories":null,"content":"Setting From Rust (1988)\nAn agent owns a fleet to buses\nBuses get old over time\nThe older the bus is, the most costly it is to maintain\nThe agent can decide to replace the bus engine with a new one, at a cost\nDynamic trade-off\nWhat is the best moment to replace the engine?\nYou don’t want to replace an engine too early\ndoesn’t change much You don’t want to replace an engine too late\navoid unnecessary maintenance costs State State: mileage of the bus\n$$s_t \\in \\lbrace 1, \u0026hellip;, 10 \\rbrace $$\nState transitions: with probability $\\lambda$ the mileage of the bus increases\n$$ s_{t+1} = \\begin{cases} \\min \\lbrace s_t + 1,10 \\rbrace \u0026amp; \\text { with probability } \\lambda \\newline s_t \u0026amp; \\text { with probability } 1 - \\lambda \\end{cases} $$\nNote that $\\lambda$ does not depend on the value of the state\nActions Action: replacement decision $$ a_t \\in \\lbrace 0, 1 \\rbrace $$\nPayoffs\nPer-period maintenance cost\nCost of replacement $$ u\\left(s_{t}, a_{t}, \\epsilon_{1 t}, \\epsilon_{2 t} ; \\theta\\right)= \\begin{cases} -\\theta_{1} s_{t}-\\theta_{2} s_{t}^{2}+\\epsilon_{0 t}, \u0026amp; \\text { if } a_{t}=0 \\newline -\\theta_{3} + \\epsilon_{1t}, \u0026amp; \\text { if } a_{t}=1 \\end{cases} $$\nSolving the Model Start with an initial expected value function $V(s_t)=0$\nCompute the alternative-specific value function $$ \\bar V(s_t) = \\begin{cases} -\\theta_1 s_t - \\theta_2 s_t^2 + \\beta \\Big[(1-\\lambda) V(s_t) + \\lambda V(\\min \\lbrace s_t+1,10 \\rbrace ) \\Big] , \u0026amp; \\text { if } a_t=0 \\newline -\\theta_3 + \\beta \\Big[(1-\\lambda) V(0) + \\lambda V(1) \\Big] , \u0026amp; \\text { if } a_t=1 \\end{cases} $$\nCompute the new expected value function $$ V\u0026rsquo;(a_t) = \\log \\Big( e^{\\bar V(a_t|s_t=0)} + e^{\\bar V(a_t|s_t=1)} \\Big) $$\nRepeat until convergence\nCode First we set the parameter values.\n# Set parameters θ = [0.13; -0.004; 3.1]; λ = 0.82; β = 0.95; Then we set the state space.\n# State space k = 10; s = Vector(1:k); Static Utility First, we can compute static utility. $$ u\\left(s_{t}, a_{t}, \\epsilon_{1 t}, \\epsilon_{2 t} ; \\theta\\right)= \\begin{cases} -\\theta_{1} s_{t}-\\theta_{2} s_{t}^{2}+\\epsilon_{0 t}, \u0026amp; \\text { if } a_{t}=0 \\newline -\\theta_{3} + \\epsilon_{1 t}, \u0026amp; \\text { if } a_{t}=1 \\end{cases} $$\nfunction compute_U(θ::Vector, s::Vector)::Matrix \u0026quot;\u0026quot;\u0026quot;Compute static utility\u0026quot;\u0026quot;\u0026quot; u1 = - θ[1]*s - θ[2]*s.^2 # Utility of not investing u2 = - θ[3]*ones(size(s)) # Utility of investing U = [u1 u2] # Combine in a matrix return U end; Value Function We can now set up the value function iteration\nfunction compute_Vbar(θ::Vector, λ::Number, β::Number, s::Vector)::Matrix \u0026quot;\u0026quot;\u0026quot;Compute value function by Bellman iteration\u0026quot;\u0026quot;\u0026quot; k = length(s) # Dimension of the state space U = compute_U(θ, s) # Static utility index_λ = Int[1:k [2:k; k]]; # Mileage index index_A = Int[1:k ones(k,1)]; # Investment index γ = Base.MathConstants.eulergamma # Euler's gamma # Iterate the Bellman equation until convergence Vbar = zeros(k, 2); Vbar1 = Vbar; dist = 1; iter = 0; while dist\u0026gt;1e-8 V = γ .+ log.(sum(exp.(Vbar), dims=2)) # Compute value expV = V[index_λ] * [1-λ; λ] # Compute expected value Vbar1 = U + β * expV[index_A] # Compute v-specific dist = max(abs.(Vbar1 - Vbar)...); # Check distance iter += 1; Vbar = Vbar1 # Update value function end return Vbar end; Solving the Model We can now solve for the value function.\n# Compute value function V_bar = compute_Vbar(θ, λ, β, s); DGP Now that we know how to compute the equilibrium, we can simulate the data.\nfunction generate_data(θ::Vector, λ::Number, β::Number, s::Vector, N::Int)::Tuple \u0026quot;\u0026quot;\u0026quot;Generate data from primitives\u0026quot;\u0026quot;\u0026quot; Vbar = compute_Vbar(θ, λ, β, s) # Solve model ε = rand(Gumbel(0,1), N, 2) # Draw shocks St = rand(s, N) # Draw states A = (((Vbar[St,:] + ε) * [-1;1]) .\u0026gt; 0) # Compute investment decisions δ = (rand(Uniform(0,1), N) .\u0026lt; λ) # Compute mileage shock St1 = min.(St .* (A.==0) + δ, max(s...)) # Compute neSr state df = DataFrame(St=St, A=A, St1=St1) # Dataframe CSV.write(\u0026quot;../data/rust.csv\u0026quot;, df) return St, A, St1 end; Generate the DAta We can now generate the data\n# Generate data N = Int(1e5); St, A, St1 = generate_data(θ, λ, β, s, N); How many investment decisions do we observe?\nprint(\u0026quot;we observe \u0026quot;, sum(A), \u0026quot; investment decisions in \u0026quot;, N, \u0026quot; observations\u0026quot;) ## we observe 19207 investment decisions in 100000 observations The Data What does the data look like?\n# Read data df = fread(\u0026quot;../data/rust.csv\u0026quot;) kable(df[1:6,], digits=4) St A St1 3 FALSE 4 9 TRUE 1 8 TRUE 1 3 FALSE 4 9 FALSE 10 10 FALSE 10 Estimation - Lambda First we can estimate the value of lambda as the probability of mileage increase\nConditional on not investing\nAnd not being in the last state (mileage cannot increase any more)\n$$ \\hat \\lambda = \\mathbb E_n \\Big[ (s_{t+1}-s_t) \\mid a_{t}=0 \\wedge s_{t}\u0026lt;10 \\Big] $$\n# Estimate lambda Δ = St1 - St; λ_ = mean(Δ[(A.==0) .\u0026amp; (St.\u0026lt;10)]); print(\u0026quot;Estimated lambda: $λ_ (true = $λ)\u0026quot;) ## Estimated lambda: 0.8206570869594549 (true = 0.82) Estimation - Theta Take a parameter guess $\\theta_0$\nCompute the alternative-specific value function $\\bar V(s_t ; \\hat \\lambda, \\theta_0)$ by iteration\nCompute the implied choice probabilities\nCompute the likelihood $$ \\mathcal{L}(\\theta) = \\prod_{t=1}^{T}\\left(\\hat{\\operatorname{Pr}}\\left(a=1 \\mid s_{t}, \\theta\\right) \\mathbb{1}\\left(a_{t}=1\\right)+\\left(1-\\hat{\\operatorname{Pr}}\\left(a=0 \\mid s_{t}, \\theta\\right)\\right) \\mathbb{1}\\left(a_{t}=0\\right)\\right) $$\nRepeat the above to find a minimum of the likelihood function\nLikelihood Function function logL_Rust(θ0::Vector, λ::Number, β::Number, s::Vector, St::Vector, A::BitVector)::Number \u0026quot;\u0026quot;\u0026quot;Compute log-likelihood functionfor Rust problem\u0026quot;\u0026quot;\u0026quot; # Compute value Vbar = compute_Vbar(θ0, λ_, β, s) # Expected choice probabilities EP = exp.(Vbar[:,2]) ./ (exp.(Vbar[:,1]) + exp.(Vbar[:,2])) # Likelihood logL = sum(log.(EP[St[A.==1]])) + sum(log.(1 .- EP[St[A.==0]])) return -logL end; We can check the likelihood at the true value:\n# True likelihood value logL_trueθ = logL_Rust(θ, λ, β, s, St, A); print(\u0026quot;The likelihood at the true parameter is $logL_trueθ\u0026quot;) ## The likelihood at the true parameter is 45937.866092460084 Estimating Theta # Select starting values θ0 = Float64[0,0,0]; # Optimize θ_R = optimize(x -\u0026gt; logL_Rust(x, λ, β, s, St, A), θ0).minimizer; print(\u0026quot;Estimated thetas: $θ_R (true = $θ)\u0026quot;) ## Estimated thetas: [0.12063838656559037, -0.003220197034620527, 3.0865668144650487] (true = [0.13, -0.004, 3.1]) Starting Values Starting values are important!\n# Not all initial values are equally good θ0 = Float64[1,1,1]; # Optimize θ_R2 = optimize(x -\u0026gt; logL_Rust(x, λ, β, s, St, A), θ0).minimizer; print(\u0026quot;Estimated thetas: $θ_R2 (true = $θ)\u0026quot;) ## Estimated thetas: [1.0, 1.0, 1.0] (true = [0.13, -0.004, 3.1]) Hotz \u0026amp; Miller Recap Hotz \u0026amp; Miller estimation procedure works as follows\nEstimate the CCPs from the data\nHotz \u0026amp; Miller inversion $$ \\hat V = \\Big[I - \\beta \\ \\sum_a P_a .* T_a \\Big]^{-1} \\ * \\ \\left( \\sum_a P_a \\ .* \\ \\bigg[ u_a + \\mathbb E [\\epsilon_a] \\bigg] \\right) $$\nCompute EP from EV $$ \\hat \\Pr(a=1 ; \\theta) = \\frac{\\exp (u_1 +\\beta T_1 \\hat V )}{\\sum_{a} \\exp (u_a +\\beta T_a \\hat V )} $$\nCompute the objective function: the (log)likelihood $$ \\mathcal{L}(\\theta) = \\prod_{t=1}^{T}\\left(\\hat{\\operatorname{Pr}}\\left(a=1 \\mid s_{t}; \\theta\\right) \\mathbb{1}\\left(a_{t}=1\\right)+\\left(1-\\hat{\\operatorname{Pr}}\\left(a=0 \\mid s_{t}; \\theta\\right)\\right) \\mathbb{1}\\left(a_{t}=0\\right)\\right) $$\nCCPs First, we need to estimate the Conditional Choice Proabilities (CCP)\ncan be done non-parametrically i.e. just look at the frequency of investment in each state # Estimate CCP P = [mean(A[St.==i]) for i=s]; CCP = [(1 .- P) P] ## 10×2 Array{Float64,2}: ## 0.952419 0.0475814 ## 0.923046 0.0769535 ## 0.894443 0.105557 ## 0.853306 0.146694 ## 0.819293 0.180707 ## 0.788935 0.211065 ## 0.747248 0.252752 ## 0.717915 0.282085 ## 0.6947 0.3053 ## 0.678452 0.321548 Transition Probabilities NeSr, we need $T$, the matrices of transition probabilities, conditional on the investment choice.\nfunction compute_T(k::Int, λ_::Number)::Array \u0026quot;\u0026quot;\u0026quot;Compute transition matrix\u0026quot;\u0026quot;\u0026quot; T = zeros(k, k, 2); # Conditional on not investing T[k,k,1] = 1; for i=1:k-1 T[i,i,1] = 1-λ_ T[i,i+1,1] = λ_ end # Conditional on investing T[:,1,2] .= 1-λ_; T[:,2,2] .= λ_; return(T) end; T What form does the transition matrix $T$ take?\n# Compute T T = compute_T(k, λ_); # Conditional on not investing T[:,:,1] ## 10×10 Array{Float64,2}: ## 0.179343 0.820657 0.0 0.0 … 0.0 0.0 0.0 ## 0.0 0.179343 0.820657 0.0 0.0 0.0 0.0 ## 0.0 0.0 0.179343 0.820657 0.0 0.0 0.0 ## 0.0 0.0 0.0 0.179343 0.0 0.0 0.0 ## 0.0 0.0 0.0 0.0 0.0 0.0 0.0 ## 0.0 0.0 0.0 0.0 … 0.0 0.0 0.0 ## 0.0 0.0 0.0 0.0 0.820657 0.0 0.0 ## 0.0 0.0 0.0 0.0 0.179343 0.820657 0.0 ## 0.0 0.0 0.0 0.0 0.0 0.179343 0.820657 ## 0.0 0.0 0.0 0.0 0.0 0.0 1.0 T (2) Instead, the transitions conditional on investing are\n# T Conditional on investing T[:,:,2] ## 10×10 Array{Float64,2}: ## 0.179343 0.820657 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 ## 0.179343 0.820657 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 ## 0.179343 0.820657 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 ## 0.179343 0.820657 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 ## 0.179343 0.820657 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 ## 0.179343 0.820657 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 ## 0.179343 0.820657 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 ## 0.179343 0.820657 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 ## 0.179343 0.820657 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 ## 0.179343 0.820657 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 Hotz \u0026amp; Miller Inversion We now have all the pieces to compute the expected value function $V$ through the Hotz \u0026amp; Miller inversion. $$ \\hat V = \\left[I - \\beta \\ \\sum_a P_a .* T_a \\right]^{-1} \\ * \\ \\left( \\sum_a P_a \\ .* \\ \\bigg[ u_a + \\mathbb E [\\epsilon_a] \\bigg] \\right) $$\nfunction HM_inversion(CCP::Matrix, T::Array, U::Matrix, β::Number)::Vector \u0026quot;\u0026quot;\u0026quot;Perform HM inversion\u0026quot;\u0026quot;\u0026quot; # Compute LHS (to be inverted) γ = Base.MathConstants.eulergamma LEFT = I - β .* (CCP[:,1] .* T[:,:,1] + CCP[:,2] .* T[:,:,2]) # Compute LHS (not to be inverted) RIGHT = γ .+ sum(CCP .* (U .- log.(CCP)) , dims=2) # Compute V EV_ = inv(LEFT) * RIGHT return vec(EV_) end; From EV to EP We can now compute the expected policy function from the expected value function $$ \\hat \\Pr(a=1 ; \\theta) = \\frac{\\exp (u_1 +\\beta T_1 \\hat V )}{\\sum_{a} \\exp (u_a +\\beta T_a \\hat V )} $$\nfunction from_EV_to_EP(EV_::Vector, T::Array, U::Matrix, β::Number)::Vector \u0026quot;\u0026quot;\u0026quot;Compute expected policy from expected value\u0026quot;\u0026quot;\u0026quot; E = exp.( U + β .* [(T[:,:,1] * EV_) (T[:,:,2] * EV_)] ) EP_ = E[:,2] ./ sum(E, dims=2) return vec(EP_) end; Likelihood We now have all the pieces to build the likelihood function $$ \\mathcal{L}(\\theta) = \\prod_{t=1}^{T} \\left(\\hat \\Pr \\left(a=1 \\mid s_{t}; \\theta\\right) \\mathbb{1} \\left(a_{t}=1\\right) + \\left(1-\\hat \\Pr \\left(a=0 \\mid s_{t}; \\theta\\right)\\right) \\mathbb{1} \\left(a_{t}=0\\right)\\right) $$\nfunction logL_HM(θ0::Vector, λ::Number, β::Number, s::Vector, St::Vector, A::BitVector, T::Array, CCP::Matrix)::Number \u0026quot;\u0026quot;\u0026quot;Compute log-likelihood function for HM problem\u0026quot;\u0026quot;\u0026quot; # Compute static utility U = compute_U(θ0, s) # Espected value by inversion EV_ = HM_inversion(CCP, T, U, β) # Implies choice probabilities EP_ = from_EV_to_EP(EV_, T, U, β) # Likelihood logL = sum(log.(EP_[St[A.==1]])) + sum(log.(1 .- EP_[St[A.==0]])) return -logL end; Estimation We can now estimate the parameters\n# Optimize θ0 = Float64[0,0,0]; θ_HM = optimize(x -\u0026gt; logL_HM(x, λ, β, s, St, A, T, CCP), θ0).minimizer; print(\u0026quot;Estimated thetas: $θ_HM (true = $θ)\u0026quot;) ## Estimated thetas: [0.12064911403839335, -0.003220614484856523, 3.086621855583483] (true = [0.13, -0.004, 3.1]) Aguirregabiria, Mira (2002) With Hotz and Miller, we have generated a mapping of the form\n$$ \\bar P(\\cdot ; \\theta) = g(h(\\hat P(\\cdot) ; \\theta); \\theta) $$\nAguirregabiria and Mira (2002): why don’t we iterate it?\nAM Likelihood Function The likelihood function in Aguirregabiria and Mira (2002) is extremely similar to Hotz and Miller (1993)\nfunction logL_AM(θ0::Vector, λ::Number, β::Number, s::Vector, St::Vector, A::BitVector, T::Array, CCP::Matrix, K::Int)::Number \u0026quot;\u0026quot;\u0026quot;Compute log-likelihood function for AM problem\u0026quot;\u0026quot;\u0026quot; # Compute static utility U = compute_U(θ0, s) EP_ = CCP[:,2] # Iterate HM mapping for _=1:K EV_ = HM_inversion(CCP, T, U, β) # Expected value by inversion EP_ = from_EV_to_EP(EV_, T, U, β) # Implies choice probabilities CCP = [(1 .- EP_) EP_] end # Likelihood logL = sum(log.(EP_[St[A.==1]])) + sum(log.(1 .- EP_[St[A.==0]])) return -logL end; Estimation We can now estimate the parameters\n# Set number of iterations K = 2; # Optimize θ0 = Float64[0,0,0]; θ_AM = optimize(x -\u0026gt; logL_AM(x, λ, β, s, St, A, T, CCP, K), θ0).minimizer; print(\u0026quot;Estimated thetas: $θ_AM (true = $θ)\u0026quot;) ## Estimated thetas: [0.12063890836521114, -0.0032202282942220464, 3.086571461772538] (true = [0.13, -0.004, 3.1]) Not much changes in our case.\nSpeed We can compare the methods in terms of speed.\n# Compare speed θ0 = Float64[0,0,0]; optimize(x -\u0026gt; logL_Rust(x, λ, β, s, St, A), θ0).time_run ## 0.5477378368377686 optimize(x -\u0026gt; logL_HM(x, λ, β, s, St, A, T, CCP), θ0).time_run ## 0.3244161605834961 optimize(x -\u0026gt; logL_AM(x, λ, β, s, St, A, T, CCP, K), θ0).time_run ## 0.35499119758605957 Even in this simple example with a very small state space, the difference is significant.\nAppendix References [references] Aguirregabiria, Victor, and Pedro Mira. 2002. “Swapping the Nested Fixed Point Algorithm: A Class of Estimators for Discrete Markov Decision Models.” Econometrica 70 (4): 1519–43.\nHotz, V Joseph, and Robert A Miller. 1993. “Conditional Choice Probabilities and the Estimation of Dynamic Models.” The Review of Economic Studies 60 (3): 497–529.\nRust, John. 1988. “Maximum Likelihood Estimation of Discrete Control Processes.” SIAM Journal on Control and Optimization 26 (5): 1006–24.\n","date":1635465600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1635465600,"objectID":"77163084f4a11a31c955652b8755f93f","permalink":"https://matteocourthoud.github.io/course/empirical-io/17_rust_1987/","publishdate":"2021-10-29T00:00:00Z","relpermalink":"/course/empirical-io/17_rust_1987/","section":"course","summary":"Setting From Rust (1988)\nAn agent owns a fleet to buses\nBuses get old over time\nThe older the bus is, the most costly it is to maintain\nThe agent can decide to replace the bus engine with a new one, at a cost","tags":null,"title":"Coding: Rust (1987)","type":"book"},{"authors":null,"categories":null,"content":"A causal inference newsletter\nNews Speeding up HelloFresh’s Bayesian AB Testing PyMC researchers tested different ideas to speed up thousands of concurrent AB tests. First, they decreased model parametrization, and sampling length, leading to marginal speed increases. However, the most gains were achieved by defining a single unpooled model made of many statistically independent models, leading to 60x speed gains.\nA tidy Package for HTE Estimation A new R package, tidyhte, estimates Heterogeneous Treatment Effects with a tidy syntax. The estimator is essentially an R-learner in the spirit of Kennedy (2022). It uses SuperLearner for fitting nuisance parameters and vimp for variable importance. Official website here.\nSpotify Shares Some Experimentation Lessons First, the researchers suggest thinking backward, starting from the decision that needs to be informed and designing the experiment accordingly. Second, they suggest running local experiments to discover local effects, an effective strategy for their expansion in the Japanese market. Last, they recommend testing changes incrementally and not in bunches.\nOld Reads Experimentation with Resource Constraints Researchers at Stitchfix show how to deal with interference bias from budget constraints: enforce virtual constraints, preventing isolated groups from competing for resources. The solution is effective in solving the bias but opens new challenges in how to enforce the new constraints and scale the estimates to the full market.\nR Guide for TMLE in Medical Research This code-first guide introduces Targeted Maximum Likelihood Estimation (TMLE) through a medical application on real data: the effects of right heart catheterization on critically ill patients in the intensive care unit. The guide starts with data exploration, then introduces outcome models (G-computation), exposure models (IPW), and lastly combines them into TMLE.\nAn Overview of Netlix Experimentation Platform Netflix\u0026rsquo;s experimentation platform is built on three pillars. The first pillar is a metrics repository where statistics are stored and shared across projects. The second pillar is a causal models library that collects causal inference methods. Lastly, a lightweight interactive visualization library to explore and report estimates. A global causal graph is not mentioned.\nThumbnack Explains the Efficiency Gains of Interleaving First, interleaving is a one-sample test. Second, interleaving controls for customer variation since each customer sees both rankings. Third, signals are stronger since consumers have to make a choice (revealed preference). The downside is generalization since the rankings during the experiment differ from the deployed ones.\nFor more causal inference resources:\nSubscribe to my stories on Medium Follow me on LinkedIn Star awesome-causal-inference, where I collect all the articles and additional causal inference resources ","date":1692489600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1692489600,"objectID":"71e986fff20e05a54e0e12ab0a74118d","permalink":"https://matteocourthoud.github.io/post/cin-2023-08-20/","publishdate":"2023-08-20T00:00:00Z","relpermalink":"/post/cin-2023-08-20/","section":"post","summary":"A causal inference newsletter\nNews Speeding up HelloFresh’s Bayesian AB Testing PyMC researchers tested different ideas to speed up thousands of concurrent AB tests. First, they decreased model parametrization, and sampling length, leading to marginal speed increases.","tags":null,"title":"XYZ","type":"post"},{"authors":null,"categories":null,"content":"A causal inference newsletter\nNews Spotify Releases its Experimentation Platform: Confidence Spotify has released its internal experimentation platform, used for 10+ years to run and evaluate A/B tests at scale. Currently, it is in closed beta, but it promises large-scale performance, usability, flexibility, but also integration with existing A/B testing tools. A confidence Github repo has been existing of a while.\nMicrosoft Calls for not Worrying About Interaction Effects Microsoft researchers have looked at four major products, each running hundreds of A/B tests. They could not find any evidence of interaction effects. This does not mean interaction effects should be ignored, but rather that, at least in this scenario, they are of sufficiently low magnitude not to worry about them.\nUpdated Version of \u0026ldquo;Statistical Challenges in Online Controlled Experiments\u0026rdquo; Ron Kohavi and co-authors have updated their paper on A/B test challenges, splitting the paper into a main paper and supplementary material. The main paper covers variance reduction, heterogeneous treatment effects, long-term effects, optimal stopping, and interference. The appendix covers stratified sampling, surrogate outcomes, and network A/B tests.\nPyMC Adds the do-Operator Python\u0026rsquo;s main probabilistic programming library has added a do-operator to do Bayesian Causal Analysis. If you know and specify the whole causal graph, it allows you to do Bayesian inference on causal quantities of interest. The repo is still experimental.\nOld Reads Embrace Overlapping A/B Tests and Avoid the Dangers of Isolating Experiments From the experience of data scientists at Meta, strong interaction effects are rare. It\u0026rsquo;s worth isolating experiments only when it\u0026rsquo;s mechanically necessary, or they might break the user experience, or you need a very precise measurement.\nAn Analysts guide to MMM While not being a standalone publication, this MMM guide in Meta\u0026rsquo;s Robyn documentation is comprehensive and detailed. It goes through all the phases of building an MMM model. Causality is mentioned shortly but vehemently: \u0026ldquo;we strongly recommend using experimental and causal results to calibrate MMM\u0026rdquo;.\nAn Intuitive Explanation of Why π is in the Normal Distribution In two dimensions, the normal distribution can be defined by two properties: the pdf depends only on the distance from the origin, and the two dimensions are independent. From here, the relationship with a circle (and hence π) but also the relationship between the normal distribution and the Central Limit Theorem (covered in another video).\nA List of Company Experimentation Platforms Denise Visser lists the different in-house experimentation platforms of different companies as of 2020.\nFor more causal inference resources:\nSubscribe to my stories on Medium Follow me on LinkedIn Star awesome-causal-inference ","date":1691280000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1691280000,"objectID":"01f9a16a4fd075f42be8b3854d36fc06","permalink":"https://matteocourthoud.github.io/post/cin-2023-08-06/","publishdate":"2023-08-06T00:00:00Z","relpermalink":"/post/cin-2023-08-06/","section":"post","summary":"A causal inference newsletter\nNews Spotify Releases its Experimentation Platform: Confidence Spotify has released its internal experimentation platform, used for 10+ years to run and evaluate A/B tests at scale. Currently, it is in closed beta, but it promises large-scale performance, usability, flexibility, but also integration with existing A/B testing tools.","tags":null,"title":"XYZ","type":"post"},{"authors":null,"categories":null,"content":"A very common task in data science is churn prediction. However, predicting churn is often just an intermediate step and rarely the final objective. Usually, what we actually care about is reducing churn, which is a separate objective, not necessarily related. In fact, for example, knowing that long-term customers are less likely to churn than new customers is not an actionable insight since we cannot increase customers\u0026rsquo; tenure. What we would like to know instead is how one (or more) treatment impacts churn. This is often referred to as churn uplift.\nIn this article, we will be going beyond both churn prediction and churn uplift and consider instead the ultimate goal of churn-prevention campaigns: increasing revenue. First of all, a policy that reduces churn might also have an impact on revenue, which should be taken into account. However, and more importantly, increasing revenue is relevant only if the customer does not churn. Vice-versa, decreasing churn is more relevant for high-revenue customers. This interaction between churn and revenue is critical in understanding the profitability of any treatment campaign and should not be overlooked.\nGifts and Subscriptions For the rest of the article, we are going to use a toy example to illustrate the main idea. Suppose we were a company interested in reducing our customer\u0026rsquo;s churn and ultimately increasing our revenue. Suppose we have decided to test a new idea: sending a gift of 1\\$ to our users. In order to test whether the treatment works, we have randomly sent it only to a subsample of our customer base.\ncost = 1 Let’s have a look at the data we have at our disposal. I import the data-generating process dgp_gift() from src.dgp. I also import some plotting functions and libraries from src.utils.\n%matplotlib inline %config InlineBackend.figure_format = 'retina' from src.utils import * from src.dgp import dgp_gift dgp = dgp_gift(n=100_000) df = dgp.generate_data() df.head() months rev_old rev_change gift churn revenue 0 3.98 3.36 0.86 0 0 7.28 1 6.28 14.41 -2.77 1 0 11.60 2 4.62 2.89 -2.21 0 0 3.59 3 3.94 0.00 -3.26 0 1 0.00 4 2.76 3.25 -3.43 0 0 5.33 We have information on 100_000 customers for which we observe the number of months they have been active customers, the revenue they generated in the last month (rev_old), the change in revenue between the last month and the previous one (rev_change), whether they were randomly sent a gift and the two outcomes of interest: churn, i.e. whether they are not active customers anymore and the revenue they have generated in the current month. We denote the outcomes with the letter Y, the treatment with the letter W and the other variables with the letter X.\nY = ['churn', 'revenue'] W = 'gift' X = ['months', 'rev_old', 'rev_change'] Note that, for simplicity, we are considering a single-period snapshot of the data and summarizing the panel structure of the data in just a couple of variables. Usually we would have a longer time series but also a longer time horizon for what concerns the outcome (e.g. customer lifetime value).\nWe can represent the underlying data generating process with the following Directed Acyclic Graph (DAG). Nodes represent variables and arrows represent potential causal relationships. I have highlighted in green the two relationships of interest: the effect of the gift on churn and revenue. Note that churn is related to revenue since churned customers by definition generate zero revenue.\nflowchart TD classDef included fill:#DCDCDC,stroke:#000000,stroke-width:3px; classDef empty width:-10px,height:-10px,fill:#000000,stroke-width:0px; W((1$ gift)) D1(( )) D2(( )) Y1((churn)) Y2((revenue)) X1((months)) X2((revenue change)) X3((revenue old)) W --- D1 X1 --- D1 D1 --\u0026gt; Y1 W --- D2 X1 --- D2 D2 --\u0026gt; Y2 Y1 --\u0026gt; Y2 X2 --\u0026gt; Y1 X3 --\u0026gt; Y1 X3 --\u0026gt; Y2 class W,Y1,Y2,X1,X2,X3 included; class D1,D2 empty; linkStyle 0,2,3,5 stroke:#2db88b,stroke-width:6px; linkStyle 1,4,6,7,8,9 stroke:#003f5c,stroke-width:6px; Importantly, past revenue and the revenue change are predictors of churn and revenue but are not related to our intervention. On the contrary, the intervention affects churn and revenue differentially depending on the customers total active months.\nWhile simplistic, this data generating process aims at captiring an important insight: variables that are good predictors of churn or revenue, are not necessarily variables that predict churn or revenue lift. We will see later how this impacts our analysis.\nLet\u0026rsquo;s start first by exploring the data.\nExploratory Data Analysis Let\u0026rsquo;s start with churn. How many customers did the company lose last month?\ndf.churn.mean() 0.19767 The company lost almost 20% of customers last month! Did the gift help in preventing churn?\nWe want to compare the churn frequency of customers that received the gift with the churn frequency of customers that did not receive the gift. Since the gift was randomized, the difference-in-means estimator is an unbiased estimator for the average treatment effect (ATE) of the gift on churn.\n$$ ATE = \\mathbb{E} \\Big[ Y \\ \\Big| \\ W = 1 \\Big] - \\mathbb{E} \\Big[ Y \\ \\Big| \\ W = 0 \\Big] $$\nWe compute the difference-in-means estimate by linear regression. We also include other covariates to improve the efficiency of the estimator.\nsmf.ols(\u0026quot;churn ~ \u0026quot; + W + \u0026quot; + \u0026quot; + \u0026quot; + \u0026quot;.join(X), data=df).fit().summary().tables[1] coef std err t P\u003e|t| [0.025 0.975] Intercept 0.3271 0.002 151.440 0.000 0.323 0.331 gift -0.1173 0.002 -51.521 0.000 -0.122 -0.113 months 0.0050 0.000 21.832 0.000 0.005 0.005 rev_old -0.0181 0.000 -108.061 0.000 -0.018 -0.018 rev_change -0.0497 0.001 -87.412 0.000 -0.051 -0.049 It looks like the gift decreased churn by around 11 percentage points, i.e. almost one-third of the baseline level of 32%! Did it also have an impact on revenue?\nAs for churn, we regress revenue on gift, our treatment variable, to estimate the average treatment effect.\nsmf.ols(\u0026quot;revenue ~ \u0026quot; + W + \u0026quot; + \u0026quot; + \u0026quot; + \u0026quot;.join(X), data=df).fit().summary().tables[1] coef std err t P\u003e|t| [0.025 0.975] Intercept 0.3691 0.010 37.910 0.000 0.350 0.388 gift 0.6317 0.010 61.560 0.000 0.612 0.652 months 0.0120 0.001 11.768 0.000 0.010 0.014 rev_old 0.8251 0.001 1092.846 0.000 0.824 0.827 rev_change 0.1457 0.003 56.777 0.000 0.141 0.151 It looks like the gift on average increased revenue by 0.63\\$, which means that it was not profitable. Does it mean that we should stop sending gifts to our customers? It depends. In fact, the gift might be effective for certain customer segments. We just need to identify them.\nTargeting Policies In this section, we will try to understand if there is a data-informed way to send the gift to customers that is profitable. In particular, we will ccompare different data-informed targeting policies with the objective of increasing revenue.\nThroughout this section, we will need some algorithms to either predict revenue, or churn, or the probability of receiving the gift. We use gradient-boosted tree models from the lightgbm library We use the same models for all policies, so that we cannot attribute differences in performance to prediction accuracy.\nfrom lightgbm import LGBMClassifier, LGBMRegressor To evaluate each policy denoted with τ, we compare its profits with the policy Π⁽¹⁾, with its profits without the policy Π⁽⁰⁾, over every single individual, in a separate validation dataset. Note that this is usually not possible, since, for each customer, we only observe one of the two potential outcomes, with or without the gift. However, this is synthetic data, so we can do oracle evaluation. If you want to know more about how to evaluate uplift models with real data, I wrote an article about it.\nhttps://towardsdatascience.com/8a078996a113\nFirst of all, let\u0026rsquo;s define profits Π as the net revenue R when the customer does not churn C. $$ \\Pi = R (1-C) $$\nTherefore, the overall effect on profits for treated individuals is given by the difference between the profits when treated Π⁽¹⁾ minus the profits when not treated Π⁽⁰⁾.\n$$ \\tau_{\\pi} = R_1 (1 - C_1) - R_0 (1 - C_0) $$\nThe effect for untreated individuals is zero.\ndef evaluate_policy(policy): data = dgp.generate_data(seed_data=4, seed_assignment=5, keep_po=True) data['profits'] = (1 - data.churn) * data.revenue baseline = (1-data.churn_c) * data.revenue_c effect = policy(data) * (1-data.churn_t) * (data.revenue_t-cost) + (1-policy(data)) * (1-data.churn_c) * data.revenue_c return np.sum(effect - baseline) 1. Target Churning Customers A first policy could be to just target churning customers. Let\u0026rsquo;s say, we send the gift only to customers with above-average redicted churn.\nmodel_churn = LGBMClassifier().fit(X=df[X], y=df['churn']) policy_churn = lambda df : (model_churn.predict_proba(df[X])[:,1] \u0026gt; df.churn.mean()) evaluate_policy(policy_churn) -5497.46 The policy is not profitable and would lead to an aggregate loss of more than 5000\\$.\nYou might think that the problem is the arbitrary threshold, but this is not the case. Below I plot the aggregate effect for all possible policy thresholds.\nx = np.linspace(0, 1, 100) y = [evaluate_policy(lambda df : (model_churn.predict_proba(df[X])[:,1] \u0026gt; k)) for k in x] fig, ax = plt.subplots(figsize=(10, 3)) sns.lineplot(x=x, y=y).set(xlabel='Churn Policy Threshold', title='Aggregate Effect'); ax.axhline(y=0, c='k', lw=3, ls='--'); As we can see, at best we can make zero losses when we decide not to give the gift to any customer.\nThe problem is that the fact that a customer is likely to churn does not imply that the gift will have any impact on their churn probability. The two measures are not completely unrelated (e.g. we cannot decrease the churning probability of customers that have 0% probability of churning), but they are not the same thing.\n2. Target revenue customers Let\u0026rsquo;s now try a different policy: we send the gift only to high-revenue customers. For example, we might send the gift only to the top-10% of customers by revenue. The idea is that, if the policy indeed decreases churn, these are the customers for whom decreasing churn is more profitable.\nmodel_revenue = LGBMRegressor().fit(X=df[X], y=df['revenue']) policy_revenue = lambda df : (model_revenue.predict(df[X]) \u0026gt; np.quantile(df.revenue, 0.9)) evaluate_policy(policy_revenue) -4730.8200000000015 The policy is again unprofitable, leading to substantial losses. As before, this is not a problem of selecting the threshold as we can see in the plot below. The best we can do is set a threshold so high that we do not treat anyone and we make zero profits.\nx = np.linspace(0, 100, 100) y = [evaluate_policy(lambda df : (model_revenue.predict(df[X]) \u0026gt; k*cost)) for k in x] fig, ax = plt.subplots(figsize=(10, 3)) sns.lineplot(x=x, y=y).set(xlabel='Revenue Policy Threshold', title='Aggregate Effect'); ax.axhline(y=0, c='k', lw=3, ls='--'); The problem is that, in our setting, the churn probability of high-revenue customers does not decrease enough to make the gift profitable. This is also partially due to the fact, often observed in reality, that high-revenue customers are also the least likely to churn, to begin with.\nLet\u0026rsquo;s now consider a more relevant set of policies: policies based on uplift.\n3. Target churn uplift customers A more sensible approach would be to target customers whose churn probability decreases the most when receiving the 1\\$ gift. We estimate churn uplift using the double-robust estimator, one of the best performing uplift models. If you are unfamiliar with meta-learners, I recommend starting from my introductory article.\nhttps://towardsdatascience.com/8a9c1e340832\nWe import the doubly-robust learner from econml, a Microsoft library.\nfrom econml.dr import DRLearner DR_learner_churn = DRLearner(model_regression=LGBMRegressor(), model_propensity=LGBMClassifier(), model_final=LGBMRegressor()) DR_learner_churn.fit(df['churn'], df[W], X=df[X]); Now that we have estimated churn uplift, we might be tempted to just target customers with a high negative uplift (negative, since we want to decrease churn). For example, we might send the gift to all customers with an estimated uplift larger than the average churn.\npolicy_churn_lift = lambda df : DR_learner_churn.effect(df[X]) \u0026lt; - np.mean(df.churn) evaluate_policy(policy_churn_lift) -3925.2400000000002 The policy is still unprofitable, leading to almost 4000\\$ in losses.\nThe problem is that we haven\u0026rsquo;t considered the cost of the policy. In fact, decreasing the churn probability is only profitable for high-revenue customers. Take the extreme case: avoiding churn of a customer that does not generate any revenue is not worth any intervention.\nTherefore, let\u0026rsquo;s only send the gift to customers whose churn probability weighted by revenue decreases more than the cost of the gift.\nmodel_revenue_1 = LGBMRegressor().fit(X=df.loc[df[W] == 1, X], y=df.loc[df[W] == 1, 'revenue']) policy_churn_lift = lambda df : - DR_learner_churn.effect(df[X]) * model_revenue_1.predict(df[X]) \u0026gt; cost evaluate_policy(policy_churn_lift) 318.03000000000003 This policy is finally profitable!\nHowever, we still have not considered one channel: the intervention might also affect the revenue of existing customers.\n4. Target revenue uplift customers A symmetric approach to the previous one would be to consider only the impact on revenue, ignoring the impact on churn. We could estimate the revenue uplift for non-churning customers and treat only customers whose incremental effect on revenue, net of churn, is greater than the cost of the gift.\nDR_learner_netrevenue = DRLearner(model_regression=LGBMRegressor(), model_propensity=LGBMClassifier(), model_final=LGBMRegressor()) DR_learner_netrevenue.fit(df.loc[df.churn==0, 'revenue'], df.loc[df.churn==0, W], X=df.loc[df.churn==0, X]); model_churn_1 = LGBMClassifier().fit(X=df.loc[df[W] == 1, X], y=df.loc[df[W] == 1, 'churn']) policy_netrevenue_lift = lambda df : DR_learner_netrevenue.effect(df[X]) * (1-model_churn_1.predict(df[X])) \u0026gt; cost evaluate_policy(policy_netrevenue_lift) 50.800000000000004 This policy is profitable as well, but ignores the effect on churn. How do we combine this poilcy with the previous one?\n5. Target revenue uplift customers The best way to efficiently combine both the effect on churn and the effect on net revenue, is simply to estimate total revenue uplift. The implied optimal policy is to treat customers whose total revenue uplift is greater than the cost of the gift.\nDR_learner_revenue = DRLearner(model_regression=LGBMRegressor(), model_propensity=LGBMClassifier(), model_final=LGBMRegressor()) DR_learner_revenue.fit(df['revenue'], df[W], X=df[X]); policy_revenue_lift = lambda df : (DR_learner_revenue.effect(df[X]) \u0026gt; cost) evaluate_policy(policy_revenue_lift) 2028.2100000000003 It looks like this is by far the best policy, generating an aggregate profit of more than 2000\\$!\nThe result is starking if we compare all the different policies.\npolicies = [policy_churn, policy_revenue, policy_churn_lift, policy_netrevenue_lift, policy_revenue_lift] df_results = pd.DataFrame() df_results['policy'] = ['churn', 'revenue', 'churn_L', 'netrevenue_L', 'revenue_L'] df_results['value'] = [evaluate_policy(policy) for policy in policies] fig, ax = plt.subplots() sns.barplot(df_results, x='policy', y='value') plt.axhline(0, c='k'); Intuition and Decomposition If we compare the different policies, it is clear that targeting high-revenue or high-churn probability customers directly were the worst choices. This is not necessarily always the case, but it happened in our simulated data because of two facts that are also common in many real scenarios:\nRevenue and churn probability are negatively correlated The effect of the gift on churn (or revenue) was not strongly negatively (or positively for revenue) correlated with the baseline values Either one of these two facts can be enough to make targeting revenue or churn a bad strategy. What one should target instead is customers with a high incremental effect. And it\u0026rsquo;s best to directly use as outcome the variable of interest, revenue in this case, whenever available.\nTo better understand the mechanism, we can decompose the aggregate effect of a policy on profits into three parts.\n$$ \\begin{aligned} \\tau_{\\pi} \u0026amp;= R_1 * (1 - C_1) - R_0 * (1 - C_0) = \\newline \u0026amp;= R_1 * (1 - C_1) - \\underbrace{\\hat{r}_1 * (1 - \\hat{c}_0) + R_1 * (1 - C_0)} _ {\\text{add and subtract}} - R_0 * (1 - C_0) = \\newline \u0026amp;= - R_1 * \\tau_c + R_1 * (1 - C_0) - R_0 * (1 - C_0) = \\newline \u0026amp;= - R_1 * \\tau_c + \\tau_r * (1 - C_0) = \\newline \u0026amp;= \\underbrace{- R_0 * \\tau_c} _ {\\text{incremental effect on churn}} + \\underbrace{\\tau_r * (1 - C_0)} _ {\\text{incremental effect on revenue}} + \\underbrace{\\tau_r * \\tau_c} _ {\\text{interaction effect}} \\end{aligned} $$\nThis implies that there are three channels that make treating a customer profitable.\nIf it\u0026rsquo;s a high-revenue customer and the treatment decreases its churn probability If it\u0026rsquo;s a non-churning customer and the treatment increases its revenue It the treatment has a strong impact on both its revenue and churn probability Targeting by churn uplift exploits only the first channel, targeting by net revenue uplift exploits only the second channel, and targeting by total revenue uplift exploits all three channels, making it the most effective method.\nBonus: weighting As highlighted by Lemmens, Gupta (2020), sometimes it might be worth weighting observations when estimating model uplift. In particular, it might be worth giving more weight to observations close to the treatment policy threshold.\nThe idea is that weighting generally decreases the efficiency of the estimator. However, we are not interested in having correct estimates for all the observations, but rather we are interested in estimating the policy threshold correctly. In fact, whether you estimate a net profit of 1\\$ or 1000\\$ it does not matter: the implied policy is the same: send the gift. However, estimating a net profit of 1\\$ rather than -1\\$ reverses the policy implications. Therefore, a large loss in accuracy away from the threshold sometimes is worth a small gain in accuracy at the threshold.\nLet\u0026rsquo;s try using negative exponential weights, decreasing in distance from the threshold.\nDR_learner_revenue_w = DRLearner(model_regression=LGBMRegressor(), model_propensity=LGBMClassifier(), model_final=LGBMRegressor()) w = np.exp(1 + np.abs(DR_learner_revenue.effect(df[X]) - cost)) DR_learner_revenue_w.fit(df['revenue'], df[W], X=df[X], sample_weight=w); policy_revenue_lift_w = lambda df : (DR_learner_revenue_w.effect(df[X]) \u0026gt; cost) evaluate_policy(policy_revenue_lift_w) 1398.19 In our case, weighting is not worth: the implied policy is still profitable, but less than the one obtained with the unweighted model, 2028\\$.\nConclusion In this article, we have why and how one should go beyond churn prediction and churn uplift modeling. In particular, one should concentrate on the final business objective of increasing profitability. This implies shifting the focus from prediction to uplift but also combining churn and revenue into a single outcome.\nAn important caveat concerns the dimension of the data available. We have used a toy dataset that highly simplifies the problem in at least two dimensions. First of all, backwards, we normally have longer time series that can (and should) be used for both prediction and modeling purposes. Second, forward, one should combine churn with a longer-run estimate of customer profitability, usually referred to as customer lifetime value.\nReferences Kennedy (2022), \u0026ldquo;Towards Optimal Doubly Robust Estimation of Heterogeneous Causal Effects\u0026rdquo;\nBonvini, Kennedy, Keele (2021), \u0026ldquo;Minimax Optimal Subgroup Identification\u0026rdquo;\nLemmens, Gupta (2020), \u0026ldquo;Managing Churn to Maximize Profits\u0026rdquo;\nRelated Articles Evaluating Uplift Models\nUnderstanding Meta Learners\nUnderstanding AIPW, the Doubly-Robust Estimator\nCode You can find the original Jupyter Notebook here:\nhttps://github.com/matteocourthoud/Blog-Posts/blob/main/notebooks/beyond_churn.ipynb\n","date":1690243200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1690243200,"objectID":"ee5afb79fae2bde88d75cb4b2fa8e019","permalink":"https://matteocourthoud.github.io/post/beyond_churn/","publishdate":"2023-07-25T00:00:00Z","relpermalink":"/post/beyond_churn/","section":"post","summary":"A very common task in data science is churn prediction. However, predicting churn is often just an intermediate step and rarely the final objective. Usually, what we actually care about is reducing churn, which is a separate objective, not necessarily related.","tags":null,"title":"Beyond Churn Prediction and Churn Uplift","type":"post"},{"authors":null,"categories":null,"content":"One of the most widespread applications of causal inference in the industry is uplift modeling, a.k.a. the estimation of Conditional Average Treatment Effects.\nWhen estimating the causal effect of a treatment (a drug, ad, product, \u0026hellip;) on an outcome of interest (a disease, firm revenue, customer satisfaction, \u0026hellip;), we are often not only interested in understanding whether the treatment works on average, but we would like to know for which subjects (patients, users, customers, \u0026hellip;) it works better or worse.\nEstimating heterogeneous incremental effects, or uplift, is an essential intermediate step to improve targeting of the policy of interest. For example, we might want to warn certain people that they are more likely to experience side effects from a drug or show an advertisement only to a specific set of customers.\nWhile there exist many methods to model uplift, it is not always clear which one to use in a specific application. Crucially, because of the fundamental problem of causal inference, the objective of interest, the uplift, is never observed, and therefore we cannot validate our estimators as we would do with a machine learning prediction algorithm. We cannot set aside a validation set and pick the best-performing model since we have no ground truth, not even in the validation set, and not even if we ran a randomized experiment.\nWhat can we do then? In this article, I try to cover the most popular methods used to evaluate uplift models. If you are unfamiliar with uplift models, I suggest first reading my introductory article.\nhttps://towardsdatascience.com/8a9c1e340832\nUplift and Promotional Emails Imagine we were working in the marketing department of a product company interested in improving our email marketing campaign. Historically, we mostly sent emails to new customers. However, now we would like to adopt a data-driven approach and target customers for whom the email has the highest positive impact on revenue. This impact is also called uplift or incrementality.\nLet\u0026rsquo;s have a look at the data we have at our disposal. I import the data-generating process dgp_promotional_email() from src.dgp. I also import some plotting functions and libraries from src.utils.\n%matplotlib inline %config InlineBackend.figure_format = 'retina' from src.utils import * from src.dgp import dgp_promotional_email dgp = dgp_promotional_email(n=500) df = dgp.generate_data() df.head() new age sales_old mail sales 0 0 32.42 0.11 1 0.03 1 1 34.92 0.36 0 0.47 2 1 41.00 0.49 0 0.40 3 0 50.02 0.35 1 0.53 4 0 33.34 0.12 1 0.02 We have information on 500 customers, for whom we observe whether they are new customers, their age, the sales they generated before the email campaign (sales_old), whether they were sent the mail, and the sales after the email campaign.\nThe outcome of interest is sales, which we denote with the letter Y. The treatment or policy that we would like to improve is the mail campaign, which we denote with the letter W. We call all the remaining variables confounders or control variables and we denote them with X.\nY = 'sales' W = 'mail' X = ['age', 'sales_old', 'new'] The Dyrected Acyclic Graph (DAG) representing the causal relationships between the variables is the following. The causal relationship of interest is depicted in green.\nflowchart TD classDef included fill:#DCDCDC,stroke:#000000,stroke-width:3px; W((mail)) Y((sales)) X1((new)) X2((age)) X3((sales old)) W --\u0026gt; Y X1 --\u0026gt; W X1 --\u0026gt; Y X2 --\u0026gt; Y X3 --\u0026gt; Y class W,Y,X1,X2,X3 included; linkStyle 0 stroke:#2db88b,stroke-width:6px; linkStyle 1,2,3,4 stroke:#003f5c,stroke-width:6px; From the DAG we see that the new customer indicator is a confounder and needs to be controlled for in order to identify the effect of mail on sales. age and sales_old instead are not essential for estimation but could be helpful for identification. For more information on DAGs and control variables, you can check my introductory article.\nhttps://towardsdatascience.com/b63dc69e3d8c\nThe objective of uplift modeling is to recover the Individual Treatment Effects (ITE) $\\tau_i$, i.e. the incremental effect on sales of sending the promotional mail. We can express the ITE as the difference between two hypothetical quantities: the potential outcome of the customer if they had received the email, $Y_i^{(1)}$, minus the potential outcome of the customer if they had not received the email, $Y_i^{(0)}$. $$ \\tau_i = Y_i^{(1)} - Y_i^{(0)} $$\nNote that for each customer, we only observe one of the two realized outcomes, depending on whether they actually received the mail or not. Therefore, the ITE are inherently unobservable. What can be estimated instead is the Conditional Average Treatment Effect (CATE) i.e., the expected individual treatment effect $\\tau_i$, conditional on covariates X. For example, the average effect of the mail on sales for older customers (age \u0026gt; 50). $$ \\tau(x) = \\mathbb{E} \\Big[ \\ \\tau_i \\ \\Big| \\ X_i = x \\Big] $$\nIn order to be able to recover the CATE, we need to make three assumptions.\nUnconfoundedness: $Y^{(0)}, Y^{(1)} \\perp W \\ | \\ X$\nOverlap: $0 \u0026lt; e(X) \u0026lt; 1$\nConsistency: $Y = W \\cdot Y^{(1)} + (1-W) \\cdot Y^{(0)}$\nWhere $e(X)$ is the propensity score i.e., the expected probability of being treated, conditional on covariates X. $$ e(x) = \\mathbb{E} \\Big[ \\ W_i \\ \\Big| \\ X_i = x \\Big] $$\nIn what follows, we will use machine learning methods to estimate the CATE $\\tau(x)$, the propensity scores $e(x)$, and the conditional expectation function (CEF) of the outcome, $\\mu(x)$ $$ \\mu(x) = \\mathbb{E} \\Big[ \\ Y_i \\ \\Big| \\ X_i = x \\Big] $$\nWe use Random Forest Regression algorithms to model the CATE and the outcome CEF, while we use Logistic Regression to model the propensity score.\nfrom sklearn.ensemble import RandomForestRegressor from sklearn.linear_model import LogisticRegressionCV model_tau = RandomForestRegressor(max_depth=2) model_y = RandomForestRegressor(max_depth=2) model_e = LogisticRegressionCV() In this article, we do not fine-tune the underlying machine learning models, but fine-tuning is strongly recommended to improve the accuracy of uplift models (for example, with auto-ml libraries like FLAML).\nUplift Models There exist many methods to model uplift or, in other words, to estimate Conditional Average Treatment Effects (CATE). Since the objective of this article is to compare methods to evaluate uplift models, we will not explain the methods in detail. For a gentle introduction, you can check my introductory article on meta learners.\nThe learners that we will consider are the following:\nS-learner or single-learner, introduced by Kunzel, Sekhon, Bickel, Yu (2017)\nT-learner or two-learner, introduced by Kunzel, Sekhon, Bickel, Yu (2017)\nX-learner or cross-learner, introduced by Kunzel, Sekhon, Bickel, Yu (2017)\nR-learner or Robinson-learner introduced by Nie, Wager (2017)\nDR-learner or doubly-robust-learner, introduced by Kennedy (2022)\nWe import all the model from Microsoft\u0026rsquo;s econml library.\nfrom src.learners_utils import * from econml.metalearners import SLearner, TLearner, XLearner from econml.dml import NonParamDML from econml.dr import DRLearner S_learner = SLearner(overall_model=model_y) T_learner = TLearner(models=clone(model_y)) X_learner = XLearner(models=model_y, propensity_model=model_e, cate_models=model_tau) R_learner = NonParamDML(model_y=model_y, model_t=model_e, model_final=model_tau, discrete_treatment=True) DR_learner = DRLearner(model_regression=model_y, model_propensity=model_e, model_final=model_tau) We fit() the models on the data, specifying the outcome variable Y, the treatment variable W and covariates X.\nnames = ['SL', 'TL', 'XL', 'RL', 'DRL'] learners = [S_learner, T_learner, X_learner, R_learner, DR_learner] for learner in learners: learner.fit(df[Y], df[W], X=df[X]) We are now ready to evaluate the models! Which model should we choose?\nOracle Loss Functions The main problem of evaluating uplift models is that, even with a validation set and even with a randomized experiment or AB test, we do not observe our metric of interest: the Individual Treatment Effects. In fact, we only observe the realized outcomes, $Y_i^{(0)}$ for untreated customers and $Y_i^{(1)}$ for treated customers. Therefore, for no customer we can compute the individual treatment effect in the validation data, $\\tau_i = Y_i^{(1)} - Y_i^{(0)}$.\nCan we still do something to evaluate our estimators?\nThe answer is yes, but before giving more details, let\u0026rsquo;s first understand what we would do if we could observe the Individual Treatment Effects $\\tau_i$.\nOracle MSE Loss If we could observe the individual treatment effects (but we don\u0026rsquo;t, hence the \u0026ldquo;oracle\u0026rdquo; attribute), we could try to measure how far our estimates $\\hat{\\tau}(X_i)$ are from the true values $\\tau_i$. This is what we normally do in machine learning when we want to evaluate a prediction method: we set aside a validation dataset and we compare predicted and true values on that data. There exist plenty of loss functions to evaluate prediction accurary, so let\u0026rsquo;s concentrate on the most popular one: the Mean Squared Error (MSE) loss.\n$$ \\mathcal{L} _ {oracle-MSE}(\\hat{\\tau}) = \\frac{1}{n} \\sum _ {i=1}^{n} \\left( \\hat{\\tau}(X_i) - \\tau(X_i) \\right)^2 $$\ndef loss_oracle_mse(data, learner): tau = learner.effect(data[X]) return np.mean((tau - data['effect_on_sales'])**2) The function compare_methods prints and plots evaluation metrics computed on a separate validation dataset.\ndef compare_methods(learners, names, loss, title=None, subtitle='lower is better'): data = dgp.generate_data(seed_data=1, seed_assignment=1, keep_po=True) results = pd.DataFrame({ 'learner': names, 'loss': [loss(data.copy(), learner) for learner in learners] }) fig, ax = plt.subplots(1, 1, figsize=(6, 4)) sns.barplot(data=results, x=\u0026quot;learner\u0026quot;, y='loss').set(ylabel='') plt.suptitle(title, y=1.02) plt.title(subtitle, fontsize=12, fontweight=None, y=0.94) return results compare_methods(learners, names, loss_oracle_mse, title='Oracle MSE Loss') learner loss 0 SL 0.002932 1 TL 0.004637 2 XL 0.000936 3 RL 0.000990 4 DRL 0.000577 In this case, we see that the T-learner clearly performs worst, with the S-learner just behind. On the other hand, the X-, R- and DR-learners perform significantly better, with the DR-learner winning the race.\nHowever, this might not be the best loss function to evaluate our uplift model. In fact, uplift modeling is just an intermediate step towards our ultimate goal: improving revenue.\nOracle Policy Gain Since our ultimate goal is to improve revenue, we could evaluate estimators by how much they increase revenue, given a certain policy function. Suppose, for example, that we had a $0.01$\\$ cost of sending an email. Then, our policy would be to treat each costumer that has a predicted Conditional Average Treatment Effect above $0.01$\\$.\ncost = 0.01 How much would our revenue actually increase? Let\u0026rsquo;s define with $d(\\hat{\\tau})$ our policy function, such that $d=1$ if $\\tau \u0026gt;= 0.1$ and $d=0$ otherwise. Then our gain (higher is better) function is: $$ \\mathcal{G} _ {oracle-POLICY}(\\hat{\\tau}) = \\frac{1}{n} \\sum _ {i=1}^{n} d(\\hat{\\tau}) (\\tau_i - c) $$\nAgain, this is an \u0026ldquo;oracle\u0026rdquo; loss function that cannot be computed in reality since we do not observe the individual treatment effects.\ndef gain_oracle_policy(data, learner): tau_hat = learner.effect(data[X]) return np.sum((data['effect_on_sales'] - cost) * (tau_hat \u0026gt; cost)) compare_methods(learners, names, gain_oracle_policy, title='Oracle Policy Gain', subtitle='higher is better') learner loss 0 SL 0.00 1 TL 4.23 2 XL 10.98 3 RL 10.43 4 DRL 12.33 In this case, the S-learner is clearly the worst performer, leading to no effect on revenues. The T-learner leads to modest gains while the X-, R- and DR- learners all lead to aggregate gains, with the X-learner slightly ahead.\nPractical Loss Functions In the previous section, we have seen two examples of loss functions that we would like to compute if we could observe the Individual Treatment Effects $\\tau_i$. However, in practice, even with a randomized experiment and even with a validation set, we do not observe the ITE,our object of interest. We will now cover some measures that try to evaluate uplift models, given this practical constraint.\nOutcome Loss The first and simplest approach is to switch to a different loss variable. While we cannot observe the Individual Treatment Effects, $\\tau_i$, we can still observe our outcome $y_i$. This is not exactly our object of interest, but we might expect an uplift model that performs well in terms of predicting $y$ to also produce good estimates of $\\tau$.\nOne such loss function could be the Outcome MSE loss, which is the usual MSE loss function for prediction methods. $$ \\mathcal{L}_{Y}(\\hat{\\mu}) = \\frac{1}{n} \\sum _ {i=1}^{n} \\Big( \\hat{\\mu}(X_i, W_i) - Y_i \\Big)^2 $$\nThe problem here is that not all models directly produce an estimate of $\\mu(x)$ and, even when they do, it is not the object of interest.\nPrediction to Prediction Loss Another very simple approach could be to compare the predictions of the model trained on the training set with the predictions of another model trained on the validation set. While intuitive, this appraoch could be extremely misleading.\ndef loss_pred(data, learner): tau = learner.effect(data[X]) learner2 = copy.deepcopy(learner).fit(data[Y], data[W], X=data[X]) tau2 = learner2.effect(data[X]) return np.mean((tau - tau2)**2) compare_methods(learners, names, loss_pred, 'Prediction to Prediction Loss') learner loss 0 SL 0.000000 1 TL 0.007342 2 XL 0.000366 3 RL 0.134137 4 DRL 0.000933 Unsurprisingly, this metric performs extremely bad, and you should never use it, since it rewards models that are consistent, irrespectively of their quality. A model that always predicts a random constant CATE for each observations would obtain a perfect score.\nDistribution Loss A different approach is to ask: how well can we match the distribution of potential outcomes? We can do this exarcise for either the treated or untreated potential outcomes. Let\u0026rsquo;s take the last case. Suppose we take the observed sales for customers that did not receive the mail and the observed sales minus the estimated CATE $\\hat{\\tau}(x)$ for customers that did receive the mail. By the unconfoundedness assumption, these two distributions of the untreated potential outcome should be similar, conditional on covariates $X$.\nTherefore, we expect the distance between the two distributions to be close if we correctly estimated the treatment effects. $$ dist \\ \\Big( \\ {Y_i, X_i | W_i=0 } \\ , \\ {Y_i - \\hat{\\tau}(X_i), X_i | W_i=1 } \\ \\Big) $$\nWe can also do the same exercise for the treated potential outcome.\n$$ dist \\ \\Big( \\ {Y_i + \\hat{\\tau}(X_i), X_i | W_i=0 } \\ , \\ {Y_i, X_i | W_i=1 } \\ \\Big) $$\nfrom dcor import energy_distance def loss_dist(data, learner): tau = learner.effect(data[X]) data.loc[data.mail==1, 'sales'] -= tau[data.mail==1] return energy_distance(data.loc[data.mail==0, [Y] + X], data.loc[data.mail==1, [Y] + X], exponent=2) compare_methods(learners, names, loss_dist, 'Distribution Loss') learner loss 0 SL 1.728523 1 TL 1.733941 2 XL 1.733993 3 RL 1.736704 4 DRL 1.735105 This measure is extremely noisy and rewards the S-learner followed by the T-learner which are actually the two worst performing models.\nAbove-below Median Difference The above-below median loss tries to answer the question: is our uplift model detecting any heterogeneity? In particular, if we take the validation set and we split the sample into above-median and below median predicted uplift $\\hat{\\tau}(x)$, how big is the actual difference in average effect, estimated with a difference-in-means estimator? We would expect better estimators to better split the sample into high-effects and low-effects.\nfrom statsmodels.formula.api import ols def loss_ab(data, learner): tau = learner.effect(data[X]) + np.random.normal(0, 1e-8, len(data)) data['above_median'] = tau \u0026gt;= np.median(tau) param = ols('sales ~ mail * above_median', data=data).fit().params[-1] return param compare_methods(learners, names, loss_ab, title='Above-below Median Difference', subtitle='higher is better') learner loss 0 SL -0.008835 1 TL 0.221423 2 XL 0.093177 3 RL 0.134629 4 DRL 0.075319 Unfortunately, the above-below median difference rewards the T-learner, which is among the worst performing models.\nIt\u0026rsquo;s important to note that the difference-in-means estimators in the two groups (above- and below- median $\\hat{\\tau}(x)$) are not guaranteed to be unbiased, even if the data came from a randomized experiment. In fact, we have split the two groups on a variable, $\\hat{\\tau}(x)$, that is highly endogenous. Therefore, the method should be used with a grain of salt.\nUplift Curve An extension of the above-below median test is the uplift curve. The idea is simple: instead of splitting the sample into two groups based on the median (0.5 quantile), why not split the data into more groups (more quantiles)?\nFor each group, we compute the difference-in-means estimate, and we plot its cumulative sum against the corresponding quantile. The result is called uplift curve. The interpretation is simple: the higher the curve, the better we are able to separate high- from low-effect observations. However, also the same disclaimer applies: the difference-in-means estimates are not unbiased. Therefore, they should be used with a grain of salt.\ndef generate_uplift_curve(df): Q = 20 df_q = pd.DataFrame() data = dgp.generate_data(seed_data=1, seed_assignment=1, keep_po=True) ate = np.mean(data[Y][data[W]==1]) - np.mean(data[Y][data[W]==0]) for learner, name in zip(learners, names): data['tau_hat'] = learner.effect(data[X]) data['q'] = pd.qcut(-data.tau_hat + np.random.normal(0, 1e-8, len(data)), q=Q, labels=False) for q in range(Q): temp = data[data.q \u0026lt;= q] uplift = (np.mean(temp[Y][temp[W]==1]) - np.mean(temp[Y][temp[W]==0])) * q / (Q-1) df_q = pd.concat([df_q, pd.DataFrame({'q': [q], 'uplift': [uplift], 'learner': [name]})], ignore_index=True) fig, ax = plt.subplots(1, 1, figsize=(8, 5)) sns.lineplot(x=range(Q), y=ate*range(Q)/(Q-1), color='k', ls='--', lw=3) sns.lineplot(x='q', y='uplift', hue='learner', data=df_q); plt.suptitle('Uplift Curve', y=1.02, fontsize=28, fontweight='bold') plt.title('higher is better', fontsize=14, fontweight=None, y=0.96) generate_uplift_curve(df) While probably not the best method to evaluate uplift models, the uplift curve is very important in understanding and implementing them. In fact, for each model, it tells us that is the expected average treatment effect (y-axis) as we increase the share of the treated population (x-axis).\nNearest Neighbor Match The last couple of methods we analyzed, aggregated data in order to understand whether the methods work on larger groups. The nearest neighbor match tries instead to understand how well an uplift model predicts individual treatment effects. However, since the ITEs are not observable, it tries to build a proxy by matching treated and control observations on observable characteristics $X$.\nFor example, if we take all treated observations ($i: W_i=1$), and we find the nearest neighbor in the control group ($NN_0(X_i)$), the corresponding MSE loss function is $$ \\mathcal{L} _ {NN}(\\hat{\\tau}) = \\frac{1}{n} \\sum _ {i: W_i=1} \\Big( \\hat{\\tau}(X_i) - (Y_i - NN_0(X_i)) \\Big)^2 $$\nfrom scipy.spatial import KDTree def loss_nn(data, learner): tau_hat = learner.effect(data[X]) nn0 = KDTree(data.loc[data[W]==0, X].values) control_index = nn0.query(data.loc[data[W]==1, X], k=1)[-1] tau_nn = data.loc[data[W]==1, Y].values - data.iloc[control_index, :][Y].values return np.mean((tau_hat[data[W]==1] - tau_nn)**2) compare_methods(learners, names, loss_nn, title='Nearest Neighbor Loss') learner loss 0 SL 0.050478 1 TL 0.051301 2 XL 0.047102 3 RL 0.046684 4 DRL 0.046652 In this case, the nearest neighbor loss performs quite well, identifying the two worse performing methods, the S- and T-learner.\nIPW Loss The Inverse Probability Weighting (IPW) loss function was first proposed by Gutierrez, Gerardy (2017), and it is the first of three metrics that we are going to see that uses a pseudo-outcome $Y^{*}$ to evaluate the estimator. Pseudo-outcomes are variables whose expected value is the Conditional Average Treatment Effect, but that are too volatile to be directly used as estimates. For a more detailed explanation of pseudo-outcomes, I suggest my article on causal regression trees. The pseudo-outcome corresponding to the IPW loss is $$ Y^* _ {IPW} = Y_i \\frac{W_i - \\hat{e}(X_i)}{\\hat{e}(X_i)(1 - \\hat{e}(X_i))} $$\nso that the corresponding loss function is $$ \\mathcal{L} _ {IPW} = \\frac{1}{n} \\sum_{i=1}^{n} \\left( \\hat{\\tau}(X_i) - Y_i \\ \\frac{W_i - \\hat{e}(X_i)}{\\hat{e}(X_i)(1 - \\hat{e}(X_i))} \\right)^2 $$\ndef loss_ipw(data, learner): tau_hat = learner.effect(data[X]) e_hat = clone(model_e).fit(data[X], data[W]).predict_proba(data[X])[:,1] tau_gg = data[Y] * (data[W] - e_hat) / (e_hat * (1 - e_hat)) return np.mean((tau_hat - tau_gg)**2) compare_methods(learners, names, loss_ipw, title='IPW Loss') learner loss 0 SL 1.170917 1 TL 1.153752 2 XL 1.172517 3 RL 1.172934 4 DRL 1.171769 The IPW loss is extremely noisy. A solution is to use its more robust variations, the R-loss or the DR-loss which we present next.\nR Loss The R-loss was introduced together with the R-learner by Nie, Wager (2017), and it is essentially the objective function of the R-learner. As for the IPW-loss, the idea is to try to match a pseudo outcome whose expected value is the Conditional Average Treatment Effect. $$ Y^* _ {R} = \\frac{Y_i - \\hat{\\mu}_W(X_i)}{W_i - \\hat{e}(X_i)} $$\nThe corresponding loss function is $$ \\mathcal{L}_{R} = \\frac{1}{n} \\sum _ {i=1}^{n} \\left( \\hat{\\tau}(X_i) - \\frac{Y_i - \\hat{\\mu}_W(X_i)}{W_i - \\hat{e}(X_i)} \\right)^2 $$\ndef loss_r(data, learner): tau_hat = learner.effect(data[X]) y_hat = clone(model_y).fit(df[X + [W]], df[Y]).predict(data[X + [W]]) e_hat = clone(model_e).fit(df[X], df[W]).predict_proba(data[X])[:,1] tau_nw = (data[Y] - y_hat) / (data[W] - e_hat) return np.mean((tau_hat - tau_nw)**2) results = compare_methods(learners, names, loss_r, title='R Loss') The R-loss is sensibly less noisy than the IPW loss and it clearly isolates the S-learner. However, it tends to favor its corresponding learner, the R-learner.\nDR Loss The DR-loss is the objective function of the DR-learner, and it was first introduced by Saito, Yasui (2020). As for the IPW- and the R-loss, the idea is to try to match a pseudo outcome, whose expected value is the Conditional Average Treatment Effect. The DR pseudo-outcome is strongly related to the AIPW estimator, also known as doubly-robust estimator, hence the DR name. $$ Y^* _ {DR} = \\hat{\\mu}_1(X_i) - \\hat{\\mu}_0(X_i) + (Y_i - \\hat{\\mu}_W(X_i)) \\ \\frac{W_i - \\hat{e}(X_i)}{\\hat{e}(X_i)(1 - \\hat{e}(X_i))} $$\nThe corresponding loss function is $$ \\mathcal{L} _ {DR} = \\frac{1}{n} \\sum _ {i=1}^{n} \\left( \\hat{\\tau}(X_i) - \\hat{\\mu}_1(X_i) + \\hat{\\mu}_0(X_i) - (Y_i - \\hat{\\mu}_W(X_i)) \\ \\frac{W_i - \\hat{e}(X_i)}{\\hat{e}(X_i)(1 - \\hat{e}(X_i))} \\right)^2 $$\ndef loss_dr(data, learner): tau_hat = learner.effect(data[X]) y_hat = clone(model_y).fit(df[X + [W]], df[Y]).predict(data[X + [W]]) mu1 = clone(model_y).fit(df[X + [W]], df[Y]).predict(data[X + [W]].assign(mail=1)) mu0 = clone(model_y).fit(df[X + [W]], df[Y]).predict(data[X + [W]].assign(mail=0)) e_hat = clone(model_e).fit(df[X], df[W]).predict_proba(data[X])[:,1] tau_nw = mu1 - mu0 + (data[Y] - y_hat) * (data[W] - e_hat) / (e_hat * (1 - e_hat)) return np.mean((tau_hat - tau_nw)**2) results = compare_methods(learners, names, loss_dr, title='DR Loss') As for the R-loss, the DR-loss tends to favor its corresponding learner, the DR-learner. However, it provides a more accurate ranking in terms of algorithms\u0026rsquo; accuracy.\nEmpirical Policy Gain The last loss function that we are going to analyze is different from all the others we have seen so far since it does not focus on how well we are able to estimate the treatment effects but rather on how well would the corresponding optimal treatment policy performs. In particular, Hitsch, Misra, Zhang (2023) propose the following gain function:\n$$ \\mathcal{G} _ {HMZ} = \\sum _ {i=1}^{n} \\left( W_i \\cdot d(\\hat{\\tau}) \\cdot \\frac{Y_i - c}{\\hat{e}(X_i)} + (1-W_i) \\cdot (1-d(\\hat{\\tau})) \\cdot \\frac{Y_i}{1-\\hat{e}(X_i)} \\right) $$\nwhere $c$ is the treatment cost and $d$ is the optimal treatment policy given the estimated CATE $\\hat{\\tau}(X_i)$. In our case, we assume an individual treatment cost of $c=0.01$\\$, so that the optimal policy is to treat every customer with an estimated CATE larger than 0.01.\nThe terms $W_i \\cdot d(X_i)$ and $(1-W_i) \\cdot (1-d(X_i))$ imply that we use for the calculation only individuals for whom the actual treatment W corresponds with the optimal one, d.\ndef gain_policy(data, learner): tau_hat = learner.effect(data[X]) e_hat = clone(model_e).fit(data[X], data[W]).predict_proba(data[X])[:,1] d = tau_hat \u0026gt; cost return np.sum((d * data[W] * (data[Y] - cost)/ e_hat + (1-d) * (1-data[W]) * data[Y] / (1-e_hat))) results = compare_methods(learners, names, gain_policy, title='Empirical Policy Gain', subtitle='higher is better') The empirical policy gain performs very well, isolating the two worst performing methods, the S- and T-learners.\nMeta Studies In this article we have introduced a wide variety of methods to evaluate uplift models, a.k.a. Conditional Average Treatment Effect estimators. We have also tested in our simulated dataset, which is a very special and limited example. How do these metrics perform in general?\nSchuler, Baiocchi, Tibshirani, Shah (2018) compares the S-loss, T-loss, R-loss, on simulated data, for the corresponding estimators. They find that the R-loss \u0026ldquo;is the validation set metric that, when optimized, most consistently leads to the selection of a high-performing model\u0026rdquo;. The authors also detect the so-called congeniality bias: metrics such as the R- or DR-loss tend to be biased towards the corresponding learner.\nCurth, van der Schaar (2023) studies a broader array of learners from a theoretical perspective. They find that \u0026ldquo;no existing selection criterion is globally best across all experimental conditions we consider\u0026rdquo;.\nMahajan, Mitliagkas, Neal, Syrgkanis (2023) is the most comprehensive study in terms of scope. The authors compare many metrics on 144 datasets and 415 estimators. They find that “no metric significantly dominates the rest” but “metrics that use DR elements seem to always be among the candidate winners”.\nConclusion In this article, we have explored multiple methods to evaluate uplift models. The main challenge is the unobservability of the variable of interest, the Individual Treatment Effects. Therefore, different methods try to evaluate uplift models either using other variables, using proxy outcomes, or approximating the effect of implied optimal policies.\nIt is hard to recommend using a single method since there is no consensus on which one performs best, neither from a theoretical nor from an empirical perspective. Loss functions that use R- and DR- elements tend to perform consistently better, but are also biased towards the corresponding learners. Understanding how these metrics work, however, can help in understanding their biases and limitations in order to make the most appropriate decisions depending on the specific scenario.\nReferences Curth, van der Schaar (2023), \u0026ldquo;In Search of Insights, Not Magic Bullets: Towards Demystification of the Model Selection Dilemma in Heterogeneous Treatment Effect Estimation\u0026rdquo;\nGutierrez, Gerardy (2017), \u0026ldquo;Causal Inference and Uplift Modeling: A review of the literature\u0026rdquo;\nHitsch, Misra, Zhang (2023), \u0026ldquo;Heterogeneous Treatment Effects and Optimal Targeting Policy Evaluation\u0026rdquo;\nKennedy (2022), \u0026ldquo;Towards optimal doubly robust estimation of heterogeneous causal effects\u0026rdquo;\nKunzel, Sekhon, Bickel, Yu (2017), \u0026ldquo;Meta-learners for Estimating Heterogeneous Treatment Effects using Machine Learning\u0026rdquo;\nMahajan, Mitliagkas, Neal, Syrgkanis (2023), \u0026ldquo;Empirical Analysis of Model Selection for Heterogeneous Causal Effect Estimation\u0026rdquo;\nNie, Wager (2017), \u0026ldquo;Quasi-Oracle Estimation of Heterogeneous Treatment Effects\u0026rdquo;\nSaito, Yasui (2020), \u0026ldquo;Counterfactual Cross-Validation: Stable Model Selection Procedure for Causal Inference Models\u0026rdquo;\nSchuler, Baiocchi, Tibshirani, Shah (2018), \u0026ldquo;A comparison of methods for model selection when estimating individual treatment effects\u0026rdquo;\nRelated Articles Understanding Meta Learners\nUnderstanding AIPW, the Doubly-Robust Estimator\nUnderstanding Causal Trees\nFrom Causal Trees to Forests\nCode You can find the original Jupyter Notebook here:\nhttps://github.com/matteocourthoud/Blog-Posts/blob/main/notebooks/evaluate_uplift.ipynb\n","date":1689120000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1689120000,"objectID":"247b3f29174958dabe803326bc45b715","permalink":"https://matteocourthoud.github.io/post/evaluate_uplift/","publishdate":"2023-07-12T00:00:00Z","relpermalink":"/post/evaluate_uplift/","section":"post","summary":"One of the most widespread applications of causal inference in the industry is uplift modeling, a.k.a. the estimation of Conditional Average Treatment Effects.\nWhen estimating the causal effect of a treatment (a drug, ad, product, \u0026hellip;) on an outcome of interest (a disease, firm revenue, customer satisfaction, \u0026hellip;), we are often not only interested in understanding whether the treatment works on average, but we would like to know for which subjects (patients, users, customers, \u0026hellip;) it works better or worse.","tags":null,"title":"Evaluating Uplift Models","type":"post"},{"authors":null,"categories":null,"content":"An introduction to the Bayesian approach to randomized experiments.\nRandomized experiments, a.k.a. AB tests, are now the established gold standard in the industry to estimate causal effects. Randomly assigning the treatment (new product, feature, UI, \u0026hellip;) to a subset of the population (users, patients, customers, \u0026hellip;) we ensure that, on average, the difference in outcomes (revenue, visits, clicks, \u0026hellip;) can be attributed to the treatment. Established companies like Booking.com report constantly running thousands of AB tests at the same time. And newer growing companies like Duolingo attribute a large chunk of their success to their culture of experimentation at scale.\nWith so many experiments, one question comes natural: in one specific experiment, can you leverage information from previous tests? How? In this post, I will try to answer these questions by introducing the Bayesian approach to AB testing. The Bayesian framework is well suited for this type of task because it naturally allows for the updating of existing knowledge (the prior) using new data. However, the method is particularly sensitive to functional form assumptions and apparently innocuous model choices can translate in sensible differences in the estimates, especially when the data is very skewed.\nSearch and Infinite Scrolling For the rest of the article, we are going to use a toy example, loosely inspired by Azavedo et al. (2019): a search engine that wants to increase its ad revenue, without sacrificing search quality. We are a company with an established experimentation culture and we continuously test new ideas on how to rank results, how to select the most relevant ads for consumers, and the user interface (UI) of the results page. Suppose that, in this specific case, we came up with a new brilliant idea: infinite scrolling! Instead of having a discrete sequence of pages, we allow users to keep scrolling down if they want to see more results.\nTo understand whether infinite scrolling works, we ran an AB test: we randomize users into a treatment and a control group. We implement infinite scrolling only for users in the treatment group. I import the data generating process dgp_infinite_scroll() from src.dgp. With respect to previous articles, I generated a new DGP parent class that handles randomization and data generation, while its children classes contain the specific use-cases. I also import some plotting functions and libraries from src.utils. To include not only code but also data and tables, I use Deepnote, a Jupyter-like web-based collaborative notebook environment.\n%matplotlib inline %config InlineBackend.figure_format = 'retina' from src.utils import * from src.dgp import DGP, dgp_infinite_scroll dgp = dgp_infinite_scroll(n=10_000) df = dgp.generate_data(true_effect=0.14) df.head() past_revenue infinite_scroll ad_revenue 0 3.76 1 3.70 1 2.40 1 1.71 2 2.98 1 4.85 3 4.24 1 4.57 4 3.87 0 3.69 We have information on $10.000$ website visitors for which we observe the monthly ad_revenue they generated, whether they were assigned to the treatment group and were using the infinite_scroll, and also the average monthly past_revenue.\nThe random treatment assignment makes the difference-in-means estimator unbiased: we expect the treatment and control group to be comparable on average, so we can causal attribute the average observed difference in outcomes to the treatment effect. We estimate the treatment effect by linear regression. We can interpret the coefficient of infinite_scroll as the estimated treatment effect.\nsmf.ols('ad_revenue ~ infinite_scroll', df).fit().summary().tables[1] coef std err t P\u003e|t| [0.025 0.975] Intercept 1.9865 0.020 101.320 0.000 1.948 2.025 infinite_scroll 0.1524 0.028 5.461 0.000 0.098 0.207 It seems that the infinite_scroll was indeed a good idea and it increase the average monthly revenue by $0.1524$$. Moreover, the effect is significantly different from zero at the 1% confidence level.\nWe could further improve the precision of the estimator by controlling for past_revenue in the regression. We do not expect a sensible change in the estimated coefficient, but the precision should improve (if you want to know more on out control variables, check my other articles on CUPED and DAGs).\nreg = smf.ols('ad_revenue ~ infinite_scroll + past_revenue', df).fit() reg.summary().tables[1] coef std err t P\u003e|t| [0.025 0.975] Intercept 0.0181 0.024 0.741 0.459 -0.030 0.066 infinite_scroll 0.1571 0.020 7.910 0.000 0.118 0.196 past_revenue 0.9922 0.010 98.655 0.000 0.972 1.012 Indeed, past_revenue is highly predictive of current ad_revenue and the precision of the estimated coefficient for infinite_scroll decreases by one-third.\nSo far, everything has been very standard. However, as we said at the beginning, suppose this is not the only experiment we ran trying to improve our browser (and ultimately ad revenue). The infinite scroll is just one idea among thousands of others that we have tested in the past. Is there a way to efficiently use this additional information?\nBayesian Statistics One of the main advantages of Bayesian statistics over the frequentist approach is that it easily allows to incorporate additional information into a model. The idea directly follows from the main results behind all Bayesian statistics: Bayes Theorem. Bayes theorem, allows you to do inference on a model by inverting the inference problem: from the probability of the model given the data, to the probability of the data given the model, a much easier object to deal with.\n$$ \\underbrace{ \\Pr \\big( \\text{model} \\ \\big| \\ \\text{data} \\big) }{\\text{posterior}} = \\underbrace{ \\Pr(\\text{model}) }{\\text{prior}} \\ \\underbrace{ \\frac{ \\Pr \\big( \\text{data} \\ \\big| \\ \\text{model} \\big) }{ \\Pr(\\text{data}) } }_{\\text{likelihood}} $$\nWe can split the right-hand side of Bayes Theorem (or Rule) into two components: the prior and the likelihood. The likelihood is the information about the model that comes from the data, the prior instead is any additional information about the model.\nFirst of all, let\u0026rsquo;s map Bayes theorem into our context. What is the data, what is the model and what is our object of interest?\nthe data which consists in our outcome variable ad_revenue, $y$, the treatment infinite_scroll, $D$ and the other variables, past_revenue and a constant, which we jointly denote as $X$ the model is the distribution of ad_revenue, given past_revenue and the infinite_scroll feature, $y | D, X$ our object of interest is the posterior $\\Pr \\big( \\text{model} \\ \\big| \\ \\text{data} \\big)$, in particular the relationship between ad_revenue and infinite_scroll X = sm.add_constant(df[['past_revenue']].values) D = df['infinite_scroll'].values y = df['ad_revenue'].values How do we use prior information in the context of AB testing, potentially including additional covariates?\nBayesian Regression Let\u0026rsquo;s use a linear model to make it directly comparable with the frequentist approach:\n$$ y_i = \\beta X_i + \\tau D_i + \\varepsilon_i \\qquad \\text{where} \\quad \\varepsilon_i \\sim N \\big( 0, \\sigma^2 \\big) $$\nThis is a parametric model with two sets of parameters: the linear coefficients $\\beta$ and $\\tau$, and the variance of the residuals $\\sigma$. An equivalent, but more Bayesian, way to write the model is:\n$$ y \\ | \\ X, D; \\beta, \\tau, \\sigma \\sim N \\Big( \\beta X + \\tau D \\ , \\sigma^2 \\Big) , $$\nwhere the semi-column separates the data from the model parameters. Differently from the frequentist approach, in Bayesian regressions we do not rely on the central limit theorem to approximate the conditional distribution of $y$, but we directly assume it is normal. Is it just a formality? Not really, but a proper comparison between the frequentist and Bayesian approach is beyond the scope of this article.\nWe are interested in doing inference on the model parameters, $\\beta$, $\\tau$, and $\\sigma$. Another core difference between the frequentist and the Bayesian approach is that the the first assumes that the model parameters are fixed (scalars), while the latter allows them to be stochastic (random variables).\nThis assumption has a very practical implication: you can easily incorporate previous information about the model parameters in the form of prior distributions. As the name says, priors contain information that was available even before looking at the data. This leads to one of the most relevant questions in Bayesian statistics: how do you chose a prior?\nPriors When choosing a prior, one analytically appealing restriction is to have a prior distribution such that the posterior belongs to the same family. These priors are called conjugate priors. For example, before seeing the data, I assume my treatment effect is normally distributed and I would like it to be normally distributed also after incorporating the information contained in the data.\nIn the case of Bayesian linear regression, the conjugate priors for $\\beta$ and $\\sigma$ are normally and inverse-gamma distributed. Let\u0026rsquo;s start a bit blindly, by taking a standard normal and inverse gamma distribution as prior.\n$$ \\beta_i \\sim N(\\boldsymbol 0, \\boldsymbol 1) \\ \\tau_i \\sim N(0,1) \\ \\sigma^2 \\sim \\Gamma^{-1} (1, 1) $$\nWe use the package PyMC to do inference. First we need to specify the model: what are the distributions of the different parameters (priors) and what is the likelihood of the data.\nimport pymc as pm with pm.Model() as baseline_model: # Priors beta = pm.MvNormal('beta', mu=np.ones(np.shape(X)[1]), cov=np.eye(np.shape(X)[1])) tau = pm.Normal('tau', mu=0, sigma=1) sigma = pm.InverseGamma('sigma', mu=1, sigma=1, initval=1) # Likelihood Ylikelihood = pm.Normal('y', mu=(X@beta + D@tau).flatten(), sigma=sigma, observed=y) PyMC has an extremely nice function that allows us to visualize the model as a graph, model_to_graphviz.\npm.model_to_graphviz(baseline_model) From the graphical representation, we can see the various model components, their distributions, and how they interact with each other.\nWe are now ready to compute the model posterior. How does it work? In short, we sample realizations of model parameters, we compute the likelihood of the data given those values and the compute the corresponding posterior.\nidata = pm.sample(model=baseline_model, draws=1000) Auto-assigning NUTS sampler... Initializing NUTS using jitter+adapt_diag... Multiprocess sampling (4 chains in 4 jobs) NUTS: [beta, tau, sigma] 100.00% [8000/8000 00:04\u0026lt;00:00 Sampling 4 chains, 0 divergences] Sampling 4 chains for 1_000 tune and 1_000 draw iterations (4_000 + 4_000 draws total) took 19 seconds. The fact that Bayesian inference requires sampling, has been historically one of the main bottlenecks of Bayesian statistics, since it makes it sensibly slower than the frequentist approach. However, this is less and less of a problem with the increased computational power of model computers.\nWe are now ready to print out results. First, with the summary() method, we can print a model summary very similar to those produced by the statsmodels package.\npm.summary(idata, hdi_prob=0.95).round(4) mean sd hdi_2.5% hdi_97.5% mcse_mean mcse_sd ess_bulk ess_tail r_hat beta[0] 0.019 0.025 -0.031 0.068 0.001 0.0 1943.0 1866.0 1.0 beta[1] 0.992 0.010 0.970 1.011 0.000 0.0 2239.0 1721.0 1.0 tau 0.157 0.021 0.117 0.197 0.000 0.0 2770.0 2248.0 1.0 sigma 0.993 0.007 0.980 1.007 0.000 0.0 3473.0 2525.0 1.0 The estimated parameters are extremely close to the ones we got with the frequentist approach, with an estimated effect of the infinite_scroll equal to $0.157$.\nIf sampling had the disadvantage of being slow, it has the advantage of being very transparent. We can directly plot the distribution of the posterior. Let\u0026rsquo;s do it for the treatment effect $\\tau$. The PyMC function plot_posterior plots the distribution of the posterior, with a black bar for the Bayesian equivalent of a 95% confidence interval.\npm.plot_posterior(idata, kind=\u0026quot;hist\u0026quot;, var_names=('tau'), hdi_prob=0.95, figsize=(6, 3), bins=30); As expected, since we chose conjugate priors, the posterior distribution looks gaussian.\nSo far we have chosen the prior without much guidance. However, suppose we had access to past experiments. How do we incorporate this specific information?\nPast Experiments Suppose the idea of the infinite scroll, was just one among a ton of other ones that we tried and tested in the past. For each idea we have the data for the corresponding experiment, with the corresponding estimated coefficient. Suppose we had a thousand of them.\npast_experiments = [dgp.generate_data(seed_data=i) for i in range(1000)] taus = [smf.ols('ad_revenue ~ infinite_scroll + past_revenue', pe).fit().params.values for pe in past_experiments] How do we use this additional information?\nNormal Prior The first idea could be to calibrate our prior to reflect the data distribution in the past. Keeping the normality assumption, we use the estimated average and standard deviations of the estimates from past experiments.\ntaus_mean = np.mean(taus, axis=0)[1] taus_mean 0.0009094486420266667 On average, had practically no effect on ad_revenue, with a average effect of $0.0009$.\ntaus_std = np.sqrt(np.cov(taus, rowvar=0)[1,1]) taus_std 0.029014447772168384 However, there was sensible variation across experiments, with a standard deviation of $0.029$.\nLet\u0026rsquo;s now estimate the\nwith pm.Model() as model_normal_prior: k = np.shape(X)[1] beta = pm.MvNormal('beta', mu=np.ones(k), cov=np.eye(k)) tau = pm.Normal('tau', mu=taus_mean, sigma=taus_std) sigma = pm.InverseGamma('sigma', mu=1, sigma=1, initval=1) Ylikelihood = pm.Normal('y', mu=(X@beta + D@tau).flatten(), sigma=sigma, observed=y) Let\u0026rsquo;s sample from the model.\nidata_normal_prior = pm.sample(model=model_normal_prior, draws=1000) Auto-assigning NUTS sampler... Initializing NUTS using jitter+adapt_diag... Multiprocess sampling (4 chains in 4 jobs) NUTS: [beta, tau, sigma] 100.00% [8000/8000 00:04\u0026lt;00:00 Sampling 4 chains, 0 divergences] Sampling 4 chains for 1_000 tune and 1_000 draw iterations (4_000 + 4_000 draws total) took 19 seconds. And plot the sample posterior distribution of the treatment effect parameter $\\tau$.\npm.plot_posterior(idata_normal_prior, kind=\u0026quot;hist\u0026quot;, var_names=('tau'), hdi_prob=0.95, figsize=(6, 3), bins=30); The estimated coefficient is sensibly smaller: $0.08$ instead of the previous estimate of $0.12$. Why is it the case?\nThe fact is that the previous coefficient of $0.12$ is extremely unlikey, given our prior. We can compute the probability of getting the same or a more extreme value, given the prior.\n1 - sp.stats.norm(taus_mean, taus_std).cdf(0.12) 2.025724712373389e-05 The probability of such value is almost zero. Therefore, the estimated coefficient has moved towards the prior mean of $0.0009$.\nStudent t Prior So far, we have assumed a normal distribution for all linear coefficients. Is it appropriate? Let\u0026rsquo;s check it visually (check here for other methods on how to compare distributions).\nsns.histplot([tau[0] for tau in taus]).set(title=r'Distribution of $\\hat{\\beta}_0$ in past experiments'); The distribution seems pretty normal. What the treatment effect paramenter $\\tau$?\nfig, ax = plt.subplots() sns.histplot([tau[1] for tau in taus], label='past experiments'); ax.axvline(reg.params['infinite_scroll'], lw=2, c='C3', ls='--', label='current experiment') plt.legend(); plt.title(r'Distribution of $\\hat{\\tau}$ in past experiments'); The distribution very heavy tailed! While at the center it looks like a normal distributions, the tails are much \u0026ldquo;fatter\u0026rdquo; and we have a couple of very extreme values. excluding the case of measurement error, this is a setting that happens often in the industry, where most ideas have extremely small or null effects and very rarely an idea is actually a breakthrough.\nOne way to model this distribution is a student-t distribution. In particular, we use a t-student with mean $0.0009$, variance $0.003$ and $1.3$ degrees of freedom to match the moments of the empirical distributions of past estimates.\nwith pm.Model() as model_studentt_prior: # Priors k = np.shape(X)[1] beta = pm.MvNormal('beta', mu=np.ones(k), cov=np.eye(k)) tau = pm.StudentT('tau', mu=taus_mean, sigma=0.003, nu=1.3) sigma = pm.InverseGamma('sigma', mu=1, sigma=1, initval=1) # Likelihood Ylikelihood = pm.Normal('y', mu=(X@beta + D@tau).flatten(), sigma=sigma, observed=y) Let\u0026rsquo;s sample from the model.\nidata_studentt_priors = pm.sample(model=model_studentt_prior, draws=1000) Auto-assigning NUTS sampler... Initializing NUTS using jitter+adapt_diag... Multiprocess sampling (4 chains in 4 jobs) NUTS: [beta, tau, sigma] 100.00% [8000/8000 00:04\u0026lt;00:00 Sampling 4 chains, 0 divergences] Sampling 4 chains for 1_000 tune and 1_000 draw iterations (4_000 + 4_000 draws total) took 18 seconds. And plot the sample posterior distribution of the treatment effect parameter $\\tau$.\npm.plot_posterior(idata_studentt_priors, kind=\u0026quot;hist\u0026quot;, var_names=('tau'), hdi_prob=0.95, figsize=(6, 3), bins=30); The estimated coefficient is now again similar to the one we got with the standard normal prior, $0.11$. However, the estimate is more precise since the confidence interval has shrunk from $[0.077, 0.016]$ to $[0.065, 0.015]$.\nWhat has happened?\nShrinking The answer lies in the shape of the different prior distributions that we have used:\nstandard normal, $N(0,1)$ normal with matched moments, $N(0, 0.03)$ t-student with matched moments, $t_{1.3}$(0, 0.003)$ t_hats = np.linspace(-0.3, 0.3, 1_000) distributions = { 'N(0,1)': sp.stats.norm(0, 1).pdf(t_hats), 'N(0, 0.03)': sp.stats.norm(0, 0.03).pdf(t_hats), '$t_{1.3}$(0, 0.003)': sp.stats.t(df=1.3).pdf(t_hats / 0.003)*300, } Let\u0026rsquo;s plot all of them together.\nfor i, (label, y) in enumerate(distributions.items()): sns.lineplot(x=t_hats, y=y, color=f'C{i}', label=label); plt.legend(); plt.title('Prior Distributions'); As we can see, all distributions are centered on zero, but they have very different shapes. The standard normal distribution is essentially flat over the $[-0.15, 0.15]$ interval. Every value has basically the same probability. The last two instead, even though they have the same mean and variance, have very different shapes.\nHow does it translate into our estimation? We can plot the implied posterior for different estimates, for each prior distribution.\ndef compute_posterior(b, prior): likelihood = sp.stats.norm(b, taus_std).pdf(t_hats) return np.average(t_hats, weights=prior*likelihood) fig, ax = plt.subplots(figsize=(7,6)) ax.axvline(0, lw=1.5, c='k'); ax.axhline(0, lw=1.5, c='k'); ax.axvline(reg.params['infinite_scroll'], lw=2, ls='--', c='darkgray'); for i, (label, y) in enumerate(distributions.items()): sns.lineplot(x=t_hats, y=[compute_posterior(t, y) for t in t_hats] , color=f'C{i}', label=label); ax.set_xlim(-0.17, 0.17); ax.set_ylim(-0.17, 0.17); plt.legend(); ax.set_xlabel('Experiment Estimate'); ax.set_ylabel('Posterior'); As we can see, the different priors transform the experimental estimates in very different ways. The standard normal prior essentially has no effect for estimates in the $[-0.15, 0.15]$ interval. The normal prior with matched moments instead shrinks each estimate by approximately 2/3. The effect of the t-student prior is instead non-linear: it shrinks small estimates towards zero, while it keeps large estimates as they are.\nMy intuition is the following. A prior distribution very skewed or with \u0026ldquo;fat tails\u0026rdquo; means that large values are rare but not impossible. In practice, it means accepting that breakthrough improvements are possible. On the other hand, for the same variance, the distribution is more concentrated around zero than a standard normal so that small values are shrunk even more.\nConclusion In this article we have seen how to extend the analysis of AB test to incorporate information from past experiments. In particular, we have seen the importance of choosing a prior. Selecting the distribution function is just as important as tuning its parameters. The shape of the prior distribution can drastically affect our inference, especially in a world with skewed distributions.\nDespite the length of the article, this was just a glimpse in the world of AB testing and Bayesian statistics. While being computationally more intensive and requiring additional assumptions, the Bayesian approach is often more natural, powerful and flexible than the frequentist one. Knowing pros and cons of both approaches is crucial to get the best of both worlds, picking the approach that work best or combining them efficiently.\nReferences E. Azevedo, A. Deng, J. Olea, G. Weyl, Empirical Bayes Estimation of Treatment Effects with Many A/B Tests: An Overview (2019). AEA Papers and Proceedings.\nA. Deng, Objective Bayesian Two Sample Hypothesis Testing for Online Controlled Experiments (2018), WWW15.\nRelated Articles The Bayesian Bootstrap\nUnderstanding CUPED\nDAGs and Control Variables\nCode You can find the original Jupyter Notebook here:\nhttps://github.com/matteocourthoud/Blog-Posts/blob/main/notebooks/bayes_ab.ipynb\n","date":1689033600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1689033600,"objectID":"2e50873dbd21801f6d40557d852077ba","permalink":"https://matteocourthoud.github.io/post/bayesian_ab_test/","publishdate":"2023-07-11T00:00:00Z","relpermalink":"/post/bayesian_ab_test/","section":"post","summary":"An introduction to the Bayesian approach to randomized experiments.\nRandomized experiments, a.k.a. AB tests, are now the established gold standard in the industry to estimate causal effects. Randomly assigning the treatment (new product, feature, UI, \u0026hellip;) to a subset of the population (users, patients, customers, \u0026hellip;) we ensure that, on average, the difference in outcomes (revenue, visits, clicks, \u0026hellip;) can be attributed to the treatment.","tags":null,"title":"Bayesian AB Testing","type":"post"},{"authors":null,"categories":null,"content":"What makes an observation \u0026ldquo;unusual\u0026rdquo;?\nIn data science, one common task is outlier detection. This is a broad term that is often misused or misunderstood. More broadly, we are often interested in understanding any observation is \u0026ldquo;unusual\u0026rdquo;. First of all, what does it mean to be unusual? In this article we are going to inspect three different ways in which an observation can be unusual: it can be unusual characteristics, it might not fit the model or it might be particularly influential in fitting the model. We will see that in linear regression the latter characteristics is a byproduct of the first two.\nImportantly, being unusual is not necessarily bad. Observations that have different characteristics from all others usually carry more information. We also expect some observations not to fit the model well, otherwise the model is likely biased (overfitting). However, \u0026ldquo;unusual\u0026rdquo; observations are also more likely to be generated by a different process. Extreme cases include measurement error or fraud, but differences can be more nuanced. Domain knowledge is always kind and dropping observations only for for statistical reasons is never wise.\nThat said, let\u0026rsquo;s have a look at some different ways in which observations can be \u0026ldquo;unusual\u0026rdquo;.\nExample Suppose we are an peer-to-peer online platform and we are interested in understanding if there is anything suspicious going on with our business. We have information about how much time our customers spend on the platform and the total value of their transactions.\nFirst, let\u0026rsquo;s have a look at the data. I import the data generating process dgp_p2p() from src.dgp and some plotting functions and libraries from src.utils. I include code snippets from Deepnote, a Jupyter-like web-based collaborative notebook environment. For our purpose, Deepnote is very handy because it allows me not only to include code but also output, like data and tables.\n%matplotlib inline %config InlineBackend.figure_format = 'retina' from src.utils import * from src.dgp import dgp_p2p df = dgp_p2p().generate_data() df.head() hours transactions 0 2.6 8.30 1 2.0 8.00 2 7.0 21.00 3 6.7 18.00 4 1.2 3.82 We have information on 50 clients for which we observe hours spent on the website and total transactions amount. Since we only have two variables we can easily inspect them using a scatterplot.\nsns.scatterplot(data=df, x='hours', y='transactions').set(title='Data Scatterplot'); The relationship between hours and transactions seems to follow a clear linear relationship. If we fit a linear model, we observe a particularly tight fit.\nsmf.ols('hours ~ transactions', data=df).fit().summary().tables[1] coef std err t P\u003e|t| [0.025 0.975] Intercept -0.0975 0.084 -1.157 0.253 -0.267 0.072 transactions 0.3452 0.009 39.660 0.000 0.328 0.363 Does any data point look suspiciously different from the others? How?\nLeverage The first metric that we are going to use to evaluate \u0026ldquo;unusual\u0026rdquo; observations is the leverage, which was first introduced by Cook (1980). The objective of the leverage is to capture how much a single point is different with respect to other data points. These data points are often called outliers and there exist a nearly amount of algorithms and rules of thumb to flag them.However the idea is the same: flagging observations that are unusual in terms of features.\nThe leverage of an observation $i$ is defined as\n$$ h_{ii} := x_i\u0026rsquo; (X\u0026rsquo;X)^{-1} x_i $$\nOne interpretation of the leverage is as a measure of distance where individual observations are compared against the average of all observations.\nAnother interpretation of the leverage is as the influence of the outcome of observation $i$, $y_i$, on the corresponding fitted value $\\hat{y_i}$.\n$$ h_{ii} = \\frac{\\partial \\hat{y}_i}{\\partial y_i} $$\nAlgebraically, the leverage of observation $i$ is the $i^{th}$ element of the design matrix $X\u0026rsquo; (X\u0026rsquo;X)^{-1} X$. Among the many properties of the leverages, is the fact that they are non-negative and their values sum to 1.\nLet\u0026rsquo;s compute the leverage of the observations in our dataset. We also flag observations that have unusual leverages (which we arbitrarily define as more than two standard deviations away from the average leverage).\nX = np.reshape(df['hours'].values, (-1, 1)) Y = np.reshape(df['transactions'].values, (-1, 1)) df['leverage'] = np.diagonal(X @ np.linalg.inv(X.T @ X) @ X.T) df['high_leverage'] = df['leverage'] \u0026gt; (np.mean(df['leverage']) + 2*np.std(df['leverage'])) Let\u0026rsquo;s plot the distribution of leverage values in our data.\nfix, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6)) sns.histplot(data=df, x='leverage', hue='high_leverage', alpha=1, bins=30, ax=ax1).set(title='Distribution of Leverages'); sns.scatterplot(data=df, x='hours', y='transactions', hue='high_leverage', ax=ax2).set(title='Data Scatterplot'); As we can see, the distribution is skewed with two observations having a unusually high leverage. Indeed, in the scatterplot these two observations are slightly separated from the rest of the distribution.\nIs this bad news? It depends. Outliers are not a problem per se. Actually, if they are genuine observations, they might carry much more information than other observations. On the other hand, they are also more likely not to be genuine observations (e.g. fraud, measurement error, \u0026hellip;) or to be inherently different from the other ones (e.g. professional users vs amateurs). In any case, we might want to investigate further and use as much context-specific information as we can.\nImportantly, the fact that an observation has a high leverage tells us information about the features of the model but nothing about the model itself. Are these users just different observations or they also behave differently?\nResiduals So far we have only talked about unusual features, but what about unusual behavior? This is what regression residuals measure.\nRegression residuals are the difference between the predicted outcome values and the observed outcome values. In a sense, they capture what the model cannot explain: the higher the residual of one observation the more it is unusual in the sense that the model cannot explain it.\nIn the case of linear regression, residuals can be written as\n$$ \\hat{e} = y - \\hat{y} = y - \\hat \\beta X $$\nIn our case, since $X$ is one dimensional (hours), we can easily visualize them.\nY_hat = X @ np.linalg.inv(X.T @ X) @ X.T @ Y plt.scatter(X, Y, s=50, label='data') plt.plot(X, Y_hat, c='k', lw=2, label='prediction') plt.vlines(X, np.minimum(Y, Y_hat), np.maximum(Y, Y_hat), color='r', lw=3, label=\u0026quot;residuals\u0026quot;); plt.legend() plt.title(f\u0026quot;Regression prediction and residuals\u0026quot;); Do some observations have unusually high residuals? Let\u0026rsquo;s plot their distribution.\ndf['residual'] = np.abs(Y - X @ np.linalg.inv(X.T @ X) @ X.T @ Y) df['high_residual'] = df['residual'] \u0026gt; (np.mean(df['residual']) + 2*np.std(df['residual'])) fix, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6)) sns.histplot(data=df, x='residual', hue='high_residual', alpha=1, bins=30, ax=ax1).set(title='Distribution of Residuals'); sns.scatterplot(data=df, x='hours', y='transactions', hue='high_residual', ax=ax2).set(title='Data Scatterplot'); Two observations have particularly high residuals. This means that for these observations, the model is not good at predicting the observed outcomes.\nIs this bad news? Not necessarily. A model that fits the observations too well is likely to be biased. However, it might still be important to understand why some users have a different relationship between hours spent and total transactions. As usual, information on the specific context is key.\nSo far we have looked at observations with \u0026ldquo;unusual\u0026rdquo; characteristics and \u0026ldquo;unusual\u0026rdquo; model fit, but what is the observation itself is distorting the model? How much our model is driven by a handful of observations?\nInfluence The concept of influence and influence functions was developed precisely to answer this question: what are influential observations? This questions were very popular in the 80\u0026rsquo;s and lost appeal for a long time until the recent need of explaining complex machine learning and AI models.\nThe general idea is to define an observation as influential if removing it significantly changes the estimated model. In linear regression, we define the influence of observation $i$ as:\n$$ \\hat{\\beta} - \\hat{\\beta}_{-i} = (X\u0026rsquo;X)^{-1} x_i e_i $$\nWhere $\\hat{\\beta}_{-i}$ is the OLS coefficient estimated omitting observation $i$.\nAs you can see, there is a tight connection to both leverage $h_{ii}$ and residuals $e_i$: influence is almost the product of the two. Indeed, in linear regression, observations with high leverage are observations that are both outliers and have high residuals. None of the two conditions alone is sufficient for an observation to have an influence on the model.\nWe can see it best in the data.\ndf['influence'] = (np.linalg.inv(X.T @ X) @ X.T).T * np.abs(Y - Y_hat) df['high_influence'] = df['influence'] \u0026gt; (np.mean(df['influence']) + 2*np.std(df['influence'])) fix, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6)) sns.histplot(data=df, x='influence', hue='high_influence', alpha=1, bins=30, ax=ax1).set(title='Distribution of Influences'); sns.scatterplot(data=df, x='hours', y='transactions', hue='high_influence', ax=ax2).set(title='Data Scatterplot'); In our dataset, there is only one observation with high influence, and it is disproportionally larger than the influence of all other observations.\nWe can now plot all \u0026ldquo;unusual\u0026rdquo; points in the same plot. I also report residuals and leverage of each point in a separate plot.\ndef plot_leverage_residuals(df): # Hue df['type'] = 'Normal' df.loc[df['high_residual'], 'type'] = 'High Residual' df.loc[df['high_leverage'], 'type'] = 'High Leverage' df.loc[df['high_influence'], 'type'] = 'High Influence' # Init figure fig, (ax1,ax2) = plt.subplots(1,2, figsize=(12,5)) ax1.plot(X, Y_hat, lw=1, c='grey', zorder=0.5) sns.scatterplot(data=df, x='hours', y='transactions', ax=ax1, hue='type').set(title='Data') sns.scatterplot(data=df, x='residual', y='leverage', hue='type', ax=ax2).set(title='Metrics') ax1.get_legend().remove() sns.move_legend(ax2, \u0026quot;upper left\u0026quot;, bbox_to_anchor=(1.05, 0.8)); plot_leverage_residuals(df) As we can see, we have one point with high residual and low leverage, one with high leverage and low residual and only one point with both high leverage and high residual: the only influential point.\nFrom the plot it is also clear why none of the two conditions alone is sufficient for an observation to rive the model. The orange point has high residual but it lies right in the middle of the distribution and therefore cannot tilt the line of best fit. The green point instead has high leverage and lies far from the center of the distribution but its perfectly aligned with the line of fit. Removing it would not change anything. The red dot instead is different from the others in terms of both characteristics and behavior and therefore tilts the fit line towards itself.\nConclusion In this post, we have seen some different ways in which observations can be \u0026ldquo;unusual\u0026rdquo;: they can have either unusual characteristics or unusual behavior. In linear regression, when an observation has both it is also influential: it tilts the model towards itself.\nIn the example of the article, we concentrated on a univariate linear regression. However, research on influence functions has recently become a hot topic because of the need to make black-box machine learning algorithms understandable. With models with millions of parameters, billions of observations and wild non-linearities, it can be very hard to establish whether a single observation is influential and how.\nReferences [1] D. Cook, Detection of Influential Observation in Linear Regression (1980), Technometrics.\n[2] D. Cook, S. Weisberg, Characterizations of an Empirical Influence Function for Detecting Influential Cases in Regression (1980), Technometrics.\n[2] P. W. Koh, P. Liang, Understanding Black-box Predictions via Influence Functions (2017), ICML Proceedings.\nCode You can find the original Jupyter Notebook here:\nhttps://github.com/matteocourthoud/Blog-Posts/blob/main/notebooks/outliers.ipynb\n","date":1689033600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1689033600,"objectID":"de5feaaf33b11b940923f5b3d34a81c1","permalink":"https://matteocourthoud.github.io/post/outliers_leverage/","publishdate":"2023-07-11T00:00:00Z","relpermalink":"/post/outliers_leverage/","section":"post","summary":"What makes an observation \u0026ldquo;unusual\u0026rdquo;?\nIn data science, one common task is outlier detection. This is a broad term that is often misused or misunderstood. More broadly, we are often interested in understanding any observation is \u0026ldquo;unusual\u0026rdquo;.","tags":null,"title":"Outliers, Leverage, and Influential Observations","type":"post"},{"authors":null,"categories":null,"content":"How to run experiments without storing individual data\nAB tests, a.k.a. randomized controlled trials, are widely recognized as the gold standard technique to compute the causal effect of a treatment (a drug, ad, product, \u0026hellip;) on an outcome of interest (a disease, firm revenue, customer satisfaction, \u0026hellip;). We randomly split a set of subjects (patients, users, customers, \u0026hellip;) into a treatment and a control group and give the treatment to the treatment group. This procedure ensures that ex-ante, the only expected difference between the two groups is caused by the treatment.\nOne potential privacy concern is that one needs to store data about many users for the whole duration of the experiment in order to estimate the effect of the treatment. This is not a problem if we can run the experiment instantaneusly, but can become an issue when the experiment duration is long. In this post, we are going to explore one solution to this problem: online regression. We will see how to estimate (conditional) average treatment effects and how to do inference, using both asymptotic approximations and bootstrapping.\n⚠️ Some parts are algebra-intense, but you can skip them if you are only interested in the intuition.\nCredit Cards and Donations Suppose, for example, that we were a fin-tech company. We have designed a new user interface (UI) for our mobile application and we would like to understand whether it slows down our transaction. In order to estimate the causal effect of the new UI on transaction speed, we plan to run an A/B test or randomized controlled trial.\nWe have one major problem: we should not store user-level information for privacy reasons.\nFirst, let\u0026rsquo;s have a look at the data. I import the data generating process dgp_credit() from src.dgp and some plotting functions and libraries from src.utils. I include code snippets from Deepnote, a Jupyter-like web-based collaborative notebook environment. For our purpose, Deepnote is very handy because it allows me not only to include code but also output, like data and tables.\n%matplotlib inline %config InlineBackend.figure_format = 'retina' from src.utils import * from src.dgp import dgp_credit I first generate the whole dataset in one-shot. We will then investigate how to perform the experimental analysis in case the data was arriving dynamically.\ndef generate_data(self, N=100, seed=0): np.random.seed(seed) # Connection speed connection = np.random.lognormal(3, 1, N) # Treatment assignment treated = np.random.binomial(1, 0.5, N) # Transfer speed #spend = np.minimum(np.random.lognormal(1 + treated + 0.1*np.sqrt(balance), 2, N), balance) speed = np.minimum(np.random.exponential(10 + 4*treated - 0.5*np.sqrt(connection), N), connection) # Generate the dataframe df = pd.DataFrame({'c': [1]*N, 'treated': treated, 'connection': np.round(connection,2), 'speed': np.round(speed,2)}) return df N = 100 df = generate_data(N) df.head() c treated connection speed 0 1 0 117.22 0.94 1 1 1 29.97 29.97 2 1 0 53.45 7.38 3 1 0 188.84 0.76 4 1 1 130.00 24.44 We have information on 100 users, for whom we observe\u0026hellip;\nFirst, let\u0026rsquo;s have a look\nmodel = smf.ols('speed ~ treated + connection', data=df).fit() model.summary().tables[1] coef std err t P\u003e|t| [0.025 0.975] Intercept 6.0740 1.079 5.630 0.000 3.933 8.215 treated 1.3939 1.297 1.075 0.285 -1.180 3.968 connection -0.0033 0.017 -0.197 0.844 -0.037 0.030 In order to understand how we can make linear regression one data point at the time, we first need a brief linear algebra recap.\nFirst of all, let\u0026rsquo;s define $y$ the dependent variable, spend in our case, and $X$ the explanatory variable, the treated indicator, the account balance and a constant.\ndef xy_from_df(df, r0, r1): return df.iloc[r0:r1,:3].to_numpy(), df.iloc[r0:r1,3].to_numpy() The estimator is given by $$ \\hat{\\beta}_{OLS} = (X\u0026rsquo;X)^{-1}X\u0026rsquo;y $$\nfrom numpy.linalg import inv X, Y = xy_from_df(df, 0, 100) inv(X.T @ X) @ X.T @ Y array([ 6.07404291e+00, 1.39385101e+00, -3.33599131e-03]) We get indeed the same exact number as with the smf.ols command!\nCan we compute $\\beta$ one observation at the time?\nThe answer is yes! Assume we had $n$ observations and we just received the $n+1$th observation: the pair $(x_{n+1}, y_{n+1})$. In order to compute $\\hat{\\beta}_{n+1}$ we need to have stored only two objects in memory\n$\\hat{\\beta}_{n}$, the previous estimate of $\\beta$ $(X_n\u0026rsquo; X_n)^{-1}$, the previous value of $(X\u0026rsquo; X)^{-1}$ First of all, how do we update $(X\u0026rsquo; X)^{-1}$? $$ \\begin{align*} (X_{n+1}\u0026rsquo; X_{n+1})^{-1} = (X_n\u0026rsquo; X_n)^{-1} - \\frac{(X_n\u0026rsquo; X_n)^{-1} x_{n+1} x_{n+1}\u0026rsquo; (X_n\u0026rsquo; X_n)^{-1}}{1 + x_{n+1}\u0026rsquo; (X_n\u0026rsquo; X_n)^{-1} x_{n+1}} \\end{align*} $$\nAfter having updated $(X\u0026rsquo; X)^{-1}$, we can update $\\hat{\\beta}$. $$ \\hat{\\beta}{n+1} = \\hat{\\beta}{n} + (X_n\u0026rsquo; X_n)^{-1} x_{n} (y_n - x_n\u0026rsquo; \\hat{\\beta}_{n}) $$\nNote that this procedure is not only privacy friendly but also memory-friendly. Our dataset is a $100 \\times 4$ matrix while $(X\u0026rsquo; X)^{-1}$ is $3 \\times 3$ matrix and $\\beta$ is a $3 \\times 1$ matrix. We are storing only 12 numbers instead of up to 400!\ndef update_xb(XiX, beta, x, y): XiX -= (XiX @ x.T @ x @ XiX) / (1 + x @ XiX @ x.T ) beta += XiX @ x.T @ (y - x @ beta) return XiX, beta We are now ready to estimate our OLS coefficient, one data point at the time. However, we cannot really start from the first observation, because we would be unable to invert the matrix $X\u0026rsquo;X$. We need at least $k+1$ observations, where $k$ is the number of variables in $X$.\nLet\u0026rsquo;s use a warm start of 10 observations to be safe.\n# Initialize XiX and beta from first 10 observations x, y = xy_from_df(df, 0, 10) XiX = inv(x.T @ x) beta = XiX @ x.T @ y # Update estimate live for n in range(10, N): x, y = xy_from_df(df, n, n+1) XiX, beta = update_xb(XiX, beta, x, y) # Print result print(beta) [ 6.07404291e+00 1.39385101e+00 -3.33599131e-03] We got exactly the same coefficient! Nice!\nHow did we get there? We can plot the evolution of out estimate $\\hat{\\beta}$ as we accumulate data. The dynamic plotting function is a bit more cumbersome, but you can find it in src.figures.\nfrom src.figures import online_regression online_regression(df, \u0026quot;fig/online_reg1.gif\u0026quot;) As we can see, as the number of data points increases, the estimate seems to less and less volatile.\nBut is it really the case? As usual, we are not just interested in the point estimate of the effect of the coupon on spending, we would also like to understand how precise this estimate is.\nInference We have seen how to estimate the treatment effect \u0026ldquo;online\u0026rdquo;: one observation at the time. Can we also compute the variance of the estimator in the same manner?\nFirst of all, let\u0026rsquo;s review what the variance of the OLS estimator looks like. Under baseline assumptions, the variance of the OLS estimator is given by: $$ \\text{Var}(\\hat{\\beta}_{OLS}) = (X\u0026rsquo;X)^{-1} \\hat{\\sigma}^2 $$\nwhere $\\hat{\\sigma}^2$ is the variance of the residuals $e = (y - X\u0026rsquo;\\hat{\\beta})$.\nThe regression table reports the standard errors of the coefficients, which are the squared elements on the diagonal of $\\text{Var}(\\hat{\\beta})$.\nmodel.summary().tables[1] coef std err t P\u003e|t| [0.025 0.975] Intercept 6.0740 1.079 5.630 0.000 3.933 8.215 treated 1.3939 1.297 1.075 0.285 -1.180 3.968 connection -0.0033 0.017 -0.197 0.844 -0.037 0.030 Let\u0026rsquo;s check that we would obtain the same numbers using matrix algebra.\nbeta = inv(X.T @ X) @ X.T @ Y np.sqrt(np.diag(inv(X.T @ X) * np.var(Y - X @ beta))) array([1.06261376, 1.27718352, 0.01669716]) Indeed, we get exactly the same numbers!\nWe already have a method to part of $\\text{Var}(\\hat{\\beta}{OLS})$ online: $(X\u0026rsquo;X)^{-1}$ update the matrix $(X\u0026rsquo;X)^{-1}$ online. How do we update $\\hat{\\sigma}^2$? This is the formula to update the sum of squared residuals $S$. $$ S{n+1} = S_{n} + \\frac{(y_{n+1} - x_{n+1}\\hat{\\beta}n)}{1 + x{n+1}\u0026rsquo; (X_n\u0026rsquo; X_n)^{-1} x_{n+1}} $$\ndef update_xbs(XiX, beta, S, x, y): S += (y - x @ beta)**2 / (1 + x @ XiX @ x.T ) XiX -= (XiX @ x.T @ x @ XiX) / (1 + x @ XiX @ x.T ) beta += XiX @ x.T @ (y - x @ beta) return XiX, beta, S[0,0] Note, the order here is very important!\n# Inizialize XiX, beta, and sigma from the first 10 observations x, y = xy_from_df(df, 0, 10) XiX = inv(x.T @ x) beta = XiX @ x.T @ y S = np.sum((y - x @ beta)**2) # Update XiX, beta, and sigma online for n in range(10, N): x, y = xy_from_df(df, n, n+1) XiX, beta, S = update_xbs(XiX, beta, S, x, y) # Print result print(np.sqrt(np.diag(XiX * S / (N - 3)))) [1.0789208 1.29678338 0.0169534 ] We indeed got the same result! Note that to get from the sum of squared residuals $S$ to the residuals variance $\\hat{\\sigma}^2$ we need to divide by the degrees of freedom: $n - k = 100 - 3$.\nAs before we have plotted the evolution of the estimate of the OLS coefficient over time, we can now augment that plot with a confidence band of +- one standard deviation.\nonline_regression(df, \u0026quot;fig/online_reg2.gif\u0026quot;, ci=True) As we can see, the estimated variance of the OLS estimator indeed decreases as the sample size increases.\nBootstrap So far we have used the asymptotic assumptions behind the Central Limit Theorem to compute the standard errors of the estimator. However, we have a particularly small sample. We further check the empirical distribution of the model residuals.\nsns.histplot(model.resid, bins=30); The residuals seem to be particularly skewed! This might be a problem in such a small sample.\nOne alternative is the bootstrap. Instead of relying on asymptotics, we approximate the distribution of our estimator by resampling our dataset with replacement. Can we bootstrap online?\nThe answer is once again yes! They key is to weight each observation with an integer weight drawn from a Poisson distribution with mean (and variance) equal to 1. We repeat this process multiple times, in parallel and then we\nThe updating rules for $(X\u0026rsquo;X)^{-1}$ and $\\hat{beta}$ become the following. $$ \\begin{align*} (X_{n+1}\u0026rsquo; X_{n+1})^{-1} = (X_n\u0026rsquo; X_n)^{-1} - \\frac{w (X_n\u0026rsquo; X_n)^{-1} x_{n+1} x_{n+1}\u0026rsquo; (X_n\u0026rsquo; X_n)^{-1}}{1 + w x_{n+1}\u0026rsquo; (X_n\u0026rsquo; X_n)^{-1} x_{n+1}} \\end{align*} $$\nand $$ \\hat{\\beta}{n+1} = \\hat{\\beta}{n} + w (X_n\u0026rsquo; X_n)^{-1} x_{n} (y_n - x_n\u0026rsquo; \\hat{\\beta}_{n}) $$\nwhere $w$ are Poisson weights. First, let\u0026rsquo;s update the updating function for $(X\u0026rsquo;X)^{-1}$ and $\\hat{beta}$.\ndef update_xbw(XiX, beta, w, x, y): XiX -= (w * XiX @ x.T @ x @ XiX) / (1 + w * x @ XiX @ x.T ) beta += w * XiX @ x.T @ (y - x @ beta) return XiX, beta We can now run the online estimation. We bootstrap 1000 different estimates of $\\hat{\\beta}$.\n# Inizialize a vector of XiXs and betas np.random.seed(0) K = 1000 x, y = xy_from_df(df, 0, 10) XiXs = [inv(x.T @ x) for k in range(K)] betas = [xix @ x.T @ y for xix in XiXs] # Update the vector of XiXs and betas online for n in range(10, N): x, y = xy_from_df(df, n, n+1) for k in range(K): w = np.random.poisson(1) XiXs[k], betas[k] = update_xbw(XiXs[k], betas[k], w, x, y) We can compute the estimated standard deviation of the treatment effect, simply by computing the standard deviation of the vector of bootstrapped coefficients.\nnp.std(betas, axis=0) array([0.95301002, 1.14186364, 0.01207962]) The estimated standard errors are slightly different from the previous values of $[1.275, 1.532, 0.020]$, but not very far apart.\nLastly, some of you might have wondered \u0026ldquo;why sampling discrete weights and not continuous ones?\u0026rdquo;. Indeed, we can. This procedure is called the Bayesian Bootstrap and you can find a more detailed explanation here.\nConclusion In this post, we have seen hot to run an experiment without storing individual-level data. How are we able to do it? In order to compute the average treatment effect, we do not need every single observation but it\u0026rsquo;s sufficient to store just a more compact representation of it.\nReferences [1] W. Chou, Randomized Controlled Trials without Data Retention (2021), Working Paper.\nRelated Articles Experiments, Peeking, and Optimal Stopping The Bayesian Bootstrap Code You can find the original Jupyter Notebook here:\nhttps://github.com/matteocourthoud/Blog-Posts/blob/main/notebooks/online_reg.ipynb\n","date":1688947200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1688947200,"objectID":"11d653865f2e9d534e2220bc44dedee6","permalink":"https://matteocourthoud.github.io/post/online_regression/","publishdate":"2023-07-10T00:00:00Z","relpermalink":"/post/online_regression/","section":"post","summary":"How to run experiments without storing individual data\nAB tests, a.k.a. randomized controlled trials, are widely recognized as the gold standard technique to compute the causal effect of a treatment (a drug, ad, product, \u0026hellip;) on an outcome of interest (a disease, firm revenue, customer satisfaction, \u0026hellip;).","tags":null,"title":"A/B Tests, Privacy and Online Regression","type":"post"},{"authors":null,"categories":null,"content":"When analyzing causal relationships, it is very hard to understand which variables to condition the analysis on, i.e. how to \u0026ldquo;split\u0026rdquo; the data so that we are comparing apples to apples. For example, if you want to understand the effect of having a tablet in class on studenta\u0026rsquo; performance, it makes sense to compare schools where students have similar socio-economic backgrounds. Otherwise, the risk is that only wealthier students can afford a tablet and, without controlling for it, we might attribute the effect to tablets instead of the socio-economic background.\nWhen the treatment of interest comes from a proper randomized experiment, we do not need to worry about conditioning on other variables. If tablets are distributed randomly across schools, and we have enough schools in the experiment, we do not have to worry about the socio-economic background of students. The only advantage of conditioning the analysis on some so-called \u0026ldquo;control variable\u0026rdquo; could be an increase in power. However, this is a different story.\nIn this post, we are going to have a brief introduction to Directed Acyclic Graphs and how they can be useful to select variables to condition a causal analysis on. Not only DAGs provide visual intuition on which variables we need to include in the analysis, but also on which variables we should not include, and why.\nDirected Acyclic Graphs Definitions Directed acyclic graphs (DAGs) provide a visual representation of the data generating process. Random variables are represented with letters (e.g. $X$) and causal relationships are represented with arrows (e.g. $\\to$). For example, we interpret\nflowchart LR classDef white fill:#FFFFFF,stroke:#000000,stroke-width:2px X((X)):::white --\u0026gt; Y((Y)):::white as $X$ (possibly) causes $Y$. We call a path between two variables $X$ and $Y$ any connection, independently of the direction of the arrows. If all arrows point forward, we call it a causal path, otherwise we call it a spurious path.\nflowchart LR classDef included fill:#DCDCDC,stroke:#000000,stroke-width:2px; classDef excluded fill:#ffffff,stroke:#000000,stroke-width:2px; X((X)) Y((Y)) Z1((Z1)) Z2((Z2)) Z3((Z3)) X --\u0026gt; Z1 Z1 --\u0026gt; Z2 Z3 --\u0026gt; Z2 Z3 --\u0026gt; Y class X,Y included; class Z1,Z2,Z3 excluded; In the example above, we have a path between $X$ and $Y$ passing through the variables $Z_1$, $Z_2$, and $Z_3$. Since not all arrows point forward, the path is spurious and there is no causal relationship of $X$ on $Y$. In fact, variable $Z_2$ is caused by both $Z_1$ and $Z_3$ and therefore blocks the path.\n$Z_2$ is called a collider.\nThe purpose of our analysis is to assess the causal relationship between two variables $X$ and $Y$. Directed acyclic graphs are useful because they provide us instructions on which other variables $Z$ we need to condition our analysis on. Conditioning the analysis on a variable means that we keep it fixed and we draw our conclusions ceteris paribus. For example, in a linear regression framework, inserting another regressor $Z$ means that we are computing the best linear approximation of the conditional expectation function of $Y$ given $X$, conditional on the observed values of $Z$.\nCausality In order to assess causality, we want to close all spurious paths between $X$ and $Y$. The questions now are:\nWhen is a path open? If it does not contain colliders. Otherwise, it is closed. How do you close an open path? You condition on at least one intermediate variable. How do you open a closed path? You condition on all colliders along the path. Suppose we are again interested in the causal relationship of $X$ on $Y$. Let\u0026rsquo;s consider the following graph\nflowchart LR classDef included fill:#DCDCDC,stroke:#000000,stroke-width:2px; classDef excluded fill:#ffffff,stroke:#000000,stroke-width:2px; X((X)) Y((Y)) Z1((Z1)) Z2((Z2)) Z3((Z3)) X --\u0026gt; Y X --\u0026gt; Z2 Z2 --\u0026gt; Y Z1 --\u0026gt; X Z1 --\u0026gt; Y X --\u0026gt; Z3 Y --\u0026gt; Z3 class X,Y included; class Z1,Z2,Z3 excluded; In this case, apart from the direct path, there are three non-direct paths between $X$ and $Y$ through the variables $Z_1$, $Z_2$, and $Z_3$.\nLet\u0026rsquo;s consider the case in which we analyze the relationship between $X$ and $Y$, ignoring all other variables.\nThe path through $Z_1$ is open but it is spurious The path through $Z_2$ is open and causal The path through $Z_3$ is closed since $Z_3$ is a collider and it is spurious Let\u0026rsquo;s draw the same graph indicating in grey variables that we are conditioning on, with dotted lines closed paths, with red lines spurious open paths, and with green lines causal open paths.\nflowchart LR classDef included fill:#DCDCDC,stroke:#000000,stroke-width:2px; classDef excluded fill:#ffffff,stroke:#000000,stroke-width:2px; X((X)) Y((Y)) Z1((Z1)) Z2((Z2)) Z3((Z3)) X --\u0026gt; Y X --\u0026gt; Z2 Z2 --\u0026gt; Y Z1 --\u0026gt; X Z1 --\u0026gt; Y X -.-\u0026gt; Z3 Y -.-\u0026gt; Z3 linkStyle 0,1,2 stroke:#00ff00,stroke-width:4px; linkStyle 3,4 stroke:#ff0000,stroke-width:4px; class X,Y included; class Z1,Z2,Z3 excluded; In this case, to assess the causal relationship between $X$ and $Y$ we need to close the path that passes through $Z_1$. We can do that by conditioning the analysis on $Z_1$.\nflowchart LR classDef included fill:#DCDCDC,stroke:#000000,stroke-width:2px; classDef excluded fill:#ffffff,stroke:#000000,stroke-width:2px; X((X)) Y((Y)) Z1((Z1)) Z2((Z2)) Z3((Z3)) X --\u0026gt; Y X --\u0026gt; Z2 Z2 --\u0026gt; Y Z1 -.-\u0026gt; X Z1 -.-\u0026gt; Y X -.-\u0026gt; Z3 Y -.-\u0026gt; Z3 linkStyle 0,1,2 stroke:#00ff00,stroke-width:4px; class X,Y,Z1 included; class Z2,Z3 excluded; Now we are able to recover the causal relationship between $X$ and $Y$ by conditioning on $Z_1$.\nWhat would happen if we were also conditioning on $Z_2$? In this case, we would close the path passing through $Z_2$ leaving only the direct path between $X$ and $Y$ open. We would then recover only the direct effect of $X$ on $Y$ and not the indirect one.\nflowchart LR classDef included fill:#DCDCDC,stroke:#000000,stroke-width:2px; classDef excluded fill:#ffffff,stroke:#000000,stroke-width:2px; X((X)) Y((Y)) Z1((Z1)) Z2((Z2)) Z3((Z3)) X --\u0026gt; Y X -.-\u0026gt; Z2 Z2 -.-\u0026gt; Y Z1 -.-\u0026gt; X Z1 -.-\u0026gt; Y X -.-\u0026gt; Z3 Y -.-\u0026gt; Z3 linkStyle 0 stroke:#00ff00,stroke-width:4px; class X,Y,Z1,Z2 included; class Z3 excluded; What would happen if we were also conditioning on $Z_3$? In this case, we would open the path passing through $Z_3$ which is a spurious path. We would then not be able to recover the causal effect of $X$ on $Y$.\nflowchart LR classDef included fill:#DCDCDC,stroke:#000000,stroke-width:2px; X((X)) Y((Y)) Z1((Z1)) Z2((Z2)) Z3((Z3)) X --\u0026gt; Y X -.-\u0026gt; Z2 Z2 -.-\u0026gt; Y Z1 -.-\u0026gt; X Z1 -.-\u0026gt; Y X --\u0026gt; Z3 Y --\u0026gt; Z3 linkStyle 0 stroke:#00ff00,stroke-width:4px; linkStyle 5,6 stroke:#ff0000,stroke-width:4px; class X,Y,Z1,Z2,Z3 included; Example: Class Size and Math Scores Suppose you are interested in the effect of class size on math scores. Are bigger classes better or worse for students\u0026rsquo; performance?\nAssume that the data generating process can be represented with the following DAG.\nflowchart TB classDef included fill:#DCDCDC,stroke:#000000,stroke-width:2px; classDef excluded fill:#ffffff,stroke:#000000,stroke-width:2px; classDef unobserved fill:#ffffff,stroke:#000000,stroke-width:2px,stroke-dasharray: 5 5; X((class size)) Y((math score)) Z1((class year)) Z2((good school)) Z3((math hours)) Z4((hist score)) U((ability)) X --\u0026gt; Y Z1 --\u0026gt; X X --\u0026gt; Z4 U --\u0026gt; Y U --\u0026gt; Z4 Z2 --\u0026gt; X Z2 --\u0026gt; Y Z2 --\u0026gt; Z4 Z3 --\u0026gt; Y class X,Y included; class Z1,Z2,Z3,Z4 excluded; class U unobserved; The variables of interest are highlighted. Moreover, the dotted line around ability indicates that this is a variable that we do not observe in the data.\nWe can now load the data and check what it looks like.\n%matplotlib inline %config InlineBackend.figure_format = 'retina' from src.utils import * from src.dgp import dgp_school df = dgp_school().generate_data() df.head() math_hours history_hours good_school class_year class_size math_score hist_score 0 3 3 1 3 15 13.009309 15.167024 1 2 3 1 3 19 13.047033 13.387456 2 2 4 0 1 25 8.330311 10.824070 3 3 4 1 3 22 11.322190 14.594394 4 3 3 1 4 15 12.338458 11.871626 What variables should we condition our regression on, in order to estimate the causal effect of class size on math scores?\nFirst of all, let\u0026rsquo;s look at what happens if we do not condition our analysis on any variable and we just regress math score on class size.\nsmf.ols('math_score ~ class_size', df).fit().summary().tables[1] coef std err t P\u003e|t| [0.025 0.975] Intercept 12.0421 0.259 46.569 0.000 11.535 12.550 class_size -0.0399 0.013 -3.025 0.003 -0.066 -0.014 The effect of class_size is negative and statistically different from zero.\nBut should we believe this estimated effect? Without controlling for anything, this is DAG representation of the effect we are capturing.\nflowchart TB classDef included fill:#DCDCDC,stroke:#000000,stroke-width:2px; classDef excluded fill:#ffffff,stroke:#000000,stroke-width:2px; classDef unobserved fill:#ffffff,stroke:#000000,stroke-width:2px,stroke-dasharray: 5 5; X((class size)) Y((math score)) Z1((class year)) Z2((good school)) Z3((math hours)) Z4((hist score)) U((ability)) X --\u0026gt; Y Z1 --\u0026gt; X X -.-\u0026gt; Z4 U --\u0026gt; Y U -.-\u0026gt; Z4 Z2 --\u0026gt; X Z2 --\u0026gt; Y Z2 --\u0026gt; Z4 Z3 --\u0026gt; Y linkStyle 0 stroke:#00ff00,stroke-width:4px; linkStyle 5,6 stroke:#ff0000,stroke-width:4px; class X,Y included; class Z1,Z2,Z3,Z4 excluded; class U unobserved; There is a spurious path passing through good school that biases our estimated coefficient. Intuitively, being enrolled in a better school improves the students\u0026rsquo; math scores and better schools might have smaller class sizes. We need to control for the quality of the school.\nsmf.ols('math_score ~ class_size + good_school', df).fit().summary().tables[1] coef std err t P\u003e|t| [0.025 0.975] Intercept 4.7449 0.247 19.176 0.000 4.259 5.230 class_size 0.2095 0.010 20.020 0.000 0.189 0.230 good_school 5.0807 0.130 39.111 0.000 4.826 5.336 Now the estimate of the effect of class size on math score is unbiased! Indeed, the true coefficient in the data generating process was $0.2$.\nflowchart TB classDef included fill:#DCDCDC,stroke:#000000,stroke-width:2px; classDef excluded fill:#ffffff,stroke:#000000,stroke-width:2px; classDef unobserved fill:#ffffff,stroke:#000000,stroke-width:2px,stroke-dasharray: 5 5; X((class size)) Y((math score)) Z1((class year)) Z2((good school)) Z3((math hours)) Z4((hist score)) U((ability)) X --\u0026gt; Y Z1 --\u0026gt; X X -.-\u0026gt; Z4 U --\u0026gt; Y U -.-\u0026gt; Z4 Z2 -.-\u0026gt; X Z2 -.-\u0026gt; Y Z2 --\u0026gt; Z4 Z3 --\u0026gt; Y linkStyle 0 stroke:#00ff00,stroke-width:4px; class X,Y,Z2 included; class Z1,Z3,Z4 excluded; class U unobserved; What would happen if we were to instead control for all variables?\nsmf.ols('math_score ~ class_size + good_school + math_hours + class_year + hist_score', df).fit().summary().tables[1] coef std err t P\u003e|t| [0.025 0.975] Intercept -0.7847 0.310 -2.529 0.012 -1.394 -0.176 class_size 0.1292 0.010 13.054 0.000 0.110 0.149 good_school 2.9815 0.170 17.533 0.000 2.648 3.315 math_hours 1.0516 0.048 21.744 0.000 0.957 1.147 class_year 0.0424 0.037 1.130 0.259 -0.031 0.116 hist_score 0.4116 0.027 15.419 0.000 0.359 0.464 The coefficient is again biased. Why?\nWe have opened a new spurious path by controlling for hist score. In fact, hist score is a collider and controlling for it has opened a path through hist score and ability that was otherwise closed.\nflowchart TB classDef included fill:#DCDCDC,stroke:#000000,stroke-width:2px; classDef excluded fill:#ffffff,stroke:#000000,stroke-width:2px; classDef unobserved fill:#ffffff,stroke:#000000,stroke-width:2px,stroke-dasharray: 5 5; X((class size)) Y((math score)) Z1((class year)) Z2((good school)) Z3((math hours)) Z4((hist score)) U((ability)) X --\u0026gt; Y Z1 --\u0026gt; X X --\u0026gt; Z4 U --\u0026gt; Y U --\u0026gt; Z4 Z2 -.-\u0026gt; X Z2 -.-\u0026gt; Y Z2 --\u0026gt; Z4 Z3 --\u0026gt; Y linkStyle 0 stroke:#00ff00,stroke-width:4px; linkStyle 2,3,4 stroke:#ff0000,stroke-width:4px; class X,Y,Z1,Z2,Z3,Z4 included; class U unobserved; The example was inspired by the following tweet.\nWe can illustrate this with Model 16 of the \u0026quot;Crash Course in Good and Bad Controls\u0026quot; (https://t.co/GcSNzhuVt2). Here X = class size, Y = math4, Z = read4, and U = student\u0026#39;s ability. Conditioning on Z opens the path X -\u0026gt; Z \u0026lt;- U -\u0026gt; Y and it is thus a \u0026quot;bad control.\u0026quot; https://t.co/KNfqtsMWwB pic.twitter.com/lUSigNYSJj\n\u0026mdash; Análise Real (@analisereal) March 12, 2022 Conclusion In this post, we have seen how to use Directed Acyclic Graphs to select control variables in a causal analysis. DAGs are very helpful tools since they provide an intuitive graphical representation of causal relationships between random variables. Contrary to common intuition that \u0026ldquo;the more information the better\u0026rdquo;, sometimes including extra variables might bias the analysis, preventing a causal interpretation of the results. In particular, we must pay attention not to include colliders that open spurious paths that would otherwise be closed.\nReferences [1] C. Cinelli, A. Forney, J. Pearl, A Crash Course in Good and Bad Controls (2018), working paper.\n[2] J. Pearl, Causality (2009), Cambridge University Press.\n[3] S. Cunningham, Chapter 3 of The Causal Inference Mixtape (2021), Yale University Press.\n","date":1688947200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1688947200,"objectID":"2132429537a02ae205b7f0955c4df6b1","permalink":"https://matteocourthoud.github.io/post/good_bad_controls/","publishdate":"2023-07-10T00:00:00Z","relpermalink":"/post/good_bad_controls/","section":"post","summary":"When analyzing causal relationships, it is very hard to understand which variables to condition the analysis on, i.e. how to \u0026ldquo;split\u0026rdquo; the data so that we are comparing apples to apples.","tags":null,"title":"DAGs and Control Variables","type":"post"},{"authors":null,"categories":null,"content":"Causal inference, machine learning and regularization bias\nIn causal inference, we often estimate causal effects by conditioning the analysis on other variables. We usually refer to these variables as control variables or confounders. In randomized control trials or AB tests, conditioning can increase the power of the analysis, by reducing imbalances that have emerged despite randomization. However, conditioning is even more important in observational studies, where, absent randomization, it might be essential to recover causal effects.\nWhen we have many control variables, we might want to select the most relevant ones, ppossibly capturing nonlinearities and interactions. Machine learning algorithms are perfect for this task. However, in these cases, we are introducing a bias that is called regularization or pre-test, or feature selection bias. In this and the next blog post, I try to explain the source of the bias and a very poweful solution called double debiased machine learning, which has been probably one of the most relevant advancement at the intersection of machine learning and causal inference of the last decade.\nPre-Testing Since this is a complex topic, let\u0026rsquo;s start with a simple example.\nSuppose we were a firm and we are interested in the effect of advertisement spending on revenue: is advertisement worth the money? There are also a lot of other things that might influence sales, therefore, we are thinking of controlling for past sales in the analysis, in order to increase the power of our analysis.\nAssume the data generating process can be represented with the following Directed Acyclic Graph (DAG). If you are not familiar with DAGs, I have written a short introduction here.\nflowchart LR classDef included fill:#DCDCDC,stroke:#000000,stroke-width:2px; classDef excluded fill:#ffffff,stroke:#000000,stroke-width:2px; classDef unobserved fill:#ffffff,stroke:#000000,stroke-width:2px,stroke-dasharray: 5 5; D((ad spend)) Z((past sales)) Y((sales)) D --\u0026gt; Y Z -- ??? --\u0026gt; Y Z --\u0026gt; D class D,Y included; class Z excluded; linkStyle 0 stroke:#00ff00,stroke-width:4px; I import the data generating process dgp_tbd() from src.dgp and some plotting functions and libraries from src.utils.\n%matplotlib inline %config InlineBackend.figure_format = 'retina' from src.utils import * from src.dgp import dgp_pretest df = dgp_pretest().generate_data() df.head() ads sales past_sales 0 16.719800 19.196620 6.624345 1 7.732222 9.287491 4.388244 2 10.923469 11.816906 4.471828 3 8.457062 9.024376 3.927031 4 13.085146 12.814823 5.865408 We have data on $1000$ different markets, in which we observe current sales, the amount spent in advertisement and past sales.\nWe want to understand ads spending is effective in increasing sales. One possibility is to regress the latter on the former, using the following regression model, also called the short model.\n$$ \\text{sales} = \\alpha \\cdot \\text{ads} + \\varepsilon $$\nShould we also include past sales in the regression? Then the regression model would be the following, also called long model.\n$$ \\text{sales} = \\alpha \\cdot \\text{ads} + \\beta \\cdot \\text{past sales} + \\varepsilon $$\nSince we are not sure whether to condition the analysis on past sales, we could let the data decide: we could run the second regression and, if the effect of past sales, $\\beta$, is statistically significant, we are good with the long model, otherwise we run the short model.\nsmf.ols('sales ~ ads + past_sales', df).fit().summary().tables[1] coef std err t P\u003e|t| [0.025 0.975] Intercept 0.1405 0.185 0.758 0.448 -0.223 0.504 ads 0.9708 0.030 32.545 0.000 0.912 1.029 past_sales 0.3381 0.095 3.543 0.000 0.151 0.525 It seems that the effect of past sales on current sales is positive and significant. Therefore, we are happy with our specification and we conclude that the effect of ads on sales is positive and significant with a 95% confidence interval of $[0.912, 1.029]$.\nThe Bias There is an issue with this procedure: we are not taking into account the fact that we have run a test to decide whether to include past_sales in the regression. The fact that we have decided to include past_sales because its coefficient is significant does have an effect on the inference on the effect of ads on sales, $\\alpha$.\nThe best way to understand the problem is through simulations. Since we have access to the data generating process dgp_pretest() (unlike in real life), we can just test what would happen if we were repeating this procedure multiple times:\nWe draw a new sample from the data generating process. We regress sales on ads and past_sales. If the coefficient of past_sales is significant at the 95% level, we keep $\\hat \\alpha_{long}$ from (2). Otherwise, we regress sales on ads only, and we keep that coefficient $\\hat \\alpha_{short}$. I write a pre_test function to implement the procedure above. I also save the coefficients from both regressions, long and short, and the chosen one, called the pre-test coefficient.\nReminder: we are pre-testing the effect of past_sales on sales but the coefficient of interest is the one of ads on sales.\ndef pre_testing(d='ads', y='sales', x='past_sales', K=1000, **kwargs): # Init alpha = {'Long': np.zeros(K), 'Short': np.zeros(K), 'Pre-test': np.zeros(K)} # Loop over simulations for k in range(K): # Generate data df = dgp_pretest().generate_data(seed=k, **kwargs) # Compute coefficients alpha['Long'][k] = smf.ols(f'{y} ~ {d} + {x}', df).fit().params[1] alpha['Short'][k] = smf.ols(f'{y} ~ {d}', df).fit().params[1] # Compute significance of beta p_value = smf.ols(f'{y} ~ {d} + {x}', df).fit().pvalues[2] # Select specification based on p-value if p_value\u0026lt;0.05: alpha['Pre-test'][k] = alpha['Long'][k] else: alpha['Pre-test'][k] = alpha['Short'][k] return alpha alphas = pre_testing() We can now plot the distributions (over simulations) of the estimated coefficients.\ndef plot_alphas(alphas, true_alpha): # Init plot fig, axes = plt.subplots(1, len(alphas), figsize=(4*len(alphas), 5), sharey=True, sharex=True) # Make one plot for each set of coefficients for i, key in enumerate(alphas.keys()): axes[i].hist(alphas[key], bins=30, lw=.1) axes[i].set_title(key) axes[i].axvline(true_alpha, c='r', ls='--') legend_text = [r'$\\alpha=%.0f$' % true_alpha, r'$\\hat \\alpha=%.4f$' % np.mean(alphas[key])] axes[i].legend(legend_text, prop={'size': 10}, loc='upper right') plot_alphas(alphas, true_alpha=1) In the plot above, I have depicted the estimated coefficients, across simulations, for the different regression specifications.\nAs we can see from the first plot, if we were always running the long regression, our estimator $\\hat \\alpha_{long}$ would be unbiased and normally distributed. However, if we were always running the short regression (second plot), our estimator $\\hat \\alpha_{short}$ would be biased.\nThe pre-testing procedure generates an estimator $\\hat \\alpha_{pretest}$ that is a mix of the two: most of the times we select the correct specification, the long regression, but sometimes the pre-test fails to reject the null hypothesis of no effect of past sales on sales, $H_0 : \\beta = 0$, and we select the incorrect specification, running the short regression.\nImportantly, the pre-testing procedure does not generate a biased estimator. As we can see in the last plot, the estimated coefficient is very close to the true value, 1. The reason is that most of the time, the number of times we select the short regression is sufficiently small not to introduce bias, but not small enough to have valid inference.\nIndeed, pre-testing distorts inference: the distribution of the estimator $\\hat \\alpha_{pretest}$ is not normal anymore, but bimodal. The consequence is that our confidence intervals for $\\alpha$ are going to have the wrong coverage (contain the true effect with a different probability than the claimed one).\nWhen is pre-testing a problem? The problem of pre-testing arises because of the bias generated by running the short regression: omitted variable bias (OVB). In you are not familiar with OVB, I have written a short introduction here. In general however, we can express the omitted variable bias introduced by regressing $Y$ on $D$ ignoring $X$ as\n$$ \\text{OVB} = \\beta \\delta \\qquad \\text{ where } \\qquad \\beta := \\frac{Cov(X, Y)}{Var(X)}, \\quad \\delta := \\frac{Cov(D, X)}{Var(D)} $$\nWhere $\\beta$ is the effect of $X$ (past sales in our example) on $Y$ (sales) and $\\delta$ is the effect of $D$ (ads) on $X$.\nPre-testing is a problem if\nWe run the short regression instead of the long one and The effect of the bias is sensible What can help improving (1), i.e. the probability of correctly rejecting the null hypothesis of zero effect of past sales, $H_0 : \\beta = 0$? The answer is simple: a bigger sample size. If we have more observations, we can more precisely estimate $\\beta$ and it is going to be less likely that we commit a type 2 error and run the short regression instead of the long one.\nLet\u0026rsquo;s simulate the estimated coefficient $\\hat \\alpha$ under different sample sizes. Remember that the sample size used until now is $N=1000$.\nNs = [100,300,1000,3000] alphas = {f'N = {n:.0f}': pre_testing(N=n)['Pre-test'] for n in Ns} plot_alphas(alphas, true_alpha=1) As we can see from the plots, as the sample size increases (left to right), the bias decreases and the distribution of the estimator $\\hat \\alpha_{pretest}$ converges to a normal distribution.\nWhat happens instead if the value of $\\beta$ was different? It is probably going to affect point (2) in the previous paragraph, but how?\nIf $\\beta$ is very small, it is going to be hard to detect it, and we will often end up running the short regression, introducing a bias. However, if $\\beta$ is very small, it also implies that the magnitude of the bias is small and therefore it is not going to affect our estimate of $\\alpha$ much\nIf $\\beta$ is very big, it is going to be easy to detect and we will often end up running the long regression, avoiding the bias (which would have been very big though).\nLet\u0026rsquo;s simulate the estimated coefficient $\\hat \\alpha$ under different values of $\\beta$. The true value used until now was $\\beta = 0.3$.\nbetas = 0.3 * np.array([0.1,0.3,1,3]) alphas = {f'beta = {b:.2f}': pre_testing(b=b)['Pre-test'] for b in betas} plot_alphas(alphas, true_alpha=1) As we can see from the plots, as the value of $\\beta$ increases, the bias first appears and then disappears. When $\\beta$ is small (left plot), we often choose the short regression, but the bias is small and the average estimate is very close to the true value. For intermediate values of $\\beta$, the bias is sensible and it has a clear effect on inference. Lastly, for large values of $\\beta$ instead (right plot), we always run the long regression and the bias disappears.\nBut when is a coefficient big or small? And big or small with respect to what? The answer is simple: with respect to the sample size, or more accurately, with respect to the inverse of the square root of the sample size, $1 / \\sqrt{n}$. The reason is deeply rooted in the Central Limit Theorem, but I won\u0026rsquo;t cover it here.\nThe idea is easier to show than to explain, so let\u0026rsquo;s repeat the same simulation as above, but now we will increase both the coefficient and the sample size at the same time.\nbetas = 0.3 * 30 / np.sqrt(Ns) alphas = {f'N = {n:.0f}': pre_testing(b=b, N=n)['Pre-test'] for n,b in zip(Ns,betas)} plot_alphas(alphas, true_alpha=1) As we can see, now that $\\beta$ is proportional to $1 / \\sqrt{n}$, the distortion is not going away, not matter the sample size. Therefore, inference will always be wrong.\nWhile a coefficient that depends on the sample size might sound not intuitive, it captures well the idea of magnitude in a world where we do inference relying on asymptotic results, first among all the Central Limit Theorem. In fact, the Central Limit Theorem relieas on an infinitely large sample size. However, with an infinite amount of data, no coefficient is small and any non-zero effect is detected with certainty.\nPre-Testing and Machine Learning So far we talked about a linear regression with only 2 variables. Where is the machine learning we were promised?\nUsually we do not have just one control variable (or confounder), but many. Moreover, we might want to be flexible with respect to the functional form through which these control variables enter the model. In general, we will assume the following model:\n$$ Y = \\alpha D + g_0(X) + u \\newline D = m_0(X) + v $$\nWhere the effect of interest is still $\\alpha$, $X$ is potentially high dimensional and we do not take a stand on the functional form through which $X$ influences $D$ or $Y$.\nIn this setting, it is natural to use a machine learning algorithm to estimate $g_0$ and $m_0$. However, machine learning algorithms usually introduce a regularization bias that is comparable to pre-testing.\nPossibly, the \u0026ldquo;simplest\u0026rdquo; way to think about it is Lasso. Lasso is linear in $X$, with a penalization term that effectively just performs the variable selection we discussed above. Therefore, if we were to use Lasso of $X$ and $D$ on $Y$ we would be introducing regularization bias and inference would be distorted. The same goes for more complex algorithms.\nLastly, you might still wonder \u0026ldquo;why is the model linear in the treatment variable $D$?\u0026rdquo;. Doing inference is much easier in linear model, not only for computational reasons but also for interpretation. Moreover, if the treatment $D$ is binary, the linear functional form is without loss of generality. A stronger assumption is the additive separability of $D$ and $g(X)$.\nConclusion In this post, I have tried to explain how does regularization bias emerges and why it can the an issue in causal inference. This problem is inherently related to settings with many control variables or where we would like to have a model-free (i.e. non-parametric) when controlling for confounders. These are exactly the settings in which machine learning algorithms can be useful.\nIn the next post, I will cover a simple and yet incredibly powerful solution to this problem: double-debiased machine learning.\nReferences [1] A. Belloni, D. Chen, V. Chernozhukov, C. Hansen, Sparse Models and Methods for Optimal Instruments With an Application to Eminent Domain (2012), Econometrica.\n[2] A. Belloni, V. Chernozhukov, C. Hansen, Inference on treatment effects after selection among high-dimensional controls (2014), The Review of Economic Studies.\n[3] V. Chernozhukov, D. Chetverikov, M. Demirer, E. Duflo, C. Hansen, W. Newey, J. Robins, Double/debiased machine learning for treatment and structural parameters (2018), The Econometrics Journal.\nRelated Articles Understanding Omitted Variable Bias Understanding The Frisch-Waugh-Lovell Theorem DAGs and Control Variables Code You can find the original Jupyter Notebook here:\nhttps://github.com/matteocourthoud/Blog-Posts/blob/main/pretest.ipynb\n","date":1688947200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1688947200,"objectID":"89d3aa00d8fc1ae33c2462f6d5256807","permalink":"https://matteocourthoud.github.io/post/regularization_bias/","publishdate":"2023-07-10T00:00:00Z","relpermalink":"/post/regularization_bias/","section":"post","summary":"Causal inference, machine learning and regularization bias\nIn causal inference, we often estimate causal effects by conditioning the analysis on other variables. We usually refer to these variables as control variables or confounders.","tags":null,"title":"Double Debiased Machine Learning (part 1)","type":"post"},{"authors":null,"categories":null,"content":"In the previous part of this blog post, we have seen how pre-testing can distort inference, i.e., how selecting control variables depending on their statistical significance results in wrong confidence intervals for the variable of interest. This bias is generally called regularization bias and also emerges in machine learning algorithms.\nIn blog post, we are going to explore a solution to the simple selection example, post-double selection, and a more general approach when we have many control variables and we do not want to assume linearity, double-debiased machine learning.\nRecap To better understand the source of the bias, in the first part of this post, we have explored the example of a firm that is interested in testing the effectiveness of an a campaign. The firm has information on its current ad spending and on the level of sales. The problem arises because the firm is uncertain on whether it should condition its analysis on the level of past sales.\nThe following Directed Acyclic Graph (DAG) summarizes the data generating process.\nflowchart LR classDef included fill:#DCDCDC,stroke:#000000,stroke-width:2px; classDef excluded fill:#ffffff,stroke:#000000,stroke-width:2px; classDef unobserved fill:#ffffff,stroke:#000000,stroke-width:2px,stroke-dasharray: 5 5; D((ad spend)) Z((past sales)) Y((sales)) D --\u0026gt; Y Z -- ??? --\u0026gt; Y Z --\u0026gt; D class D,Y included; class Z excluded; linkStyle 0 stroke:#00ff00,stroke-width:4px; I import the data generating process dgp_tbd() from src.dgp and some plotting functions and libraries from src.utils.\n%matplotlib inline %config InlineBackend.figure_format = 'retina' from src.utils import * from src.dgp import dgp_pretest df = dgp_pretest().generate_data() df.head() ads sales past_sales 0 16.719800 19.196620 6.624345 1 7.732222 9.287491 4.388244 2 10.923469 11.816906 4.471828 3 8.457062 9.024376 3.927031 4 13.085146 12.814823 5.865408 We have data on $1000$ different markets, in which we observe current sales, the amount spent in advertisement and past sales.\nWe want to understand ads spending is effective in increasing sales. One possibility is to regress the latter on the former, using the following regression model, also called the short model.\n$$ \\text{sales} = \\alpha \\cdot \\text{ads} + \\varepsilon $$\nShould we also include past sales in the regression? Then the regression model would be the following, also called long model.\n$$ \\text{sales} = \\alpha \\cdot \\text{ads} + \\beta \\cdot \\text{past sales} + \\varepsilon $$\nOne naive approach would be to let the data decide: we could run the second regression and, if the effect of past sales, $\\beta$, is statistically significant, we are good with the long model, otherwise we run the short model. This procedure is called pre-testing.\nThe problem with this procedure is that it introduces a bias that is called regularization or pre-test bias. Pre-testing ensures that this bias is small enough not to distort the estimated coefficient. However, it does not ensure that it is small enough not to distort the confidence intervals around the estimated coefficient.\nIs there a solution? Yes!\nPost-Double Selection The solution is called post-double selection. The method was first introduced in Belloni, Chernozhukov, Hansen (2014) and later expanded in a variety of papers.\nThe authors assume the following data generating process:\n$$ y = \\alpha D + \\beta X + u \\newline D = \\delta X + v $$\nIn our example, $Y$ corresponds to sales, $D$ corresponds to ads, $X$ corresponds to past_sales and the effect of interest is $\\alpha$. In our example, $X$ is 1-dimensional for simplicity, but generally we are interested in cases where X is high-dimensional, potentially even having more dimensions than the number of observations. In that case, variable selection is essential in linear regression since we cannot have more features than variables (the OLS coefficients are not uniquely determined anymore).\nPost-double selection consists in the following procedure.\nReduced Form selection: lasso $Y$ on $X$. Select the statistically significant variables in the set $S_{RF} \\subseteq X$ First Stage selection: regress $D$ on $X$. Select the statistically significant variables in the set $S_{FS} \\subseteq X$ Regress $Y$ on $D$ and the union of the selected variables in the first two steps, $S_{FS} \\cup S_{RF}$ The authors show that this procedure produces confidence intervals for the coefficient of interest $\\alpha$ that have the correct coverage, i.e. the correct probability of type 1 error.\nNote (1): this procedure is always less parsimonious, in terms of variable selection, than pre-testing. In fact, we still select all the variables we would have selected with pre-testing but, in the first stage, we might select additional variables.\nNote (2): the terms first stage and reduced form come from the intrumental variables literature in econometrics. Indeed, the first application of post-double selection was to select instrumental variables in Belloni, Chen, Chernozhukov, Hansen (2012).\nNote (3): the name post-double selection comes from the fact that now we are not performing variable selection once but twice.\nIntuition The idea behind post-double selection is: bound the omitted variables bias. In case you are not familiar with it, I wrote a separate blog post on omitted variable bias.\nIn our setting, we can express the omitted variable bias as\n$$ \\text{OVB} = \\beta \\delta \\qquad \\text{ where } \\qquad \\beta := \\frac{Cov(X, Y)}{Var(X)}, \\quad \\delta := \\frac{Cov(D, X)}{Var(D)} $$\nAs we can see, the omitted variable bias comes from the product of two quantities related to the omitted variable $X$:\nIts partial correlation with the outcome $Y$, $\\beta$ Its partial correlation with the variable of interest $D$, $\\delta$ With pre-testing, we ensure that the partial correlation between $X$ the outcome $Y$, $\\beta$, is small. In fact, we omit $Z$ when we shouldn\u0026rsquo;t (i.e. we commit a type 2 error) rarely. What do small and rarely mean?\nWhen we are selecting a variable because of its significance, we ensure that it dimension is smaller than $\\frac{c}{\\sqrt{n}}$ for some number $c$, where $n$ is the sample size.\nTherefore, with pre-testing, we ensure that, no matter what the value of $\\delta$ is, the dimension of the bias is smaller than $\\frac{c}{\\sqrt{n}}$ which means that it converges to zero for sufficiently large $n$. This is why the pre-testing estimator is still consistent.\nHowever, in order for our confidence intervals to have the right coverage, this is not enough. In practice, we need the bias to converge to zero faster than $\\frac{1}{\\sqrt{n}}$. Why?\nTo get an intuition for this result, we need to turn to the Central Limit Theorem. The CLT tells us that for large $n$ the distribution of the sample average of a random variable $X$ converges to a normal distribution with mean $\\mu$ and standard deviation $\\frac{\\sigma}{\\sqrt{n}}$, where $\\mu$ and $\\sigma$ are the mean and standard deviation of $X$. To do inference, we usually apply the Central Limit Theorem to our estimator to get its asymptotic distribution, which in turn allows us to build confidence intervals (using the mean and the standard deviation). Therefore, if the bias is not sensibly smaller than the standard deviation of the estimator, the confidence intervals are going to be wrong. Therefore, we need the bias to converge to zero faster than the standard deviation, i.e. faster than $\\frac{1}{\\sqrt{n}}$.\nIn our setting, the omitted variable bias is $\\beta \\gamma$ and we want it to converge to zero faster than $\\frac{1}{\\sqrt{n}}$. Post-double selection guarantees that\nReduced form selection (pre-testing): any \u0026ldquo;missing\u0026rdquo; variable $j$ has $|\\beta_j| \\leq \\frac{c}{\\sqrt{n}}$ First stage selection (additional): any \u0026ldquo;missing\u0026rdquo; variable $j$ has $|\\delta_j| \\leq \\frac{c}{\\sqrt{n}}$ As a consequence, as long as the number of omitted variables is finite, the omitted variable bias is going to converge to zero at a rate $\\frac{1}{n}$, which is faster than $\\frac{1}{\\sqrt{n}}$. Problem solved!\nApplication Let\u0026rsquo;s now go back to our example and test the post-double selection procedure. In practice, we want to do the following:\nFirst Stage selection: regress ads on past_sales. Check if past_sales is statistically significant Reduced Form selection: regress sales on past_sales. Check if past_sales is statistically significant Regress sales on ads and include past_sales only if it was significant in either one of the two previous regressions I update the pre_test function from the first part of the post to compute also the post-double selection estimator.\ndef pre_test(d='ads', y='sales', x='past_sales', K=1000, **kwargs): # Init alphas = pd.DataFrame({'Long': np.zeros(K), 'Short': np.zeros(K), 'Pre-test': np.zeros(K), 'Post-double': np.zeros(K)}) # Loop over simulations for k in range(K): # Generate data df = dgp_pretest().generate_data(seed=k, **kwargs) # Compute coefficients alphas['Long'][k] = smf.ols(f'{y} ~ {d} + {x}', df).fit().params[1] alphas['Short'][k] = smf.ols(f'{y} ~ {d}', df).fit().params[1] # Compute significance of beta and gamma p_value_ydx = smf.ols(f'{y} ~ {d} + {x}', df).fit().pvalues[2] p_value_yx = smf.ols(f'{y} ~ {x}', df).fit().pvalues[1] p_value_dx = smf.ols(f'{d} ~ {x}', df).fit().pvalues[1] # Select pre-test specification based on regression of y on d and x if p_value_ydx\u0026lt;0.05: alphas['Pre-test'][k] = alphas['Long'][k] else: alphas['Pre-test'][k] = alphas['Short'][k] # Select post-double specification based on regression of y on d and x if p_value_yx\u0026lt;0.05 or p_value_dx\u0026lt;0.05: alphas['Post-double'][k] = alphas['Long'][k] else: alphas['Post-double'][k] = alphas['Short'][k] return alphas alphas = pre_test() We can now plot the distributions (over simulations) of the estimated coefficients.\ndef plot_alphas(alphas, true_alpha): # Init plot K = len(alphas.columns) fig, axes = plt.subplots(1, K, figsize=(4*K, 5), sharey=True, sharex=True) # Make one plot for each set of coefficients for i, key in enumerate(alphas.columns): axes[i].hist(alphas[key].values, bins=30, lw=.1, color=f'C{int(i==3)*2}') axes[i].set_title(key) axes[i].axvline(true_alpha, c='r', ls='--') legend_text = [rf'$\\alpha=${true_alpha}', rf'$\\hat \\alpha=${np.mean(alphas[key]):.4f}'] axes[i].legend(legend_text, prop={'size': 10}, loc='upper right') plot_alphas(alphas, true_alpha=1) As we can see, the post-double selection estimator always correctly selects the long regression and therefore has the correct distribution.\nDouble-checks In the last post, we ran some simulations in order to investigate when pre-testing bias emerges. We saw that pre-testing is a problem for\nSmall sample sizes $n$ Intermediate values of $\\beta$ When the value of $\\beta$ depends on the sample size Let\u0026rsquo;s check that post-double selection removes regularization bias in all the previous cases.\nFirst, let\u0026rsquo;s simulate the distribution of the post-double selection estimator $\\hat \\alpha_{postdouble}$ for different sample sizes.\nNs = [100,300,1000,3000] alphas = {f'N = {n:.0f}': pre_test(N=n) for n in Ns} def compare_alphas(alphas, true_alpha): # Init plot fig, axes = plt.subplots(1, len(alphas), figsize=(4*len(alphas), 5), sharey=True, sharex=True) # Make one plot for each set of coefficients for i, key in enumerate(alphas.keys()): axes[i].hist(alphas[key]['Pre-test'], bins=30, lw=.1, alpha=0.5) axes[i].hist(alphas[key]['Post-double'], bins=30, lw=.1, alpha=0.5, color='C2') axes[i].set_title(key) axes[i].axvline(true_alpha, c='r', ls='--') axes[i].legend([rf'$\\alpha=${true_alpha}', 'Pre-test', 'Post-double'], prop={'size': 10}, loc='upper right') compare_alphas(alphas, true_alpha=1) For small samples, the distribution of the pre-testing estimator is not normal but rather bimodal. From the plots we can see that the post-double estimator is gaussian also in small sample sizes.\nNow we repeat the same exercise, but for different values of $\\beta$, the coefficient of past_sales on sales.\nbetas = 0.3 * np.array([0.1,0.3,1,3]) alphas = {f'beta = {b:.2f}': pre_test(b=b) for b in betas} compare_alphas(alphas, true_alpha=1) Again, the post-double selection estimator has a gaussian distribution irrespectively of the value of $\\beta$, while he pre-testing estimator suffers from regularization bias.\nFor the last simulation, we change both the coefficient and the sample size at the same time.\nbetas = 0.3 * 30 / np.sqrt(Ns) alphas = {f'N = {n:.0f}': pre_test(b=b, N=n) for n,b in zip(Ns,betas)} compare_alphas(alphas, true_alpha=1) Also in this last case, the post-double selection estimator performs well and inference is not distorted.\nDouble Debiased Machine Learning So far, we only have analyzed a linear, univariate example. What happens if the dimension of $X$ increases and we do not know the functional form through which $X$ affects $Y$ and $D$? In these cases, we can use machine learning algorithms to uncover these high-dimensional non-linear relationships.\nChernozhukov, Chetverikov, Demirer, Duflo, Hansen, Newey, and Robins (2018) investigate this setting. In particular, the authors consider the following partially linear model.\n$$ Y = \\alpha D + g(X) + u \\ D = m(X) + v $$\nwhere $Y$ is the outcome variable, $D$ is the treatment to interest and $X$ is a potentially high-dimensional set of control variables.\nNaive approach A naive approach to estimation of $\\alpha$ using machine learning methods would be, for example, to construct a sophisticated machine learning estimator for learning the regression function $\\alpha D$ + $g(X)$.\nSplit the sample in two: main sample and auxiliary sample [why? see note below] Use the auxiliary sample to estimate $\\hat g(X)$ Use the main sample to compute the orthogonalized component of $Y$ on $X$: $\\ \\hat u = Y - \\hat{g} (X)$ Use the main sample to estimate the residualized OLS estimator from regressing $\\hat u$ on $D$ $$ \\hat \\alpha = \\left( D\u0026rsquo; D \\right) ^{-1} D\u0026rsquo; \\hat u $$\nThis estimator is going to have two problems:\nSlow rate of convergence, i.e. slower than $\\sqrt(n)$ It will be biased because we are employing high dimensional regularized estimators (e.g. we are doing variable selection) Note (1): so far we have not talked about it, but variable selection procedure also introduce another type of bias: overfitting bias. This bias emerges because of the fact that the sample used to select the variables is the same that is used to estimate the coefficient of interest. This bias is easily accounted for with sample splitting: using different sub-samples for the selection and the estimation procedures.\nNote (2): why can we use the residuals from step 3 to estimate $\\alpha$ in step 4? Because of the Frisch-Waugh-Lovell theorem. If you are not familiar with it, I have written a blog post on the Frisch-Waugh-Lovell theorem here.\nOrthogonalization Now consider a second construction that employs an orthogonalized formulation obtained by directly partialling out the effect of $X$ from $D$ to obtain the orthogonalized regressor $v = D − m(X)$.\nSplit the sample in two: main sample and auxiliary sample\nUse the auxiliary sample to estimate $\\hat g(X)$ from\n$$ Y = \\alpha D + g(X) + u \\ $$\nUse the auxiliary sample to estimate $\\hat m(X)$ from\n$$ D = m(X) + v $$\nUse the main sample to compute the orthogonalized component of $D$ on $X$ as\n$$ \\hat v = D - \\hat m(X) $$\nUse the main sample to estimate the double-residualized OLS estimator as\n$$ \\hat \\alpha = \\left( \\hat{v}\u0026rsquo; D \\right) ^{-1} \\hat{v}\u0026rsquo; \\left( Y - \\hat g(X) \\right) $$\nThe estimator is root-N consistent! This means that not only the estimator converges to the true value as the sample sizes increases (i.e. it\u0026rsquo;s consistent), but also its standard deviation does (i.e. it\u0026rsquo;s root-N consistent).\nHowever, the estimator still has a lower rate of convergence because of sample splitting. The problem is solved by inverting the split sample, re-estimating the coefficient and averaging the two estimates. Note that this procedure is valid since the two estimates are independent by the sample splitting procedure.\nA Cautionary Tale Before we conclude, I have to mention a recent research paper by Hünermund, Louw, and Caspi (2022), in which the authors show that double-debiased machine learning can easily backfire, if we apply blindly.\nThe problem is related to bad control variables. If you have never heard this term, I have written an introductory blog post on good and bad control variables here. In short, conditioning the analysis on additional features is not always good for causal inference. Depending on the setting, there might exist variables that we want to leave out of our analysis since their inclusion can bias the coefficient of interest, preventing a causal interpretation. The simplest example is variables that are common outcomes, of both the treatment $D$ and outcome variable $Y$.\nThe double-debiased machine learning model implicitly assumes that the control variables $X$ are (weakly) common causes to both the outcome $Y$ and the treatment $D$. If this is the case, and no further mediated/indirect relationship exists between $X$ and $Y$, there is no problem. However, if, for example, some variable among the controls $X$ is a common effect instead of a common cause, its inclusion will bias the coefficient of interest. Moreover, this variable is likely to be highly correlated either with the outcome $Y$ or with the treatment $D$. In the latter case, this implies that post-double selection might include it in cases in which simple selection would have not. Therefore, in presence of bad control variables, doule-debiased machine learning might be even worse than simple pre-testing.\nIn short, as for any method, it is crucial to have a clear understanding of the method\u0026rsquo;s assumptions and to always check for potential violations.\nConclusion In this post, we have seen how to use post-double selection and, more generally, double debiased machine learning to get rid of an important source of bias: regularization bias.\nThis contribution by Victor Chernozhukov and co-authors has been undoubtedly one of the most relevant advances in causal inferences in the last decade. It is now widely employed in the industry and included in the most used causal inference packages, such as EconML (Microsoft) and causalml (Uber).\nIf you (understandably) feel the need for more material on double-debiased machine learning, but you do not feel like reading academic papers (also very understandable), here is a good compromise.\nIn this video lecture, Victor Chernozhukov himself presents the idea. The video lecture is relatively heavy on math and statistics, but you cannot get a more qualified and direct source than this!\nReferences [1] A. Belloni, D. Chen, V. Chernozhukov, C. Hansen, Sparse Models and Methods for Optimal Instruments With an Application to Eminent Domain (2012), Econometrica.\n[2] A. Belloni, V. Chernozhukov, C. Hansen, Inference on treatment effects after selection among high-dimensional controls (2014), The Review of Economic Studies.\n[3] V. Chernozhukov, D. Chetverikov, M. Demirer, E. Duflo, C. Hansen, W. Newey, J. Robins, Double/debiased machine learning for treatment and structural parameters (2018), The Econometrics Journal.\n[4] P. Hünermund, B. Louw, I. Caspi, Double Machine Learning and Automated Confounder Selection - A Cautionary Tale (2022), working paper.\nRelated Articles Double Debiased Machine Learning (part 1) Understanding Omitted Variable Bias Understanding The Frisch-Waugh-Lovell Theorem DAGs and Control Variables Code You can find the original Jupyter Notebook here:\nhttps://github.com/matteocourthoud/Blog-Posts/blob/main/pds.ipynb\n","date":1688947200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1688947200,"objectID":"da6e880b0debc7655a4d55efe6d86c28","permalink":"https://matteocourthoud.github.io/post/double_ml/","publishdate":"2023-07-10T00:00:00Z","relpermalink":"/post/double_ml/","section":"post","summary":"In the previous part of this blog post, we have seen how pre-testing can distort inference, i.e., how selecting control variables depending on their statistical significance results in wrong confidence intervals for the variable of interest.","tags":null,"title":"Double Debiased Machine Learning (part 2)","type":"post"},{"authors":null,"categories":null,"content":"An introduction to the delta method for inference on ratio metrics.\nWhen we run an experiment, we are often not only interested in the effect of a treatment (new product, new feature, new interface, \u0026hellip;) on revenue, but in it\u0026rsquo;s cost-effectiveness. In other words, is the investment worth the cost? Common examples include investments in computing resources, returns on advertisement, but also click-through rates and other ratio metrics.\nWhen we investigate causal effects, the gold standard is randomized control trials, a.k.a. AB tests. Randomly assigning the treatment to a subset of the population (users, patients, customers, \u0026hellip;) we ensure that, on average, the difference in outcomes can be attributed to the treatment. However, when the object of interest is cost-effectiveness, AB tests present some additional problems since we are not just interested in one treatment effect, but in the ratio of two treatment effects, the outcome of the investment over its cost.\nIn this post we are going to see how to analyze randomized experiments when the object of interest is the return on investment (ROI). We are going to explore alternative metrics to measure whether an investment paid off. We will also introduce a very powerful tool for inference with complex metrics: the delta method. While the algebra can be intense, the result is simple: we can compute the confidence interval for our ratio estimator using a simple linear regression.\nInvesting in Cloud Computing To better illustrate the concepts, we are going to use a toy example throughout the article: suppose we were an online marketplace and we wanted to invest in cloud computing: we want to increase the computing power behind our internal search engine, by switching to a higher tier server. The idea is that the faster search will improve the user experience, potentially leading to higher sales. Therefore, the question is: is the investment worth the cost? The object of interest is the return on investment (ROI).\nDifferently from usual AB tests or randomized experiments, we are not interested in a single causal effect, but in the ratio of two metrics: the effect on revenue and the effect on cost. We will still use a randomized control trial or AB test to estimate the ROI: we randomly assign groups of users to either the treatment or the control group. The treated users will benefit from the faster cloud machines, while the control users will use the old slower machines. Randomization ensures that we can estimate the impact of the new machines on either cost or revenue by comparing users in the treatment and control group: the difference in their average is an unbiased estimator of the average treatment effect. However, things are more complicated for their ratio.\nI import the data generating process dgp_cloud() from src.dgp. With respect to previous articles, I generated a new DGP parent class that handles randomization and data generation, while its children classes contain the specific use-cases. I also import some plotting functions and libraries from src.utils. To include not only code but also data and tables, I use Deepnote, a Jupyter-like web-based collaborative notebook environment.\n%matplotlib inline %config InlineBackend.figure_format = 'retina' from src.utils import * from src.dgp import dgp_cloud, DGP dgp = dgp_cloud(n=10_000) df = dgp.generate_data(seed_assignment=6) df.head() new_machine cost revenue 0 1 3.14 20.90 1 0 3.77 33.57 2 1 3.16 24.31 3 0 2.36 20.35 4 0 1.65 12.60 The data contains information on the total cost and revenue for a set of $10.000$ users over a period of a month. We also have information on the treatment: whether the search engine was running on the old or new machines. As it often happens with business metrics, both distributions of cost and revenues are very skewed. Moreover, most people do not buy anything and therefore generate zero revenue, even though they still use the platform, generating positive costs.\nfix, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5)) sns.histplot(df.cost, ax=ax1, color='C0').set(title='Distribution of Cost') sns.histplot(df.revenue, ax=ax2, color='C1').set(title='Distribution of Revenue'); We can compute the difference-in-means estimate for cost and revenue by regressing the outcome on the treatment indicator.\nsmf.ols('cost ~ new_machine', data=df).fit().summary().tables[1] coef std err t P\u003e|t| [0.025 0.975] Intercept 2.9617 0.043 69.034 0.000 2.878 3.046 new_machine 0.5152 0.060 8.563 0.000 0.397 0.633 The average cost has increased by $0.5152$$ per user. What about revenue?\nsmf.ols('revenue ~ new_machine', data=df).fit().summary().tables[1] coef std err t P\u003e|t| [0.025 0.975] Intercept 25.9172 0.425 60.950 0.000 25.084 26.751 new_machine 1.0664 0.596 1.788 0.074 -0.103 2.235 The average revenue per user has also increased, by $1.0664$$. So, was the investment profitable?\nTo answer this question, we first have to decide which metric to use as our outcome metric. In case of ratio metrics, this is not trivial.\nAverage Return or Return of the Average? It is very tempting to approach this problem saying: it is true that we have two variables, by we can just compute their ratio, and then analyze everything as usual, using a single variable: the individual level return.\n$$ \\rho_i = \\frac{\\text{individual revenue}}{\\text{individual cost}} = \\frac{R_i}{C_i} $$\nWhat happens if we analyze the experiment using this single metric?\ndf[\u0026quot;rho\u0026quot;] = df[\u0026quot;revenue\u0026quot;] / df[\u0026quot;cost\u0026quot;] smf.ols(\u0026quot;rho ~ new_machine\u0026quot;, df).fit().summary().tables[1] coef std err t P\u003e|t| [0.025 0.975] Intercept 6.6898 0.044 150.832 0.000 6.603 6.777 new_machine -0.7392 0.062 -11.893 0.000 -0.861 -0.617 The estimated effect is negative and significant, $-0.7392$! It seems like the the new machines were not a good investment, and the returns have decreased by $74%$.\nThis result seems to contradict our previous estimates. We have seen before that the revenue has increased on average more than the cost ($0.9505$ vs $0.5076$). Why is it the case? The problem is that we are giving the same weight to heavy users and light users. Let\u0026rsquo;s use a simple example with two users. The first one (blue) is a light user and before was costing $1$ $ and returning $10$ $, while now is costing $4$ $ and returning $20$ $. The other user (violet) is a heavy user and before was costing $10$ $ and returning $100$ $ and now is costing $20$ $ and returning $220$ $.\nThe average return is -3x: on average the return per user has decreased by $300%$. However, the total return per user is $1000%$: the increase in cost of $13$$ has generated $130$$ in revenue! The results are wildly different and entirely driven by the weight of the two users: the effect of the heavy user is low in relative terms but high in absolute terms, while it\u0026rsquo;s the opposite for the light user. The average relative effect is therefore mostly driven by the light user, while the relative average effect is mostly driven by the heavy user.\nWhich metric is more relevant in our setting? We talking about return on investment, we are usually interested in understanding whether we got a return on the money we spend. Therefore, the total return is more interesting than the average return.\nFrom now on, the object of interest will be the return on investment (ROI), given by the expected increase in revenue over the expected increase in cost, and we will denote it with the greek letter rho, $\\rho$.\n$$ \\rho = \\frac{\\text{incremental revenue}}{\\text{incremental cost}} = \\frac{\\mathbb E [\\Delta R]}{\\mathbb E [\\Delta C]} $$\nWe can estimate the ROI as the ratio of the two previous estimates: the average difference in revenue between the treatment and control group, over the average difference in cost between the treatment and control group.\n$$ \\hat{\\rho} = \\frac{\\mathbb E_n [\\Delta R]}{\\mathbb E_n [\\Delta C]} $$\nNote a subtle but crucial difference with respect to the previous formula: we have replaced the expected values $\\mathbb E$ with the empirical expectation operators $\\mathbb E_n$, also known as the sample average. The difference in notation is minimal, but the conceptual difference is huge. The first, $\\mathbb E$, is a theoretical concept, while the second, $\\mathbb E_n$, is empirical: it is a number that depends on the actual data. I personally like the notation since it highlights the close link between the two concepts (the second is the empirical counterpart of the first), while also making it clear that the second crucially depends on the sample size $n$.\ndef estimate_roi(df): Delta_C = df.loc[df.new_machine==1, \u0026quot;cost\u0026quot;].mean() - df.loc[df.new_machine==0, \u0026quot;cost\u0026quot;].mean() Delta_R = df.loc[df.new_machine==1, \u0026quot;revenue\u0026quot;].mean() - df.loc[df.new_machine==0, \u0026quot;revenue\u0026quot;].mean() return Delta_R / Delta_C estimate_roi(df) 2.0698235970047887 The estimate is $2.0698$: each additional dollar spent in the new machines translated in $2.0698$ extra dollars in revenue. Sounds great!\nBut how much should we trust this number? Is it significantly different form one, or it is just driven by noise?\nInference To answer this question, we would like to compute a confidence interval for our estimate. How do we compute a confidence interval for a ratio metric? The first step is to compute the standard deviation of the estimator. One method that is always available is the bootstrap: resample the data with replacement multiple times and use the distribution of the estimates over samples to compute the standard deviation of the estimator.\nLet\u0026rsquo;s try it in our case. I compute the standard deviation over $10.000$ bootstrapped samples, using the function pd.DataFrame().sample() with the options frac=1 to obtain a dataset of the same size and replace=True to sample with replacement.\nboot_estimates = [estimate_roi(df.sample(frac=1, replace=True, random_state=i)) for i in range(10_000)] np.std(boot_estimates) 0.9790730538161984 The bootstrap estimate of the standard deviation is equal to $0.979$. How good is it?\nSince we fully control the data generating process, we can simulate the \u0026ldquo;true\u0026rdquo; distribution of the estimator. We do that for $10.000$ simulations and we compute the resulting standard deviation of the estimator.\nnp.std(dgp.evaluate_f_redrawing_outcomes(estimate_roi, 10_000)) 1.0547776958025372 The estimated variance of the estimator using the \u0026ldquo;true\u0026rdquo; data generating process is slightly higher but very similar, around $1.055$.\nThe issue with the bootstrap is that it is very computational intense since it requires repeating the estimating procedure thousands of times. We are now going to explore another extremely powerful alternative that requires a single estimation step, the delta method. The delta method generally allows us to do inference on functions of random variable, therefore its applications are broader than ratios.\n⚠️ Warning: the next section is going to be algebra-intense. If you want, you can skip it and go straight to the last section.\nThe Delta Method What is the delta method? In short, it is an incredibly powerful asymptotic inference method for functions of random variables, that exploits Taylor expansions. In short, the delta method requires four ingredients\nOne or more random variables A function The Central Limit Theorem Taylor expansions I will assume some basic knowledge of all four concepts. Suppose we had a set of realizations $X_1$, \u0026hellip;, $X_n$ of a random variable that satisfy the requirements for the Central Limit Theorem (CLT): independence, identically distributions with expected value $\\mu$, and finite variance $\\sigma^2$. Under these conditions, the CLT tells us that the sample average $\\mathbb E_n[X]$ converges in distribution to a normal distribution, or more precisely\n$$ \\sqrt{n} \\ \\frac{ \\mathbb E_n[X] - \\mu}{\\sigma} \\ \\overset{D}{\\to} \\ N(0, 1) $$\nWhat does the equation mean? It reads \u0026ldquo;the normalized sample average, scaled by a factor $\\sqrt{n}$, converges in distribution to a standard normal distribution, i.e. it is approximately Gaussian for a sufficiently large sample.\nNow, suppose we were interested in a function of the sample average $f\\big(\\mathbb E_n[X]\\big)$. Note that this is different from the sample average of the function $\\mathbb E_n\\big[f(X)\\big]$. The delta method tells us what the function of the sample average converges to.\n$$ \\sqrt{n} \\ \\frac{ f\\big(\\mathbb E_n[X]\\big) - f(\\mu)}{\\sigma} \\ \\overset{D}{\\to} \\ N \\big(0, f\u0026rsquo;(\\mu)^2 \\big) $$\n, where $f\u0026rsquo;(\\mu)^2$ is the derivative of the function $f$, evaluated at $\\mu$.\nWhat is the intuition behind this formula? We now have a new term inside the expression of the variance, the squared first derivative $f\u0026rsquo;(\\mu)^2$ ($\\neq$ second derivative). If the derivative of the function is low, the variance decreases since different inputs translate into similar outputs. On the contrary, if the derivative of the function is high, the variance of the distribution is amplified, since different inputs translate into even more different outputs.\nThe result directly follows from the Taylor approximation of $f \\big(\\mathbb E_n[X]\\big)$\n$$ f\\big(\\mathbb E_n[X]\\big) = f(\\mu) + f\u0026rsquo;(\\mu) (\\mathbb E_n[X] - \\mu) + \\text{residual} $$\nImportantly, asymptotically, the last term disappears and the linear approximation holds exactly!\nHow is this connected to the ratio estimator? We need a bit more math and to switch from a single dimension to two dimensions in order to understand that. In our case, we have a bivariate function of two random variables, $\\Delta R$ and $\\Delta C$, which returns their ratio. In the case of a multivariate function $f$, the asymptotic variance of the estimator is given by\n$$ \\text{AVar} \\big( \\hat{\\rho} \\big) = \\nabla \\hat{\\rho}\u0026rsquo; \\Sigma_n \\nabla \\hat{\\rho} $$\nwhere, $\\nabla$ indicates the gradient of the function, i.e. the vector of directional derivatives, and $\\Sigma_n$ is the empirical variance-covariance matrix of $X$. In our case, they correspond to\n$$ \\nabla \\hat{\\rho} = \\begin{bmatrix} \\frac{1}{\\mathbb E_n [\\Delta C]} \\newline - \\frac{\\mathbb E_n [\\Delta R]}{\\mathbb E_n [\\Delta C]^2} \\end{bmatrix} $$\nand\n$$ \\Sigma_n = \\begin{bmatrix} \\text{Var}_n (\\Delta R) \u0026amp; \\text{Cov}_n (\\Delta R, \\Delta C) \\newline \\text{Cov}_n (\\Delta R, \\Delta C) \u0026amp; \\text{Var}_n (\\Delta C) \\newline \\end{bmatrix} $$\n, where the subscripts $n$ indicate the empirical counterparts, as for the expected value.\nCombining the previous three equations together with a little matrix algebra, we get the formula of the asymptotic variance of the return on investment estimator.\n$$ \\begin{align*} \\text{AVar} \\big( \\hat{\\rho} \\big) \u0026amp;= \\frac{1}{\\mathbb E_n[\\Delta C]^2} \\text{Var}_n(\\Delta R) - 2 \\frac{\\mathbb E_n[\\Delta R]}{\\mathbb E_n[\\Delta C]^3} \\text{Cov}_n(\\Delta R, \\Delta C) + \\frac{\\mathbb E_n[\\Delta R]^2}{\\mathbb E_n[\\Delta C]^4} \\text{Var}_n(\\Delta C) \\end{align*} $$\nSince the estimator is given by $\\hat{\\rho} = \\frac{\\mathbb E_n[\\Delta R]}{\\mathbb E_n[\\Delta C]}$, we can rewrite the asymptotic variance as\n$$ \\begin{align*} \\text{AVar} \\big( \\hat{\\rho} \\big) = \\frac{1}{\\mathbb E_n[\\Delta C]^2} \\text{Var}_n \\Big( \\Delta R - \\hat{\\rho} \\Delta C \\Big) \\end{align*} $$\nThe last expression is very interesting because it suggests that we can rewrite the asymptotic variance of our estimator as the variance of a difference-in-means estimator for a new auxiliary variable. In fact, we can rewrite the above expression as\n$$ \\begin{align*} \\text{AVar} \\big( \\hat{\\rho} \\big) = \\text{Var}_n \\Big( \\Delta \\tilde R \\Big) \\qquad \\text{where} \\quad \\tilde R = \\frac{R - \\hat{\\rho} \\ C}{| \\mathbb E [\\Delta C] |} \\end{align*} $$\nThis expression is incredibly useful because it gives us intuition and allows us to estimate the standard deviation of our estimator by linear regression.\nInference with Linear Regression Did you skip the previous section? No problem!\nAfter some algebra, we concluded that we can estimate the variance of a difference-in-means estimator for an auxiliary variable defined as\n$$ \\tilde R = \\frac{R - \\hat{\\rho} \\ C}{| \\mathbb E_n [\\Delta C] |} $$\nThis expression might seem obscure at first, but it is incredibly useful. In fact, it gives us (1) an intuitive interpretation of the variance of the estimator and (2) a practical way to estimate it.\nInterpretation first! How should we read the above expression? We can estimate the variance of the empirical estimator as the variance of a difference-in-means estimator, for a new variable $\\tilde R$ that we can easily compute from the data. We just need to take the revenue $R$, subtract the cost $C$ multiplied by the estimated ROI $\\rho$ and scale it down by the expected cost difference $|\\mathbb E_n[\\Delta C]|$. We can interpret this variable as the baseline revenue, i.e. the revenue not affected by the investment. The fact that it is scaled by the expected cost difference tells us that its variance will be decreasing in the total investment: the more we spend, the more precisely we can estimate the return on that expenditure.\nNow, let\u0026rsquo;s estimate the variance of the ROI estimator, in four steps.\nWe need to estimate the return on investment $\\hat \\rho$. rho_hat = estimate_roi(df) The term $| \\mathbb E_n[\\Delta C] |$ is the absolute difference in average cost between the treatment and control group. abs_Delta_C = np.abs(df.loc[df.new_machine==1, \u0026quot;cost\u0026quot;].mean() - df.loc[df.new_machine==0, \u0026quot;cost\u0026quot;].mean()) We now have all the ingredients to generate the auxiliary variable $\\tilde R$. df['revenue_tilde'] = (df['revenue'] - rho_hat * df['cost']) / abs_Delta_C The variance of the treatment-control difference $\\Delta \\tilde R$ can be directly computed by linear regression, as in randomized controlled trials for difference-in-means estimators (see Agrist and Pischke, 2008). smf.ols('revenue_tilde ~ new_machine', df).fit().summary().tables[1] coef std err t P\u003e|t| [0.025 0.975] Intercept 38.4067 0.653 58.771 0.000 37.126 39.688 new_machine -2.01e-14 0.917 -2.19e-14 1.000 -1.797 1.797 The estimated standard error of the ROI is $0.917$, very close to the bootstrap estimate of $0.979$ and the simulated value of $1.055$. However, with respect to bootstrapping, the delta method allowed us to compute it in a single step, making it sensibly faster (around $1000$ times on my local machine).\nNote that this estimated standard deviation implies a 95% confidence interval of $2.0698 +- 1.96 \\times 0.917$, equal to $[-0.2735, 3.8671]$. This might seem like good news since the confidence interval does not cover zero. However, note that in this case, a more interesting null hypothesis is that the ROI is equal to 1: we are breaking even. A value larger than 1 implies profits, while a value lower than 1 implies losses. In our case, we cannot reject the null hypothesis that the investment in new machines was not profitable.\nConclusion In this article, we have explored a very common causal inference problem: assessing the return on investment. Whether it\u0026rsquo;s a physical investment in new hardware, a virtual cost, or advertisement expenditure, we are interested in understanding whether this incremental cost has paid off. The additional complications come from the fact that we are studying not one, but two causal quantities, intertwined.\nWe have first explored and compared different outcome metrics to assess whether the investment paid off. Then, we have introduced an incredibly powerful method to do inference with complex random variables: the delta method. In the particular case of ratios, the delta method delivers a very insightful and practical functional form for the asymptotic variance of the estimator that can be estimated with a simple linear regression.\nReferences [1] A. Deng, U. Knoblich, J. Lu, Applying the Delta Method in Metric Analytics: A Practical Guide with Novel Ideas (2018).\n[2] R. Budylin, A. Drutsa, I. Katsev, V. Tsoy, Consistent Transformation of Ratio Metrics for Efficient Online Controlled Experiments (2018). ACM.\n[3] J. Angrist, J. Pischke, Mostly harmless econometrics: An empiricist\u0026rsquo;s companion (2009). Princeton university press.\nRelated Articles The Bayesian Bootstrap\nOutliers, Leverage, Residuals, and Influential Observations\nA/B Tests, Privacy, and Online Regression\nCode You can find the original Jupyter Notebook here:\nhttps://github.com/matteocourthoud/Blog-Posts/blob/main/notebooks/delta.ipynb\n","date":1688947200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1688947200,"objectID":"4253d081092abf61d2ff5390e4348a92","permalink":"https://matteocourthoud.github.io/post/delta_method/","publishdate":"2023-07-10T00:00:00Z","relpermalink":"/post/delta_method/","section":"post","summary":"An introduction to the delta method for inference on ratio metrics.\nWhen we run an experiment, we are often not only interested in the effect of a treatment (new product, new feature, new interface, \u0026hellip;) on revenue, but in it\u0026rsquo;s cost-effectiveness.","tags":null,"title":"Experiments on Returns on Investment","type":"post"},{"authors":null,"categories":null,"content":"In the decade preceding the Second World War, there was a massive increase in industrial production of war materials, so there was a need to ensure that products, especially munitions, were reliable. The testing of war materials is not only expensive but also destructive since, for example, bullets need to be fired in order to be tested.\nTherefore, the U.S. government was presented with the following dilemma: how many bullets should one fire out of a batch before declaring the batch reliable? Clearly, if we were to fire all the bullets, we would know the exact amount of functioning bullets in a crate. However, there would be no bullets left to use.\nBecause of the growing relevance of these statistical problems, in 1939, a group of prominent statisticians and economists joined forces at Columbia University\u0026rsquo;s Statistical Research Group (SGR). The group included, among others, W. Allen Wallis, Jacob Wolfowitz and Abraham Wald. According to Wallis himself the SGR group was \u0026ldquo;composed of what surely must be the most extraordinary group of statisticians ever organized, taking into account both number and quality\u0026rdquo;[2].\nTheir work was of first order importance and classified, to the point that Wallis reports:\nIt is said that as Wald worked on sequential analysis his pages were snatched away and given a security classification. Being still an \u0026ldquo;enemy alien\u0026rdquo;, he did not have a security clearance so, the story has it, he was not allowed to know of his results. [Wallis (1980)]\nIndeed, the group worked under the pressure from the U.S. Army to deliver fast practical solutions that could be readily deployed on the field. For example, Wallis reports that\nduring the Battle of the Bulge in December 1944, several high-ranking Army officers flew to Washington from the battle, spent a day discussing the best settings on proximity fuzes for air bursts of artillery shells against ground troops, and flew back to the battle to put into effect advice from, among others, Milton Friedman, whose earlier studies of the fuzes had given him extensive and accurate knowledge of the way the fuzes actually performed. [Wallis (1980)]\nThe most prominent result that came out of the SGR experience was undoubtedly the Sequential Probability Ratio Test. The idea first came to Wallis and Friedman that realized that\nit might pay to use a test which would not be as efficient as the classical tests if a sample of exactly N were to be taken, but which would more than offset this disadvantage by providing a good chance of terminating early when used sequentially. [Wallis (1980)]\nThe two economists exposed the idea to the statistician Jacob Wolfowitz who initially\nseemed to be something distasteful about the idea of people so ignorant of mathematics as Milton and I venturing to meddle with such sacred ideas as those of most powerful statistics, etc. No doubt this antipathy was strengthened by our calling the new tests \u0026ldquo;supercolossal\u0026rdquo; on the grounds that they are more powerful than \u0026ldquo;most powerful\u0026rdquo; tests. [Wallis (1980)]\nUltimately, the two economists managed to draw the attention of both Wolfowitz and Wald that started to formally work on the idea. The results remained top secret until the end of the war when Wald published his Sequential Tests of Statistical Hypotheses article.\nIn this post, after a quick introduction to hypothesis testing, we are going to explore the Sequential Probability Ratio Test and implement it in Python.\nHypothesis Testing When we design an A/B test or, more generally, an experiment, the standard steps are the following\nDefine a null hypothesis $H_0$, usually a zero effect of the experiment on a metric of interest\nfor example, no effect of a drug on mortality Define a significance level $\\alpha$, usually equal to 0.05, it represents the maximum probability of rejecting the null hypothesis when it is true\nfor example, the probability of claiming that the drug is effective in reducing mortality, when it\u0026rsquo;s not effective Define an alternative hypothesis $H_1$, usually the minimum effect size that we would like to detect\nfor example, a decrease in mortality by 1% Define a power level $1-\\beta$, usually equal to 0.8 ($\\beta=0.2$), it represents the minimum probability of rejecting the null hypothesis $H_0$, when the alternative $H_1$ is true\nfor example, the probability of claiming that the drug is ineffective, when it\u0026rsquo;s effective Pick a test statistic whose distribution is known under both hypotheses, usually the sample average of the metric of interest\nfor example, the average mortality rate of patients Compute the minimum sample size, in order to achieve the desired power level $1-\\beta$, given all the test parameters\nThen, we run the test and, depending on the realized value of the test statistic, we decide whether to reject the null hypothesis or not. In particular, we reject the null hypothesis if the p-value, i.e. the probability of observing under the null hypothesis a statistic as or more extreme than the sample statistic, is lower than the significance level $\\alpha$.\nRemember that rejecting the null hypothesis does not imply accepting the alternative hypothesis.\nPeeking Suppose that halfway through the experiment we were to peek at the data and notice that, for that intermediate value of the test statistic, we would reject the null hypothesis. Should we stop the experiment? If we do, what happens?\nThe answer is that we should not stop the experiment. If we do, the test would not achieve the desired significance level or, in other terms, our confidence intervals would have the wrong coverage.\nLet\u0026rsquo;s see what I mean with an example. Suppose our data generating process is a standard normal distribution with unknown mean $\\mu$ and known variance $\\sigma=1$: $X \\sim N(\\mu,1)$.\nThe hypothesis that we wish to test is\n$$ \\begin{align} H_0: \\quad \u0026amp; \\mu = 0 \\newline H_1: \\quad \u0026amp; \\mu = 0.1 \\end{align} $$\nAfter each observation $n$, we compute the z test statistic\n$$ z = \\frac{\\bar X_n - \\mu_0}{\\frac{\\sigma}{\\sqrt{n}}} = \\frac{\\bar X_n - 0}{\\frac{1}{\\sqrt{n}}} = \\bar X_n * \\sqrt{n} $$\nwhere $\\bar X_n$ is the sample mean from a sample $X_1, X_2, \u0026hellip;, X_n$, of size $n$, $\\sigma$ is the standard deviation of the population, and $\\mu_0$ is the population mean, under the null hypothesis. The term in the denominator, $\\frac{\\sigma}{\\sqrt{n}}$, is the variance of the sample mean. Under the null hypothesis of zero mean, the test statistic is distributed as a standard normal distribution with zero mean and unit variance, $N(0,1)$.\nLet\u0026rsquo;s code the test in Python. I import some code from utils to make the plots prettier.\n%matplotlib inline %config InlineBackend.figure_format = 'retina' from src.utils import * zstat = lambda x: np.mean(x) * np.sqrt(len(x)) zstat.__name__ = 'z-statistic' Suppose we want a test with significance level $\\alpha=0.05$ and power $1-\\beta=0.8$. What sample size $n$ do we need?\nWe need a sample size such that\nThe probability of rejecting the null hypothesis $H_0$, when $H_0$ is true, is at most $\\alpha=0.05$ The probability of not rejecting the null hypothesis $H_0$, when $H_0$ is false (i.e. $H_1$ is true), is at most $\\beta=0.2$ I.e. we need to find a critical value $c$ such that\n$c = \\mu_0 + z_{0.95} * \\frac{\\sigma}{\\sqrt{n}}$ $c = \\mu_1 - z_{0.8} * \\frac{\\sigma}{\\sqrt{n}}$ where $z_{p}$ is the CDF inverse (or percent point function) at $p$, and $\\mu_i$ are the values of the mean under the different hypotheses.\nIf we do not know the sign of the unknown mean $\\mu$, we have to run a two-sided test. This means that the maximum probability of type 1 error on each side of the distribution has to be $\\alpha/2 = 0.025$, implying $z_{0.975} = 1.96$.\nCombining the two expressions together we can solve for the required minimum sample size.\n$$ n : \\mu_0 + z_{0.975} * \\frac{\\sigma}{\\sqrt{n}} = \\mu_1 - z_{0.8} * \\frac{\\sigma}{\\sqrt{n}} $$\nso that\n$$ n = \\left( \\sigma * \\frac{z_{0.975} + z_{0.8}}{\\mu_0 + \\mu_1} \\right)^2 = \\left( 1 * \\frac{1.96 + 0.84}{0 + 0.1} \\right)^2 = 784.89 $$\nfrom scipy.stats import norm n = ( (norm.ppf(0.975) + norm.ppf(0.8)) / 0.1 )**2 print(f\u0026quot;Minimum sample size: {n}\u0026quot;) Minimum sample size: 784.8879734349086 We need at least 785 observations.\nWe can get a better intuition by graphically plotting the two distributions with the critical value. I wrote a function plot_test to draw a standard hypothesis testing setting.\nfrom src.figures import plot_test plot_test(mu0=0, mu1=0.1, alpha=0.05, n=n) The critical value is such that, given the distributions under the two hypothesis, the rejection area in red is equal to $\\alpha$. The sample size $n$ is such that it shrinks the variance of the two distributions so that the area in green is equal to $\\beta$.\nLet\u0026rsquo;s now simulate an experiment in which we draw an ordered sequence of observations and, after each observation, we compute the value of the test statistic.\ndef experiment(f_stat, mu=0, n=785, seed=1): np.random.seed(seed) # Set seed I = np.arange(1, n+1) # Observation index x = np.random.normal(mu, 1, n) # Observation value stat = [f_stat(x[:i]) for i in I] # Value of the test statistic so far df = pd.DataFrame({'i': I, 'x': x, f_stat.__name__: stat}) # Generate dataframe return df Let\u0026rsquo;s have a look at what a sample looks like.\ndf = experiment(zstat) df.head() i x z-statistic 0 1 1.624345 1.624345 1 2 -0.611756 0.716009 2 3 -0.528172 0.279678 3 4 -1.072969 -0.294276 4 5 0.865408 0.123814 We can now plot the time trend of the test statistic as we accumulate observations during the sampling process. I also mark with horizontal lines the values for rejection of the null hypothesis of a test with $\\alpha = 0.05$: $z_{0.025} = -1.96$ and $z_{0.975} = 1.96$.\ndef plot_experiment(df, ybounds, **kwargs): sns.lineplot(data=df, x='i', y=df.columns[2], **kwargs) for ybound in ybounds: sns.lineplot(x=df['i'], y=ybound, lw=1.5, color='black') plt.title(f'{df.columns[2]} with sequential sampling') plt.yticks([0, ybounds[0], ybounds[1]]) plot_experiment(df, ybounds=[-1.96, 1.96]) In this case, the test never crosses the critical values. Therefore, peeking does not have an effect. We would not have stopped the experiment prematurely.\nWhat would happen if we were repeating the experiment many times? Since the data is generated under the null hypothesis, $H_0: \\mu = 0$, we expect to reject it only $\\alpha=5%$ of the times.\nLet\u0026rsquo;s simulate the data-generating process $K=100$ times.\ndef simulate_experiments(f_stat, ybounds, xmin=0, early_stop=False, mu=0, K=100, n=785, **kwargs): # Count experiment durations stops = np.zeros(K) * n # Perform K simulations for k in range(K): # Draw data df = experiment(f_stat, mu=mu, seed=k, n=n) vals = df[f_stat.__name__].values # If early stop, check early violations (during sampling) if early_stop: violations = (vals[xmin:] \u0026gt; max(ybounds)) + (vals[xmin:] \u0026lt; min(ybounds)) if early_stop and any(violations): end = 1 + xmin + np.where(violations)[0][0] plot_experiment(df.iloc[:end, :], ybounds, **kwargs) stops[k] = end * np.sign(df[f_stat.__name__].values[end]) # Otherwise, only check violations of last value elif (vals[-1] \u0026gt; max(ybounds)) or (vals[-1] \u0026lt; min(ybounds)): plot_experiment(df, ybounds, **kwargs) stops[k] = len(df) * np.sign(vals[-1]) # Plot all other observations in grey else: plot_experiment(df, ybounds, color='grey', alpha=0.1, lw=1) # Print diagnostics pct_up = sum(stops\u0026gt;0)/sum(stops!=0)*100 print(f'Bounds crossed: {sum(stops!=0)} ({pct_up:.0f}% upper, {100-pct_up:.0f}% lower)') print(f'Average experiment duration: {(sum(np.abs(stops)) + n*sum(stops==0))/ len(stops) :.0f}') We plot the distribution of the z-statistic over samples .\nsimulate_experiments(zstat, ybounds=[-1.96, 1.96], early_stop=False); Bounds crossed: 3 (67% upper, 33% lower) Average experiment duration: 785 In the figure above, I have highlighted the experiments for which we reject the null hypothesis without peeking, i.e. given the value of the z test statistic at the end of the sampling process. Only in 3 experiments the final value lies outside the critical values, so that we reject the null hypothesis. This means a rejection rate of 3% which is very close to the expected rejection rate of $\\alpha=0.05$ (under the null).\nWhat if instead we were impatient and, after collecting the first 100 observations, we were stopping as soon as we saw the z-statistic crossing the boundaries?\nstops_zstat_h0 = simulate_experiments(zstat, xmin=99, ybounds=[-1.96, 1.96], early_stop=True, lw=2); plt.vlines(100, ymin=plt.ylim()[0], ymax=plt.ylim()[1], color='k', lw=1, ls='--'); Bounds crossed: 29 (45% upper, 55% lower) Average experiment duration: 644 In the figure above, I have highlighted the experiments in which the values of the z-statistic crosses one of the boundaries, from the 100th observation onwards. This happens in 29 simulations out of 100, which implies a rejection rate of 25%, which is very far from the expected rejection rate of $\\alpha=0.05$ (under the null hypothesis). Peaking distorts the significance level of the test.\nPotential solutions are:\nsequential probability ratio tests sequential triangular testing group sequential testing Before analyzing these sequential testing procedures, we first need to introduce the likelihood ratio test.\nLikelihood Ratio Test The likelihood ratio test is a test that tries to assess the likelihood that the observed data was generated by either one of two competing statistical models. In order to perform the likelihood ratio test for hypothesis testing, we need the data generating process to be fully specified under both hypotheses. For example, this would be the case with the following hypotheses:\n$$ \\begin{align} H_0: \\quad \u0026amp; \\mu=0 \\newline H_1: \\quad \u0026amp; \\mu=0.1 \\end{align} $$\nIn this case, we say that the statistical test is fully specified. If the alternative hypothesis was $H_1: \\mu \\neq 0$, then the data generating process would not be specified under the alternative hypothesis. When a statistical test is fully specified, we can compute the likelihood ratio as the the ratio of the likelihood function under the two hypotheses.\n$$ \\Lambda (X) = \\frac{\\mathcal L (\\theta_1 \\ | \\ X)}{\\mathcal L (\\theta_0 \\ | \\ X)} $$\nThe likelihood-ratio test provides a decision rule as follows:\nIf $\\Lambda\u0026gt;c$, reject $H_{0}$; If $\\Lambda\u0026lt;c$, do not reject $H_{0}$; If $\\Lambda =c$, reject with probability $q$ The values $c$ and $q$ are usually chosen to obtain a specified significance level $\\alpha$.\nThe Neyman–Pearson lemma states that this likelihood-ratio test is the most powerful among all level $\\alpha$ tests for this case.\nSpecial Case: testing mean of normal distribution Let\u0026rsquo;s go back to our example where data is coming from a normal distribution with unknown mean $\\mu$ and known variance $\\sigma^2$ and we want to perform the following test\n$$ \\begin{align} H_0: \\quad \u0026amp; \\mu = 0 , \\newline H_1: \\quad \u0026amp; \\mu = 0.1 \\end{align} $$\nThe likelihood of the normal distribution with unknown mean $\\mu$ and known variance $\\sigma^2$ is\n$$ \\mathcal L(\\mu) = \\left( \\frac{1}{\\sqrt{2 \\pi} \\sigma } \\right)^n e^{- \\sum_{i=1}^{n} \\frac{(X_i - \\mu)^2}{2 \\sigma^2}} $$\nSo that the likelihood ratio under the two hypotheses is\n$$ \\Lambda(X) = \\frac{\\mathcal L (0.1, \\sigma^2)}{\\mathcal L (0, \\sigma^2)} = \\frac{e^{- \\sum_{i=1}^{n} \\frac{(X_i - 0.1)^2}{2 \\sigma^2}}}{e^{- \\sum_{i=1}^{n} \\frac{(X_i)^2}{2 \\sigma^2}}} $$\nWe now have all the ingredients to move on to the final purpose of this blog post: the Sequential Probability Ratio Test.\nSequential Probability Ratio Test Given a pair of fully specified hypotheses, say $H_{0}$ and $H_{1}$, the first step of the sequential probability ratio test is to calculate the log-likelihood ratio test $\\log (\\Lambda_{i})$, as new data arrive: with $S_{0}=0$, then, for $i=1,2,\u0026hellip;,$\n$$ S_{i} = S_{i-1} + \\log(\\Lambda_{i}) $$\nThe stopping rule is a simple thresholding scheme:\n$S_{i}\\geq b$: Accept $H_{1}$ $S_{i}\\leq a$: Accept $H_{0}$ $a\u0026lt;S_{i}\u0026lt;b$: continue monitoring (critical inequality) where $a$ and $b$ ($-\\infty\u0026lt;a\u0026lt;0\u0026lt;b\u0026lt;\\infty$) depend on the desired type I and type II errors, $\\alpha$ and $\\beta$.\nWald (1945) shows that the choice of the following boundaries delivers a test with expected probability of type 1 and 2 error not greater than $\\alpha$ and $\\beta$, respectively.\n$$ a \\approx \\log {\\frac {\\beta }{1-\\alpha }} \\quad \\text{and} \\quad b \\approx \\log {\\frac {1-\\beta }{\\alpha }} $$\nThe equations are approximations because of the discrete nature of the data generating process.\nWald and Wolfowitz (1948) have proven that a test with these boundaries is the most powerful sequential probability ratio test, i.e. all SPR tests with the same power and significance require at least the same amount of observations.\nSpecial Case: testing null effect Let\u0026rsquo;s go back to our example where data is coming from a normal distribution with unknown mean $\\mu$ and known variance $\\sigma^2$ and hypotheses $H_0: \\ \\mu = 0$ and $H_1: \\ \\mu = 0.1$.\nWe have seen that the likelihood ratio with a sample of size $n$ is\n$$ \\Lambda(X) = \\frac{\\mathcal L (0.1, \\sigma^2)}{\\mathcal L (0, \\sigma^2)} = \\frac{e^{- \\sum_{i=1}^{n} \\frac{(X_i - 0.1)^2}{2 \\sigma^2}}}{e^{- \\sum_{i=1}^{n} \\frac{(X_i)^2}{2 \\sigma^2}}} $$\nTherefore, the log-likelihood (easier to compute) is\n$$ \\log (\\Lambda(X)) = \\left( \\sum_{i=1}^{n} \\frac{(X_i)^2}{2 \\sigma^2} \\right) - \\left( \\sum_{i=1}^{n} \\frac{(X_i - 0.1)^2}{2 \\sigma^2} \\right) $$\nSimulation We are now ready to perform some simulations. First, let\u0026rsquo;s code the log likelihood ratio test statistic that we have just computed.\nlog_lr = lambda x: (np.sum((x)**2) - np.sum((x-0.1)**2) ) / 2 log_lr.__name__ = 'log likelihood-ratio' We now repeat the same experiment we did at the beginning, with one difference: we will compute the log likelihood ratio as a statistic. The data generating process has $\\mu=0$, as under the null hypothesis.\ndf = experiment(log_lr, ) df.head() i x log likelihood-ratio 0 1 1.624345 0.157435 1 2 -0.611756 0.091259 2 3 -0.528172 0.033442 3 4 -1.072969 -0.078855 4 5 0.865408 0.002686 Let\u0026rsquo;s now compute the optimal bounds, given significance level $\\alpha=0.05$ and power $1-\\beta=0.8$.\nalpha = 0.05 beta = 0.2 a = np.log( beta / (1-alpha) ) b = np.log( (1-beta) / alpha ) print(f'Optimal bounds : [{a:.3f}, {b:.3f}]') Optimal bounds : [-1.558, 2.773] Since significance and (one minus) power are different, the bound for the null hypothesis is much closer than the bound for the alternative hypothesis. This means that, in case of an intermediate effect of $\\mu=0.05$, we will be more likely to accept the null hypothesis $H_0: \\mu = 0$ than the alternative $H_1: \\mu = 0.1$.\nWe can plot the distribution of the likelihood ratio over samples drawn under the null hypothesis $H_0: \\mu = 0$.\nplot_experiment(df, ybounds=[a,b]) In this particular case, the test is inconclusive within our sampling framework. We need to collect more data in order to come to a decision.\nplot_experiment(experiment(log_lr, n=789), ybounds=[a,b]); It takes 789 observations to reach to a conclusion, while before the sample size was 785. This test procedure can require a larger sample size than the previous one. Is it true on average?\nWhat would happen if we were to repeat the experiment $K=100$ times?\nsimulate_experiments(log_lr, ybounds=[a, b], early_stop=True, lw=1.5); Bounds crossed: 96 (4% upper, 96% lower) Average experiment duration: 264 We get a decision for 96 simulations out of 100 and for 96% of them, it\u0026rsquo;s the correct decision. Therefore, our rejection rate is very close to the expected $\\alpha=0.05$ (under the null hypothesis).\nHowever, for 4 experiments, the test is inconclusive. What would happen if we were to sample until we reach a conclusion in each experiment?\nsimulate_experiments(log_lr, ybounds=[a,b], early_stop=True, lw=1.5, n=1900); Bounds crossed: 100 (4% upper, 96% lower) Average experiment duration: 275 As we can see from the plot, in one particularly unlucky experiment, we need to collect 1900 observations before coming to a conclusion. However, despite this outlier, the average experiment duration is an astounding 275 samples, almost a third of the original sample size of 785.\nWhat would happen if instead the alternative hypothesis $H_1: \\mu = 0.1$ was true?\nsimulate_experiments(log_lr, ybounds=[a,b], early_stop=True, mu=0.1, lw=1, n=2100); Bounds crossed: 100 (84% upper, 16% lower) Average experiment duration: 443 In this case, we make the correct decision in only 84% of the simulations, which is very close to the expected value of 80% (under the alternative hypothesis), i.e. the power of the experiment, 1-β.\nMoreover, also under the alternative hypothesis we need a significantly lower sample size: just 443 observation, on average.h a conclusion in 78/100 experiments we need just 1/3 of the samples!\nConclusion In this post, we have seen the dangers of peeking during a randomized experiment. Prematurely stopping a test can be dangerous since it distorts inference, biasing the expected rejection rates.\nDoes it mean that we always need to perform tests with a pre-specified sample size? No! There exist procedures that allow for optimal stopping. These procedures were born for a specific purpose: reducing the sample size as much as possible, without sacrificing accuracy. The first and most known is the Sequential Probability Ratio Test, defined by Wallis as \u0026ldquo;the most powerful and seminal statistical ideas of the past third of a century\u0026rdquo; (in 1980).\nThe SPRT was not only a powerful tool during war time but keeps being used today for very practical purposes (see for example Netflix, Uber).\nReferences [1] A. Wald, Sequential tests of statistical hypotheses (1945), The Annal of Mathematical Statistics.\n[2] A. Wald and J Wolfowitz, Optimum character of the sequential probability ratio test (1948), The Annals of Mathematical Statistics.\n[3] W. A. Wallis, The Statistical Research Group, 1942–1945 (1980), Journal of the American Statistical Association.\nCode You can find the original Jupyter Notebook here:\nhttps://github.com/matteocourthoud/Blog-Posts/blob/main/optimal_stopping.ipynb\n","date":1688947200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1688947200,"objectID":"0b40b79a667a0362644e1579efb424d3","permalink":"https://matteocourthoud.github.io/post/optimal_stopping/","publishdate":"2023-07-10T00:00:00Z","relpermalink":"/post/optimal_stopping/","section":"post","summary":"In the decade preceding the Second World War, there was a massive increase in industrial production of war materials, so there was a need to ensure that products, especially munitions, were reliable.","tags":null,"title":"Experiments, Peeking, and Optimal Stopping","type":"post"},{"authors":null,"categories":null,"content":"How to use regression trees to do policy targeting.\nIn my previous blog post, we have seen how to use causal trees to estimate heterogeneous treatment effects of a policy. If you haven\u0026rsquo;t read it, I recommend starting there first, since we are going to take the content of that article for granted and start from there.\nWhy heterogenous treatment effects (HTE)? The estimation of heterogeneous treatments effects is important because it allows us to do targeting. Knowing which customers are more likely to react to a discount allows a company to spend less money by offering fewer but better targeted discounts. This works also for negative effects: knowing for which patients a certain drug has side effects allows a pharmaceutical company to warn or exclude them from the treatment. There is also a more subtle advantage of estimating heterogeneous treatment effects: knowing for whom a treatment works allows us to better understand how a treatment works. Knowing that the effect of a discount does not depend on the income of its recipient but rather by its buying habits tells us that maybe it is not a matter of money, but rather a matter of attention or loyalty.\nIn this article, we will explore an extention of causal trees: causal forests. Exactly as random forests extend regression trees by averaging multiple bootstrapped trees together, causal forests extend causal trees. The main difference comes from the inference perspective, which is less straighforward. We are also going to see how to compare outputs of different HTE estimation algorithms and how to use them for policy targeting.\nOnline Discounts For the rest of the article, we resume the toy example used in the causal trees article: we assume we are an online store and we are interested in understanding whether offering discounts to new customers increases their expenditure in the store.\nTo understand whether and how much the discounts are effective we run an A/B test: whenever a new user visits our online store, we randomly decide whether to offer them the discount or not. I import the data-generating process dgp_online_discounts() from src.dgp. I also import some plotting functions and libraries from src.utils. To include not only code but also data and tables, I use Deepnote, a Jupyter-like web-based collaborative notebook environment.\n%matplotlib inline %config InlineBackend.figure_format = 'retina' from src.utils import * from src.dgp import dgp_online_discounts dgp = dgp_online_discounts(n=100_000) df = dgp.generate_data() df.head() time device browser region discount spend 0 10.78 mobile edge 9 0 0.46 1 0.57 desktop firefox 9 1 11.04 2 3.74 mobile safari 7 0 1.81 3 13.37 desktop other 5 0 31.90 4 0.71 mobile explorer 2 0 15.42 We have data on 100.000 store visitors, for whom we observe the time of the day the acessed the website, the device they use, their browser, and their geographical region. We also see whether they were offered the discount, our treatment, and what is their spend, the outcome of interest.\nSince the treatment was randomly assigned, we can use a simple difference-in-means estimator to estimate the treatment effect. We expect the treatment and control group to be similar, except for the discount, therefore we can causally attribute any difference in spend to the discount.\nsmf.ols('spend ~ discount', df).fit().summary().tables[1] coef std err t P\u003e|t| [0.025 0.975] Intercept 5.0306 0.045 110.772 0.000 4.942 5.120 discount 1.9492 0.064 30.346 0.000 1.823 2.075 The discount seems to be effective: on average the spend in the treatment group increases by 1.95$. But are all customers equally affected?\nTo answer this question, we would like to estimate heterogeneous treatment effects, possibly at the individual level.\nCausal Forests There are many different options to compute heterogeneous treatment effects. The simplest one is to interact the outcome of interest with a dimension of heterogeneity. The problem with this approach is which variable to pick. Sometimes we have prior information that might guide out actions; for example, we might know that mobile users on average spend more than desktop users. Other times, we might be interested in one dimension for business reasons; for example we might want to invest more in a certain region. However, when we do not extra information we would like this process to be data-driven.\nIn the previous article we have explored one data-drive approach to estimate heterogeneous treatment effects: causal trees. We will now expand them to causal forests. However, before we start, we have to give an introduction to its non-causal cousing: random forests.\nRandom forests, as the name suggests, are an extension of regression trees, adding two separate sources of randomness of top of them. In particular, a random forest algorithm takes the predictions of many different regression trees, each trained on a bootstrapped sample of the data, and averages them together. This procedure is generally known as bagging, boostrap-aggregating, and can be applied to any prediction algorithm and is not specific to random forest. The additional source of randomness comes from feature selection since at each split, only a random subset of all the features $X$ is considered for the optimal split.\nThese two extra sources of randomness are extremely important and controbute to a superior performance of random forests. First of all, bagging allows random forests to produce smoother prediction than regression trees by averaging multiple discrete predictions. Random feature selection instead allows random forests to explore the feature space more in depth, allowing it to discover more interations than simple regression trees. In fact, there might be interactions between variables that are on their own not very predictive (and therefore would not generate splits) but together very powerful.\nCausal Forests are the equivalent of random forests, but for the estimation of heterogeneous treatment effects, exaxtly as for causal trees and regression trees. Exactly as for Causal Trees, we have a fundamental problem: we are interested in predicting an object that we do not observe: the individua treatment effects $\\tau_i$. The solution is to create an auxiliary outcome variable $Y^*$ whose expected value for each single observation is exactly the treatment effect.\n$$ Y_i^* = \\frac{Y_i}{D_i \\cdot p(X_i) - (1-D_i) \\cdot (1-p(X_i))} $$\nIf you want to know more details on why this variable is unbiased for the individual treatment effect, have a look at my previous post where I go more in detail. In short, you can interpret $Y_i^*$ as the difference-in-means estimator for a single observation.\nOnce we have an outcome variable, there are still a couple of things we need to do in order to use Random Forests to estimate heterogeneous treatment effects. First, we need to build trees that have an equal number of treated and control units in each leaf. Second, we need to use different samples to build the tree and evaluate it, i.e. compute the average outcome per leaf. This procedure is often referred to as honest trees and it\u0026rsquo;s extremely helpful for inference, since we can treat the sample of each leaf as independent from the tree structure.\nBefore we go into the estimation, let\u0026rsquo;s first generate dummy variables for our categorical variables, device, browser and region.\ndf_dummies = pd.get_dummies(df[dgp.X[1:]], drop_first=True) df = pd.concat([df, df_dummies], axis=1) X = ['time'] + list(df_dummies.columns) We can now estimate the heterogeneous treatment effects using the Random Forest algorithm. Luckily, we don\u0026rsquo;t have to do all this by hand, but there is a great implementation of Causal Trees and Forests in Microsoft\u0026rsquo;s EconML package. We will use the CausalForestDML function. We set a seed for reproducibility.\nfrom econml.dml import CausalForestDML np.random.seed(0) forest_model = CausalForestDML(max_depth=3) forest_model = forest_model.fit(Y=df[dgp.Y], X=df[X], T=df[dgp.D]) Differently from Causal Trees, Causal Forests are harder to interpret since we cannot visualize every single tree. We can use the SingleTreeCateInterpreter function to plot an equivalent representation of the Causal Forest algorithm.\nfrom econml.cate_interpreter import SingleTreeCateInterpreter %matplotlib inline intrp = SingleTreeCateInterpreter(max_depth=2).interpret(forest_model, df[X]) intrp.plot(feature_names=X, fontsize=12) We can interpret the tree diagram exactly as for the Causal Tree model. On the top, we can see the average $Y^*$ in the data, $1.917$. Starting from there, the data gets split into different branches, according to the rules highlighted at the top of each node. For example, the first node splits the data into two groups of size $46878$ and $53122$ depending on whether the time is later than $11.295$. At the bottom, we have our final partitions, with the predicted values. For example, the leftmost leaf contains $40191$ observation with time earlier than $11.295$ and non-Safari browser, for which we predict a spend of $0.264$. Darker node colors indicate higher prediction values.\nThe problem with this representation is that, differently from the case of Causal Trees, it is only an interpretation of the model. Since Causal Forests are made of many bootstrapped trees, there is no way to directly inspect each decision tree. One way to understand which feature is most important in detemining the tree split is the so-called feature importance.\nfig, ax = plt.subplots() sns.barplot(x=X, y=forest_model.feature_importances()[0], color='C0').set( title='Feature Importances', ylabel='Importance') ax.set_xticklabels(ax.get_xticklabels(), rotation=45, ha=\u0026quot;right\u0026quot;); Clearly time is the first dimension of heterogeneity, followed by device (mobile in particular) and browser (safari in particular). Other dimensions do not matter much.\nLet\u0026rsquo;s now check the model performance.\nPerformance Since we control the data generating process, we can do something that is not possible with real data: check the predicted treatment effects against the true ones. The generate_potential_outcomes() function loads the data with both potential outcomes for each observation, under both treatment (outcome_t) and control (outcome_c). Let\u0026rsquo;s start first by evaluating how well the algorithm predicts the effects along the discrete dimensions of the data.\ndef compute_discrete_effects(df, hte_model): temp_df = df.copy() temp_df.time = 0 temp_df = dgp.add_treatment_effect(temp_df) temp_df = temp_df.rename(columns={'effect_on_spend': 'True'}) temp_df['Predicted'] = hte_model.effect(temp_df[X]) df_effects = pd.DataFrame() for var in X[1:]: for effect in ['True', 'Predicted']: v = temp_df.loc[temp_df[var]==1, effect].mean() - temp_df[effect][temp_df[var]==0].mean() effect_var = {'Variable': [var], 'Effect': [effect], 'Value': [v]} df_effects = pd.concat([df_effects, pd.DataFrame(effect_var)]).reset_index(drop=True) return df_effects, temp_df['Predicted'].mean() df_effects, avg_effect_notime = compute_discrete_effects(df, forest_model) fig, ax = plt.subplots() sns.barplot(data=df_effects, x=\u0026quot;Variable\u0026quot;, y=\u0026quot;Value\u0026quot;, hue=\u0026quot;Effect\u0026quot;, ax=ax).set( xlabel='', ylabel='', title='Heterogeneous Treatment Effects') ax.set_xticklabels(ax.get_xticklabels(), rotation=45, ha=\u0026quot;right\u0026quot;); The Causal Forest algorithm is pretty good at predicting the treatment effects related to the categorical variables. As for Causal Trees, this is expected since the algorithm has a very discrete nature. However, differently from Causal Trees, the predictions are more nuanced.\nWe can now do a more relevant test: how well the algorithm performs with a continuous variable such as time? First, let\u0026rsquo;s again isolate the predicted treatment effects on time and ignore the other covariates.\ndef compute_time_effect(df, hte_model, avg_effect_notime): df_time = df.copy() df_time[[X[1:]] + ['device', 'browser', 'region']] = 0 df_time = dgp.add_treatment_effect(df_time) df_time['predicted'] = hte_model.effect(df_time[X]) + avg_effect_notime return df_time df_time = compute_time_effect(df, forest_model, avg_effect_notime) We now plot the predicted treatment effects against the true ones, along the time dimension.\nsns.scatterplot(x='time', y='effect_on_spend', data=df_time, label='True') sns.scatterplot(x='time', y='predicted', data=df_time, label='Predicted').set( ylabel='', title='Heterogeneous Treatment Effects') plt.legend(title='Effect'); We can now fully appreciate the difference between Causal Trees and Forests: while in the case of Causal Trees the estimates were essentially a very coarse step function, we can now see how Causal Forests produce smoother estimates.\nWe have now explored the model, it\u0026rsquo;s time to use it!\nPolicy Suppose that we were considering offering a 4$ discount to new customers that visit our online store.\ncost = 4 For which customers is the discount effective? We have estimated an average treatment effect of 1.9492$ which means that the discount is not really profitable on average. However, we are now able to target single individuals and we can offer the discount only to a subset of the incoming customers. We will now explore how to do policy targeting and in order to get a better understanding of the quality of the targeting, we will use the Causal Tree model as a reference point.\nWe build a Causal Tree using the same CausalForestDML function but restricting the number of estimators and the forest size to 1.\nfrom econml.dml import CausalForestDML np.random.seed(0) tree_model = CausalForestDML(n_estimators=1, subforest_size=1, inference=False, max_depth=3) tree_model = tree_model.fit(Y=df[dgp.Y], X=df[X], T=df[dgp.D]) Next, we split the dataset into a train and a test set. The idea is very similar to cross-validation: we use the training set to train the model - in our case the estimator for the heterogeneous treatment effects - and the test set to assess its quality. The main difference is that we do not observe the true outcome in the test dataset. But we can still use the train-test split to compare in-sample predictions with out-of-sample predictions.\nWe put 80% of all observations in the training set and 20% in the test set.\ndf_train, df_test = df.iloc[:80_000, :], df.iloc[20_000:,] First, let\u0026rsquo;s retrain the models only on the training sample.\nnp.random.seed(0) tree_model = tree_model.fit(Y=df_train[dgp.Y], X=df_train[X], T=df_train[dgp.D]) forest_model = forest_model.fit(Y=df_train[dgp.Y], X=df_train[X], T=df_train[dgp.D]) Now we can decide on a targeting policy, i.e. decide to which customers we offer the discount. The answer seems simple: we offer the discount to all the customers for whom we anticipate a treatment effect larger than the cost, 4$.\nA visualization tool that allows us to understand on whom the treatment is effective and how, is the so-called Treatment Operative Characteristic (TOC) curve. The name is remindful of the much more famous receiver operating characteristic (ROC) curve that plots the true positive rate against the false positive rate for different thresholds of a binary classifier. The idea is similar: we plot the average treatment effect for different shares of the treated population. At one extreme, when all customers are treated, and the curve takes value equal to the average treatement effect, while at the other extreme, when only one customer is treated, and the curve takes value equal to the maximum treatment effect.\nNow let\u0026rsquo;s compute the curve.\ndef compute_toc(df, hte_model, cost, truth=False): df_toc = pd.DataFrame() for q in np.linspace(0, 1, 101): if truth: df = dgp.add_treatment_effect(df_test) effect = df['effect_on_spend'] else: effect = hte_model.effect(df[X]) ate = np.mean(effect[effect \u0026gt;= np.quantile(effect, 1-q)]) temp = pd.DataFrame({'q': [q], 'ate': [ate]}) df_toc = pd.concat([df_toc, temp]).reset_index(drop=True) return df_toc df_toc_tree = compute_toc(df_train, tree_model, cost) df_toc_forest = compute_toc(df_train, forest_model, cost) Now we can plot the Treatment Operating Curves for the two CATE estimators.\ndef plot_toc(df_toc, cost, ax, color, title): ax.axhline(y=cost, lw=2, c='k') ax.fill_between(x=df_toc.q, y1=cost, y2=df_toc.ate, where=(df_toc.ate \u0026gt; cost), color=color, alpha=0.3) if any(df_toc.ate \u0026gt; cost): q = df_toc_tree.loc[df_toc.ate \u0026gt; cost, 'q'].values[-1] else: q = 0 ax.axvline(x=q, ymin=0, ymax=0.36, lw=2, c='k', ls='--') sns.lineplot(data=df_toc, x='q', y='ate', ax=ax, color=color).set( title=title, ylabel='ATT', xlabel='Share of treated', ylim=[1.5, 8.5]) ax.text(0.7, cost+0.1, f'Discount cost: {cost:.0f}$', fontsize=12) fix, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 6)) plot_toc(df_toc_tree, cost, ax1, 'C0', 'TOC - Causal Tree') plot_toc(df_toc_forest, cost, ax2, 'C1', 'TOC - Causal Forest') As expected, the TOC curve is decreasing for both estimators since the average effect decreases as we increase the share of treated customers. In other words, the more selective we are in releasing discounts, the higher the effect of the coupon, per customer. I have also plotted an horizontal line with the discount cost so that we can interpret the shaded area below the TOC curve and above the cost line as the expected profits.\nThe two algorims predict a similar share of treated, around 20%, with the Causal Forest algorithm targeting slightly more customers. However, they predict very different profits. The Causal Tree algorithm predicts a small and constant margin, while the Causal Forest algorithm predicts a larger and steeper margin. Which algorithm is more accurate?\nIn order to compare them, we can evaluate them in the test set. We take the model trained on the training set, we predict the treatment effects and we compare them with the predictions from a model trained on the test set. Note that, differently from machine learning standard testing procedures, there is a substantial difference: in our case, we cannot evaluate our predictions against the ground truth, since the treatment effects are not observed. We can only compare two predictions with each other.\ndef compute_effect_test(df_test, hte_model, cost, ax, title, truth=False): df_test['Treated'] = hte_model.effect(df_test[X]) \u0026gt; cost if truth: df_test = dgp.add_treatment_effect(df_test) df_test['Effect'] = df_test['effect_on_spend'] else: np.random.seed(0) hte_model_test = copy.deepcopy(hte_model).fit(Y=df_test[dgp.Y], X=df_test[X], T=df_test[dgp.D]) df_test['Effect'] = hte_model_test.effect(df_test[X]) df_test['Cost Effective'] = df_test['Effect'] \u0026gt; cost tot_effect = ((df_test['Effect'] - cost) * df_test['Treated']).sum() sns.barplot(data=df_test, x='Cost Effective', y='Treated', errorbar=None, width=0.5, ax=ax, palette=['C3', 'C2']).set( title=title + '\\n', ylim=[0,1]) ax.text(0.5, 1.08, f'Total effect: {tot_effect:.2f}', fontsize=14, ha='center') return fix, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5)) compute_effect_test(df_test, tree_model, cost, ax1, 'Causal Tree') compute_effect_test(df_test, forest_model, cost, ax2, 'Causal Forest') It seems that the Causal Tree model performs better than the Causal Forest model, with a total net effect of $8386$$ against $4948$$. From the plot we can also understand the source of the discrepancy. The Causal Forest algorithm tends to be more restrictive and treat fewer customers, making no false positives but also having a lot of false negatives. On the other hand, the Causal Tree algorithm, is much more generous and distributes the discount to mamy more new customers. This translates in both more true positives but also false positives. The net effect seem to favor the causal tree algorithm.\nNormally, we would stop here since there is not much more we can do. However, in our case, we have access to the true data generating process. Therefore we can check the ground-truth accuracy of the two algorithms.\nFirst, let\u0026rsquo;s compare them in terms of prediction error of the treatment effects. For each algorithm we compute the mean squared error of the treatment effects.\nfrom sklearn.metrics import mean_squared_error as mse def compute_mse_test(df_test, hte_model): df_test = dgp.add_treatment_effect(df_test) print(f\u0026quot;MSE = {mse(df_test['effect_on_spend'], hte_model.effect(df_test[X])):.4f}\u0026quot;) compute_mse_test(df_test, tree_model) compute_mse_test(df_test, forest_model) MSE = 0.9035 MSE = 0.5555 The Random Forest model better predicts the average treatment effect, with a mean squared error of $0.5555$ instead of $0.9035$.\nDoes this map into a better targeting? We can now replicate the same barplot we did above, to understand how well the two algorithms perform in terms of policy targeting.\nfix, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5)) compute_effect_test(df_test, tree_model, cost, ax1, 'Causal Tree', True) compute_effect_test(df_test, forest_model, cost, ax2, 'Causal Forest', True) The plot is very similar, but the result differ substantially. In fact, the Causal Forest algorithm now outperforms the Causal Tree algorithm with a total effect of $10395$$ compared to $8828$$. Why this sudden difference?\nTo better understand the source of the discrepancy let\u0026rsquo;s plot the TOC based on the ground truth.\ndf_toc = compute_toc(df_test, tree_model, cost, True) fix, ax = plt.subplots(1, 1, figsize=(7, 5)) plot_toc(df_toc, cost, ax, 'C2', 'TOC - Ground Truth') As we can see, the TOC is very skewed and there exist a few customers with very high average treatment effects. The Random Forest algorothm is better able to indentify them and therefore is overall more effective, despite targeting fewer customers.\nConclusion In this post, we have seen a very powerful algorithm for the estimation of heterogeneous treatment effects: causal forests. Causal forests are built on the same principle of causal trees, but benefit from a much deeper exploration of the parameter space and bagging.\nWe have also seen how to use the estimates of the heterogeneous treatment effects to perform policy targeting. By identifying users with the highest treatment effects, we are able to make profitable a policy that wouldn\u0026rsquo;t be otherwise. We have also see how the objective of policy targeting might differ from the objective of heterogeneous treatment effect estimation since the tails of the distribution might be more relevant than the average.\nReferences S. Athey, G. Imbens, Recursive partitioning for heterogeneous causal effects (2016), PNAS.\nS. Wager, S. Athey, Estimation and Inference of Heterogeneous Treatment Effects using Random Forests (2018), Journal of the American Statistical Association.\nS. Athey, J. Tibshirani, S. Wager, Generalized Random Forests (2019). The Annals of Statistics.\nM. Oprescu, V. Syrgkanis, Z. Wu, Orthogonal Random Forest for Causal Inference (2019). Proceedings of the 36th International Conference on Machine Learning.\nRelated Articles DAGs and Control Variables\nMatching, Weighting, or Regression?\nUnderstanding Meta Learners\nUnderstanding AIPW, the Doubly-Robust Estimator\nUnderstanding Causal Trees\nCode You can find the original Jupyter Notebook here:\nhttps://github.com/matteocourthoud/Blog-Posts/blob/main/notebooks/causal_forests.ipynb\n","date":1688947200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1688947200,"objectID":"59e96353605e7508a8766be21fb75aeb","permalink":"https://matteocourthoud.github.io/post/causal_forests/","publishdate":"2023-07-10T00:00:00Z","relpermalink":"/post/causal_forests/","section":"post","summary":"How to use regression trees to do policy targeting.\nIn my previous blog post, we have seen how to use causal trees to estimate heterogeneous treatment effects of a policy. If you haven\u0026rsquo;t read it, I recommend starting there first, since we are going to take the content of that article for granted and start from there.","tags":null,"title":"From Causal Trees to Forests","type":"post"},{"authors":null,"categories":null,"content":"When we want to visualize the relationship between two continuous variables, the go-to plot is the scatterplot. It\u0026rsquo;s a very intuitive visualization tool that allows us to directly look at the data. However, when we have a lot of data and/or when the data is skewed, scatterplots can be too noisy to be informative.\nIn this blog post, I am going to review a very powerful alternative to the scatterplot to visualize correlations between two variables: the binned scatterplot. Binned scatterplots are not only a great visualization tool, but they can also be used to do inference on the conditional distribution of the dependent variable.\nThe Scatterplot Let\u0026rsquo;s start with an example. Suppose we are an online marketplace where multiple firms offer goods that consumer can efficiently browse, compare and buy. Our dataset consists in a snapshot of the firms active on the marketplace.\nLet\u0026rsquo;s load the data and have a look at it. You can find the code for the data generating process here.\n%matplotlib inline %config InlineBackend.figure_format = 'retina' from src.utils import * from src.dgp import dgp_marketplace df = dgp_marketplace().generate_data(N=10_000) df.head() age sales online products 0 0.312777 450.858091 0 2 1 1.176221 1121.882449 1 3 2 1.764048 2698.714549 0 1 3 1.082742 1627.746386 0 3 4 3.156503 1464.593939 0 2 We have information on 10.000 firms. For each firm we know:\nage: the age of the firm sales: the monthly sales from last month online: whether the firm is only active online products: the number of products that the firm offers Suppose we are interested in understanding the relationship between age and sales. What is the life-cycle of sales?\nLet\u0026rsquo;s start with a simple scatterplot of sales over age.\nsns.scatterplot(x='age', y='sales', data=df); plt.title(\u0026quot;Sales by firm's age\u0026quot;); The plot is extremely noisy. We have a lot of observations, therefore, it is very difficult to visualize them all. If we had to guess, we could say that the relationship looks negative (sales decrease with age), but it would be a very uninformed guess.\nWe are now going to explore some plausible tweaks and alternatives.\nScatterplot Alternatives What can we do when we have an extremely dense scatterplot? One solution could be to plot the density of the observations, instead of the observations themselves.\nThere are multiple solutions in Python to visualize the density of a 2-dimensional distribution. A very useful one is seaborn jointplot. jointplot plots the joint distribution of two variables, together with the marginal distributions along the axis. The default option is the scatterplot, but one can also choose to add a regression line (reg), change the plot to a histogram (hist), a hexplot (hex), or a kernel density estimate (kde).\nLet\u0026rsquo;s try the hexplot, which is basically a histogram of the data, where the bins are hexagons, in the 2-dimensional space.\ns = sns.jointplot(x='age', y='sales', data=df, kind='hex', ); s.ax_joint.grid(False); s.ax_marg_y.grid(False); s.fig.suptitle(\u0026quot;Sales by firm's age\u0026quot;); Not much has changed. It looks like the distributions of age and sales are both very skewed and, therefore, most of the action is concentrated in a very small subspace.\nMaybe we could remove outliers and zoom-in on the area where most of the data is located. Let\u0026rsquo;s zoom-in on the bottom-left corner, on observations what have age \u0026lt; 3 and sales \u0026lt; 3000.\ns = sns.jointplot(x='age', y='sales', data=df.query(\u0026quot;age \u0026lt; 3 \u0026amp; sales \u0026lt; 3000\u0026quot;), kind=\u0026quot;hex\u0026quot;); s.ax_joint.grid(False); s.ax_marg_y.grid(False); s.fig.suptitle(\u0026quot;Sales by firm's age\u0026quot;); Now there is much less empty space, but it does not look like we are going far. The joint distribution is still too skewed. This is the case when the data follows some power distribution, as it\u0026rsquo;s often the case with business data.\nOne solution is to transform the variable, by taking the natural logarithm.\ndf['log_age'] = np.log(df['age']) df['log_sales'] = np.log(df['sales']) We can now plot the relationship between the logarithms of age and sales.\ns = sns.jointplot(x='log_age', y='log_sales', data=df, kind='hex'); s.ax_joint.grid(False); s.ax_marg_y.grid(False); s.fig.suptitle(\u0026quot;Sales by firm's age\u0026quot;, y=1.02); The logarithm definitely helped. Now the data is more spread across space, which means that the visualization is more informative. Moreover, it looks like there is no relationship between the two variables.\nHowever, there is still too much noise. Maybe data visualization alone is not sufficient do draw a conclusion.\nLet\u0026rsquo;s swap to a more structured approach: linear regression. Let\u0026rsquo;s linearly regress log_sales on log_age.\nsmf.ols('log_sales ~ log_age', df).fit().summary().tables[1] coef std err t P\u003e|t| [0.025 0.975] Intercept 7.3971 0.015 478.948 0.000 7.367 7.427 log_age 0.1690 0.010 16.888 0.000 0.149 0.189 The regression coefficient for log_age is positive and statistically significant (i.e. different from zero). It seems that all previous visualizations were very misleading. From none of the graphs above we could have guessed such a strong positive relationship.\nHowever, maybe this relationship is different for online-only firms and the rest of the sample. We need to control for this variable in order to avoid Simpson\u0026rsquo;s Paradox and, more generally, bias.\nWith linear regression, we can condition the analysis on covariates. Let\u0026rsquo;s add the binary indicator for online-only firms and the variable counting the number of products to the regression.\nsmf.ols('log_sales ~ log_age + online + products', df).fit().summary().tables[1] coef std err t P\u003e|t| [0.025 0.975] Intercept 6.5717 0.037 176.893 0.000 6.499 6.644 log_age 0.0807 0.010 7.782 0.000 0.060 0.101 online 0.1447 0.027 5.433 0.000 0.092 0.197 products 0.3456 0.014 24.110 0.000 0.317 0.374 The coefficient for log_age is still positive and statistically significant, but its magnitude has halved.\nWhat should we conclude? It seems that sales increase over age, on average. However, this pattern might be very non-linear.\nWithin the linear regression framework, one approach could be to add extra terms such as polynomials (age^2) or categorical features (e.g. age \u0026lt; 2). However, it would be really cool if there was a more flexible (i.e. non-parametric) approach that could inform us on the relationship between firm age and sales.\nIf only\u0026hellip;\nThe Binned Scatterplot The binned scatterplot is a very powerful tool that provides a flexible and parsimonious way of visualizing and summarizing conditional means (and not only) in large datasets.\nThe idea behind the binned scatterplot is to divide the conditioning variable, age in our example, into equally sized bins or quantiles, and then plot the conditional mean of the dependent variable, sales in our example, within each bin.\nDetails Cattaneo, Crump, Farrell, Feng (2021) have built an extremely good package for binned scatterplots in R, binsreg. Moreover, they have ported the package to Python. We can install binsreg directly from pip using pip install binsreg. You can find more information on the Python package here, while the original and detailed R package documentation can be found here.\nThe most important choice when building a binned scatterplot is the number of bins. The trade-off is the usual bias-variance trade-off. By picking a higher number of bins, we have more points in the graph. In the extreme, we end up having a standard scatterplot (assuming the conditioning variable is continuous). On the other hand, by decreasing the number bins, the plot will be more stable. However, in the extreme, we will have a single point representing the sample mean.\nCattaneo, Crump, Farrell, Feng (2021) prove that, in the basic binned scatterplot, the number of bins that minimizes the mean squared error is proportional to $n^{1/3}$, where $n$ is the number of observations. Therefore, in general, more observations lead to more bins.\nStarr and Goldfarb (2020) add the following consideration:\n\u0026ldquo;However other elements are also important. For example, holding the distribution of x constant, the more curvilinear the true relationship between x and y is, the more bins the algorithm will select (otherwise mean squared error will increase). This implies that even with large n, few bins will be chosen for relatively flat relationships. The calculation of the optimal number of bins in a basic binned scatterplot thus takes into account the amount and location of variation in the data available to identify the relationship between x and y.\u0026rdquo;\nIt is strongly recommended to use the default optimal number of bins. However, one can also set a customized number of bins in binsreg with the nbins option.\nBinned scatterplots however, do not just compute conditional means, for optimally chosen intervals, but they can also provide inference for these means. In particular, we can build confidence intervals around each data point. In the binsreg package, the option ci adds confidence intervals to the estimation results. The option takes as input a tuple of parameters (p, s) and uses a piecewise polynomial of degree p with s smoothness constraints to construct the confidence intervals. By default, the confidence intervals are not included in the plot. For what concerns the choice of p and s, the package documentation reports:\n\u0026ldquo;Recommended specification is ci=c(3,3), which adds confidence intervals based on cubic B-spline estimate of the regression function of interest to the binned scatter plot.\u0026rdquo;\nBinsreg One problem with the Python version of the package, is that is not very Python-ish. Therefore, I have wrapped the binsreg package into a function binscatter that takes care of cleaning and formatting the output in a nicely readable Pandas DataFrame.\nimport binsreg def binscatter(**kwargs): # Estimate binsreg est = binsreg.binsreg(**kwargs) # Retrieve estimates df_est = pd.concat([d.dots for d in est.data_plot]) df_est = df_est.rename(columns={'x': kwargs.get(\u0026quot;x\u0026quot;), 'fit': kwargs.get(\u0026quot;y\u0026quot;)}) # Add confidence intervals if \u0026quot;ci\u0026quot; in kwargs: df_est = pd.merge(df_est, pd.concat([d.ci for d in est.data_plot])) df_est = df_est.drop(columns=['x']) df_est['ci'] = df_est['ci_r'] - df_est['ci_l'] # Rename groups if \u0026quot;by\u0026quot; in kwargs: df_est['group'] = df_est['group'].astype(df[kwargs.get(\u0026quot;by\u0026quot;)].dtype) df_est = df_est.rename(columns={'group': kwargs.get(\u0026quot;by\u0026quot;)}) return df_est We can now proceed to estimate and visualize the binned scatterplot for age based on sales.\n# Estimate binsreg df_est = binscatter(x='age', y='sales', data=df, ci=(3,3)) df_est.head() group age bin isknot mid sales ci_l ci_r ci 0 Full Sample 0.016048 0 0 0 1653.865445 1362.722061 1893.998686 531.276626 1 Full Sample 0.049295 1 0 0 1666.329034 1486.504692 1890.922562 404.417871 2 Full Sample 0.086629 2 0 0 1937.095012 1727.248438 2124.811346 397.562909 3 Full Sample 0.125955 3 0 0 1972.484136 1801.125187 2243.034755 441.909568 4 Full Sample 0.167636 4 0 0 2142.560866 1937.677738 2405.785562 468.107824 The binscatter function outputs a dataset in which, for each bin of the conditioning variable, age, we have values and confidence intervals for the outcome variable, sales.\nWe can now plot the estimates.\n# Plot binned scatterplot sns.scatterplot(x='age', y='sales', data=df_est); plt.errorbar('age', 'sales', yerr='ci', data=df_est, ls='', lw=3, alpha=0.2); plt.title(\u0026quot;Sales by firm's age\u0026quot;); The plot is quite revealing. Now the relationship looks extremely non-linear with a sharp increase in sales at the beginning of the lifetime of a firm, followed by a plateau.\nMoreover, the plot is also telling us information regarding the distributions of age and sales. In fact, the plot is more dense on the left, where the distribution of age is concentrated. Also, confidence intervals are tighter on the left, where most of the conditional distribution of sales lies.\nAs we already discussed in the previous section, it might be important to control for other variables. For example, the number of products, since firms that sell more products probably survive longer in the markets and also make more sales.\nbinsreg allows to condition the analysis on any number of variables, with the w option.\n# Estimate binsreg df_est = binscatter(x='age', y='sales', w=['products'], data=df, ci=(3,3)) # Plot binned scatterplot sns.scatterplot(x='age', y='sales', data=df_est); plt.errorbar('age', 'sales', yerr='ci', data=df_est, ls='', lw=3, alpha=0.2); plt.title(\u0026quot;Sales by firm's age\u0026quot;); Conditional on number of products, the shape of the sales life-cycle changes further. Now, after an initial increase in sales, we observe a gradual decrease over time.\nDo online-only firms have different sales life-cycles with respect to mixed online-offline firms? We can produce different binned scatterplots by group using the option by.\n# Estimate binsreg df_est = binscatter(x='age', y='sales', by='online', w=['products'], data=df, ci=(3,3)) # Plot binned scatterplot sns.scatterplot(x='age', y='sales', data=df_est, hue='online'); plt.errorbar('age', 'sales', yerr='ci', data=df_est.query(\u0026quot;online==0\u0026quot;), ls='', lw=3, alpha=0.2); plt.errorbar('age', 'sales', yerr='ci', data=df_est.query(\u0026quot;online==1\u0026quot;), ls='', lw=3, alpha=0.2); plt.title(\u0026quot;Sales by firm's age\u0026quot;); From the binned scatterplot, we can see that online products have on average shorter lifetimes, with a higher initial peak in sales, followed by a sharper decline.\nConclusion In this blog post, we have analyzed a very powerful data visualization tool: the binned scatterplot. In particular, we have seen how to use the binsreg package to automatically pick the optimal number of bins and perform non-parametric inference on conditional means. However, the binsreg package offers much more than that and I strongly recommend checking its manual more in depth.\nReferences [1] E Starr, B Goldfarb, Binned Scatterplots: A Simple Tool to Make Research Easier and Better (2020), Strategic Management Journal.\n[2] M. D. Cattaneo, R. K. Crump, M. H. Farrell, Y. Feng, On Binscatter (2021), working paper.\n[3] P. Goldsmith-Pinkham, Lecture 6. Linear Regression II: Semiparametrics and Visualization, Applied Metrics PhD Course.\n","date":1688947200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1688947200,"objectID":"a344e22cdf5228dc6dc1b5e2f207b7f9","permalink":"https://matteocourthoud.github.io/post/binned_scatterplot/","publishdate":"2023-07-10T00:00:00Z","relpermalink":"/post/binned_scatterplot/","section":"post","summary":"When we want to visualize the relationship between two continuous variables, the go-to plot is the scatterplot. It\u0026rsquo;s a very intuitive visualization tool that allows us to directly look at the data.","tags":null,"title":"Goodbye Scatterplot, Welcome Binned Scatterplot","type":"post"},{"authors":null,"categories":null,"content":"A complete guide to comparing distributions, from visualization to statistical tests\nComparing the empirical distribution of a variable across different groups is a common problem in data science. In particular, in causal inference the problem often arises when we have to assess the quality of randomization.\nWhen we want to assess the causal effect of a policy (or UX feature, ad campaign, drug, \u0026hellip;), the golden standard in causal inference are randomized control trials, also known as A/B tests. In practice, we select a sample for the study and we randomly split it into a control and a treatment group, and we compare the outcomes between the two groups. Randomization ensures that only difference between the two groups is the treatment, on average, so that we can attribute outcome differences to the treatment effect.\nThe problem is that, despite randomization, the two groups are never identical. However, sometimes, they are not even \u0026ldquo;similar\u0026rdquo;. For example, we might have more males in one group, or older people, etc.. (we usually call these characteristics, covariates or control variables). When it happens, we cannot be certain anymore that the difference in the outcome is only due to the treatment and cannot be attributed to the inbalanced covariates instead. Therefore, it is always important, after randomization, to check whether all observed variables are balanced across groups and whether there are no systematic differences. Another option, to be certain ex-ante that certain covariates are balanced, is stratified sampling.\nIn this blog post, we are going to see different ways to compare two (or more) distributions and assess the magnitude and significance of their difference. We are going to consider two different approaches, visual and statistical. The two approaches generally trade-off intuition with rigor: from plots we can quickly assess and explore differences, but it\u0026rsquo;s hard to tell whether these differences are systematic or due to noise.\nExample Let\u0026rsquo;s assume we need to perform an experiment on a group of individuals and we have randomized them into a treatment and control group. We would like them to be as comparable as possible, in order to attribute any difference between the two groups to the treatment effect alone. We also have divided the treatment group in different arms for testing different treatments (e.g. slight variations of the same drug).\nFor this example, I have simulated a dataset of 1000 individuals, for whom we observe a set of characteristics. I import the data generating process dgp_rnd_assignment() from src.dgp and some plotting functions and libraries from src.utils.\n%matplotlib inline %config InlineBackend.figure_format = 'retina' from src.utils import * from src.dgp import dgp_rnd_assignment df = dgp_rnd_assignment().generate_data() df.head() Group Arm Gender Age Income 0 control NaN 0 29.0 568.44 1 control NaN 1 32.0 596.45 2 treatment arm 3 0 29.0 380.86 3 control NaN 0 25.0 476.38 4 treatment arm 4 1 32.0 628.28 We have information on $1000$ individuals, for which we observe gender, age and weekly income. Each individual is assigned either to the treatment or control group and treated individuals are distributed across four treatment arms.\nTwo Groups - Plots Let\u0026rsquo;s start with the simplest setting: we want to compare the distribution of income across the treatment and control group. We first explore visual approaches and the statistical approaches. The advantage of the first is intuition while the advantage of the second is rigor.\nFor most visualizations I am going to use Python\u0026rsquo;s seaborn library.\nBoxplot A first visual approach is the boxplot. The boxplot is a good trade-off between summary statistics and data visualization. The center of the box represents the median while the borders represent the first (Q1) and third quartile (Q3), respectively. The whiskers instead, extend to the first data points that are more than 1.5 times the interquartile range (Q3 - Q1) outside the box. The points that fall outside of the whiskers are plotted individually and are usually considered outliers.\nTherefore, the boxplot provides both summary statistics (the box and the whiskers) and direct data visualization (the outliers).\nsns.boxplot(data=df, x='Group', y='Income'); plt.title(\u0026quot;Boxplot\u0026quot;); It seems that the income distribution in the treatment group is slightly more dispersed: the orange box is larger and its whiskers cover a wider range. However, the issue with the boxplot is that it hides the shape of the data, telling us some summary statistics but not showing us the actual data distribution.\nHistogram The most intuitive way to plot a distribution is the histogram. The histogram groups the data into equally wide bins and plots the number of observations within each bin.\nsns.histplot(data=df, x='Income', hue='Group', bins=50); plt.title(\u0026quot;Histogram\u0026quot;); There are multiple issues with this plot:\nSince the two groups have a different number of observations, the two histograms are not comparable The number of bins is arbitrary We can solve the first issue using the stat option to plot the density instead of the count and setting the common_norm option to False to use the same normalization.\nsns.histplot(data=df, x='Income', hue='Group', bins=50, stat='density', common_norm=False); plt.title(\u0026quot;Density Histogram\u0026quot;); Now the two histograms are comparable!\nHowever, an important issue remains: the size of the bins is arbitrary. In the extreme, if we bunch the data less, we end up with bins with at most one observation, if we bunch the data more, we end up with a single bin. In both cases, if we exaggerate, the plot loses informativeness. This is a classical bias-variance trade-off.\nKernel Density One possible solution is to use a kernel density function that tries to approximate the histogram with a continuous function, using kernel density estimation (KDE).\nsns.kdeplot(x='Income', data=df, hue='Group', common_norm=False); plt.title(\u0026quot;Kernel Density Function\u0026quot;); From the plot, it seems that the estimated kernel density of income has \u0026ldquo;fatter tails\u0026rdquo; (i.e. higher variance) in the treatment group, while the average seems similar across groups.\nThe issue with kernel density estimation is that it is a bit of a black-box and might mask relevant features of the data.\nCumulative Distribution A more transparent representation of the two distribution is their cumulative distribution function. At each point of the x axis (income) we plot the percentage of data points that have an equal or lower value. The main advantages of the cumulative distribution function are that\nwe do not need to make any arbitrary choice (e.g. number of bins) we do not need to perform any approximation (e.g. with KDE), but we represent all data points sns.histplot(x='Income', data=df, hue='Group', bins=len(df), stat=\u0026quot;density\u0026quot;, element=\u0026quot;step\u0026quot;, fill=False, cumulative=True, common_norm=False); plt.title(\u0026quot;Cumulative distribution function\u0026quot;); How should we interpret the graph?\nSince the two lines cross more or less at 0.5 (y axis), it means that their median is similar\nSince the orange line is above the blue line on the left and below the blue line on the left, it means that the distribution of the treatment group as fatter tails\nQQ Plot A related method is the qq-plot, where q stands for quantile. The qq-plot plots the quantiles of the two distributions against each other. If the distributions are the same, we should get a 45 degree line.\nThere is no native qq-plot function in Python and, while the statsmodels package provides a qqplot function, it is quite cumbersome. Therefore, we will do it by hand.\nFirst, we need to compute the quartiles of the two groups, using the percentile function.\nincome = df['Income'].values income_t = df.loc[df.Group=='treatment', 'Income'].values income_c = df.loc[df.Group=='control', 'Income'].values df_pct = pd.DataFrame() df_pct['q_treatment'] = np.percentile(income_t, range(100)) df_pct['q_control'] = np.percentile(income_c, range(100)) Now we can plot the two quantile distributions against each other, plus the 45-degree line, representing the benchmark perfect fit.\nplt.figure(figsize=(8, 8)) plt.scatter(x='q_control', y='q_treatment', data=df_pct, label='Actual fit'); sns.lineplot(x='q_control', y='q_control', data=df_pct, color='r', label='Line of perfect fit'); plt.xlabel('Quantile of income, control group') plt.ylabel('Quantile of income, treatment group') plt.legend() plt.title(\u0026quot;QQ plot\u0026quot;); The qq-plot delivers a very similar insight with respect to the cumulative distribution plot: income in the treatment group has the same median (lines cross in the center) but wider tails (dots are below the line on the left end and above on the right end).\nTwo Groups - Tests So far, we have seen different ways to visualize differences between distributions. The main advantage of visualization is intuition: we can eyeball the differences and intuitively assess them.\nHowever, we might want to be more rigorous and try to assess the statistical significance of the difference between the distributions, i.e. answer the question \u0026ldquo;is the observed difference systematic or due to sampling noise?\u0026rdquo;.\nWe are now going to analyze different tests to discern two distributions from each other.\nT-test The first and most common test is the student t-test. T-tests are generally used to compare means. In this case, we want to test whether the means of the income distribution is the same across the two groups. The test statistic for the two-means comparison test is given by:\n$$ stat = \\frac{|\\bar x_1 - \\bar x_2|}{\\sqrt{s^2 / n }} $$\nWhere $\\bar x$ is the sample mean and $s$ is the sample standard deviation. Under mild conditions, the test statistic is asymptotically distributed as a student t distribution.\nWe use the ttest_ind function from scipy to perform the t-test. The function returns both the test statistic and the implied p-value.\nfrom scipy.stats import ttest_ind stat, p_value = ttest_ind(income_c, income_t) print(f\u0026quot;t-test: statistic={stat:.4f}, p-value={p_value:.4f}\u0026quot;) t-test: statistic=-1.5549, p-value=0.1203 The p-value of the test is $0.12$, therefore we do not reject the null hypothesis of no difference in means across treatment and control groups.\nNote: the t-test assumes that the variance in the two samples is the same so that its estimate is computed on the joint sample. Welch’s t-test allows for unequal variances in the two samples.\nStandardized Mean Difference (SMD) In general, it is good practice to always perform a test for difference in means on all variables across the treatment and control group, when we are running a randomized control trial or A/B test.\nHowever, since the denominator of the t-test statistic depends on the sample size, the t-test has been criticized for making p-values hard to compare across studies. In fact, we may obtain a significant result in an experiment with very small magnitude of difference but large sample size while we may obtain a non-significant result in an experiment with large magnitude of difference but small sample size.\nOne solution that has been proposed is the standardized mean difference (SMD). As the name suggests, this is not a proper test statistic, but just a standardized difference, which can be computed as:\n$$ SMD = \\frac{|\\bar x_1 - \\bar x_2|}{\\sqrt{(s^2_1 + s^2_2) / 2}} $$\nUsually a value below $0.1$ is considered a \u0026ldquo;small\u0026rdquo; difference.\nIt is good practice to collect average values of all variables across treatment and control group and a measure of distance between the two — either the t-test or the SMD — into a table that is called balance table. We can use the create_table_one function from the causalml library to generate it. As the name of the function suggests, the balance table should always be the first table you present when performing an A/B test.\nfrom causalml.match import create_table_one df['treatment'] = df['Group']=='treatment' create_table_one(df, 'treatment', ['Gender', 'Age', 'Income']) Control Treatment SMD Variable n 704 296 Age 32.40 (8.54) 36.42 (7.76) 0.4928 Gender 0.51 (0.50) 0.58 (0.49) 0.1419 Income 524.59 (117.35) 538.75 (160.15) 0.1009 In the first two columns, we can see the average of the different variables across the treatment and control groups, with standard errors in parenthesis. In the last column, the values of the SMD indicate a standardized difference of more than $0.1$ for all variables, suggesting that the two groups are probably different.\nMann–Whitney U Test An alternative test is the Mann–Whitney U test. The null hypothesis for this test is that the two groups have the same distribution, while the alternative hypothesis is that one group has larger (or smaller) values than the other.\nDifferently from the other tests we have seen so far, the Mann–Whitney U test is agnostic to outliers and concentrates on the center of the distribution.\nThe test procedure is the following.\nCombine all data points and rank them (in increasing or decreasing order)\nCompute $U_1 = R_1 - n_1(n_1 + 1)/2$, where $R_1$ is the sum of the ranks for data points in the first group and $n_1$ is the number of points in the first group.\nCompute $U_2$ similarly for the second group.\nThe test statistic is given by $stat = min(U_1, U_2)$.\nUnder the null hypothesis of no systematic rank differences between the two distributions (i.e. same median), the test statistic is asymptotically normally distributed with known mean and variance.\nThe intuition behind the computation of $R$ and $U$ is the following: if the values in the first sample were all bigger than the values in the second sample, then $R_1 = n_1(n_1 + 1)/2$ and, as a consequence, $U_1$ would then be zero (minimum attainable value). Otherwise, if the two samples were similar, $U_1$ and $U_2$ would be very close to $n_1 n_2 / 2$ (maximum attainable value).\nWe perform the test using the mannwhitneyu function from scipy.\nfrom scipy.stats import mannwhitneyu stat, p_value = mannwhitneyu(income_t, income_c) print(f\u0026quot; Mann–Whitney U Test: statistic={stat:.4f}, p-value={p_value:.4f}\u0026quot;) Mann–Whitney U Test: statistic=106371.5000, p-value=0.6012 We get a p-value of 0.6 which implies that we do not reject the null hypothesis of no difference between the two distributions.\nNote: as for the t-test, there exists a version of the Mann–Whitney U test for unequal variances in the two samples, the Brunner-Munzel test.\nPermutation Tests A non-parametric alternative is permutation testing. The idea is that, under the null hypothesis, the two distributions should be the same, therefore shuffling the group labels should not significantly alter any statistic.\nWe can chose any statistic and check how its value in the original sample compares with its distribution across group label permutations. For example, let\u0026rsquo;s use as a test statistic the difference of sample means between the treatment and control group.\nsample_stat = np.mean(income_t) - np.mean(income_c) stats = np.zeros(1000) for k in range(1000): labels = np.random.permutation((df['Group'] == 'treatment').values) stats[k] = np.mean(income[labels]) - np.mean(income[labels==False]) p_value = np.mean(stats \u0026gt; sample_stat) print(f\u0026quot;Permutation test: p-value={p_value:.4f}\u0026quot;) Permutation test: p-value=0.0530 The permutation test gives us a p-value of $0.056$, implying a weak non-rejection of the null hypothesis at the 5% level.\nHow do we interpret the p-value? It means that the difference in means in the data is larger than $1 - 0.0560 = 94.4%$ of the differences in means across the permuted samples.\nWe can visualize the test, by plotting the distribution of the test statistic across permutations against its sample value.\nplt.hist(stats, label='Permutation Statistics', bins=30); plt.axvline(x=sample_stat, c='r', ls='--', label='Sample Statistic'); plt.legend(); plt.xlabel('Income difference between treatment and control group') plt.title('Permutation Test'); As we can see, the sample statistic is quite extreme with respect to the values in the permuted samples, but not excessively.\nChi-Squared Test The chi-squared test is a very powerful test that is mostly used to test differences in frequencies.\nOne of the least known applications of the chi-squared test, is testing the similarity between two distributions. The idea is to bin the observations of the two groups. If the two distributions were the same, we would expect the same frequency of observations in each bin. Importantly, we need enough observations in each bin, in order for the test to be valid.\nI generate bins corresponding to deciles of the distribution of income in the control group and then I compute the expected number of observations in each bin in the treatment group, if the two distributions were the same.\n# Init dataframe df_bins = pd.DataFrame() # Generate bins from control group _, bins = pd.qcut(income_c, q=10, retbins=True) df_bins['bin'] = pd.cut(income_c, bins=bins).value_counts().index # Apply bins to both groups df_bins['income_c_observed'] = pd.cut(income_c, bins=bins).value_counts().values df_bins['income_t_observed'] = pd.cut(income_t, bins=bins).value_counts().values # Compute expected frequency in the treatment group df_bins['income_t_expected'] = df_bins['income_c_observed'] / np.sum(df_bins['income_c_observed']) * np.sum(df_bins['income_t_observed']) df_bins bin income_c_observed income_t_observed income_t_expected 0 (232.26, 380.496] 70 46 29.075391 1 (380.496, 425.324] 70 30 29.075391 2 (425.324, 456.795] 70 24 29.075391 3 (456.795, 488.83] 71 26 29.490754 4 (488.83, 513.66] 70 18 29.075391 5 (513.66, 540.048] 70 19 29.075391 6 (540.048, 576.664] 71 21 29.490754 7 (576.664, 621.022] 70 25 29.075391 8 (621.022, 682.003] 70 42 29.075391 9 (682.003, 973.46] 71 41 29.490754 We can now perform the test by comparing the expected (E) and observed (O) number of observations in the treatment group, across bins. The test statistic is given by\n$$ stat = \\sum _{i=1}^{n} \\frac{(O_i - E_i)^{2}}{E_i} $$\nwhere the bins are indexed by $i$ and $O$ is the observed number of data points in bin $i$ and $E$ is the expected number of data points in bin $i$. Since we generated the bins using deciles of the distribution of income in the control group, we expect the number of observations per bin in the treatment group to be the same across bins. The test statistic is asymptocally distributed as a chi-squared distribution.\nTo compute the test statistic and the p-value of the test, we use the chisquare function from scipy.\nfrom scipy.stats import chisquare stat, p_value = chisquare(df_bins['income_t_observed'], df_bins['income_t_expected']) print(f\u0026quot;Chi-squared Test: statistic={stat:.4f}, p-value={p_value:.4f}\u0026quot;) Chi-squared Test: statistic=32.1432, p-value=0.0002 Differently from all other tests so far, the chi-squared test strongly rejects the null hypothesis that the two distributions are the same. Why?\nThe reason lies in the fact that the two distributions have a similar center but different tails and the chi-squared test tests the similarity along the whole distribution and not only in the center, as we were doing with the previous tests.\nThis result tells a cautionary tale: it is very important to understand what you are actually testing before drawing blind conclusions from a p-value!\nKolmogorov-Smirnov Test The idea of the Kolmogorov-Smirnov test, is to compare the cumulative distributions of the two groups. In particular, the Kolmogorov-Smirnov test statistic is the maximum absolute difference between the two cumulative distributions.\n$$ stat = \\sup _{x} \\ \\Big| \\ F_1(x) - F_2(x) \\ \\Big| $$\nWhere $F_1$ and $F_2$ are the two cumulative distribution functions and $x$ are the values of the underlying variable. The asymptotic distribution of the Kolmogorov-Smirnov test statistic is Kolmogorov distributed.\nTo better understand the test, let\u0026rsquo;s plot the cumulative distribution functions and the test statistic. First, we compute the cumulative distribution functions.\ndf_ks = pd.DataFrame() df_ks['Income'] = np.sort(df['Income'].unique()) df_ks['F_control'] = df_ks['Income'].apply(lambda x: np.mean(income_c\u0026lt;=x)) df_ks['F_treatment'] = df_ks['Income'].apply(lambda x: np.mean(income_t\u0026lt;=x)) df_ks.head() Income F_control F_treatment 0 216.36 0.000000 0.003378 1 232.26 0.001420 0.003378 2 243.15 0.001420 0.006757 3 259.88 0.002841 0.006757 4 262.82 0.002841 0.010135 We now need to find the point where the absolute distance between the cumulative distribution functions is largest.\nk = np.argmax( np.abs(df_ks['F_control'] - df_ks['F_treatment'])) ks_stat = np.abs(df_ks['F_treatment'][k] - df_ks['F_control'][k]) We can visualize the value of the test statistic, by plotting the two cumulative distribution functions and the value of the test statistic.\ny = (df_ks['F_treatment'][k] + df_ks['F_control'][k])/2 plt.plot('Income', 'F_control', data=df_ks, label='Control') plt.plot('Income', 'F_treatment', data=df_ks, label='Treatment') plt.errorbar(x=df_ks['Income'][k], y=y, yerr=ks_stat/2, color='k', capsize=5, mew=3, label=f\u0026quot;Test statistic: {ks_stat:.4f}\u0026quot;) plt.legend(loc='center right'); plt.title(\u0026quot;Kolmogorov-Smirnov Test\u0026quot;); From the plot, we can see that the value of the test statistic corresponds to the distance between the two cumulative distributions at income~650. For that value of income, we have the largest imbalance between the two groups.\nWe can now perform the actual test using the kstest function from scipy.\nfrom scipy.stats import kstest stat, p_value = kstest(income_t, income_c) print(f\u0026quot; Kolmogorov-Smirnov Test: statistic={stat:.4f}, p-value={p_value:.4f}\u0026quot;) Kolmogorov-Smirnov Test: statistic=0.0974, p-value=0.0355 The p-value is below 5%: we reject the null hypothesis that the two distributions are the same, with 95% confidence.\nNote 1: The KS test is too conservative and rejects the null hypothesis too rarely. Lilliefors test corrects this bias using a different distribution for the test statistic, the Lilliefors distribution.\nNote 2: the KS test uses very little information since it only compares the two cumulative distributions at one point: the one of maximum distance. The Anderson-Darling test and the Cramér-von Mises test instead compare the two distributions along the whole domain, by integration (the difference between the two lies in the weighting of the squared distances).\nMultiple Groups - Plots So far we have only considered the case of two groups: treatment and control. But that if we had multiple groups? Some of the methods we have seen above scale well, while others don\u0026rsquo;t.\nAs a working example, we are now going to check whether the distribution of income is the same across treatment arms.\nBoxplot The boxplot scales very well, when we have a number of groups in the single-digits, since we can put the different boxes side-by-side.\nsns.boxplot(x='Arm', y='Income', data=df.sort_values('Arm')); plt.title(\u0026quot;Boxplot, multiple groups\u0026quot;); From the plot, it looks like the distribution of income is different across treatment arms, with higher numbered arms having a higher average income.\nViolin Plot A very nice extension of the boxplot that combines summary statistics and kernel density estimation is the violinplot. The violinplot plots separate densities along the y axis so that they don\u0026rsquo;t overlap. By default, it also adds a miniature boxplot inside.\nsns.violinplot(x='Arm', y='Income', data=df.sort_values('Arm')); plt.title(\u0026quot;Violin Plot, multiple groups\u0026quot;); As for the boxplot, the violin plot suggests that income is different across treatment arms.\nRidgeline Plot Lastly, the ridgeline plot plots multiple kernel density distributions along the x-axis, making them more intuitive than the violin plot but partially overlapping them. Unfortunately, there is no default ridgeline plot neither in matplotlib nor in seaborn. We need to import it from joypy.\nfrom joypy import joyplot joyplot(df, by='Arm', column='Income', colormap=sns.color_palette(\u0026quot;crest\u0026quot;, as_cmap=True)); plt.xlabel('Income'); plt.title(\u0026quot;Ridgeline Plot, multiple groups\u0026quot;); Again, the ridgeline plot suggests that higher numbered treatment arms have higher income. From this plot it is also easier to appreciate the different shapes of the distributions.\nMultiple Groups - Tests Lastly, let\u0026rsquo;s consider hypothesis tests to compare multiple groups. For simplicity, we will concentrate on the most popular one: the F-test.\nF-test With multiple groups, the most popular test is the F-test. The F-test compares the variance of a variable across different groups. This analysis is also called analysis of variance, or ANOVA.\nIn practice, the F-test statistic is\n$$ \\text{stat} = \\frac{\\text{between-group variance}}{\\text{within-group variance}} = \\frac{\\sum_{g} \\big( \\bar x_g - \\bar x \\big) / (G-1)}{\\sum_{g} \\sum_{i \\in g} \\big( \\bar x_i - \\bar x_g \\big) / (N-G)} $$\nWhere $G$ is the number of groups, $N$ is the number of observations, $\\bar x$ is the overall mean and $\\bar x_g$ is the mean within group $g$. Under the null hypothesis of group independence, the f-statistic is F-distributed.\nfrom scipy.stats import f_oneway income_groups = [df.loc[df['Arm']==arm, 'Income'].values for arm in df['Arm'].dropna().unique()] stat, p_value = f_oneway(*income_groups) print(f\u0026quot;F Test: statistic={stat:.4f}, p-value={p_value:.4f}\u0026quot;) F Test: statistic=9.0911, p-value=0.0000 The test p-value is basically zero, implying a strong rejection of the null hypothesis of no differences in the income distribution across treatment arms.\nConclusion In this post we have see a ton of different ways to compare two or more distributions, both visually and statistically. This is a primary concern in many applications, but especially in causal inference where we use randomization to make treatment and control group as comparable as possible.\nWe have also seen how different methods might be better suited for different situations. Visual methods are great to build intuition, but statistical methods are essential for decision-making, since we need to be able to assess the magnitude and statistical significance of the differences.\nReferences [1] Student, The Probable Error of a Mean (1908), Biometrika.\n[2] F. Wilcoxon, Individual Comparisons by Ranking Methods (1945), Biometrics Bulletin.\n[3] B. L. Welch, The generalization of \u0026ldquo;Student\u0026rsquo;s\u0026rdquo; problem when several different population variances are involved (1947), Biometrika.\n[4] H. B. Mann, D. R. Whitney, On a Test of Whether one of Two Random Variables is Stochastically Larger than the Other (1947), The Annals of Mathematical Statistics.\n[5] E. Brunner, U. Munzen, The Nonparametric Behrens-Fisher Problem: Asymptotic Theory and a Small-Sample Approximation (2000), Biometrical Journal.\n[6] A. N. Kolmogorov, Sulla determinazione empirica di una legge di distribuzione (1933), Giorn. Ist. Ital. Attuar..\n[7] H. Cramér, On the composition of elementary errors (1928), Scandinavian Actuarial Journal.\n[8] R. von Mises, Wahrscheinlichkeit statistik und wahrheit (1936), Bulletin of the American Mathematical Society.\n[9] T. W. Anderson, D. A. Darling, Asymptotic Theory of Certain \u0026ldquo;Goodness of Fit\u0026rdquo; Criteria Based on Stochastic Processes (1953), The Annals of Mathematical Statistics.\nRelated Articles Goodbye Scatterplot, Welcome Binned Scatterplot Code You can find the original Jupyter Notebook here:\nhttps://github.com/matteocourthoud/Blog-Posts/blob/main/notebooks/distr.ipynb\n","date":1688947200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1688947200,"objectID":"e801ea74490d9118f36e79fe0f2ccb53","permalink":"https://matteocourthoud.github.io/post/comparing_distributions/","publishdate":"2023-07-10T00:00:00Z","relpermalink":"/post/comparing_distributions/","section":"post","summary":"A complete guide to comparing distributions, from visualization to statistical tests\nComparing the empirical distribution of a variable across different groups is a common problem in data science. In particular, in causal inference the problem often arises when we have to assess the quality of randomization.","tags":null,"title":"How to Compare Two or More Distributions","type":"post"},{"authors":null,"categories":null,"content":"An introduction to quantile regression.\nIn A/B tests, a.k.a. randomized controlled trials, we usually estimate the average treatment effect (ATE): effect of a treatment (a drug, ad, product, \u0026hellip;) on an outcome of interest (a disease, firm revenue, customer satisfaction, \u0026hellip;), where the \u0026ldquo;average\u0026rdquo; is taken over the test subjects (patients, users, customers, \u0026hellip;). The ATE is a very useful quantity since it tells us the effect that we can expect if we were to treat a new subject with the same treatment.\nHowever, sometimes we might be interested in quantities different from the average, such as the median. The median is an alternative measure of central tendency that is more robust to outliers and is often more informative with skewed distributions. More generally, we might want to estimate the effect for different quantiles of the outcome distribution. A common use-case is studying the impact of a UI change on the loading time of a website: a slightly heavier website might translate in an imperceptible change for most users, but a big change for a few users with very slow connections. Another common use-case is studying the impact of a product change on a product that is bought by few people: do existing customers buy it more or are we attracting new customers?\nThese questions are hard to answer with linear regression that estimates the average treatment effect. A more suitable tool is quantile regression that can instead estimate the median treatment effect. In this article we are going to cover a brief introduction to quantile regression and the estimation of quantile treatment effects.\nLoyalty Cards and Spending Suppose we were an online store and we wanted to increase sales. We decide to offer our customers a loyalty card that grants them discounts as they increase their spend in the store. We would like to assess if the loyalty card is effective in increasing sales so we run an A/B test: we offer the loyalty card only to a subset of our customers, at random.\nI import the data generating process dgp_loyalty() from src.dgp. I also import some plotting functions and libraries from src.utils. To include not only code but also data and tables, I use Deepnote, a Jupyter-like web-based collaborative notebook environment.\n%matplotlib inline %config InlineBackend.figure_format = 'retina' from src.utils import * from src.dgp import dgp_loyalty Now, let\u0026rsquo;s have a look at the data. We have information on $10.000$ customers, for whom we observe their spend and whether they were offered the loyalty card. We also observe some demographics, like age and gender.\ndf = dgp_loyalty().generate_data() df.head() loyalty spend age gender 0 1 0.0 30 Male 1 0 0.0 26 Male 2 1 0.0 27 Male 3 1 0.0 29 Male 4 1 0.0 23 Female Interestingly, we notice that the outcome of interest, spend, seems to have a lot of zeros. Let\u0026rsquo;s dig deeper.\nMean vs Median Before analyzing our experiment, let\u0026rsquo;s have a look at our outcome variable, spend. We first inspect it using centrality measures. We have two main options: the mean and the median.\nFirst of all, what are they? The mean captures the average value, while the median captures the value in the middle of the distribution. In general, the mean is mathematically more tractable and easier to interpret, while the median is more robust to outliers. You can find plenty of articles online comparing the two measures and suggesting which one is more appropriate and when. Let\u0026rsquo;s have a look at the mean and median spend.\nnp.mean(df['spend']) 28.136224 np.median(df['spend']) 0.0 How do we interpret these two numbers? People spend on average 28\\$ on our store. However, more than 50% of people don\u0026rsquo;t spend anything. As we can see, both measures are very informative and, to a certain extent, complementary. We can better understand the distribution of spend by plotting its histogram.\nsns.histplot(data=df, x=\u0026quot;spend\u0026quot;).set(ylabel='', title='Spending Distribution'); As previewed by the values of the mean and the median, the distribution of spend is very skewed, with more than 5000 customers (out of 10000) not spending anything.\nOne natural question then is: are we interested in the effect of the loyalty card on average spend or on median spend? The first would tell us if customers spend more on average, while the second would tell us if the average customer spends more.\nLinear regression can tell us the effect of the loyalty card on average spend. However, what can we do if we were interested in the effect of the loyalty card on median spend (or other quantiles)? The answer is quantile regression.\nQuantile Regression With linear regression, we try to estimate the conditional expectation function of an outcome variable $Y$ (spend in our example) with respect to one or more explanatory variables $X$ (loyalty in our example).\n$$ \\mathbb E \\big[ Y \\big| X \\big] $$\nIn other words, we want to find a function $f$ such that $f(X) = \\mathbb E[Y|X]$. We do so, by solving the following minimization problem:\n$$ \\hat f(X) = \\arg \\min_{f} \\mathbb E \\big[ Y - f(X) \\big]^2 $$\nIt can be shown that the function $f$ that solves this minimization is indeed the conditional expectation of $Y$, with respect to $X$.\nSince $f(X)$ can be infinite dimensional, we usually estimate a parametric form of $f(X)$. The most common one is the linear form $f(X) = \\beta X$, where $\\beta$ is estimated by solving the corresponding minimization problem:\n$$ \\hat \\beta = \\arg \\min_{\\beta} \\mathbb E \\big[ Y - \\beta X \\big]^2 $$\nThe linear form is not just convenient, but it can be interpreted as the best local approximation of $f(X)$, referring to Taylor\u0026rsquo;s expansion.\nWith quantile regression, we do the same. The only difference is that, instead of estimating the conditional expectation of $Y$ with respect to $X$, we want to estimate the $q$-quantile of $Y$ with respect to $X$.\n$$ \\mathbb Q_q \\big[ Y \\big| X \\big] $$\nFirst of all, what is a quantile? The Wikipedia definition says\n\u0026ldquo;In statistics and probability, quantiles are cut points dividing the range of a probability distribution into continuous intervals with equal probabilities, or dividing the observations in a sample in the same way. Common quantiles have special names, such as quartiles (four groups), deciles (ten groups), and percentiles (100 groups).\u0026rdquo;\nFor example, the 0.1-quantile represents the value that sits on the right of 10% of the mass of the distribution. The median is the 0.5-quantile (or, equivalently, the $50^{th}$ percentile or the $5^{th}$ decile) and corresponds with the value in the center of the distribution. Let\u0026rsquo;s see a simple example with a log-normal distribution. I plot the three quartiles that divide the data in four equally sized bins.\ndata = np.random.lognormal(0, 1, 1_000_000); sns.histplot(data).set(title='Lognormal Distribution', xlim=(0,10)) plt.axvline(x=np.percentile(data, 25), c='C8', label='25th percentile') plt.axvline(x=np.median(data), c='C1', label='Median (50th pct)') plt.axvline(x=np.percentile(data, 75), c='C3', label='75th percentile') plt.legend(); As we can see, the three quartiles divide the data into four bins, of equal size.\nSo, what is the objective of quantile regression? The objective is to find a function $f$ such that $f(X) = F^{-1}(y_q)$, where $F$ is the cumulative distribution function of $Y$ and $y_q$ is the $q$-quantile of the distribution of $Y$.\nHow do we do this? It can be shown with a little linear algebra that we can obtain the conditional quantile as the solution of the following minimization problem:\n$$ \\hat f(X) = \\arg \\min_{f} \\mathbb E \\big[ \\rho_q (Y - f(X)) \\big] = \\arg \\min_{f} \\ (1-q) \\int_{-\\infty}^{f(x)} (y - f(x)) \\text{d} F(y) + q \\int_{f(x)}^{\\infty} (y - f(x)) \\text{d} F(y) $$\nWhere $\\rho_q$ is an auxiliary weighting function with the following shape.\nWhat is the intuition behind the objective function?\nThe idea is that we can interpret the equation as follows\n$$ \\mathbb E \\big[ \\rho_q (Y - f(X)) \\big] = (1-q) \\underset{\\color{red}{\\text{mass of distribution below }f(x)}}{\\int_{-\\infty}^{f(x)} (y - f(x)) \\text{d} F(y)} + q \\underset{\\color{red}{\\text{mass of distribution above }f(x)}}{\\int_{f(x)}^{\\infty} (y - f(x)) \\text{d} F(y)} \\overset{\\color{blue}{\\text{if } f(x) = y_q}}{=} - (1-q) q + q (1-q) = 0 $$\nSo that, when $f(X)$ corresponds with the quantile $y_q$, the value of the objective function is zero.\nExactly as before, we can estimate a parametric form of $f$ and, exactly as before, we can interpret it as a best local approximation (not trivially though, see Angrist, Chernozhukov, and Fernández-Val (2006)).\n$$ \\hat \\beta_q = \\arg \\min_{\\beta} \\mathbb E \\big[ \\rho_q (Y - \\beta X ) \\big] $$\nWe wrote $\\hat \\beta_q$ to indicate that this is the coefficient for the best linear approximation of the conditional $q$-quantile function.\nHow do we estimate a quantile regression?\nEstimation The statsmodels package allows us to estimate quantile regression with the the quantreg() function. We just need to specify the quantile $q$ when we fit the model. Let\u0026rsquo;s use $q=0.5$, which corresponds with the median.\nsmf.quantreg(\u0026quot;spend ~ loyalty\u0026quot;, data=df).fit(q=0.5).summary().tables[1] coef std err t P\u003e|t| [0.025 0.975] Intercept 8.668e-07 0.153 5.66e-06 1.000 -0.300 0.300 loyalty 3.4000 0.217 15.649 0.000 2.974 3.826 Quantile regression estimates a positive coefficient for loyalty. How does this estimate compare with linear regression?\nsmf.ols(\u0026quot;spend ~ loyalty\u0026quot;, data=df).fit().summary().tables[1] coef std err t P\u003e|t| [0.025 0.975] Intercept 25.6583 0.564 45.465 0.000 24.552 26.765 loyalty 4.9887 0.801 6.230 0.000 3.419 6.558 The estimated coefficient with linear regression is higher. What does it mean? We will spend more time on the interpretation of quantile regression coefficients later.\nCan we condition the analysis on other variables? We suspect that spend is also affected by other variables and we want to increase the precision of our estimate by also conditioning the analysis on age and gender. We can just add the variables to the quantreg() model.\nsmf.quantreg(\u0026quot;spend ~ loyalty + age + gender\u0026quot;, data=df).fit(q=0.5).summary().tables[1] coef std err t P\u003e|t| [0.025 0.975] Intercept -50.5353 1.053 -47.977 0.000 -52.600 -48.471 gender[T.Male] -20.2963 0.557 -36.410 0.000 -21.389 -19.204 loyalty 4.5747 0.546 8.374 0.000 3.504 5.646 age 2.3663 0.026 92.293 0.000 2.316 2.417 The coefficient of loyalty increases when we condition the analysis on age and gender. This is true also for linear regresssion.\nsmf.ols(\u0026quot;spend ~ loyalty + age + gender\u0026quot;, data=df).fit().summary().tables[1] coef std err t P\u003e|t| [0.025 0.975] Intercept -57.4466 0.911 -63.028 0.000 -59.233 -55.660 gender[T.Male] -26.3170 0.482 -54.559 0.000 -27.262 -25.371 loyalty 3.9101 0.473 8.272 0.000 2.983 4.837 age 2.7688 0.022 124.800 0.000 2.725 2.812 There are a couple of things that we haven\u0026rsquo;t mentioned yet. The first one is inference. How do we compute confidence intervals and p-values for our estimates in quantile regression?\nInference The asymptotic variance of the estimate $a$ of the quantile $q$ of a distribution $F$ is given by\n$$ AVar(y) = q(1-q) f^{-2}(y) $$\nwhere $f$ is the density function of $F$. This expression can be decomposed into two components: $q(1-q)$ and $f^{-2}(y)$.\nThe first component, $q(1-q)$, basically tells us that the variance of a quantile is higher the more the quantile is closer to the center of the distribution. Why is that so? First, we need to think about when the quantile of a point changes in response to a change in the value of a second point. The quantile changes when the second point swaps from left to right (or viceversa) of the first point. This is intuitively very easy if the first point lies in the middle of the distribution, but very hard if it lies at the extreme.\nThe second component, $f^{-2}(a)$, instead tells us that this side swapping is more likely if the first point is surrounded by a lot of points.\nImportantly, estimating the variance of a quantile requires an estimate of the whole distribution of $Y$. This is done via approximation and it can be computationally very intensive. However, alternative procedures like the bootstrap or the bayesian bootstrap are always available.\nThe second thing that we haven\u0026rsquo;t talked about yet is the interpretation of the estimated coefficients. We got a lower coefficient of loyalty on spend with median regression. What does it mean?\nInterpretation The interpretation of linear regression coefficients is straightforward: each coefficient is the derivative of the conditional expectation function $\\mathbb E[Y|X]$ with respect to one dimension of $X$. In our case, we can interpret the regression coefficient of loyalty as the average spend increase from being offered a loyalty card. Crucially, here \u0026ldquo;average\u0026rdquo; means that this holds true for each customer, on average.\nHowever, the interpretation of quantile regression coefficients is tricky. Before, we were tempted to say that the loyalty card increases the spend of the median customer by 3.4\\$. But what does it mean? Is it the same median customer that spends more or do we have a different median customer? This might seem like a philosophical question but it has important implications on reporting of quantile regression results. In the first case, we are making a statement that, as for the interpretation of linear regression coefficients, applies to a single individual. In the second case, we are making a statement about the distribution.\nChernozhukov and Hansen (2005) show that a strong but helpful assumption is rank invariance: assuming that the treatment does not shift the relative composition of the distribution. In other words, if we rank people by spend before the experiment, we assume that this ranking is not affected by the introduction of the loyalty card. If I was spending less than you before, I might spend more afterwards, but still less than you (for any two people).\nUnder this assumption, we can interpret the quantile coefficients as marginal effects for single individuals sitting at different points of the outcome distribution, as in the first interpretation provided above. Moreover, we can report the treatment effect for many quantiles and interpret each one of them as a local effect for a different individual. Let\u0026rsquo;s plot the distribution of treatment effects, for different quantiles of spend.\ndef plot_quantile_TE(df, formula, q, varname): df_results = pd.DataFrame() for q in np.arange(q, 1-q, q): qreg = smf.quantreg(formula, data=df).fit(q=q) temp = pd.DataFrame({'q': [q], 'coeff': [qreg.params[varname]], 'std': [qreg.bse[varname]], 'ci_lower': [qreg.conf_int()[0][varname]], 'ci_upper': [qreg.conf_int()[1][varname]]}) df_results = pd.concat([df_results, temp]).reset_index(drop=True) # Plot fig, ax = plt.subplots() sns.lineplot(data=df_results, x='q', y='coeff') ax.fill_between(data=df_results, x='q', y1='ci_lower', y2='ci_upper', alpha=0.1); plt.axhline(y=0, c=\u0026quot;k\u0026quot;, lw=2, zorder=1) ols_coeff = smf.ols(formula, data=df).fit().params[varname] plt.axhline(y=ols_coeff, ls=\u0026quot;--\u0026quot;, c=\u0026quot;C1\u0026quot;, label=\u0026quot;OLS coefficient\u0026quot;, zorder=1) plt.legend() plt.title(\u0026quot;Estimated coefficient, by quantile\u0026quot;) plot_quantile_TE(df, formula=\u0026quot;spend ~ loyalty\u0026quot;, varname='loyalty', q=0.05) This plot is extremely insightful: for almost half of the customers, the loyalty card has no effect. On the other hand, customers that were already spending something end up spending even more (around 10/12\\$ more). This is a very powerful insight that we would have missed with linear regression that estimated an average effect of 5\\$.\nWe can repeat the same exercise, conditioning the analysis on gender and age.\nplot_quantile_TE(df, formula=\u0026quot;spend ~ loyalty + age + gender\u0026quot;, varname='loyalty', q=0.05) Conditioning on other covariates removes a lot of the heterogeneity in treatment effects. The loyalty card increases spending for most people, it\u0026rsquo;s demographic characteristics that are responsible for no spending to begin with.\nConclusion In this article, we have explored a different causal estimand: median treatment effects. How does it compare with the average treatment effect that we usually estimate? The pros and cons are closely related to the pros and cons of the median with respect to the mean as a measure of central tendency. Median treatment effects are more informative on what is the effect on the average subject and are more robust to outliers. However, they are much more computationally intensive and they require strong assumptions for identification, such as rank invariance.\nReferences [1] R. Koenker, Quantile Regression (1996), Cambridge University Press.\n[1] R. Koenker, K. Hallock, Quantile Regression, (2001), Journal of Economic Perspectives.\n[2] V. Chernozhukov, C. Hansen, An IV Model of Quantile Treatment Effects (2005), Econometrica.\n[3] J. Angrist, V. Chernozhukov, I. Fernández-Val, Quantile Regression under Misspecification, with an Application to the U.S. Wage Structure (2006), Econometrica.\nRelated Articles DAGs and Control Variables The Bayesian Bootstrap Goodbye Scatterplot, Welcome Binned Scatterplot Code You can find the original Jupyter Notebook here:\nhttps://github.com/matteocourthoud/Blog-Posts/blob/main/notebooks/quantile_reg.ipynb\n","date":1688947200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1688947200,"objectID":"5517fe2a3e5b8f4dd887476d9f7fdafd","permalink":"https://matteocourthoud.github.io/post/quantile_regression/","publishdate":"2023-07-10T00:00:00Z","relpermalink":"/post/quantile_regression/","section":"post","summary":"An introduction to quantile regression.\nIn A/B tests, a.k.a. randomized controlled trials, we usually estimate the average treatment effect (ATE): effect of a treatment (a drug, ad, product, \u0026hellip;) on an outcome of interest (a disease, firm revenue, customer satisfaction, \u0026hellip;), where the \u0026ldquo;average\u0026rdquo; is taken over the test subjects (patients, users, customers, \u0026hellip;).","tags":null,"title":"Mean vs Median Causal Effect","type":"post"},{"authors":null,"categories":null,"content":"In causal inference, bias is extremely problematic because it makes inference not valid. Bias generally means that an estimator will not deliver the estimate of the true effect, on average.\nThis is why, in general, we prefer estimators that are unbiased, at the cost of a higher variance, i.e. more noise. Does it mean that every biased estimator is useless? Actually no. Sometimes, with domain knowledge, we can still draw causal conclusions even with a biased estimator.\nIn this post, we are going to review a specific but frequent source of bias, omitted variable bias (OVB). We are going to explore the causes of the bias and leverage these insights to make causal statements, despite the bias.\nTheory Suppose we are interested in the effect of a variable $D$ on a variable $y$. However, there is a third variable $Z$ that we do not observe and that is correlated with both $D$ and $Y$. Assume the data generating process can be represented with the following Directed Acyclic Graph (DAG). If you are not familiar with DAGs, I have written a short introduction here.\nflowchart LR classDef included fill:#DCDCDC,stroke:#000000,stroke-width:2px; classDef excluded fill:#ffffff,stroke:#000000,stroke-width:2px; classDef unobserved fill:#ffffff,stroke:#000000,stroke-width:2px,stroke-dasharray: 5 5; D((D)) Z((Z)) Y((Y)) D --\u0026gt; Y Z --\u0026gt; D Z --\u0026gt; Y class D,Y excluded; class Z unobserved; Since there is a backdoor path from $D$ to $y$ passing through $Z$, we need to condition our analysis on $Z$ in order to recover the causal effect of $D$ on $y$. If we could observe $Z$, we would run a linear regression of $y$ on $D$ and $Z$ to estimate the following model:\n$$ y = \\alpha D + \\gamma Z + \\varepsilon $$\nwhere $\\alpha$ is the effect of interest. This regression is usually referred to as the long regression since it includes all variables of the model.\nHowever, since we do not observe $Z$, we have to estimate the following model:\n$$ y = \\alpha D + u $$\nThe corresponding regression is usually referred to as the short regression since it does not include all the variables of the model\nWhat is the consequence of estimating the short regression when the true model is the long one?\nIn that case, the OLS estimator of $\\alpha$ is\n$$ \\begin{align} \\hat \\alpha \u0026amp;= \\frac{Cov(D, y)}{Var(D)} = \\newline \u0026amp;= \\frac{Cov(D, \\alpha D + \\gamma Z + \\varepsilon)}{Var(D)} = \\newline \u0026amp;= \\frac{Cov(D, \\alpha D)}{Var(D)} + \\frac{Cov(D, \\gamma Z)}{Var(D)} + \\frac{Cov(D, \\varepsilon)}{Var(D)} = \\newline \u0026amp;= \\alpha + \\underbrace{ \\gamma \\frac{Cov(D, Z)}{Var(D)} }_{\\text{omitted variable bias}} \\end{align} $$\nTherefore, we can write the omitted variable bias as\n$$ \\text{OVB} = \\gamma \\delta \\qquad \\text{ where } \\qquad \\delta := \\frac{Cov(D, Z)}{Var(D)} $$\nThe beauty of this formula is its interpretability: the omitted variable bias consists in just two components, both extremely easy to interpret.\n$\\gamma$: the effect of $Z$ on $y$ $\\delta$: the effect of $D$ on $Z$ Additional Controls What happens if we had additional control variables in the regression? For example, assume that besides the variable of interest $D$, we also observe a vector of other variables $X$ so that the long regression is\n$$ y = \\alpha D + \\beta X + \\gamma Z + \\varepsilon $$\nThanks to the Frisch-Waugh-Lowell theorem, we can simply partial-out $X$ and express the omitted variable bias in terms of $D$ and $Z$.\n$$ \\text{OVB} = \\gamma \\times \\frac{Cov(D^{\\perp X}, Z^{\\perp X})}{Var(D^{\\perp X})} $$\nwhere $D^{\\perp X}$ are the residuals from regressing $D$ on $X$ and $Z^{\\perp X}$ are the residuals from regressing $Z$ on $X$. If you are not familiar with Frisch-Waugh-Lowell theorem, I have written a short note here.\nChernozhukov, Cinelli, Newey, Sharma, Syrgkanis (2022) further generalize to analysis the the setting in which the control variables $X$ and the unobserved variables $Z$ enter the long model with a general functional form $f$\n$$ y = \\alpha D + f(Z, X) + \\varepsilon $$\nYou can find more details in their paper, but the underlying idea remains the same.\nExample Suppose we were a researcher interested in the relationship between education and wages. Does investing in education pay off in terms of future wages? Suppose we had data on wages for people with different years of education. Why not looking at the correlation between years of education and wages?\nThe problem is that there might be many unobserved variables that are correlated with both education and wages. For simplicity, let\u0026rsquo;s concentrate on ability. People of higher ability might decide to invest more in education just because they are better in school and they get more opportunities. On the other hand, they might also get higher wages afterwards, purely because of their innate ability.\nWe can represent the data generating process with the following Directed Acyclic Graph (DAG).\nflowchart TD classDef included fill:#DCDCDC,stroke:#000000,stroke-width:2px; classDef excluded fill:#ffffff,stroke:#000000,stroke-width:2px; classDef unobserved fill:#ffffff,stroke:#000000,stroke-width:2px,stroke-dasharray: 5 5; D((education)) Z((ability)) Y((wage)) X1((age)) X2((gender)) D --\u0026gt; Y Z --\u0026gt; D Z --\u0026gt; Y X1 --\u0026gt; Y X2 --\u0026gt; Y class D,Y included; class X1,X2 excluded; class Z unobserved; Let\u0026rsquo;s load and inspect the data. I import the data generating process from src.dgp and some plotting functions and libraries from src.utils.\n%matplotlib inline %config InlineBackend.figure_format = 'retina' from src.utils import * from src.dgp import dgp_educ_wages df = dgp_educ_wages().generate_data(N=50) df.head() age gender education wage 0 62 male 6.0 3800.0 1 44 male 8.0 4500.0 2 63 male 8.0 4700.0 3 33 male 7.0 3500.0 4 57 female 6.0 4000.0 We have information on 300 individuals, for which we observe their age, their gender, the years of education, and the current monthly wage.\nSuppose we were directly regressing wage on education.\nshort_model = smf.ols('wage ~ education + gender + age', df).fit() short_model.summary().tables[1] coef std err t P\u003e|t| [0.025 0.975] Intercept 2657.8864 444.996 5.973 0.000 1762.155 3553.618 gender[T.male] 335.1075 132.685 2.526 0.015 68.027 602.188 education 95.9437 38.752 2.476 0.017 17.940 173.948 age 12.3120 6.110 2.015 0.050 0.013 24.611 The coefficient of education is positive and significant. However, we know there might be an omitted variable bias, because we do not observe ability. In terms of DAGs, there is a backdoor path from education to wage passing through ability that is not blocked and therefore biases our estimate.\nflowchart TD classDef included fill:#DCDCDC,stroke:#000000,stroke-width:2px; classDef excluded fill:#ffffff,stroke:#000000,stroke-width:2px; classDef unobserved fill:#ffffff,stroke:#000000,stroke-width:2px,stroke-dasharray: 5 5; D((education)) Z((ability)) Y((wage)) X1((age)) X2((gender)) D --\u0026gt; Y Z --\u0026gt; D Z --\u0026gt; Y X1 --\u0026gt; Y X2 --\u0026gt; Y class D,Y included; class X1,X2 excluded; class Z unobserved; linkStyle 0 stroke:#00ff00,stroke-width:4px; linkStyle 1,2 stroke:#ff0000,stroke-width:4px; Does it mean that all our analysis is garbage? Can we still draw some causal conclusion from the regression results?\nDirection of the Bias If we knew the signs of $\\gamma$ and $\\delta$, we could infer the sign of the bias, since it\u0026rsquo;s the product of the two signs.\n$$ \\text{OVB} = \\gamma \\delta \\qquad \\text{ where } \\qquad \\gamma := \\frac{Cov(Z, y)}{Var(Z)}, \\quad \\delta := \\frac{Cov(D, Z)}{Var(D)} $$\nwhich in our example is\n$$ \\text{OVB} = \\gamma \\delta \\qquad \\text{ where } \\qquad \\gamma := \\frac{Cov(\\text{ability}, \\text{wage})}{Var(\\text{ability})}, \\quad \\delta := \\frac{Cov(\\text{education}, \\text{ability})}{Var(\\text{education})} $$\nLet\u0026rsquo;s analyze the two correlations separately:\nThe correlation between ability and wage is most likely positive The correlation between ability and education is most likely positive Therefore, the bias is most likely positive. From this, we can conclude that our estimate from the regression on wage on education is most likely an overestimate of the true effect, which is most likely smaller.\nThis might seem like a small insight, but it\u0026rsquo;s actually huge. Now we can say with confidence that one year of education increases wages by at most 95 dollars per month, which is a much more informative statement than just saying that the estimate is biased.\nIn general, we can summarize the different possible effects of the bias in a 2-by-2 table.\nFurther Sensitivity Analysis Can we say more about the omitted variable bias without making strong assumptions?\nThe answer is yes! In particular, we can ask ourselves: how strong should the partial correlations $\\gamma$ and $\\delta$ be in order to overturn our conclusion?\nIn our example, we found a positive correlation between education and wages in the data. However, we know that we are omitting ability in the regression. The question is: how strong should the correlation between ability and wage, $\\gamma$, and between ability and education, $\\delta$, be in order to make the effect not significant or even negative?\nCinelli and Hazlett (2020) show that we can transform this question in terms of residual variation explained, i.e. the coefficient of determination, $R^2$. The advantage of this approach is interpretability. It is much easier to make a guess about the percentage of variance explained than to make a guess about the magnitude of a conditional correlation.\nThe authors wrote a companion package sensemakr to conduct the sensitivity analysis. You can find a detailed description of the package here.\nWe will now use the Sensemakr function. The main arguments of the Sensemakr function are:\nmodel: the regression model we want to analyze treatment: the feature/covariate of interest, in our case education The question we will try to answer is the following:\nHow much of the residual variation in education (x axis) and wage (y axis) does ability need to explain in order for the effect of education on wages to change sign?\nimport sensemakr sensitivity = sensemakr.Sensemakr(model = short_model, treatment = \u0026quot;education\u0026quot;) sensitivity.plot() plt.xlabel(\u0026quot;Partial $R^2$ of ability with education\u0026quot;); plt.ylabel(\u0026quot;Partial $R^2$ of ability with wage\u0026quot;); In the plot, we see how the partial (because conditional on age and gender) $R^2$ of ability with education and wage affects the estimated coefficient of education on wage. The $(0,0)$ coordinate, marked with a triangle, corresponds to the current estimate and reflects what would happen if ability had no explanatory power for both wage with education: nothing. As the explanatory power of ability grows (moving upwards and rightwards from the triangle), the estimated coefficient decreases, as marked by the level curves, until it becomes zero at the dotted red line.\nHow should we interpret the plot? We can see that we need ability to explain around 30% of the residual variation in both education and wage in order for the effect of education on wages to disappear, corresponding to the red line.\nOne question that you might (legitimately) have now is: what is 30%? Is it big or is it small? We can get a sense of the magnitude of the partial $R^2$ by benchmarking the results with the residual variance explained by another observed variable. Let\u0026rsquo;s use age for example.\nThe Sensemakr function accepts the following optional arguments:\nbenchmark_covariates: the covariate to use as a benchmark kd and ky: these arguments parameterize how many times stronger the unobserved variable (ability) is related to the treatment (kd) and to the outcome (ky) in comparison to the observed benchmark covariate (age). In our example, setting kd and ky equal to $[0.5, 1, 2]$ means we want to investigate the maximum strength of a variable half, same, or twice as strong as age (in explaining education and wage variation). sensitivity = sensemakr.Sensemakr(model = short_model, treatment = \u0026quot;education\u0026quot;, benchmark_covariates = \u0026quot;age\u0026quot;, kd = [0.5, 1, 2], ky = [0.5, 1, 2]) sensitivity.plot() plt.xlabel(\u0026quot;Partial $R^2$ of ability with education\u0026quot;); plt.ylabel(\u0026quot;Partial $R^2$ of ability with wage\u0026quot;); It looks like even if ability had twice as much explanatory power as age, the effect of education on wage would still be positive. But would it be statistically significant?\nWe can repeat the same exercise, looking at the t-statistic instead of the magnitude of the coefficient. We just need to set the sensitivity_of option in the plotting function equal to t-value.\nThe question that we are trying to answer in this case is:\nHow much of the residual variation in education (x axis) and wage (y axis) does ability need to explain in order for the effect of education on wages to become not significant?\nsensitivity.plot(sensitivity_of = 't-value') plt.xlabel(\u0026quot;Partial $R^2$ of ability with education\u0026quot;); plt.ylabel(\u0026quot;Partial $R^2$ of ability with wage\u0026quot;); From the plot, we can see, we need ability to explain around 5% to 10% of the residual variation in both education and wage in order for the effect of education on wage not to be significant. In particular, the red line plots the level curve for the t-statistic equal to 2.01, corresponding to a 5% significance level. From the comparison with age, we see that a slightly stronger explanatory power (bigger than 1.0x age) would be sufficient to make the coefficient of education on wage not statistically significant.\nConclusion In this post, I have introduced the concept of omitted variable bias. We have seen how it\u0026rsquo;s computed in a simple linear model and how we can exploit qualitative information about the variables to make inference in presence of omitted variable bias.\nThese tools are extremely useful since omitted variable bias is essentially everywhere. First of all, there are always factors that we do not observe, such as ability in our toy example. However, even if we could observe everything, omitted variable bias can also emerge in the form of model misspecification. Suppose that wages depended on age in a quadratic way. Then, omitting the quadratic term from the regression introduces bias, which can be analyzed with the same tools we have used for ability.\nReferences [1] C. Cinelli, C. Hazlett, Making Sense of Sensitivity: Extending Omitted Variable Bias (2019), Journal of the Royal Statistical Society.\n[2] V. Chernozhukov, C. Cinelli, W. Newey, A. Sharma, V. Syrgkanis, Long Story Short: Omitted Variable Bias in Causal Machine Learning (2022), working paper.\nRelated Articles The FWL Theorem, Or How To Make Regressions Intuitive DAGs and Control Variables Code You can find the original Jupyter Notebook here:\nhttps://github.com/matteocourthoud/Blog-Posts/blob/main/ovb.ipynb\n","date":1688947200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1688947200,"objectID":"b43ca6c4e05c675a1512ae55609b750d","permalink":"https://matteocourthoud.github.io/post/omitted_variable_bias/","publishdate":"2023-07-10T00:00:00Z","relpermalink":"/post/omitted_variable_bias/","section":"post","summary":"In causal inference, bias is extremely problematic because it makes inference not valid. Bias generally means that an estimator will not deliver the estimate of the true effect, on average.","tags":null,"title":"Omitted Variable Bias And How To Deal With It","type":"post"},{"authors":null,"categories":null,"content":"What makes an observation \u0026ldquo;unusual\u0026rdquo;?\nIn data science, one common task is outlier detection. This is a broad term that is often misused or misunderstood. More broadly, we are often interested in understanding any observation is \u0026ldquo;unusual\u0026rdquo;. First of all, what does it mean to be unusual? In this article we are going to inspect three different ways in which an observation can be unusual: it can be unusual characteristics, it might not fit the model or it might be particularly influential in fitting the model. We will see that in linear regression the latter characteristics is a byproduct of the first two.\nImportantly, being unusual is not necessarily bad. Observations that have different characteristics from all others usually carry more information. We also expect some observations not to fit the model well, otherwise the model is likely biased (overfitting). However, \u0026ldquo;unusual\u0026rdquo; observations are also more likely to be generated by a different process. Extreme cases include measurement error or fraud, but differences can be more nuanced. Domain knowledge is always kind and dropping observations only for for statistical reasons is never wise.\nThat said, let\u0026rsquo;s have a look at some different ways in which observations can be \u0026ldquo;unusual\u0026rdquo;.\nExample Suppose we are an peer-to-peer online platform and we are interested in understanding if there is anything suspicious going on with our business. We have information about how much time our customers spend on the platform and the total value of their transactions.\nFirst, let\u0026rsquo;s have a look at the data. I import the data generating process dgp_p2p() from src.dgp and some plotting functions and libraries from src.utils. I include code snippets from Deepnote, a Jupyter-like web-based collaborative notebook environment. For our purpose, Deepnote is very handy because it allows me not only to include code but also output, like data and tables.\n%matplotlib inline %config InlineBackend.figure_format = 'retina' from src.utils import * from src.dgp import dgp_p2p df = dgp_p2p().generate_data() df.head() hours transactions 0 2.6 8.30 1 2.0 8.00 2 7.0 21.00 3 6.7 18.00 4 1.2 3.82 We have information on 50 clients for which we observe hours spent on the website and total transactions amount. Since we only have two variables we can easily inspect them using a scatterplot.\nsns.scatterplot(data=df, x='hours', y='transactions').set(title='Data Scatterplot'); The relationship between hours and transactions seems to follow a clear linear relationship. If we fit a linear model, we observe a particularly tight fit.\nsmf.ols('hours ~ transactions', data=df).fit().summary().tables[1] coef std err t P\u003e|t| [0.025 0.975] Intercept -0.0975 0.084 -1.157 0.253 -0.267 0.072 transactions 0.3452 0.009 39.660 0.000 0.328 0.363 Does any data point look suspiciously different from the others? How?\nLeverage The first metric that we are going to use to evaluate \u0026ldquo;unusual\u0026rdquo; observations is the leverage, which was first introduced by Cook (1980). The objective of the leverage is to capture how much a single point is different with respect to other data points. These data points are often called outliers and there exist a nearly amount of algorithms and rules of thumb to flag them.However the idea is the same: flagging observations that are unusual in terms of features.\nThe leverage of an observation $i$ is defined as\n$$ h_{ii} := x_i\u0026rsquo; (X\u0026rsquo;X)^{-1} x_i $$\nOne interpretation of the leverage is as a measure of distance where individual observations are compared against the average of all observations.\nAnother interpretation of the leverage is as the influence of the outcome of observation $i$, $y_i$, on the corresponding fitted value $\\hat{y_i}$.\n$$ h_{ii} = \\frac{\\partial \\hat{y}_i}{\\partial y_i} $$\nAlgebraically, the leverage of observation $i$ is the $i^{th}$ element of the design matrix $X\u0026rsquo; (X\u0026rsquo;X)^{-1} X$. Among the many properties of the leverages, is the fact that they are non-negative and their values sum to 1.\nLet\u0026rsquo;s compute the leverage of the observations in our dataset. We also flag observations that have unusual leverages (which we arbitrarily define as more than two standard deviations away from the average leverage).\nX = np.reshape(df['hours'].values, (-1, 1)) Y = np.reshape(df['transactions'].values, (-1, 1)) df['leverage'] = np.diagonal(X @ np.linalg.inv(X.T @ X) @ X.T) df['high_leverage'] = df['leverage'] \u0026gt; (np.mean(df['leverage']) + 2*np.std(df['leverage'])) Let\u0026rsquo;s plot the distribution of leverage values in our data.\nfix, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6)) sns.histplot(data=df, x='leverage', hue='high_leverage', alpha=1, bins=30, ax=ax1).set(title='Distribution of Leverages'); sns.scatterplot(data=df, x='hours', y='transactions', hue='high_leverage', ax=ax2).set(title='Data Scatterplot'); As we can see, the distribution is skewed with two observations having a unusually high leverage. Indeed, in the scatterplot these two observations are slightly separated from the rest of the distribution.\nIs this bad news? It depends. Outliers are not a problem per se. Actually, if they are genuine observations, they might carry much more information than other observations. On the other hand, they are also more likely not to be genuine observations (e.g. fraud, measurement error, \u0026hellip;) or to be inherently different from the other ones (e.g. professional users vs amateurs). In any case, we might want to investigate further and use as much context-specific information as we can.\nImportantly, the fact that an observation has a high leverage tells us information about the features of the model but nothing about the model itself. Are these users just different observations or they also behave differently?\nResiduals So far we have only talked about unusual features, but what about unusual behavior? This is what regression residuals measure.\nRegression residuals are the difference between the predicted outcome values and the observed outcome values. In a sense, they capture what the model cannot explain: the higher the residual of one observation the more it is unusual in the sense that the model cannot explain it.\nIn the case of linear regression, residuals can be written as\n$$ \\hat{e} = y - \\hat{y} = y - \\hat \\beta X $$\nIn our case, since $X$ is one dimensional (hours), we can easily visualize them.\nY_hat = X @ np.linalg.inv(X.T @ X) @ X.T @ Y plt.scatter(X, Y, s=50, label='data') plt.plot(X, Y_hat, c='k', lw=2, label='prediction') plt.vlines(X, np.minimum(Y, Y_hat), np.maximum(Y, Y_hat), color='r', lw=3, label=\u0026quot;residuals\u0026quot;); plt.legend() plt.title(f\u0026quot;Regression prediction and residuals\u0026quot;); Do some observations have unusually high residuals? Let\u0026rsquo;s plot their distribution.\ndf['residual'] = np.abs(Y - X @ np.linalg.inv(X.T @ X) @ X.T @ Y) df['high_residual'] = df['residual'] \u0026gt; (np.mean(df['residual']) + 2*np.std(df['residual'])) fix, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6)) sns.histplot(data=df, x='residual', hue='high_residual', alpha=1, bins=30, ax=ax1).set(title='Distribution of Residuals'); sns.scatterplot(data=df, x='hours', y='transactions', hue='high_residual', ax=ax2).set(title='Data Scatterplot'); Two observations have particularly high residuals. This means that for these observations, the model is not good at predicting the observed outcomes.\nIs this bad news? Not necessarily. A model that fits the observations too well is likely to be biased. However, it might still be important to understand why some users have a different relationship between hours spent and total transactions. As usual, information on the specific context is key.\nSo far we have looked at observations with \u0026ldquo;unusual\u0026rdquo; characteristics and \u0026ldquo;unusual\u0026rdquo; model fit, but what is the observation itself is distorting the model? How much our model is driven by a handful of observations?\nInfluence The concept of influence and influence functions was developed precisely to answer this question: what are influential observations? This questions were very popular in the 80\u0026rsquo;s and lost appeal for a long time until the recent need of explaining complex machine learning and AI models.\nThe general idea is to define an observation as influential if removing it significantly changes the estimated model. In linear regression, we define the influence of observation $i$ as:\n$$ \\hat{\\beta} - \\hat{\\beta}_{-i} = (X\u0026rsquo;X)^{-1} x_i e_i $$\nWhere $\\hat{\\beta}_{-i}$ is the OLS coefficient estimated omitting observation $i$.\nAs you can see, there is a tight connection to both leverage $h_{ii}$ and residuals $e_i$: influence is almost the product of the two. Indeed, in linear regression, observations with high leverage are observations that are both outliers and have high residuals. None of the two conditions alone is sufficient for an observation to have an influence on the model.\nWe can see it best in the data.\ndf['influence'] = (np.linalg.inv(X.T @ X) @ X.T).T * np.abs(Y - Y_hat) df['high_influence'] = df['influence'] \u0026gt; (np.mean(df['influence']) + 2*np.std(df['influence'])) fix, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6)) sns.histplot(data=df, x='influence', hue='high_influence', alpha=1, bins=30, ax=ax1).set(title='Distribution of Influences'); sns.scatterplot(data=df, x='hours', y='transactions', hue='high_influence', ax=ax2).set(title='Data Scatterplot'); In our dataset, there is only one observation with high influence, and it is disproportionally larger than the influence of all other observations.\nWe can now plot all \u0026ldquo;unusual\u0026rdquo; points in the same plot. I also report residuals and leverage of each point in a separate plot.\ndef plot_leverage_residuals(df): # Hue df['type'] = 'Normal' df.loc[df['high_residual'], 'type'] = 'High Residual' df.loc[df['high_leverage'], 'type'] = 'High Leverage' df.loc[df['high_influence'], 'type'] = 'High Influence' # Init figure fig, (ax1,ax2) = plt.subplots(1,2, figsize=(12,5)) ax1.plot(X, Y_hat, lw=1, c='grey', zorder=0.5) sns.scatterplot(data=df, x='hours', y='transactions', ax=ax1, hue='type').set(title='Data') sns.scatterplot(data=df, x='residual', y='leverage', hue='type', ax=ax2).set(title='Metrics') ax1.get_legend().remove() sns.move_legend(ax2, \u0026quot;upper left\u0026quot;, bbox_to_anchor=(1.05, 0.8)); plot_leverage_residuals(df) As we can see, we have one point with high residual and low leverage, one with high leverage and low residual and only one point with both high leverage and high residual: the only influential point.\nFrom the plot it is also clear why none of the two conditions alone is sufficient for an observation to rive the model. The orange point has high residual but it lies right in the middle of the distribution and therefore cannot tilt the line of best fit. The green point instead has high leverage and lies far from the center of the distribution but its perfectly aligned with the line of fit. Removing it would not change anything. The red dot instead is different from the others in terms of both characteristics and behavior and therefore tilts the fit line towards itself.\nConclusion In this post, we have seen some different ways in which observations can be \u0026ldquo;unusual\u0026rdquo;: they can have either unusual characteristics or unusual behavior. In linear regression, when an observation has both it is also influential: it tilts the model towards itself.\nIn the example of the article, we concentrated on a univariate linear regression. However, research on influence functions has recently become a hot topic because of the need to make black-box machine learning algorithms understandable. With models with millions of parameters, billions of observations and wild non-linearities, it can be very hard to establish whether a single observation is influential and how.\nReferences [1] D. Cook, Detection of Influential Observation in Linear Regression (1980), Technometrics.\n[2] D. Cook, S. Weisberg, Characterizations of an Empirical Influence Function for Detecting Influential Cases in Regression (1980), Technometrics.\n[2] P. W. Koh, P. Liang, Understanding Black-box Predictions via Influence Functions (2017), ICML Proceedings.\nCode You can find the original Jupyter Notebook here:\nhttps://github.com/matteocourthoud/Blog-Posts/blob/main/notebooks/outliers.ipynb\n","date":1688947200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1688947200,"objectID":"1a8f1e89d25244954cbcaf05f48d3b09","permalink":"https://matteocourthoud.github.io/post/outliers_levarage/","publishdate":"2023-07-10T00:00:00Z","relpermalink":"/post/outliers_levarage/","section":"post","summary":"What makes an observation \u0026ldquo;unusual\u0026rdquo;?\nIn data science, one common task is outlier detection. This is a broad term that is often misused or misunderstood. More broadly, we are often interested in understanding any observation is \u0026ldquo;unusual\u0026rdquo;.","tags":null,"title":"Outliers, Leverage, and Influential Observations","type":"post"},{"authors":null,"categories":null,"content":"INTRO\nSelf-Driving Cars Suppose you were a ride-sharing platform and you want to test the effect of self-driving cars in your fleet.\nAs you can imagine, there are many limitations to running an AB/test for this type of feature. First of all, it\u0026rsquo;s complicated to randomize individual rides. Second, it\u0026rsquo;s a very expensive intervention. Third, and statistically most important, you cannot run this intervention at the ride level. The problem is that there are spillover effects from treated to control units: if indeed self-driving cars are more efficient, it means that they can serve more customers in the same amount of time, reducing the customers available to normal drivers (the control group). This spillover contaminates the experiment and prevents a causal interpretation of the results.\nFor all these reasons, we select only one city at random to run this experiment\u0026hellip; (drum roll)\u0026hellip; Miami!\nI generate a simulated dataset in which we observe a panel of U.S. cities over time. The revenue data is made up, while the socio-economic variables are taken from the OECD database. I import the data generating process dgp_selfdriving() from src.dgp. I also import some plotting functions and libraries from src.utils.\n%matplotlib inline %config InlineBackend.figure_format = 'retina' from src.utils import * from src.dgp import dgp_selfdriving treatment_year = 2013 treated_city = 'Miami' df = dgp_selfdriving().generate_data(year=treatment_year, city=treated_city) df.head() city year density employment gdp population treated post revenue 0 Atlanta 2003 290 0.629761 6.4523 4.267538 False False 25.713947 1 Atlanta 2004 295 0.635595 6.5836 4.349712 False False 23.852279 2 Atlanta 2005 302 0.645614 6.6998 4.455273 False False 24.332397 3 Atlanta 2006 313 0.648573 6.5653 4.609096 False False 23.816017 4 Atlanta 2007 321 0.650976 6.4184 4.737037 False False 25.786902 len(df.city.unique()) 46 We have information on the largest 46 U.S. cities for the period 2002-2019. The panel is balanced, which means that we observe all cities for all time periods.\nIs the treated unit, Miami, comparable to the rest of the sample? Let\u0026rsquo;s use the create_table_one function from Uber\u0026rsquo;s causalml package to produce a covariate balance table, containing the average value of our observable characteristics, across treatment and control groups. As the name suggests, this should always be the first table you present in causal inference analysis.\nfrom causalml.match import create_table_one create_table_one(df, 'treated', ['density', 'employment', 'gdp', 'population', 'revenue']) Control Treatment SMD Variable n 765 17 density 256.63 (172.90) 364.94 (19.61) 0.8802 employment 0.63 (0.05) 0.60 (0.04) -0.5266 gdp 6.07 (1.16) 5.12 (0.29) -1.1124 population 3.53 (3.81) 5.85 (0.31) 0.861 revenue 25.25 (2.45) 23.86 (2.39) -0.5737 As expected, the groups are not balanced: Miami is more densely populated, poorer, larger and has lower employment rate than the other cities in the US in our sample.\nWe are interested in understanding the impact of the introduction of self-driving cars on revenue.\nOne initial idea could be to analyze the data as we would in an A/B test, comparing control and treatment group. We can estimate the treatment effect as a difference in means in revenue between the treatment and control group, after the introduction of self-driving cars.\nsmf.ols('revenue ~ treated', data=df[df['post']==True]).fit().summary().tables[1] coef std err t P\u003e|t| [0.025 0.975] Intercept 26.6006 0.127 210.061 0.000 26.351 26.850 treated[T.True] -0.7156 0.859 -0.833 0.405 -2.405 0.974 The effect of self-driving cars seems to be negative but not significant.\nThe main problem here is that we have a single treated unit: Miami. It\u0026rsquo;s very hard to argue that Miami is comparable to other cities. Randomization ensures that this simple estimator is unbiased, ex-ante. However, with a single treated unit, the estimator suffers from severe small sample bias .\nOne alternative procedure, is to compare revenue before and after the treatment, within the city of Miami.\nsmf.ols('revenue ~ post', data=df[df['city']==treated_city]).fit().summary().tables[1] coef std err t P\u003e|t| [0.025 0.975] Intercept 22.4485 0.534 42.044 0.000 21.310 23.587 post[T.True] 3.4364 0.832 4.130 0.001 1.663 5.210 The effect of self-driving cars seems to be positive and statistically significant.\nHowever, the problem of this procedure is that there might have been many other things happening after 2016. It\u0026rsquo;s a very strong stretch to attribute all differences to self-driving cars.\nWe can better understand this concern if we plot the time trend of revenue over cities. First, we need to reshape the data into a wide format, with one column per city and one row per year.\ndf = df.pivot(index='year', columns='city', values='revenue').reset_index() Now, let\u0026rsquo;s plot the revenue over time for Miami and for the other cities.\ncities = [c for c in df.columns if c!='year'] df['Other Cities'] = df[[c for c in cities if c != treated_city]].mean(axis=1) def plot_lines(df, line1, line2, year, hline=True): sns.lineplot(x=df['year'], y=df[line1].values, label=line1) sns.lineplot(x=df['year'], y=df[line2].values, label=line2) plt.axvline(x=year, ls=\u0026quot;:\u0026quot;, color='C2', label='Self-Driving Cars', zorder=1) plt.legend(); plt.title(\u0026quot;Average revenue per day (in M$)\u0026quot;); Since we are talking about Miami, let\u0026rsquo;s use an appropriate color palette.\nsns.set_palette(sns.color_palette(['#f14db3', '#0dc3e2', '#443a84'])) plot_lines(df, treated_city, 'Other Cities', treatment_year) As we can see, revenue seems to be increasing after the treatment in Miami. But it\u0026rsquo;s a very volatile time series. And revenue was increasing also in the rest of the country. It\u0026rsquo;s very hard from this plot to attribute the change to self-driving case.\nCan we do better?\nSynthetic Control The answer is yes! Synthetic control methods were first introduced by Abadie, Diamond and Hainmueller (2010) and allow us to do causal inference when we have as few as one treated unit and many control units and we observe them over time.\nSetting We assume that for a panel of i.i.d. subjects $i = 1, \u0026hellip;, n$ over time $t=1, \u0026hellip;,T$ we observed a set of variables $(X_{it}, D_i, Y_{it})$ that includes\na treatment assignment $D_i \\in \\lbrace 0, 1 \\rbrace$ (treated) a response $Y_{i,t} \\in \\mathbb R$ (revenue) a feature vector $X_{i,t} \\in \\mathbb R^n$ (population, density, employment and GDP) Moreover, one unit (Miami in our case) is treated at time $t^*$ (2016 in our case). We distinguish time periods before treatment and time periods after treatment.\nCrucially, treatment $D_i$ is not randomly assigned, therefore a difference in means between the treated unit(s) and the control group is not an unbiased estimator of the average treatment effect.\nThe Problem The problem is that, as usual, we do not observe the counterfactual outcome for treated units, i.e. we do not know what would have happened to them, if they had not been treated. This is known as the fundamental problem of causal inference.\nThe simplest approach, would be just to compare pre and post periods. This is called the event study approach.\nHowever, we can do better than this. In fact, even though treatment was not randomly assigned, we still have access to some units that were not treated.\nFor the outcome variable we have the following setup\n$$ Y = \\begin{bmatrix} Y_{t, post} \\ \u0026amp; Y_{c, post} \\newline Y_{t, pre} \\ \u0026amp; Y_{c, pre} \\end{bmatrix} $$\nwhich we can rewrite as\n$$ Y = \\begin{bmatrix} Y^{(1)} _ {t, post} \\ \u0026amp; Y^{(0)} _ {c, post} \\newline Y^{(0)} _ {t, pre} \\ \u0026amp; Y^{(0)} _ {c, pre} \\end{bmatrix} $$\nWe basically have a missing data problem since we do not observe $Y^{(0)} _ {t, post}$.\nThe Solution Following Doudchenko and Inbens (2018), we can formulate an estimate of the counterfactual outcome for the treated unit as a linear combination of the observed outcomes for the control units.\n$$ \\hat Y^{(0)} _ {t, post} = \\alpha + \\sum_{i \\in c} \\beta_{i} Y^{(0)} _ {i, post} $$\nwhere\nthe constant $\\alpha$ allows for different averages between the two groups the weights $\\beta_i$ are allowed to vary across control units $i$ (otherwise, it would be a difference-in-differences) How should we choose which weights to use? We want our synthetic control to approximate the outcome as closely as possible, before the treatment. The first approach could be to define the weights as\n$$ \\hat \\beta = \\arg \\min_{\\beta} || \\boldsymbol X_{t, pre} - \\boldsymbol \\beta \\boldsymbol X_{c, pre} || = \\sqrt{ \\sum_{p} \\left( X_{t, p, pre} - \\sum_{i \\in c} \\beta_{p} X_{c, p, pre} \\right)^2 } $$\nI.e. the weights are such that they minimize the distance between observable characteristics of control units $X_c$ and the treated unit $X_t$ before the treatment.\nYou might notice a very close similarity to linear regression. Indeed, we are doing something very similar.\nIn linear regression, we usually have many units (observations), few exogenous features and one endogenous feature and we try to express the endogenous feature as a linear combination of the endogenous features, for each unit.\nWith synthetic control, we instead have many time periods (features), few control units and a single treated unit and we try to express the treated unit as a linear combination of the control units, for each time period\nIn the end, we are doing the same thing, but with a transposed dataset.\nBack to self-driving cars Let\u0026rsquo;s go back to the data now! First, we write a synth_predict function that takes as input a model that is trained on control cities and tries to predict the outcome of the treated city, Miami, before the introduction of self-driving cars.\ndef synth_predict(df, model, city, year): other_cities = [c for c in cities if c not in ['year', city]] y = df.loc[df['year'] \u0026lt;= year, city] X = df.loc[df['year'] \u0026lt;= year, other_cities] df[f'Synthetic {city}'] = model.fit(X, y).predict(df[other_cities]) return model Let\u0026rsquo;s estimate the model via linear regression.\nfrom sklearn.linear_model import LinearRegression coef = synth_predict(df, LinearRegression(), treated_city, treatment_year).coef_ How well did we match pre-self-driving cars revenue in Miami? What is the implied effect of self-driving cars?\nWe can visually answer both questions by plotting the actual revenue in Miami against the predicted one.\nplot_lines(df, treated_city, f'Synthetic {treated_city}', treatment_year) It looks like self-driving cars had a sensible positive effect on revenue in Miami: the predicted trend is lower than the actual data and diverges right after the introduction of self-driving cars.\nOn the other hand, we are clearly overfitting: the pre-treatment predicted revenue line is perfectly overlapping with the actual data. Given the high variability of revenue in Miami, this is suspicious, to say the least.\nAnother problem concerns the weights. Let\u0026rsquo;s plot them.\ndf_states = pd.DataFrame({'city': [c for c in cities if c!=treated_city], 'ols_coef': coef}) plt.figure(figsize=(10, 10)) sns.barplot(data=df_states, x='ols_coef', y='city'); We have many negative weights, which do not make much sense from a causal inference perspective. I can understand that Miami can be expressed as a combination of 0.2 St. Louis, 0.15 Oklahoma and 0.15 Hartford. But what does it mean that Miami is -0.15 Milwaukee?\nSince we would like to interpret our synthetic control as a weighted average of untreated states, all weights should be positive and they should sum to one.\nTo address both concerns (weighting and overfitting), we need to impose some restrictions on the weights.\nExtensions Weights To solve the problems of overweighting and negative weights, Abadie, Diamond and Hainmueller (2010) propose the following weights:\n$$ \\hat \\beta = \\arg \\min_{\\beta} || \\boldsymbol X_t - \\boldsymbol \\beta \\boldsymbol X_c || = \\sqrt{ \\sum_{p} \\left( X_{t, p} - \\sum_{i \\in c} \\beta_{p} X_{c, p} \\right)^2 } \\quad \\text{s.t.} \\quad \\sum_{p} \\beta_p = 1 \\quad \\text{and} \\quad \\beta_p \\geq 0 \\quad \\forall p $$\nWhich means, a set of weights $\\beta$ such that\nweighted observable characteristics of the control group $X_c$, match the observable characteristics of the treatment group $X_t$, before the treatment\nthey sum to 1\nand are not negative.\nWith this approach we get an interpretable counterfactual as a weighted avarage of untreated units.\nLet\u0026rsquo;s write now our own objective function. I create a new class SyntheticControl() which has both a loss function, as described above, a method to fit it and predict the values for the treated unit.\nfrom toolz import partial from scipy.optimize import fmin_slsqp class SyntheticControl(): # Loss function def loss(self, W, X, y) -\u0026gt; float: return np.sqrt(np.mean((y - X.dot(W))**2)) # Fit model def fit(self, X, y): w_start = [1/X.shape[1]]*X.shape[1] self.coef_ = fmin_slsqp(partial(self.loss, X=X, y=y), np.array(w_start), f_eqcons=lambda x: np.sum(x) - 1, bounds=[(0.0, 1.0)]*len(w_start), disp=False) self.mse = self.loss(W=self.coef_, X=X, y=y) return self # Predict def predict(self, X): return X.dot(self.coef_) We can now repeat the same procedure as before, but using the SyntheticControl method instead of the simple, unconstrained LinearRegression.\ndf_states['coef_synth'] = synth_predict(df, SyntheticControl(), treated_city, treatment_year).coef_ plot_lines(df, treated_city, f'Synthetic {treated_city}', treatment_year) As we can see, now we are not overfitting anymore. The actual and predicted revenue pre-treatment are close but not identical. The reason is that the non-negativity constraint is constraining most coefficients to be zero (as Lasso does).\nIt looks like the effect is again negative. However, let\u0026rsquo;s plot the difference between the two lines to better visualize the magnitude.\ndef plot_difference(df, city, year, vline=True, hline=True, **kwargs): sns.lineplot(x=df['year'], y=df[city] - df[f'Synthetic {city}'], **kwargs) if vline: plt.axvline(x=year, ls=\u0026quot;:\u0026quot;, color='C2', lw=3, label='Self-driving Cars', zorder=100) plt.legend() if hline: sns.lineplot(x=df['year'], y=0, lw=3, color='k', zorder=1) plt.title(\u0026quot;Estimated effect of self-driving cars\u0026quot;); plot_difference(df, treated_city, treatment_year) The difference is clearly positive and slightly increasing over time.\nWe can also visualize the weights to interpret the estimated counterfactual (what would have happened in Miami, without self-driving cars).\nplt.figure(figsize=(10, 10)) sns.barplot(data=df_states, x='coef_synth', y='city'); As we can see, now we are expressing revenue in Miami as a linear combination of just a couple of cities: Tampa, St. Louis and, to a lower extent, Las Vegas. This makes the whole procedure very transparent.\nInference What about inference? Is the estimate significantly different from zero? Or, more practically, \u0026ldquo;how unusual is this estimate under the null hypothesis of no policy effect?\u0026rdquo;.\nWe are going to perform a randomization/permutation test in order to answer this question. The idea is that if the policy has no effect, the effect we observe for Miami should not be significantly different from the effect we observe for any other city.\nTherefore, we are going to replicate the procedure above, but for all other cities and compare them with the estimate for Miami.\nfrom matplotlib.offsetbox import OffsetImage, AnnotationBbox fig, ax = plt.subplots() for city in cities: synth_predict(df, SyntheticControl(), city, treatment_year) plot_difference(df, city, treatment_year, vline=False, alpha=0.2, color='C1', lw=3) plot_difference(df, treated_city, treatment_year) ax.add_artist(AnnotationBbox(OffsetImage(mpimg.imread('fig/miami.png'), zoom=0.25), (2015, 2.7), frameon=False)); --------------------------------------------------------------------------- NameError Traceback (most recent call last) Input In [21], in \u0026lt;module\u0026gt; 6 plot_difference(df, city, treatment_year, vline=False, alpha=0.2, color='C1', lw=3) 7 plot_difference(df, treated_city, treatment_year) ----\u0026gt; 8 ax.add_artist(AnnotationBbox(OffsetImage(mpimg.imread('fig/miami.png'), zoom=0.25), (2015, 2.7), frameon=False)) NameError: name 'mpimg' is not defined From the graph we notice two things. First, the effect for California is quite extreme and therefore likely not to be driven by random noise.\nSecond, we also notice that there are a couple of states for which we cannot fit the pre-trend very well. This is expected since, for each state, we are building the counterfactual trend as a convex combination of all other states. States that are quite extreme in terms of cigarette consumpion are very useful to build the counterfactuals of other states, but it\u0026rsquo;s hard to build a counterfactual for them. Not to bias the analysis, let\u0026rsquo;s exclude states for which we cannot build a \u0026ldquo;good enough\u0026rdquo; counterfectual, in terms of pre-treatment MSE.\n$$ MSE_{pre} = \\frac{1}{n} \\sum_{t \\in \\text{pre}} \\left( Y_t - \\hat Y_t \\right)^2 $$\nmse_treated = synth_predict(df, SyntheticControl(), treated_city, treatment_year).mse fig, ax = plt.subplots() for city in cities: mse = synth_predict(df, SyntheticControl(), city, treatment_year).mse if mse \u0026lt; 2 * mse_treated: plot_difference(df, city, treatment_year, vline=False, alpha=0.2, color='C1', lw=3) plot_difference(df, treated_city, treatment_year) ax.add_artist(AnnotationBbox(OffsetImage(mpimg.imread('fig/miami.png'), zoom=0.25), (2015, 2.7), frameon=False)); After exluding extreme observations, it looks like the effect for California is very unusual, especially if we consider a one-sided hypothesis test (it feels weird to assume that the policy could ever increase cigarette sales).\nOne statistic that the authors suggest to perform a randomization test is the ratio between pre-treatment MSE and post-treatment MSE.\n$$ \\lambda = \\frac{MSE_{post}}{MSE_{pre}} = \\frac{\\frac{1}{n} \\sum_{t \\in \\text{post}} \\left( Y_t - \\hat Y_t \\right)^2 }{\\frac{1}{n} \\sum_{t \\in \\text{pre}} \\left( Y_t - \\hat Y_t \\right)^2 } $$\nWe can compute a p-value as the number of observations with higher ratio.\nlambdas = {} for city in cities: mse_pre = synth_predict(df, SyntheticControl(), city, treatment_year).mse mse_tot = np.mean((df[f'Synthetic {city}'] - df[city])**2) lambdas[city] = (mse_tot - mse_pre) / mse_pre print(f\u0026quot;p-value: {np.mean(np.fromiter(lambdas.values(), dtype='float') \u0026gt; lambdas[treated_city]):.4}\u0026quot;) It seems that only $4.3%$ of the cities had a larger MSE ratio. We can visualize the distribution of the statistic under permutation with a histogram.\nfig, ax = plt.subplots() _, bins, _ = plt.hist(lambdas.values(), bins=20, color=\u0026quot;C1\u0026quot;); plt.hist([lambdas[treated_city]], bins=bins) plt.title('Ratio of $MSE_{post}$ and $MSE_{pre}$ across cities'); ax.add_artist(AnnotationBbox(OffsetImage(plt.imread('fig/miami.png'), zoom=0.25), (2.7, 1.6), frameon=False)); Indeed, the statistic for Miami is quite extreme.\nSynthetic Control vs Other Methods What are the advantages and disadvantages of synthetic control methods with respect to other methods?\nAs long as we use positive weights that are constrained to sum to one, the method avoids extrapolation: we will never go out of the support of the data\nIt can be \u0026ldquo;pre-registered\u0026rdquo; in the sense that you don\u0026rsquo;t need post-treatment observations to build the method: could avoid p-hacking and cherry picking\nWeights make the counterfactual analysis explicit: one can look at the weights and understand which comparison we are making\nIt\u0026rsquo;s a bridge between quantitative and qualitative research: can be used to inspect single-treated unit cases\nConclusion TBD\nReferences [1] A. Abadie, A. Diamond and J. Hainmueller, Synthetic Control Methods for Comparative Case Studies: Estimating the Effect of California’s Tobacco Control Program (2010), Journal of the American Statistical Association.\n[2] A. Abadie, J. L\u0026rsquo;Hour, A Penalized Synthetic Control Estimator for Disaggregated Data (2020), Journal of the American Statistical Association.\n[3] N. Doudchenko, G. Imbens, Balancing, Regression, Difference-In-Differences and Synthetic Control Methods: A Synthesis (2017), working paper.\n[] Matrix completion methods for causal panel data models\n[] Using Synthetic Controls: Feasibility, Data Requirements, and Methodological Aspects\nCode You can find the original Jupyter Notebook here:\nhttps://github.com/matteocourthoud/Blog-Posts/blob/main/notebooks/synthetic_control.ipynb\n","date":1688947200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1688947200,"objectID":"b5d7c5d08438854e3a113705c275ed8c","permalink":"https://matteocourthoud.github.io/post/synthetic_control/","publishdate":"2023-07-10T00:00:00Z","relpermalink":"/post/synthetic_control/","section":"post","summary":"INTRO\nSelf-Driving Cars Suppose you were a ride-sharing platform and you want to test the effect of self-driving cars in your fleet.\nAs you can imagine, there are many limitations to running an AB/test for this type of feature.","tags":null,"title":"Synthetic Control","type":"post"},{"authors":null,"categories":null,"content":"A short guide to a simple and powerful alternative to the bootstrap\nIn causal inference we do not want just to compute treatment effect, we also want to do inference (duh!). In some cases, it\u0026rsquo;s very easy to compute the asymptotic difference of an estimator, thanks to the central limit theorem. This is the case of computing the average treatment effect in AB tests or randomized controlled trials, for example. However, in other settings, inference is more complicated. The most frequent setting is the computation of quantities that are not sums or averages, such as the median treatment effect, for example. In these cases, we cannot rely on the central limit theorem. What can we do then?\nThe bootstrap is the answer! It is a very powerful procedure to compute the distribution of an estimator, without needing any knowledge of the data generating process. It is also very intuitive and simple to implement: just re-sample your data with replacement a lot of times and compute your estimator on the re-computed sample.\nCan we do better? The answer is yes! The Bayesian Bootstrap is a powerful procedure that in a lot of setting performs better than the bootstrap. In particular, it\u0026rsquo;s usually faster, can give tighter confidence intervals and prevents a lot of corner cases of the bootstrap. In this article we are going to explore this simple but powerful procedure more in detail.\nThe Bootstrap Bootstrap is a procedure to compute properties of an estimator by random re-sampling with replacement from the data. It was first introduced by Efron (1979). The procedure is very simple and consists in the following steps.\nSuppose you have access to an i.i.d. sample $\\lbrace X_i \\rbrace_{i=1}^n$ and you want to compute a statistic $\\theta$ using an estimator $\\hat \\theta(X)$. You can approximate the distribution of $\\hat \\theta$ by\nSample $n$ observations with replacement from your sample $\\lbrace \\tilde X_i \\rbrace_{i=1}^n$ Compute the estimator $\\hat \\theta_{bootstrap}(\\tilde X)$ Repeat steps 1 and 2 a large number of times The distribution of $\\hat \\theta_{bootstrap}$ is a good approximation of the distribution of $\\hat \\theta$.\nWhy is the bootstrap so powerful?\nFirst of all, it\u0026rsquo;s easy to implement. It does not require you to do anything more than what you were already doing: estimating $\\theta$. You just need to do it a lot of times. Indeed, the main disadvantage of the bootstrap is its computational speed. If estimating $\\theta$ once is slow, bootstrapping it is prohibitive.\nSecond, the bootstrap makes no distributional assumption. It only assumes a representative sample from your population, where observations are independent from each other. This assumption might be violated when observations are tightly connected with each other, such when studying social networks.\nIs bootstrap just weighting?\nIn the end, what we are doing is assigning integer weights to our observations, such that their sum adds up to $n$. Such distribution is the multinomial distribution.\nLet\u0026rsquo;s have a look at what a multinomial distribution look like by drawing a sample of size 10.000.\n%matplotlib inline %config InlineBackend.figure_format = 'retina' from src.utils import * N = 10_000 np.random.seed(1) bootstrap_weights = np.random.multinomial(N, np.ones(N)/N) np.sum(bootstrap_weights) 10000 First of all, we check that indeed the weights sum up to 1000, or equivalently, we generated a re-sample of the same size of the data.\nWe can now plot the distribution of weights.\nsns.countplot(bootstrap_weights, color='C0').set(title='Bootstrap Weights'); As we can see, around 3600 observations got zero weight, however a couple of observations got a weights of 6. Or equivalently, around 3600 observations did not get re-sampled while a couple of observations got samples as many as 6 times.\nNow you might have a spontaneous question: why not use continuous weights instead of discrete ones?\nVery good question! The Bayesian Bootstrap is the answer.\nThe Bayesian Bootstrap The Bayesian bootstrap was introduced by Rubin (1981) and it\u0026rsquo;s based on a very simple idea: why not draw a smoother distribution of weights? The continuous equivalent of the multinomial distribution is the Dirichelet distribution. Below I plot the probability distribution of Multinomial and Dirichelet weights for a single observation (they are Poisson and Gamma distributed, respectively).\nfrom scipy.stats import gamma, poisson x1 = np.arange(0, 8, 0.001) x2 = np.arange(0, 8, 1) sns.barplot(x2, poisson.pmf(x2, mu=1), color='C0', label='Multinomial Weights'); plt.plot(x1, gamma.pdf(x1, a=1.0001), color='C1', label='Dirichlet Weights'); plt.legend() plt.title('Distribution of Bootstrap Weights'); The Bayesian Bootstrap has many advantages.\nThe first and most intuitive one is that it delivers estimates that are much more smooth than the normal bootstrap, because of its continuous weighting scheme. Moreover, the continuous weighting scheme prevents corner cases from emerging, since no observation will ever receive zero weight. For example, in linear regression, no problem of collinearity emerges, if there wasn\u0026rsquo;t one in the original sample. Lastly, being a Bayesian method, we gain interpretation: the estimated distribution of the estimator can be interpreted as the posterior distribution with an uninformative prior. Let\u0026rsquo;s now draw a set a Dirichlet weights.\nbayesian_weights = np.random.dirichlet(alpha=np.ones(N), size=1)[0] * N np.sum(bayesian_weights) 10000.000000000005 The weights naturally sum to (approximately) 1, so we have to scale them by a factor N.\nAs before, we can plot the distribution of weights, with the difference that now we have continuous weights, so we have to approximate the distribution.\nsns.histplot(bayesian_weights, color='C1').set(title='Dirichlet Weights'); As you might have noticed, the Dirichelet distirbution has a parameter $\\alpha$ that we have set to 1 for all observations. What does it do?\nThe $\\alpha$ parameter essentially governs both the absolute and relative probability of being samples. Increasing $\\alpha$ for all observations makes the distribution less skewed so that all observations have a more similar weight. For $\\alpha \\to \\infty$, all observations receiver the same weight and we are back to the original sample.\nHow should we pick $\\alpha$? Shao and Tu (1995) suggest the following.\nThe distribution of the random weight vector does not have to be restricted to the Diri(l, \u0026hellip; , 1). Later investigations found that the weights having a scaled Diri(4, \u0026hellip; ,4) distribution give better approximations (Tu and Zheng, 1987)\nLet\u0026rsquo;s have a look at how a Dirichelet distribution with $\\alpha = 4$ for all observations compare to our previous distribution with $\\alpha = 1$ for all observations.\nbayesian_weights2 = np.random.dirichlet(np.ones(N) * 4, 1)[0] * N sns.histplot(bayesian_weights, color='C1') sns.histplot(bayesian_weights2, color='C2').set(title='Comparing Dirichlet Weights'); plt.legend([r'$\\alpha = 1$', r'$\\alpha = 4$']); The new distribution is much less skewed and more concentrated around the average value of 1.\nExamples Let\u0026rsquo;s have a look at a couple of examples, where we compare both inference procedures.\nMean of a Skewed Distribution First, let\u0026rsquo;s have a look at one of the simplest and most common estimators: the sample mean.\nnp.random.seed(2) X = pd.Series(np.random.pareto(2, 100)) sns.histplot(X).set(title='Sample from Pareto Distribution'); def classic_boot(df, estimator, seed=1): df_boot = df.sample(n=len(df), replace=True, random_state=seed) estimate = estimator(df_boot) return estimate classic_boot(X, np.mean) 0.7079805545831946 def bayes_boot(df, estimator, seed=1): np.random.seed(seed) w = np.random.dirichlet(np.ones(len(df)), 1)[0] result = estimator(df, weights=w) return result bayes_boot(X, np.average) 1.0378495251293498 from joblib import Parallel, delayed def bootstrap(boot_method, df, estimator, K): r = Parallel(n_jobs=8)(delayed(boot_method)(df, estimator, seed=i) for i in range(K)) return r def compare_boot(df, boot1, boot2, estimator, title, K=1000): s1 = bootstrap(boot1, df, estimator, K) s2 = bootstrap(boot2, df, estimator, K) df = pd.DataFrame({'Estimate': s1 + s2, 'Estimator': ['Classic']*K + ['Bayes']*K}) sns.histplot(data=df, x='Estimate', hue='Estimator') plt.legend([f'Bayes: {np.mean(s2):.2f} ({np.std(s2):.2f})', f'Classic: {np.mean(s1):.2f} ({np.std(s1):.2f})']) plt.title(f'Bootstrap Estimates of {title}') compare_boot(X, classic_boot, bayes_boot, np.average, 'Sample Mean') In this setting, both procedures give a very similar answer.\nWhich one is faster?\nimport time def compare_time(df, boot1, boot2, estimator, K=1000): t1, t2 = np.zeros(K), np.zeros(K) for k in range(K): # Classic bootstrap start = time.time() boot1(df, estimator) t1[k] = time.time() - start # Bayesian bootstrap start = time.time() boot2(df, estimator) t2[k] = time.time() - start print(f\u0026quot;Bayes wins {np.mean(t1 \u0026gt; t2)*100}% of the time (by {np.mean((t1 - t2)/t1*100):.2f}%)\u0026quot;) compare_time(X, classic_boot, bayes_boot, np.average) Bayes wins 99.8% of the time (by 82.89%) The Bayesian bootstrap is faster than the classical bootstrap 100% of the simulations, and by an impressive 83%!\nNo Weighting? No Problem What if we have an estimator that does not accept weights, such as the median? We can do two-level sampling.\ndef twolv_boot(df, estimator, seed=1): np.random.seed(seed) w = np.random.dirichlet(np.ones(len(df))*4, 1)[0] df_boot = df.sample(n=len(df)*10, replace=True, weights=w, random_state=seed) result = estimator(df_boot) return result np.random.seed(1) X = pd.Series(np.random.normal(0, 10, 1000)) compare_boot(X, classic_boot, twolv_boot, np.median, 'Sample Median') In this setting, the Bayesian Bootstrap is also more precise than the classical bootstrap.\nLogistic Regression with Rare Outcome Let\u0026rsquo;s now explore the first of two settings in which the classical bootstrap might fall into corner cases. Suppose we observed a feature $x$, normally distributed, and a binary outcome $y$. We are interested in the relationship between the two variables.\nN = 100 np.random.seed(1) x = np.random.normal(0, 1, N) y = np.rint(np.random.normal(x, 1, N) \u0026gt; 2) df = pd.DataFrame({'x': x, 'y': y}) df.head() x y 0 1.624345 0.0 1 -0.611756 0.0 2 -0.528172 0.0 3 -1.072969 0.0 4 0.865408 0.0 In this case, we observe a positive outcome only in 10 observations out of 100.\nnp.sum(df['y']) 10.0 Since the outcome is binary, we fit a logistic regression model.\nsmf.logit('y ~ x', data=df).fit(disp=False).summary().tables[1] coef std err z P\u003e|z| [0.025 0.975] Intercept -4.0955 0.887 -4.618 0.000 -5.834 -2.357 x 2.7664 0.752 3.677 0.000 1.292 4.241 Can we bootstrap the distribution of our estimator? Let\u0026rsquo;s try to compute the logistic regression coefficient over 1000 bootstrap samples.\nestimate_logit = lambda df: smf.logit('y ~ x', data=df).fit(disp=False).params[1] for i in range(1000): try: classic_boot(df, estimate_logit, seed=i) except Exception as e: print(f'Error for bootstrap number {i}: {e}') Error for bootstrap number 92: Perfect separation detected, results not available Error for bootstrap number 521: Perfect separation detected, results not available Error for bootstrap number 545: Perfect separation detected, results not available Error for bootstrap number 721: Perfect separation detected, results not available Error for bootstrap number 835: Perfect separation detected, results not available For 5 samples out of 1000, we are unable to compute the estimate. This would not have happened with then bayesian bootstrap.\nThis might seem like an innocuous issue in this case: we can just drop those observations. Let\u0026rsquo;s conclude with a much more dangerous example.\nSuppose we observed a binary feature $x$ and a continuous outcome $y$. We are again interested in the relationship between the two variables.\nN = 100 np.random.seed(1) x = np.random.binomial(1, 5/N, N) y = np.random.normal(1 + 2*x, 1, N) df = pd.DataFrame({'x': x, 'y': y}) df.head() x y 0 0 1.315635 1 0 -1.022201 2 0 0.693796 3 0 1.827975 4 0 1.230095 Let\u0026rsquo;s compare the two bootstrap estimators of the regression coefficient of $y$ on $x$.\nestimate_beta = lambda df, **kwargs: smf.wls('y ~ x', data=df, **kwargs).fit().params[1] compare_boot(df, classic_boot, bayes_boot, estimate_beta, 'beta') The classic bootstrap procedure estimates a 50% larger variance of our estimator. Why? If we look more closely, we seen that in almost 20 re-samples, we get a very unusual estimate of zero!\nThe problem is that in some samples we might not have have any observations with $x=1$. Therefore, in these re-samples, the estimated coefficient is zero. This does not happen with the Bayesian bootstrap, since it does not drop any observation.\nThe problematic part here is that we are not getting any error message or warning. This bias is very sneaky and could easily go unnoticed!\nConclusion The article was inspired by the following tweet by Brown University professor Peter Hull\nOk, so I come bearing good news for ~93% of you: esp. those bootstraping complex models (e.g. w/many FEs)\nInstead of resampling, which can be seen as reweighting by a random integer W that may be zero, you can reweight by a random non-zero non-integer W https://t.co/Rpm1GmomHg\n\u0026mdash; Peter Hull (@instrumenthull) January 29, 2022 Indeed, besides being a simple and intuitive procedure, the Bayesian Bootstrap is not part of the standard econometrics curriculum in economic graduate schools.\nReferences [1] B. Efron Bootstrap Methods: Another Look at the Jackknife (1979), The Annals of Statistics.\n[2] D. Rubin, The Bayesian Bootstrap (1981), The Annals of Statistics.\n[3] A. Lo, A Large Sample Study of the Bayesian Bootstrap (1987), The Annals of Statistics.\n[4] J. Shao, D. Tu, Jacknife and Bootstrap (1995), Springer.\nCode You can find the original Jupyter Notebook here:\nhttps://github.com/matteocourthoud/Blog-Posts/blob/main/notebooks/bayes_boot.ipynb\n","date":1688947200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1688947200,"objectID":"cf14b1dea595f3b0054773d1801d4983","permalink":"https://matteocourthoud.github.io/post/bayesian_bootstrap/","publishdate":"2023-07-10T00:00:00Z","relpermalink":"/post/bayesian_bootstrap/","section":"post","summary":"A short guide to a simple and powerful alternative to the bootstrap\nIn causal inference we do not want just to compute treatment effect, we also want to do inference (duh!","tags":null,"title":"The Bayesian Bootstrap","type":"post"},{"authors":null,"categories":null,"content":"An introduction to the Frisch-Waugh-Lowell theorem and how to use it to gain intuition in linear regressions\nThe Frisch-Waugh-Lowell theorem is a simple but yet powerful theorem that allows us to reduce multivariate regressions to univariate ones. This is extremely useful when we are interested in the relationship between two variables, but we still need to control for other factors, as it is often the case in causal inference.\nIn this blog post, I am going to introduce the Frisch-Waugh-Lowell theorem and illustrate some interesting applications.\nThe Theorem The theorem was first published by Ragnar Frisch and Frederick Waugh in 1933. However, since its proof was lengthy and cumbersome, Michael Lovell in 1963 provided a very simple and intuitive proof and his name was added to the theorem name.\nThe theorem states that, when estimating a model of the form\n$$ y_i = \\beta_1 x_{i,1} + \\beta_2 x_{i,2} + \\varepsilon_i $$\nthen, the following estimators of $\\beta_1$ are equivalent:\nthe OLS estimator obtained by regressing $y$ on $x_1$ and $x_2$ the OLS estimator obtained by regressing $y$ on $\\tilde x_1$ where $\\tilde x_1$ is the residual from the regression of $x_1$ on $x_2$ the OLS estimator obtained by regressing $\\tilde y$ on $\\tilde x_1$ where $\\tilde y$ is the residual from the regression of $y$ on $x_2$ Interpretation What did we actually learn?\nThe Frisch-Waugh-Lowell theorem is telling us that there are multiple ways to estimate a single regression coefficient. One possibility is to run the full regression of $y$ on $x$, as usual.\nHowever, we can also regress $x_1$ on $x_2$, take the residuals, and regress $y$ only those residuals. The first part of this process is sometimes referred to as partialling-out (or orthogonalization, or residualization) of $x_1$ with respect to $x_2$. The idea is that we are isolating the variation in $x_1$ that is orthogonal to $x_2$. Note that $x_2$ can be also be multi-dimensional (i.e. include multiple variables and not just one).\nWhy would one ever do that?\nThis seems like a way more complicated procedure. Instead of simply doing the regression in 1 step, now we need to do 2 or even 3 steps. It\u0026rsquo;s not intuitive at all. The main advantage comes from the fact that we have reduced a multivariate regression to a univariate one, making more tractable and more intuitive.\nWe will later explore more in detail three applications:\ndata visualization computational speed further applications for inference However, let\u0026rsquo;s first explore the theorem more in detail with an example.\nExample Suppose we were a retail chain, owning many different stores in different locations. We come up with a brilliant idea to increase sales: give away discounts in the form of coupons. We print a lot of coupons and we distribute them around.\nTo understand whether our marketing strategy worked, in each store, we check the average daily sales and which percentage of shoppers used a coupon. However, there is one problem: we are worried that higher income people are less likely to use the discount, but usually they spend more. To be safe, we also record the average income in the neighborhood of each store.\nWe can represent the data generating process with a Directed Acyclic Graph (DAG). If you are not familiar with DAGs, I have written a short introduction to Directed Acyclic Graphs here.\nflowchart LR classDef included fill:#DCDCDC,stroke:#000000,stroke-width:2px; classDef excluded fill:#ffffff,stroke:#000000,stroke-width:2px; classDef unobserved fill:#ffffff,stroke:#000000,stroke-width:2px,stroke-dasharray: 5 5; X1((coupons)) X2((income)) X3((weekday)) Y((sales)) X1 --\u0026gt; Y X2 --\u0026gt; X1 X2 --\u0026gt; Y X3 --\u0026gt; Y class X1,X2,X3,Y excluded; Let\u0026rsquo;s load and inspect the data. I import the data generating process from src.dgp and some plotting functions and libraries from src.utils.\n%matplotlib inline %config InlineBackend.figure_format = 'retina' from src.utils import * from src.dgp import dgp_store_coupons df = dgp_store_coupons().generate_data(N=50) df.head() sales coupons income dayofweek 0 821.7 0.199 66.243 2 1 602.3 0.245 43.882 3 2 655.1 0.162 44.718 5 3 625.8 0.269 39.270 4 4 696.6 0.186 58.654 1 We have information on 50 stores, for which we observe the percentage of customers that use coupons, daily sales (in thousand $), average income of the neighborhood (in thousand $), and day of the week.\nSuppose we were directly regressing sales on coupon usage. What would we get? I represent the result of the regression graphically, using seaborn regplot.\nsns.regplot(x=\u0026quot;coupons\u0026quot;, y=\u0026quot;sales\u0026quot;, data=df, ci=False, line_kws={'color':'r', 'label':'linear fit'}) plt.legend() plt.title(f\u0026quot;Sales and coupon usage\u0026quot;); It looks like coupons were a bad idea: in stores where coupons are used more, we observe lower sales.\nHowever, it might just be that people with higher income are using less coupons, while also spending more. If this was true, it could bias our results. In terms of the DAG, it means that we have a backdoor path passing through income, generating a non-causal relationship.\nflowchart LR classDef included fill:#DCDCDC,stroke:#000000,stroke-width:2px; classDef excluded fill:#ffffff,stroke:#000000,stroke-width:2px; classDef unobserved fill:#ffffff,stroke:#000000,stroke-width:2px,stroke-dasharray: 5 5; X1((coupons)) X2((income)) X3((weekday)) Y((sales)) X1 --\u0026gt; Y X2 --\u0026gt; X1 X2 --\u0026gt; Y X3 --\u0026gt; Y class X1,Y included; class X2,X3 excluded; linkStyle 1,2 stroke:#ff0000,stroke-width:4px; In order to recover the causal effect of coupons on sales we need to condition our analysis on income. This will block the non-causal path passing through income, leaving only the direct path from coupons to sales open, allowing us to estimate the causal effect.\nflowchart LR classDef included fill:#DCDCDC,stroke:#000000,stroke-width:2px; classDef excluded fill:#ffffff,stroke:#000000,stroke-width:2px; classDef unobserved fill:#ffffff,stroke:#000000,stroke-width:2px,stroke-dasharray: 5 5; X1((coupons)) X2((income)) X3((weekday)) Y((sales)) X1 --\u0026gt; Y X2 -.-\u0026gt; X1 X2 -.-\u0026gt; Y X3 --\u0026gt; Y class X1,X2,Y included; class X3 excluded; linkStyle 0 stroke:#00ff00,stroke-width:4px; Let\u0026rsquo;s implement this, by including income in the regression.\nsmf.ols('sales ~ coupons + income', df).fit().summary().tables[1] coef std err t P\u003e|t| [0.025 0.975] Intercept 161.4982 33.253 4.857 0.000 94.601 228.395 coupons 218.7548 50.058 4.370 0.000 118.052 319.458 income 9.5094 0.480 19.818 0.000 8.544 10.475 Now the estimated effect of coupons on sales is positive and significant. Coupons were actually a good idea after all.\nVerifying the Theorem Let\u0026rsquo;s now verify that the Frisch-Waugh-Lowell theorem actually holds. In particular, we want to check whether we get the same coefficient if, instead of regressing sales on coupons and income, we were\nregressing coupons on income computing the residuals coupons_tilde, i.e. the variation in coupons not explained by income regressing sales on coupons_tilde df['coupons_tilde'] = smf.ols('coupons ~ income', df).fit().resid smf.ols('sales ~ coupons_tilde - 1', df).fit().summary().tables[1] coef std err t P\u003e|t| [0.025 0.975] coupons_tilde 218.7548 1275.236 0.172 0.865 -2343.929 2781.438 Yes, the coefficient is the same! However, the standard errors now have increased a lot and the estimated coefficient is not significantly different from zero anymore.\nA better approach is to add a further step and repeat the same procedure also for sales:\nregressing sales on income computing the residuals sales_tilde, i.e. the variation in sales not explained by income and finally regress sales_tilde on coupons_tilde.\ndf['sales_tilde'] = smf.ols('sales ~ income', df).fit().resid smf.ols('sales_tilde ~ coupons_tilde - 1', df).fit().summary().tables[1] coef std err t P\u003e|t| [0.025 0.975] coupons_tilde 218.7548 49.025 4.462 0.000 120.235 317.275 The coefficient is still exactly the same, but now also the standard errors are almost identical.\nProjection What is partialling-out (or residualization, or orthogonalization) actually doing? What is happening when we take the residuals of coupons with respect to income?\nWe can visualize the procedure in a plot. First, let\u0026rsquo;s actually display the residuals of coupons with respect to income.\ndf[\u0026quot;coupons_hat\u0026quot;] = smf.ols('coupons ~ income', df).fit().predict() ax = sns.regplot(x=\u0026quot;income\u0026quot;, y=\u0026quot;coupons\u0026quot;, data=df, ci=False, line_kws={'color':'r', 'label':'linear fit'}) ax.vlines(df[\u0026quot;income\u0026quot;], np.minimum(df[\u0026quot;coupons\u0026quot;], df[\u0026quot;coupons_hat\u0026quot;]), np.maximum(df[\u0026quot;coupons\u0026quot;], df[\u0026quot;coupons_hat\u0026quot;]), linestyle='--', color='k', alpha=0.5, linewidth=1, label=\u0026quot;residuals\u0026quot;); plt.legend() plt.title(f\u0026quot;Coupons usage, income and residuals\u0026quot;); The residuals are the vertical dotted lines between the data and the linear fit, i.e. the part of the variation in coupons unexplained by income.\nBy partialling-out, we are removing the linear fit from the data and keeping only the residuals. We can visualize this procedure with a gif. I import the code from the src.figures file that you can find here.\nfrom src.figures import gif_projection gif_projection(x='income', y='coupons', df=df, gifname=\u0026quot;gifs/fwl.gif\u0026quot;) The original distribution of the data is on the left in blue, the partialled-out data in on the right in green. As we can see, partialling-out removes both the level and the trend in coupons that is explained by income.\nMultiple Controls We can use the Frisch-Waugh-Theorem also when we have multiple control variables. Suppose that we wanted to also include day of the week in the regression, to increase precision.\nsmf.ols('sales ~ coupons + income + dayofweek', df).fit().summary().tables[1] coef std err t P\u003e|t| [0.025 0.975] Intercept 124.2721 28.764 4.320 0.000 66.182 182.362 dayofweek[T.2] 7.7703 14.607 0.532 0.598 -21.729 37.270 dayofweek[T.3] 15.0895 11.678 1.292 0.204 -8.495 38.674 dayofweek[T.4] 28.2762 9.868 2.866 0.007 8.348 48.204 dayofweek[T.5] 44.0937 10.214 4.317 0.000 23.467 64.720 dayofweek[T.6] 50.7664 13.130 3.866 0.000 24.249 77.283 dayofweek[T.7] 57.3142 12.413 4.617 0.000 32.245 82.383 coupons 192.0262 39.140 4.906 0.000 112.981 271.071 income 9.8152 0.404 24.314 0.000 9.000 10.630 We can perform the same procedure as before, but instead of partialling-out only income, now we partial out both income and day of the week.\ndf['coupons_tilde'] = smf.ols('coupons ~ income + dayofweek', df).fit().resid df['sales_tilde'] = smf.ols('sales ~ income + dayofweek', df).fit().resid smf.ols('sales_tilde ~ coupons_tilde - 1', df).fit().summary().tables[1] coef std err t P\u003e|t| [0.025 0.975] coupons_tilde 192.0262 35.803 5.363 0.000 120.078 263.974 We still get exactly the same coefficient!\nApplications Let\u0026rsquo;s now inspect some useful applications of the FWL theorem.\nData Visualization One of the advantages of the Frisch-Waugh-Theorem is that it allows us to estimate the coefficient of interest from a univariate regression, i.e. with a single explanatory variable (or feature).\nTherefore, we can now represent the relationship of interest graphically. Let\u0026rsquo;s plot the residual sales against the residual coupons.\nsns.regplot(x=\u0026quot;coupons_tilde\u0026quot;, y=\u0026quot;sales_tilde\u0026quot;, data=df, ci=False, line_kws={'color':'r', 'label':'linear fit'}) plt.legend() plt.title(f\u0026quot;Residual sales and residual coupons\u0026quot;); Now it\u0026rsquo;s evident from the graph that the conditional relationship between sales and coupons is positive.\nOne problem with this approach is that the variables are hard to interpret: we now have negative values for both sales and coupons. Weird.\nWhy did it happen? It happened because when we partialled-out the variables, we included the intercept in the regression, effectively de-meaning the variables (i.e. normalizing their values so that their mean is zero).\nWe can solve this problem by scaling both variables, adding their mean.\ndf['coupons_tilde_scaled'] = df['coupons_tilde'] + np.mean(df['coupons']) df['sales_tilde_scaled'] = df['sales_tilde'] + np.mean(df['sales']) Now the magnitudes of the two variables are interpretable again.\nsns.regplot(x=\u0026quot;coupons_tilde_scaled\u0026quot;, y=\u0026quot;sales_tilde_scaled\u0026quot;, data=df, ci=False, line_kws={'color':'r', 'label':'linear fit'}) plt.legend() plt.title(f\u0026quot;Residual sales scaled and residual coupons scaled\u0026quot;); Is this a valid approach or did it alter our estimates? We can can check it by running the regression with the scaled partialled-out variables.\nsmf.ols('sales_tilde_scaled ~ coupons_tilde_scaled', df).fit().summary().tables[1] coef std err t P\u003e|t| [0.025 0.975] Intercept 641.6486 10.017 64.054 0.000 621.507 661.790 coupons_tilde_scaled 192.0262 36.174 5.308 0.000 119.294 264.758 The coefficient is exactly the same as before!\nComputational Speed Another application of the Frisch-Waugh-Lovell theorem is to increase the computational speed of linear estimators. For example it is used to compute efficient linear estimators in presence of high-dimensional fixed effects (day of the week in our example).\nSome packages that exploit the Frisch-Waugh-Lovell theorem include\nreghdfe in Stata pyhdfe in Python However, it\u0026rsquo;s important to also mention the fixest package in R, which is also exceptionally efficient in running regressions with high dimensional fixed effects.\nInference and Machine Learning Another important application of the FWL theorem sits at the intersection of machine learning and causal inference. I am referring to the work on post-double selection by Belloni, Chernozhukov, Hansen (2013) and the follow up work on \u0026ldquo;double machine learning\u0026rdquo; by Chernozhukov, Chetverikov, Demirer, Duflo, Hansen, Newey, Robins (2018).\nI plan to cover both applications in future posts, but I wanted to start with the basics. Stay tuned!\nReferences [1] R. Frisch and F. V. Waugh, Partial Time Regressions as Compared with Individual Trends (1933), Econometrica.\n[2] M. C. Lowell, Seasonal Adjustment of Economic Time Series and Multiple Regression Analysis (1963), Journal of the American Statistical Association.\n","date":1688947200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1688947200,"objectID":"cffbb4b8a4da9f6bae1e03036d26dbb9","permalink":"https://matteocourthoud.github.io/post/fwl_theorem/","publishdate":"2023-07-10T00:00:00Z","relpermalink":"/post/fwl_theorem/","section":"post","summary":"An introduction to the Frisch-Waugh-Lowell theorem and how to use it to gain intuition in linear regressions\nThe Frisch-Waugh-Lowell theorem is a simple but yet powerful theorem that allows us to reduce multivariate regressions to univariate ones.","tags":null,"title":"The FWL Theorem, Or How To Make All Regressions Intuitive","type":"post"},{"authors":null,"categories":null,"content":"An introduction to doubly-robust estimation of conditional average treatment effects (CATE)\nWhen estimating causal effects, the gold standard is randomized controlled trials or AB tests. By randomly exposing units to a treatment we make sure that individuals in both groups are comparable, on average, and any difference we observe can be attributed to the treatment effect alone.\nHowever, often the treatment and control groups are not perfectly comparable. This could be due to the fact that randomization was not perfect or available. Not always we can randomize a treatment, for ethical or practical reasons. And even when we can, sometimes we do not have enough individuals or units so that differences between groups are seizable. This happens often, for example, when randomization is not done at the individual level, but at a higher level of aggregation, for example zipcodes, counties or even states.\nIn a previous post, I have introduced and compared a series of methods that compute conditional average treatment effects (CATE) from observational or experimental data. Some of these methods require the researcher to specify and estimate the distribution of the outcome of interest, given the treatment and the observable characteristics (e.g. meta learners). Other methods require the researcher to specify and estimate the probability of being treated, given the observable characteristics (e.g. IPW).\nIn this post, we are going to see a procedure that combines both methods and is robust to misspecification of either method\u0026rsquo;s model: the Augmented Inverse Probability Weighted estimator (AIPW).\nTLDR; AIPW greatly improves both IPW and meta-learners, and you should always use it!\nExample Assume we had blog on statistics and causal inference 😇. To improve user experience, we are considering releasing a dark mode, and we would like to understand whether this new feature increases the time users spend on our blog.\nThis example is borrowed from my last post on the estimation of conditional average treatment effects (CATE). You can find the original post here. If you remember the setting, you can skip this introduction.\nWe are not a sophisticated company, therefore we do not run an AB test but we simply release the dark mode and we observe whether users select it or not and the time they spend on the blog. We know that there might be selection: users that prefer the dark mode could have different reading preferences and this might complicate our causal analysis.\nWe can represent the data generating process with the following Directed Acyclic Graph (DAG).\nflowchart TB classDef included fill:#DCDCDC,stroke:#000000,stroke-width:2px; classDef excluded fill:#ffffff,stroke:#000000,stroke-width:2px; classDef unobserved fill:#ffffff,stroke:#000000,stroke-width:2px,stroke-dasharray: 5 5; X1((gender)) X2((age)) X3((hours)) D((dark mode)) Y((read time)) D --\u0026gt; Y X1 --\u0026gt; Y X1 --\u0026gt; D X2 --\u0026gt; D X3 --\u0026gt; Y class D,Y included; class X1,X2,X3 excluded; We generate the simulated data using the data generating process dgp_darkmode() from src.dgp. I also import some plotting functions and libraries from src.utils.\n%matplotlib inline %config InlineBackend.figure_format = 'retina' from src.utils import * from src.dgp import dgp_darkmode dgp = dgp_darkmode() df = dgp.generate_data() df.head() read_time dark_mode male age hours 0 14.4 False 0 43.0 65.6 1 15.4 False 1 55.0 125.4 2 20.9 True 0 23.0 642.6 3 20.0 False 0 41.0 129.1 4 21.5 True 0 29.0 190.2 We have informations on 300 users for whom we observe whether they select the dark_mode (the treatment), their weekly read_time (the outcome of interest) and some characteristics like gender, age and total hours previously spend on the blog.\nWe would like to estimate the effect of the new dark_mode on users\u0026rsquo; read_time. As a first approach, we might naively compute the effect as a difference in means, assuming that the treatment and control sample are comparable. We can estimate the difference in means by regressing read_time on dark_mode.\nsmf.ols(\u0026quot;read_time ~ dark_mode\u0026quot;, data=df).fit().summary().tables[1] coef std err t P\u003e|t| [0.025 0.975] Intercept 19.1748 0.402 47.661 0.000 18.383 19.967 dark_mode[T.True] -0.4446 0.571 -0.779 0.437 -1.568 0.679 Individuals that select the dark_mode spend on average 0.44 hours less on the blog, per week. Should we conclude that dark_mode is a bad idea? Is this a causal effect?\nThe problem is that we did not run an AB test or randomized control trial, therefore users that selected the dark_mode might not be directly comparable with users that didn\u0026rsquo;t. Can we verify this concern? Partially. We can only check characteristics that we observe, gender, age and total hours in our setting. We cannot check if users differ along other dimensions that we don\u0026rsquo;t observe.\nLet\u0026rsquo;s use the create_table_one function from Uber\u0026rsquo;s causalml package to produce a covariate balance table, containing the average value of our observable characteristics, across treatment and control groups. As the name suggests, this should always be the first table you present in causal inference analysis.\nWe did not randomize the dark_mode so that users that selected it might not be directly comparable with users that didn\u0026rsquo;t. Can we verify this concern? Partially. We can only check characteristics that we observe, gender, age and total hours in our setting. We cannot check if users differ along other dimensions that we don\u0026rsquo;t observe.\nLet\u0026rsquo;s use the create_table_one function from Uber\u0026rsquo;s causalml package to produce a covariate balance table, containing the average value of our observable characteristics, across treatment and control groups. As the name suggests, this should always be the first table you present in causal inference analysis.\nfrom causalml.match import create_table_one X = ['male', 'age', 'hours'] table1 = create_table_one(df, 'dark_mode', X) table1 Control Treatment SMD Variable n 151 149 age 46.01 (9.79) 39.09 (11.53) -0.6469 hours 337.78 (464.00) 328.57 (442.12) -0.0203 male 0.34 (0.47) 0.66 (0.48) 0.6732 There seems to be some difference between treatment (dark_mode) and control group. In particular, users that select the dark_mode are older, have spent less hours on the blog and they are more likely to be males.\nWhat can we do? If we assume that all differences between treatment and control group are observable, we can solve the problem by performing conditional analysis.\nConditional Analysis We assume that for a set of subjects $i = 1, \u0026hellip;, n$ we observed a set of variables $(D_i, Y_i, X_i)$ that includes\na treatment assignment $D_i \\in \\lbrace 0, 1 \\rbrace$ (dark_mode) a response $Y_i \\in \\mathbb R$ (read_time) a feature vector $X_i \\in \\mathbb R^n$ (gender, age and hours) We are interested in estimating the conditional average treatment effect (CATE).\n$$ \\tau(x) = \\mathbb E \\Big[ Y_i^{(1)} - Y_i^{(0)} \\ \\Big| \\ X_i = x \\Big] $$\nWhere $Y_i^{(d)}$ indicates the potential outcome of individual $i$ under treatment status $d$. We also make the following assumptions.\nAssumption 1 : unconfoundedness (or ignorability, or selection on observables)\n$$ \\big \\lbrace Y_i^{(1)} , Y_i^{(0)} \\big \\rbrace \\ \\perp \\ D_i \\ | \\ X_i $$\ni.e. conditional on observable characteristics $X$, the treatment assignment $D$ is as good as random. What we are effectively assuming is that there is no other characteristics that we do not observe that could impact both whether a user selects the dark_mode and their read_time. This is a strong assumption that is more likely to be satisfied the more individual characteristics we observe.\nAssumption 2: overlap (or common support)\n$$ \\exists \\eta \u0026gt; 0 \\ : \\ \\eta \\leq \\mathbb E \\left[ D_i = 1 \\ \\big | \\ X_i = x \\right] \\leq 1-\\eta $$\ni.e. no observation is deterministically assigned to the treatment or control group. This is a more technical assumption that basically means that for any level of gender, age or hours, there could exist an individual that select the dark_mode and one that doesn\u0026rsquo;t. Differently from the unconfoundedness assumption, the overal assumption is testable.\nAssumption 3: stable unit treatment value (SUTVA)\n$$ Y^{(d)} \\perp D $$\ni.e. the potential outcome does not depend on the treatment status. In our case, we are ruling out the fact that another user selecting dark_mode might affect my effect of dark_mode on read_time. The most common setting where SUTVA is violated is in presence of network effects: if a friend of mine uses a social network increases my utility from using it.\nIPW and Meta-Learners Two alternative ways to perform conditional analysis are\nIPW: balance observations by their conditional treatment assignment probability and then estimate the treatment effect as a weighted difference in means Meta Learners: predict the potential outcomes from observable characteristics and estimate treatment effects as the difference between observed and counterfactual outcomes These two alternative procedures exploit the fact that we observe individual characteristics $X$ in different ways:\nIPW exploits $X$ to predict the treatment assignment $D$ and estimate the propensity scores $e(X) = \\mathbb{E} [D | X]$ Meta Learners exploit $X$ to predict the counterfactual outcomes $Y^{(d)}$ and estimate the response function $\\mu(X)^{(d)} = \\mathbb{E} [Y | D, X]$ Can we combine the two procedures and get the best of both worlds?\nYes, with the AIPW or double-robust estimator.\nThe AIPW Estimator The Augmented Inverse Propensity Weighted estimator is given by\n$$ \\hat \\tau_{AIPW} = \\frac{1}{n} \\sum_{i=1}^{n} \\left( \\hat \\mu^{(1)}(X_i) - \\hat \\mu^{(0)}(X_i) + \\frac{D_i }{\\hat e(X_i)} \\left( Y_i - \\hat \\mu^{(1)}(X_i) \\right) - \\frac{(1-D_i) }{1-\\hat e(X_i)} \\left( Y_i - \\hat \\mu^{(0)}(X_i) \\right) \\right) $$\nwhere $\\mu^{(d)}(x)$ is the response function, i.e. the expected value of the outcome, conditional on observable characteristics $x$ and treatment status $d$, and $e(X)$ is the propensity score.\n$$ \\mu^{(d)}(x) = \\mathbb E \\left[ Y_i \\ \\big | \\ X_i = x, D_i = d \\right] \\qquad ; \\qquad e(x) = \\mathbb E \\left[ D_i = 1 \\ \\big | \\ X_i = x \\right] $$\nThe formula of the AIPW estimator seems very cryptic at first, so let\u0026rsquo;s dig deeper and try to understand it.\nDecomposition The best way to understand the AIPW formula is to decompose it into two parts.\nThe first way is to decompose the AIPW estimator into a S-learner estimator and an adjustment factor.\n$$ \\hat \\tau_{AIPW} = \\hat \\tau_{S-learn} + \\widehat{\\text{adj}}_{S-learn} $$\nwhere\n$$ \\begin{aligned} \\hat \\tau_{S-learn} =\u0026amp; \\frac{1}{n} \\sum_{i=1}^{n} \\left( \\hat \\mu^{(1)}(X_i) - \\hat \\mu^{(0)}(X_i) \\right) \\newline \\widehat {adj} _ {S-learn} =\u0026amp; \\frac{1}{n} \\sum_{i=1}^{n} \\left(\\frac{D_i }{\\hat e(X_i)} \\left( Y_i - \\hat \\mu^{(1)}(X_i) \\right) - \\frac{(1-D_i) }{1-\\hat e(X_i)} \\left( Y_i - \\hat \\mu^{(0)}(X_i) \\right) \\right) \\end{aligned} $$\nThe adjustment is essentially an IPW estimator performed on the residuals of the S-learner.\nThe second way to decompose the AIPW estimator into the IPW estimator and an adjustment factor.\n$$ \\hat \\tau_{AIPW} = \\hat \\tau_{IPW} + \\widehat{\\text{adj}}_{IPW} $$\nwhere\n$$ \\begin{aligned} \\hat \\tau_{IPW} \u0026amp;= \\frac{1}{n} \\sum _ {i=1}^{n} \\left( \\frac{D_i Y_i}{\\hat e(X_i)} - \\frac{(1-D_i) Y_i}{1-\\hat e(X_i)} \\right) \\newline \\widehat {adj} _ {IPW} \u0026amp;= \\frac{1}{n} \\sum_{i=1}^{n} \\left( \\frac{\\hat e(X_i) - D_i}{\\hat e(X_i)} \\hat \\mu^{(1)}(X_i) - \\frac{(1-\\hat e(X_i)) - (1-D_i)}{1-\\hat e(X_i)} \\hat \\mu^{(0)}(X_i) \\right) \\end{aligned} $$\nThe adjustment is essentially an S-learner estimator weighted by the residual treatment probabilities.\nDouble Robustness Why is the AIPW estimator so compelling? The reason is that it just needs one of the two predictions, $\\hat \\mu$ or $\\hat e$, to be right in order to be unbiased (i.e. correct on average). Let\u0026rsquo;s check it.\nIf $\\hat \\mu$ is correctly specified, i.e. $\\mathbb E \\left[ \\hat \\mu^{(d)}(x) \\right] = \\mathbb E \\left[ Y_i \\ \\big | \\ X_i = x, D_i = d \\right]$, then $\\hat \\tau_{AIPW}$ is unbiased, even if $\\hat e$ is misspecified.\n$$ \\begin{aligned} \\hat \\tau_{AIPW} \u0026amp;\\overset{p}{\\to} \\mathbb E \\Big[ \\hat \\tau_{S-learn} + \\widehat{\\text{adj}}_{S-learn} \\Big] = \\newline \u0026amp;= \\mathbb E \\left[ \\hat \\mu^{(1)}(X_i) - \\hat \\mu^{(0)}(X_i) + \\frac{D_i \\left( Y_i - \\hat \\mu^{(1)}(X_i) \\right)}{\\hat e(X_i)} - \\frac{(1-D_i) \\left( Y_i - \\hat \\mu^{(0)}(X_i) \\right)}{1-\\hat e(X_i)} \\right] = \\newline \u0026amp;= \\mathbb E \\left[ \\hat \\mu^{(1)}(X_i) - \\hat \\mu^{(0)}(X_i) \\right] = \\newline \u0026amp;= \\mathbb E \\left[ Y^{(1)} - Y^{(0)} \\right] = \\newline \u0026amp;= \\tau \\end{aligned} $$\nThe intuition is that, if $\\hat \\mu$ is correctly specified, $\\hat \\tau_{S-learn}$ is unbiased and the adjustment factor vanishes, since the residuals $\\left( Y_i - \\hat \\mu^{(t)}(X_i) \\right)$ converge to zero.\nOn the other hand, if $\\hat e$ is correctly specified, i.e. $\\mathbb E \\left[\\hat e(x) \\right] = \\mathbb E \\left[ D_i = 1 \\ \\big | \\ X_i = x \\right]$, then $\\hat \\tau_{AIPW}$ is unbiased, even if $\\hat \\mu$ is misspecified.\n$$ \\begin{aligned} \\hat \\tau_{AIPW} \u0026amp; \\overset{p}{\\to} \\mathbb E \\Big[ \\hat \\tau_{IPW} + \\widehat{\\text{adj}}_{IPW} \\Big] = \\newline \u0026amp;= \\mathbb E \\left[ \\frac{D_i Y_i}{\\hat e(X_i)} - \\frac{(1-D_i) Y_i }{1-\\hat e(X_i)} + \\frac{\\hat e(X_i) - D_i}{\\hat e(X_i)} \\hat \\mu^{(1)}(X_i) - \\frac{(1-\\hat e(X_i)) - (1-D_i)}{1-\\hat e(X_i)} \\hat \\mu^{(0)}(X_i) \\right] = \\newline \u0026amp;= \\mathbb E \\left[ \\frac{D_i Y_i}{\\hat e(X_i)} - \\frac{(1-D_i) Y_i }{1-\\hat e(X_i)}\\right] = \\newline \u0026amp;= \\mathbb E \\left[ Y^{(1)} - Y^{(0)} \\right] = \\newline \u0026amp;= \\tau \\end{aligned} $$\nThe intuition is that, if $\\hat e$ is correctly specified, $\\hat \\tau_{IPW}$ is unbiased and the adjustment factor vanishes, since the residuals $\\left( D_i - \\hat e (X_i) \\right)$ converge to zero.\nBest Practices 1. Check Covariate Balance\nBoth IPW and AIPW were built for settings in which the treatment $D$ is not unconditionally randomly assigned, but might depend on some observables $X$. This information can be checked in two ways:\nProduce a balance table, summarizing the covariates across treatment arms. If unconditional randomization does not hold, we expect to see significant differences across some observables Plot the estimated propensity scores. If unconditional randomization holds, we expect the propensity scores to be constant 2. Check the Overlap Assumption\nAnother assumption that we can check is the overlap assumption, i.e. $\\exists \\eta \\ : \\ \\eta \\leq \\mathbb E \\left[ D_i = 1 \\ \\big | \\ X_i = x \\right] \\leq 1-\\eta$. To check this assumption we can simply check the bounds of the predicted propensity scores. If the overlap assumption is violated, we end up dividing some term of the estimator by zero.\n3. Use Cross-Fitting\nWhenever we build a prediction, it is best practice to exclude observation $i$ when estimating $\\hat \\mu^{(d)} (X_i)$ or $\\hat e (X_i)$. This procedure is generally known as cross-fitting in the machine learning literature. While there are many possible ways to perform cross-fitting, the simplest one is the following:\nSplit the sample in two at random Use sample 1 to estimate $\\hat \\mu^{(d)} (X_i)$ and $\\hat e (X_i)$ Use sample 2 to estimate $\\hat{\\tau}_{AIPW, 1}$ Repeat (2) and (3) swapping samples to estimate $\\hat{\\tau}_{AIPW, 2}$ Compute $\\hat{\\tau}_{AIPW}$ as the average of the two estimates Steps (2) and (3) ensure that the estimator is not overfitting. Steps (4) and (5) ensure that the estimator is efficient, using all the data for all steps and not just half. Kennedy (2022) shows that this procedure produces much more precise estimates than existing methods and provide formal results on error bounds. In particular, their main result is the following:\n\u0026ldquo;The bound on the DR-Learner error given in Theorem 2 shows that it can only deviate from the oracle error by at most a (smoothed) product of errors in the propensity score and regression estimators, thus allowing faster rates for estimating the CATE even when the nui- sance estimates converge at slower rates. Importantly the result is agnostic about the methods used, and requires no special tuning or undersmoothing.\u0026rdquo;\nBack to the Data Let\u0026rsquo;s now build and explore the AIPW estimator in our dataset on blog reading time and dark mode.\nPropensity Scores First, let\u0026rsquo;s estimate the propensity scores $e(X)$.\ndef estimate_e(df, X, D, model_e): e = model_e.fit(df[X], df[D]).predict_proba(df[X])[:,1] return e We estimate them by logistic regression using the LogisticRegression methods from the sklearn package.\nfrom sklearn.linear_model import LogisticRegression df['e'] = estimate_e(df, X, \u0026quot;dark_mode\u0026quot;, LogisticRegression()) Let\u0026rsquo;s check if the bounded support assumption is satisfied, by plotting the estimated propensity scores, across treatment and control groups.\nsns.histplot(data=df, x='e', hue='dark_mode', bins=30, stat='density', common_norm=False).\\ set(ylabel=\u0026quot;\u0026quot;, title=\u0026quot;Distribution of Propensity Scores\u0026quot;); The distribution of propensity scores is different between two groups, but it\u0026rsquo;s generally overlapping.\nWe can now use the propensity scores to build the IPW estimator.\nw = 1 / (e * df[\u0026quot;dark_mode\u0026quot;] + (1-e) * (1-df[\u0026quot;dark_mode\u0026quot;])) smf.wls(\u0026quot;read_time ~ dark_mode\u0026quot;, weights=w, data=df).fit().summary().tables[1] coef std err t P\u003e|t| [0.025 0.975] Intercept 18.6099 0.412 45.159 0.000 17.799 19.421 dark_mode[T.True] 1.0620 0.582 1.826 0.069 -0.083 2.207 Note that the computed standard errors are not exact, since we are ignoring the extra uncertainty that comes from the estimation of the propensity scores $e(X)$.\nResponse Function Let\u0026rsquo;s now estimate the second building block of the AIPW estimator: the response function $\\mu(X)$.\ndef estimate_mu(df, X, D, y, model_mu): mu = model_mu.fit(df[X + [D]], df[y]) mu0 = mu.predict(df[X + [D]].assign(dark_mode=0)) mu1 = mu.predict(df[X + [D]].assign(dark_mode=1)) return mu0, mu1 Let\u0026rsquo;s start by estimating $\\mu(X)$ with linear regression.\nfrom sklearn.linear_model import LinearRegression mu0, mu1 = estimate_mu(df, X, \u0026quot;dark_mode\u0026quot;, \u0026quot;read_time\u0026quot;, LinearRegression()) print(np.mean(mu1-mu0)) 1.3858099131476969 We have computed the meta learner estimate of the average treatment effect as the difference in means between the two estimated response functions, $\\mu^{(1)}(X)$ and $\\mu^{(0)}(X)$.\nNote that we can use any estimator to get the response function, I used linear regression for simplicity.\nEstimating AIPW We now have all the building blocks to compute the AIPW estimator!\naipw = mu1 - mu0 + df[\u0026quot;dark_mode\u0026quot;] / e * (df[\u0026quot;read_time\u0026quot;] - mu1) - (1-df[\u0026quot;dark_mode\u0026quot;]) / (1-e) * (df[\u0026quot;read_time\u0026quot;] - mu0) print(np.mean(aipw)) 1.3153774511905783 We can also compute it directly using the LinearDRLearner function from Microsoft\u0026rsquo;s EconML library.\nfrom econml.drlearner import LinearDRLearner model = LinearDRLearner(model_propensity=LogisticRegression(), model_regression=LinearRegression(), random_state=1) model.fit(Y=df[\u0026quot;read_time\u0026quot;], T=df[\u0026quot;dark_mode\u0026quot;], X=df[X]); The model directly gives us the average treatment effect.\nmodel.ate_inference(X=df[X].values, T0=0, T1=1).summary().tables[0] Uncertainty of Mean Point Estimate mean_point stderr_mean zstat pvalue ci_mean_lower ci_mean_upper 1.417 0.541 2.621 0.009 0.358 2.477 The estimate is statistically different from zero and the confidence interval includes the true value of 2.\nNote that we got a different estimate because the LinearDRLearner function also performed cross-fitting in the background, which we did not before.\nAssessment Let\u0026rsquo;s now assess the main property of the AIPW estimator: its double robustness. To do so, we compare it with its two parents: the IPW estimator and the S-learner.\ndef compare_estimators(X_e, X_mu, D, y, seed): df = dgp_darkmode().generate_data(seed=seed) e = estimate_e(df, X_e, D, LogisticRegression()) mu0, mu1 = estimate_mu(df, X_mu, D, y, LinearRegression()) slearn = mu1 - mu0 ipw = (df[D] / e - (1-df[D]) / (1-e)) * df[y] aipw = slearn + df[D] / e * (df[y] - mu1) - (1-df[D]) / (1-e) * (df[y] - mu0) return np.mean((slearn, ipw, aipw), axis=1) We use the joblib library to run the simulations in parallel and speed up the process.\nfrom joblib import Parallel, delayed def simulate_estimators(X_e, X_mu, D, y): r = Parallel(n_jobs=8)(delayed(compare_estimators)(X_e, X_mu, D, y, i) for i in range(100)) df_tau = pd.DataFrame(r, columns=['S-learn', 'IPW', 'AIPW']) plot = sns.boxplot(data=pd.melt(df_tau), x='variable', y='value', linewidth=2); plot.set(title=\u0026quot;Distribution of $\\hat τ$ and its components\u0026quot;, xlabel='', ylabel='') plot.axhline(2, c='r', ls=':'); First, let\u0026rsquo;s assume that we use all variables for both models, $\\mu(X)$ and $e(X)$. In this case, both models are well specified and we expect all estimators to perform well.\nWe plot the distribution of the three estimators across 100 simulations.\nsimulate_estimators(X_e=X, X_mu=X, D=\u0026quot;dark_mode\u0026quot;, y=\u0026quot;read_time\u0026quot;) Indeed, all estimator are unbiased and deliver very similar estimates.\nWhat happens if we misspecify one of the two models? Let\u0026rsquo;s start by (correctly) assuming that gender and age influence the probability of selecting dark_mode and (wrongly) assuming that only previous hours influence the weekly read_time. In this case, the propensity score $e(X)$ is well specified, while the response function $\\mu(X)$ is misspecified.\nsimulate_estimators(X_e=['male', 'age'], X_mu=['hours'], D=\u0026quot;dark_mode\u0026quot;, y=\u0026quot;read_time\u0026quot;) As expected, the S-learner is biased since we have misspecified $\\mu(X)$, while IPW isn\u0026rsquo;t. AIPW picks the best of both worlds and is unbiased.\nLet\u0026rsquo;s now explore the alternative misspecification. We (wrongly) assume that only age influences the probability of selecting dark_mode and (correctly) assume that both gender and previous hours influence the weekly read_time. In this case, the propensity score $e(X)$ is misspecified, while the response function $\\mu(X)$ is correctly specified.\nsimulate_estimators(['age'], ['male', 'hours'], D=\u0026quot;dark_mode\u0026quot;, y=\u0026quot;read_time\u0026quot;) In this case, the S-learner is unbiased, while IPW isn\u0026rsquo;t, since we have misspecified $e(X)$. Again, AIPW picks the best of both worlds and is unbiased.\nConclusion In this article we have seen a method to estimate conditional average treatment effects (CATE), that is robust to model misspecification: the Augmented Inverse Propensity Weighted (AIPW) estimator. The AIPW estimator takes the best out of two existing estimators: the IPW estimator and the S-learner. It requires the estimation of both the propensity score function $\\mathbb{E} [ D | X ]$ and the response function $\\mathbb{E} [ Y | D, X ]$ and it is unbiased even if one of the two functions is misspecified.\nThis estimator is now a standard and it is included all the most important causal inference packages such as Microsoft\u0026rsquo;s EconML, Uber\u0026rsquo;s causalml and Stanford researchers\u0026rsquo; R package grf.\nReferences [1] J. Robins, A. Rotzniski, J. P. Zhao, Estimation of regression coefficients when some regressors are not always observed (1994), Journal of the American Statistical Associations.\n[2] A. Glyn, K. Quinn, An Introduction to the Augmented Inverse Propensity Weighted Estimator (2010), Political Analysis.\n[3] E. Kennedy, Towards optimal doubly robust estimation of heterogeneous causal effects (2022), working paper.\nRelated Articles DAGs and Control Variables Matching, Weighting, or Regression? Understanding Meta Learners Code You can find the original Jupyter Notebook here:\nhttps://github.com/matteocourthoud/Blog-Posts/blob/main/notebooks/aipw.ipynb\n","date":1688947200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1688947200,"objectID":"bc7e85f56e381bf28f3ee62ab0bf1da0","permalink":"https://matteocourthoud.github.io/post/aipw/","publishdate":"2023-07-10T00:00:00Z","relpermalink":"/post/aipw/","section":"post","summary":"An introduction to doubly-robust estimation of conditional average treatment effects (CATE)\nWhen estimating causal effects, the gold standard is randomized controlled trials or AB tests. By randomly exposing units to a treatment we make sure that individuals in both groups are comparable, on average, and any difference we observe can be attributed to the treatment effect alone.","tags":null,"title":"Understanding AIPW","type":"post"},{"authors":null,"categories":null,"content":"How to use regression trees to estimate heterogeneous treatment effects.\nIn causal inference we are usually interested in estimating the causal effect of a treatment (a drug, ad, product, \u0026hellip;) on an outcome of interest (a disease, firm revenue, customer satisfaction, \u0026hellip;). However, knowing that a treatment works on average is often not sufficient and we would like to know for which subjects (patients, users, customers, \u0026hellip;) it works better or worse, i.e. we would like to estimate heterogeneous treatment effects.\nEstimating heterogeneous treatments effects allows us to do targeting. Knowing which customers are more likely to react to a discount allows a company to spend less money by offering fewer but better targeted discounts. This works also for negative effects: knowing for which patients a certain drug has side effects allows a pharmaceutical company to warn or exclude them from the treatment. There is also a more subtle advantage of estimating heterogeneous treatment effects: knowing for whom a treatment works allows us to better understand how a treatment works. Knowing that the effect of a discount does not depend on the income of its recipient but rather by its buying habits tells us that maybe it is not a matter of money, but rather a matter of attention or loyalty.\nIn this article, we will explore the estimation of heterogeneous treatment effects using a modified version of regression trees (and forests). From a machine learning perspective, there are two fundamental differences between causal trees and predictive trees. First of all, the target is the treatment effect, which is an inherently unobservable object. Second, we are interested in doing inference, which means quantifying the uncertainty of our estimates.\nOnline Discounts For the rest of the article, we are going to use a toy example, for the sake of exposition: suppose we were an online shop and we are interested in understanding whether offering discounts to new customers increases their expenditure. In particular, we would like to know if offering discounts is more effective for some customers with respect to others, since we would prefer not to give discounts to customers that would spend anyways. Moreover, it could also be that spamming customers with pop-ups could deter them from buying, having the opposite effect.\nTo understand whether and how much the discounts are effective we run an A/B test: whenever a new user visits our online shop, we randomly decide whether to offer them the discount or not. I import the data-generating process dgp_online_discounts() from src.dgp. With respect to previous articles, I generated a new DGP parent class that handles randomization and data generation, while its children classes contain specific use cases. I also import some plotting functions and libraries from src.utils. To include not only code but also data and tables, I use Deepnote, a Jupyter-like web-based collaborative notebook environment.\n%matplotlib inline %config InlineBackend.figure_format = 'retina' from src.utils import * from src.dgp import dgp_online_discounts dgp = dgp_online_discounts(n=100_000) df = dgp.generate_data() df.head() time device browser region discount spend 0 10.78 mobile edge 9 0 0.46 1 0.57 desktop firefox 9 1 11.04 2 3.74 mobile safari 7 0 1.81 3 13.37 desktop other 5 0 31.90 4 0.71 mobile explorer 2 0 15.42 We have data on 100.000 website visitors, for whom we observe the time of the day, the device they use, their browser and their geographical region. We also see whether they were offered the discount, our treatment, and what is their spend, the outcome of interest.\nSince the treatment was randomly assigned, we can use a simple difference-in-means estimator to estimate the treatment effect. We expect the treatment and control group to be similar, except for the discount, therefore we can causally attribute any difference in spend to the discount.\nsmf.ols('spend ~ discount', df).fit().summary().tables[1] coef std err t P\u003e|t| [0.025 0.975] Intercept 5.0306 0.045 110.772 0.000 4.942 5.120 discount 1.9492 0.064 30.346 0.000 1.823 2.075 The discount seems to be effective: on average the spend in the treatment group increases by 3.86$. But are all customers equally affected?\nTo answer this question, we would like to estimate heterogeneous treatment effects, possibly at the individual level.\nConditional Average Treatment Effects There are many possible ways to estimate heterogenous treatment effects. The most common is to split the population in groups based on some observable characteristic, which in our case could be the device, the browser or the geographical region. Once you have decided which variable to split your data on, you can simply interact the treatment variable (discount) with the dimension of treatment heterogeneity. Let\u0026rsquo;s take device for example.\nsmf.ols('spend ~ discount * device', df).fit().summary().tables[1] coef std err t P\u003e|t| [0.025 0.975] Intercept 5.0006 0.064 78.076 0.000 4.875 5.126 device[T.mobile] 0.0602 0.091 0.664 0.507 -0.118 0.238 discount 1.2264 0.091 13.527 0.000 1.049 1.404 discount:device[T.mobile] 1.4447 0.128 11.261 0.000 1.193 1.696 How do we interpret the regression results? The effect of the discount on customers\u0026rsquo; spend is $1.22$$ but it increases by a further $1.44$$ if the customer is accessing the website from a mobile device.\nSplitting is easy for categorical variables, but for a continuous variable like time it is not intuitive where to split. Every hour? And which dimension is more informative? It would be temping to try all possible splits, but the more we split the data, the more it is likely that we find spurious results (i.e. we overfit, in machine learning lingo). It would be great if we could let the data speak and select the minimum and most informative splits.\nIn a separate post, I have shown how the so-called meta-learners take this approach to causal inference. The idea is to predict the outcome conditional on the treatment status for each observation, and then compare the predicted conditional on treatment, with the predicted outcome conditional on control. The difference is the individual treatment effect.\nThe problem with meta-learners is that they use all their degrees of freedom in predicting the outcome. However, we are interested to predict treatment effect heterogeneity. If most of the variation in the outcome is not in the treatment dimension, we will get very poor estimates of the treatment effects.\nIs it possible to instead directly concentrate on the prediction of individual treatment effects? Let\u0026rsquo;s define $Y$ the outcome of interest, spend in our case, $D$ the treatment, the discount, and $X$ other observable characteristics. The ideal objective function is\n$$ \\sum_i \\Big [ ( \\tau_i - \\hat \\tau_i(X))^2 \\Big ] $$\nwhere $\\tau_i$ is the treatment effect of individual $i$. However, this objective function is unfeasible since we do not observe $\\tau_i$.\nBut, turns out that there is a way to get an unbiased estimate of the individual treatment effect. The idea is to use an auxiliary outcome variable, whose expected value for each individual is the individual treatment effect. This variable is\n$$ Y_i^* = \\frac{Y_i}{D_i \\cdot p(X_i) - (1-D_i) \\cdot (1-p(X_i))} $$\nwhere $p(X_i)$ is the propensity score of observation $i$, i.e. its probability of being treated. A crucial assuption here is unconfoundedness, also known as ignorability or selection on observables. In short, we will assume that, conditional on some observables $X$ the treatment assignment is as good as random.\n$$ \\left\\lbrace Y_i^{(0)}, Y_i^{(1)} \\right \\rbrace \\ \\perp \\ D_i | X_i $$\nwhere $Y_i^{(0)}$ and $Y_i^{(1)}$ denote the control and treated potential outcomes, respectively. In our case, we have randomized assignment, therefore we do not have to worry about unconfoundedness, unless the randomization went wrong.\nIn randomized experiments, the propensity score is known since randomization is fully under control of the experimenter. For example, in our case, the probability of treatment was 50%. In quasi-experimental studies instead, when the treatment probability is not known, it has to be estimated. Even in randomized experiments, it is always better to estimate rather than inpute the propensity scores, since it guards against sampling variation in the randomization. For more details on the propensity scores and how they are used in causal inference, I have a separate post here.\nLet\u0026rsquo;s first generate dummy variables for our categorical variables, device, browser and region.\ndf_dummies = pd.get_dummies(df[dgp.X[1:]], drop_first=True) df = pd.concat([df, df_dummies], axis=1) X = ['time'] + list(df_dummies.columns) We fit a LogisticRegression and use it to predict the treatment probability, i.e. construct the propensity score.\nfrom sklearn.linear_model import LogisticRegression df['pscore'] = LogisticRegression().fit(df[X], df[dgp.D]).predict_proba(df[X])[:,1] sns.histplot(data=df, x='pscore', hue='discount').set( title='Predicted propensity scores', xlim=[0,1], xlabel='Propensity Score'); As expected, most propensity scores are very close to 0.5, the probability of treatment used in randomization.\nWe now have all the elements to compute our auxiliary outcome variable $Y^*$.\ndf['y_star'] = df[dgp.Y[0]] / (df[dgp.D] * df['pscore'] - (1-df[dgp.D]) * (1-df['pscore'])) As we said before, the idea is to use $Y^*$ as the target of a prediction problem, since the expected value is exactly the individual treatment effect. Let\u0026rsquo;s check its average in the data.\ndf['y_star'].mean() 1.94501174385229 Indeed its average is almost identical to the previously estimated average treatment effect of 3.85.\nHow is it possible that, with a single observation and an estimate of the propensity score, we can estimate the individual treatment effect? What are the drawbacks?\nThe intuition is to approach the problem from a different perspective: ex-ante, before the experiment. Imagine that our dataset had a single observation, $i$. We know that the treatment probability is $p(X_i)$, the propensity score. Therefore, in expectation, our dataset has $p(X_i)$ observations in the treatment group and $1 - p(X_i)$ observations in the control group. The rest is business as usual: we estimate the treatment effect as the difference in average outcomes between the two groups! And indeed that is what we would do:\n$$ Y_i^* = \\frac{Y_i D_i}{p(X_i)} - \\frac{Y_i (1-D_i)}{1-p(X_i)} $$\nThe only difference is that we have a single observation.\nThis trick comes at a cost: $Y_i^*$ is an unbiased estimator for the individual treatment effect, but has a very high variance. This is immediately visible by plotting its distribution.\nfig, ax = plt.subplots() sns.histplot(df['y_star'], ax=ax).set(title='Distribution of Auxiliary Variable'); We are now ready to estimate heterogeneous treatment effects, by translating the causal inference problem into a prediction problem, predicting the auxiliary outcome $Y^*$, given observable characteristics $X$.\nCausal Trees In the previous section, we have see that we can transform the estimation of heterogeneous treatment effects into a prediction problem, where the outcome is the auxiliary outcome variable\n$$ Y_i^* = \\frac{Y_i}{T_i * e_i - (1-T_i) * (1-e_i)} $$\nWe can in principle use any machine learning algorithm at this point to estimate individual treatment effects. However, regression trees have particularly convenient characteristics.\nFirst of all, how do regression trees work? Without going too much in detail, they are an algorithm that recursively partitions the data in bins such that the outcome $Y$ within each bin is as homogeneous as possible and the outcome across bins is as heterogeneous as possible. The predicted values are simply the averages within each bin.\nThe averaging part is one of the big advantages of regression trees for inference since we know very well how to do inference with averages, with the Central Limit Theorem. The second advantage is that trees are very interpretable, since we can directly plot the data partition as a tree structure. We will see more of this later. Last but not least, regression trees are still at the core the best performing predictive algorithms with tabular data, as of 2022.\nLet\u0026rsquo;s use the DecisionTreeRegressor function from sklearn to fit our regression tree and estimate heterogeneous treatment effects of discounts on customers\u0026rsquo; spend.\nfrom sklearn.tree import DecisionTreeRegressor tree = DecisionTreeRegressor(max_depth=2).fit(df[X], df['y_star']) df['y_hat'] = tree.predict(df[X]) We have restricted the tree to have a maximum depth of 2 and at least 30 observation per partition (also called leaf) so that we can easily plot the tree and visualize the estimated groups and treatment effects.\nfrom sklearn.tree import plot_tree plot_tree(tree, filled=True, fontsize=12, feature_names=X, impurity=False, rounded=True); How should we interpret the tree? On the top, we can see the average $Y^*$ in the data, 3.851. Starting from there, the data gets split into different branches, according to the rules highlighted at the top of each node. For example, the first node splits the data into two groups of size 42970 and 57030 depending on whether the time is later than 10.365. At the bottom, we have our final partitions, with the predicted values. For example, the leftmost leaf contains 36846 observation with time earlier than 10.365 and non-Safari browser, for which we predict a spend of 1.078. Darker node colors indicate higher prediction values.\nShould we believe these estimates? Not really, because of a couple of reasons. The first problem is that we have an unbiased estimate of the average treatment effect only if, within each leaf, we have the same number of treated and control units. This is not automatically the case with an off-the-shelf DecisionTreeRegressor().\nMoreover, we have used the same data to generate the tree and evaluate it. This generates some bias because of overfitting. We can split the sample in 2 and use different data to generate the tree and compute the predictions. These trees are called honest trees.\nGenerating Splits Last but not least, how should the tree be generated? The default rule to generate splits with the DecisionTreeRegressor function is the squared_error and there is no restriction on the minimum number of observations per leaf. Other commonly used rules include, mean absolute error, Gini\u0026rsquo;s impurity, and Shannon\u0026rsquo;s information. Which one performs better depends on the specific application, but the general objective is always prediction accuracy, broadly defined.\nIn our case instead, the objective is inference: we want to uncover heterogeneous treatment effects that are statistically different from each other. There is no value in generating different treatment effects if they are statistically indistinguishable. Moreover (but strongly related), when building the tree and generating the data partitions, we have to take into account that, since we use honest trees, we will use different data to estimate the within-leaf treatment effects.\nAthey and Imbens (2016) use an modified version of the Mean Squared Error (MSE) as splitting criterion, the Expanded Mean Squared Error (EMSE):\n$$ EMSE = \\mathbb{E} \\Big[ \\big( Y_i - \\hat \\mu(X_i)\\big)^2 - Y_i^2 \\Big] $$\nwhere the main difference is given by the additional term $Y_i^2$, the squared outcome variable. In our case, we can rewrite it as\n$$ EMSE = \\mathbb{E} \\Big[ \\big( Y^_i - \\hat \\tau(X_i)\\big)^2 - {Y^_i}^2 \\Big] $$\nWhy is this a sensible error loss? Because we can rewrite it as the sum of the squared mean μ and the estimator\u0026rsquo;s variance.\n$$ \\begin{aligned} EMSE \u0026amp;= \\mathbb{E} \\Big[ \\big( Y^_i - \\hat \\tau(X_i)\\big)^2 - {Y^_i}^2 \\Big] = \\newline \u0026amp;= \\mathbb{E} \\Big[ \\big( Y^_i - \\tau(X_i)\\big)^2 - {Y^_i}^2 \\Big] - \\mathbb{E} \\Big[ \\big( \\hat \\tau(X_i) - \\tau(X_i)\\big)^2 \\Big] = \\newline \u0026amp;= \\mathbb{E} \\Big[ \\mathbb{V} \\big (\\hat \\tau(X_i)^2 \\big) \\Big] - \\mathbb{E} \\Big[ \\tau(X_i)^2 \\Big] \\end{aligned} $$\nImplementation Luckily, there are multiple libraries where the so-called causal trees are implemented. We import CausalForestDML from Microsoft\u0026rsquo;s EconML library, one of the best libraries for causal inference.\nfrom econml.dml import CausalForestDML np.random.seed(0) tree_model = CausalForestDML(n_estimators=1, subforest_size=1, inference=False, max_depth=3) tree_model = tree_model.fit(Y=df[dgp.Y], X=df[X], T=df[dgp.D]) We have restricted the number of estimators to 1 to have a single tree instead of multiple ones, the so-called random forests that we will cover in a separate article.\nfrom econml.cate_interpreter import SingleTreeCateInterpreter %matplotlib inline intrp = SingleTreeCateInterpreter(max_depth=2).interpret(tree_model, df[X]) intrp.plot(feature_names=X, fontsize=12) As we can see, the tree representation looks extremely similar to the one we got before using the DecisionTreeRegressor function. However, now the model not only reports estimates of the conditional average treatment effects, but also the standard errors of the estimates (at the bottom). How were they computed?\nInference Honest trees, besides improving the out-of-sample prediction accuracy of the model, have another great implication: they allow us to compute standard errors as if the tree structure was exogenous. In fact, since the data used to compute the predictions is independent from the data used to build the tree (split the data), we can just treat the tree structure as independent from the estimated treatment effects. As a consequence, we can estimate the standard errors of the the estimates as standard errors of difference between sample averages, as in a standard AB test.\nIf we had used the same data to build the tree and estimate the treatment effects, we would have introduced bias, because of the spurious correlation between the covariates and the outcomes. This bias usually disappears for very large sample sizes, but honest trees do not require than.\nPerformance How well does the model perform? Since we control the data generating process, we can do something that is not possible with real data: check the predicted treatment effects against the true ones. The generate_potential_outcomes() function loads the data with both potential outcomes for each observation, under both treatment (outcome_t) and control (outcome_c).\ndef compute_discrete_effects(df, hte_model): temp_df = df.copy() temp_df.time = 0 temp_df = dgp.add_treatment_effect(temp_df) temp_df = temp_df.rename(columns={'effect_on_spend': 'True'}) temp_df['Predicted'] = hte_model.effect(temp_df[X]) df_effects = pd.DataFrame() for var in X[1:]: for effect in ['True', 'Predicted']: v = temp_df[effect][temp_df[var]==1].mean() - temp_df[effect][temp_df[var]==0].mean() effect_var = {'Variable': [var], 'Effect': [effect], 'Value': [v]} df_effects = pd.concat([df_effects, pd.DataFrame(effect_var)]).reset_index(drop=True) return df_effects, temp_df['Predicted'].mean() df_effects_tree, avg_effect_notime_tree = compute_discrete_effects(df, tree_model) fig, ax = plt.subplots() sns.barplot(data=df_effects_tree, x=\u0026quot;Variable\u0026quot;, y=\u0026quot;Value\u0026quot;, hue=\u0026quot;Effect\u0026quot;, ax=ax).set( xlabel='', ylabel='', title='Heterogeneous Treatment Effects') ax.set_xticklabels(ax.get_xticklabels(), rotation=45, ha=\u0026quot;right\u0026quot;); The causal tree is pretty good at detecting the heterogeneous treatment effects for the categorical variables. It only missed the heterogeneity in the third region.\nHowever, this is also where we expect a tree model to perform particularly well: where the effects are discrete. How well does it do on our continuous variable, time? First, let\u0026rsquo;s again isolate the predicted treatment effects on time and ignore the other covariates.\ndef compute_time_effect(df, hte_model, avg_effect_notime): df_time = df.copy() df_time[[X[1:]] + ['device', 'browser', 'region']] = 0 df_time = dgp.add_treatment_effect(df_time) df_time['predicted'] = hte_model.effect(df_time[X]) + avg_effect_notime return df_time df_time_tree = compute_time_effect(df, tree_model, avg_effect_notime_tree) We now plot the predicted treatment effects against the true ones, along the time dimension.\nsns.scatterplot(x='time', y='effect_on_spend', data=df_time_tree, label='True') sns.scatterplot(x='time', y='predicted', data=df_time_tree, label='Predicted').set( ylabel='', title='Heterogeneous Treatment Effects') plt.legend(title='Effect'); From the plot, we can appreciate the discrete nature of causal trees: the model is only able to split the continuous variable into 5 bins. These bins are close to the true treatment effects, but they fail to capture a big chunk of the treatment effect heterogeneity.\nCan these predictions be improved? The answer is yes, and we will explore how in the next post.\nConclusion In this article, we have seen how to use causal trees to estimate heterogeneous treatment effects. The main insight comes from the definition of an auxiliary outcome variable that allows us to frame the inference problem as a prediction problem. While we can then use any algorithm to predict treatment effects, regression trees are particularly useful because of their interpretability, prediction accuracy, and feature of generating prediction as subsample averages.\nThe work by Athey and Imbens (2016) on regression trees to compute heterogeneous treatment effects brought together two separate literatures, causal inference and machine learning in a very fruitful synergy. The causal inference literature (re)discovered the inference benefits of sample splitting, that allows us to do correct inference even when the data partition is complex and hard to analyze. On the other hand, splitting the tree generation phase from the within-leaf prediction phase has strong benefits in terms of prediction accuracy, by safeguarding against overfitting.\nReferences S. Athey, G. Imbens, Recursive partitioning for heterogeneous causal effects (2016), PNAS.\nS. Wager, S. Athey, Estimation and Inference of Heterogeneous Treatment Effects using Random Forests (2018), Journal of the American Statistical Association.\nS. Athey, J. Tibshirani, S. Wager, Generalized Random Forests (2019). The Annals of Statistics.\nM. Oprescu, V. Syrgkanis, Z. Wu, Orthogonal Random Forest for Causal Inference (2019). Proceedings of the 36th International Conference on Machine Learning.\nRelated Articles DAGs and Control Variables\nMatching, Weighting, or Regression?\nUnderstanding Meta Learners\nUnderstanding AIPW, the Doubly-Robust Estimator\nCode You can find the original Jupyter Notebook here:\nhttps://github.com/matteocourthoud/Blog-Posts/blob/main/notebooks/causal_trees.ipynb\n","date":1688947200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1688947200,"objectID":"801c3459d81ea394bb44113ad5d513d6","permalink":"https://matteocourthoud.github.io/post/causal_trees/","publishdate":"2023-07-10T00:00:00Z","relpermalink":"/post/causal_trees/","section":"post","summary":"How to use regression trees to estimate heterogeneous treatment effects.\nIn causal inference we are usually interested in estimating the causal effect of a treatment (a drug, ad, product, \u0026hellip;) on an outcome of interest (a disease, firm revenue, customer satisfaction, \u0026hellip;).","tags":null,"title":"Understanding Causal Trees","type":"post"},{"authors":null,"categories":null,"content":"Problems and solutions of linear regression with multiple treatments\nIn many causal inference settings, we might be interested in the effect of not just one treatment, but many mutually exclusive treatments. For example, we might want to test alternative UX designs, or drugs, or policies. Depending on the context, there might be many reasons why we want to test different treatments at the same time, but generally it can help reducing the sample size, as we need just a single control group. A simple way to recover the different treatment effects is a linear regression of the outcome of interest on the different treatment indicators.\nHowever, in causal inference, we often condition the analysis on other observable variables (often called control variables), either to increase power or, especially in quasi-experimental settings, to identify a causal parameter instead of a simple correlation. There are cases in which adding control variables can backfire, but otherwise, we usually think that the regression framework is still able to recover the average treatment effect.\nIn a breakthrough paper, Goldsmith-Pinkham, Hull and Kolesár (2022) have recently shown that in case of multiple and mutually-exclusive treatments with control variables, the regression coefficients do not identify a causal effect. However, not everything is lost: the authors propose a simple solution to this problem that still makes use of linear regression.\nIn this blog post, I am going to go through a simple example illustrating the nature of the problem and the solution proposed by the authors.\nMultiple Treatments Example Suppose we are an online store and we are not satisfied with our current checkout page. In particular, we would like to change our checkout button to increase the probability of a purchase. Our UX designer comes up with two alternative checkout buttons, which are displayed below.\nIn order to understand which button to use, we run an A/B test, or randomized control trial. In particular, when people arrive at the checkout page, we show them one of the three options, at random. Then, for each user, we record the revenue generated which is our outcome of interest.\nI generate a synthetic dataset using dgp_buttons() from src.dgp as data generating process. I also import plotting functions and standard libraries from src.utils.\n%matplotlib inline %config InlineBackend.figure_format = 'retina' from src.utils import * from src.dgp import dgp_buttons dgp = dgp_buttons() df = dgp.generate_data() df.head() group revenue mobile 0 button1 8.927335 0 1 default 13.613456 1 2 button2 4.777628 0 3 default 8.909049 0 4 default 10.160347 0 We have information on 2000 users, for which we observe their checkout button (default, button1 or button2), the revenue they generate and whether they connected from desktop or mobile.\nWe notice too late that we have a problem with randomization. We showed button1 more frequently to desktop users and button2 more frequently to mobile users. The control group that sees the default button instead is balanced.\nWhat should we do? What happens if we simply compare revenue across groups? Let\u0026rsquo;s do it by regressing revenue on group dummy variables.\nsmf.ols('revenue ~ group', data=df).fit().summary().tables[1] coef std err t P\u003e|t| [0.025 0.975] Intercept 11.6553 0.149 78.250 0.000 11.363 11.948 group[T.button1] -0.5802 0.227 -2.556 0.011 -1.026 -0.135 group[T.button2] -0.5958 0.218 -2.727 0.006 -1.024 -0.167 From the regression results we estimate a negative and significant effect for both buttons. Should we believe these estimates? Are they causal?\nIt is unlikely that what we have estimated are the true treatment effects. In fact, there might be substantial differences in purchase attitudes between desktop and mobile users. Since we do not have a comparable number of mobile and desktop users across treatment arms, it might be that the observed differences in revenue are due to the device used and not the button design.\nBecause of this, we decide to condition our analysis on the device used and we include the mobile dummy variable in the regression.\nsmf.ols('revenue ~ group + mobile', data=df).fit().summary().tables[1] coef std err t P\u003e|t| [0.025 0.975] Intercept 9.1414 0.110 82.905 0.000 8.925 9.358 group[T.button1] 0.3609 0.141 2.558 0.011 0.084 0.638 group[T.button2] -1.0326 0.134 -7.684 0.000 -1.296 -0.769 mobile 4.7181 0.116 40.691 0.000 4.491 4.946 Now the coefficient of button1 is positive and significant. Should we recommend its implementation?\nThe answer is surprisingly no. Goldsmith-Pinkham, Hull, Kolesár (2022) show that this type of regression does not identify the average treatment effect when:\nthere are mutually exclusive treatment arms (in our case, groups) we are controlling for some variable $X$ (in our case, mobile) there treatment effects are heterogeneous in $X$ This is true even if the treatment is \u0026ldquo;as good as random\u0026rdquo; once we condition on $X$.\nIndeed, in our case, the true treatment effects are the ones reported in the following table.\nThe first button has no effect on revenue, irrespectively of the device, while the second button has a positive effect for mobile users and a negative effect for desktop users. Our (wrong) regression specification instead estimates a positive effect of the first button.\nLet\u0026rsquo;s now dig more in detail into the math, to understand why this is happening.\nTheory This section borrows heavily from Goldsmith-Pinkham, Hull, Kolesár (2022). For a great summary of the paper, I recommend this excellent Twitter thread by one of the authors, Paul Goldsmith-Pinkham.\nEconomists love using linear regression to estimate treatment effects — it turns out that there are perils to this method, but also amazing perks\nCome with me in this 🧵 if you want to learn… https://t.co/eDsRLkZFZe\n\u0026mdash; Paul Goldsmith-Pinkham (@paulgp) June 7, 2022 Single Treatment Arm Suppose we are interested in the effect of a treatment $D$ on an outcome $Y$. First, let\u0026rsquo;s consider the standard case of a single treatment arm so that the treatment variable is binary, $D \\in \\lbrace 0 , 1 \\rbrace$. Also consider a single binary control variable $X \\in \\lbrace 0 , 1 \\rbrace$. We also assume that treatment assignment is as good as random, conditionally on $X$. This means that there might be systematic differences between the treatment and control group, however, these differences are fully accounted for by $X$. Formally we write\n$$ \\left( Y_i^{(0)}, Y_i^{(1)} \\right) \\ \\perp \\ D_i \\ | \\ X_i $$\nWhere $Y_i^{(d)}$ denotes the potential outcome of individual $i$ when its treatment status is $d$. For example, $Y_i^{(0)}$ indicates the outcome of individual $i$ in case it is not treated. This notation comes from Rubin\u0026rsquo;s potential outcomes framework. We can write the individual treatment effect of individual $i$ as\n$$ \\tau_i = Y_i^{(1)} - Y_i^{(0)} $$\nIn this setting, the regression of interest is\n$$ Y_i = \\alpha + \\beta D_i + \\gamma X_i + u_i $$\nThe coefficient of interest is $\\beta$.\nAngrist (1998) shows that the regression coefficient $\\beta$ identifies the average treatment effect. In particular, $\\beta$ identifies a weighted average of the within-group $x$ average treatment effect $\\tau (x)$ with convex weights. In this particular setting, we can write it as\n$$ \\beta = \\lambda \\tau(0) + (1 - \\lambda) \\tau(1) \\qquad \\text{where} \\qquad \\tau (x) = \\mathbb E \\big[ Y_i^{(1)} - Y_i^{(0)} \\ \\big| \\ X_i = x \\big] $$\nThe weights $\\lambda$ and $(1-\\lambda)$ are given by the within-group treatment variance. Hence, the OLS estimator gives less weight to groups where we have less treatment variance, i.e., where treatment is more imbalanced. Groups where treatment is distributed 50-50 get the most weight.\n$$ \\lambda = \\frac{ \\text{Var} \\big(D_i \\ \\big| \\ X_i = 0 \\big) \\Pr \\big(X_i=0 \\big)}{\\sum_{x \\in \\lbrace 0 , 1 \\rbrace} \\text{Var} \\big(D_i \\ \\big| \\ X_i = x \\big) \\Pr \\big( X_i=x \\big)} \\in [0, 1] $$\nThe weights can be derived using the Frisch-Waugh-Lowell theorem to express $\\beta_1$ as the OLS coefficient of a univariate regression of $Y$ on $D_{i, \\perp X}$, where $D_{i, \\perp X}$ are the residuals from regressing $D$ on $X$. If you are not familiar with the Frisch-Waugh-Lowell theorem, I wrote an introductory blog post here.\n$$ \\beta_1 = \\frac{ \\mathbb E \\big[ D_{i, \\perp X} Y_i \\big] }{ \\mathbb E \\big[ D_{i, \\perp X}^2 \\big] } = \\underbrace{ \\frac{\\mathbb E \\big[ D_{i, \\perp X} Y_i(0) \\big]}{\\mathbb E \\big[ D_{i, \\perp X}^2 \\big]} } _ {=0} + \\frac{\\mathbb E \\big[ D_{i, \\perp X} D_i \\tau_i \\big]}{\\mathbb E \\big[ D_{i, \\perp X}^2 \\big]} = \\frac{\\mathbb E \\big[ \\text{Var} (D_i | X_i) \\ \\tau(X_i) \\big]}{\\mathbb E \\big[ \\text{Var}(D_i | X_i) \\big]} $$\nThe first term of the central expression disappears because the residual $D_{i, \\perp X}$ is by construction mean independent of the control variable $X_i$, i.e.\n$$ \\mathbb E \\big[ D_{i, \\perp X} | X_i \\big] = 0 $$\nThis mean independence property is crucial to obtain an unbiased estimate and its failure in the multiple-treatment case is the source of the contamination bias.\nMultiple Treatment Arms Let\u0026rsquo;s now consider the case of multiple treatment arms, $D \\in \\lbrace 0, 1, 2 \\rbrace$, where $1$ and $2$ indicate two mutually-exclusive treatments. We still assume conditional ignorability, i.e., treatment assignment is as good as random, conditional on $X$.\n$$ \\left( Y_i^{(0)}, Y_i^{(1)}, Y_i^{(2)} \\right) \\ \\perp \\ D_i \\ | \\ X_i $$\nIn this case, we have two different individual treatment effects, one per treatment.\n$$ \\tau_{i1} = Y_i^{(1)} - Y_i^{(0)} \\qquad \\text{and} \\qquad \\tau_{i2} = Y_i^{(2)} - Y_i^{(0)} $$\nThe regression of interest is\n$$ Y_i = \\alpha + \\beta_1 D_{i1} + \\beta_2 D_{i2} + \\gamma X_i + u_i $$\nDoes the OLS estimator of $\\beta_1$ and $\\beta_2$ identify an average treatment effect?\nIt would be very tempting to say yes. In fact, it looks like not much has changed with respect to the previous setup. We just have one extra treatment, but the potential outcomes are still conditionally independent of it. Where is the issue?\nLet\u0026rsquo;s concentrate on $\\beta_1$ (the same applies to $\\beta_2$). As before, can rewrite $\\beta_1$ using the Frisch-Waugh-Lowell theorem as the OLS coefficient of a univariate regression of $Y_i$ on $D_{i1, \\perp X, D_2}$, where $D_{i1, \\perp X, D_2}$ are the residuals from regressing $D_1$ on $D_2$ and $X$.\n$$ \\beta_1 = \\frac{ \\mathbb E \\big[D_{i1, \\perp X, D_2} Y_i \\big] }{ \\mathbb E \\big[ D_{i1, \\perp X, D_2}^2 \\big]} = \\underbrace{ \\frac{ \\mathbb E \\big[ D_{i1, \\perp X, D_2} Y_i(0) \\big] }{\\mathbb E \\big[ D_{i1, \\perp X, D_2}^2 \\big]} } _ {=0} + \\frac{ \\mathbb E \\big[ D_{i1, \\perp X, D_2} D_{i1} \\tau_{i1} \\big] }{ \\mathbb E \\big[ D_{i1, \\perp X, D_2}^2 \\big]} + \\color{red}{ \\underbrace{ \\color{black}{ \\frac{ \\mathbb E \\big[ D_{i1, \\perp X, D_2} D_{i2} \\tau_{i2} \\big] }{ \\mathbb E \\big[ D_{i1, \\perp X, D_2}^2 \\big]}} } _ { \\neq 0} } $$\nThe problem is the last term. Without the last term, we could still write $\\beta_1$ as a convex combination of the individual treatment effects. However, the last term biases the estimator by adding a component that depends on the treatment effect of $D_2$, $\\tau_2$. Why does this term not disappear?\nThe problem is that $D_{i1, \\perp X, D_2}$ is not mean independent of $D_{i2}$, i.e.\n$$ \\mathbb E \\big[ D_{i1, \\perp X, D_2} D_{i2} \\ \\big| \\ X_i \\big] \\neq 0 $$\nThe reason lies in the fact that the treatments are mutually exclusive. This implies that when $D_{i1}=1$, $D_{i2}$ must be zero, regardless of the value of $X_i$. Therefore, the last term does not cancel out and it introduces a contamination bias.\nSolution Goldsmith-Pinkham, Hull, Kolesár (2022) show that a simple estimator, first proposed by Imbens and Wooldridge (2009), is able to remove the bias. The procedure is the following.\nDe-mean the control variable: $\\tilde X = X - \\bar X$ Regress $Y$ on the interaction between the treatment indicators $D$ and the demeaned control variable $\\tilde X$ The OLS estimators of $\\beta_1$ and $\\beta_2$ are unbiased estimators of the average treatment effects. It also just requires a linear regression. Moreover, this estimator is unbiased also for continuous control variables $X$, not only for a binary one as we have considered so far.\nWhy was this estimator initially proposed by Imbens and Wooldridge (2009)? Let\u0026rsquo;s analyze two parts separately: the interaction term between $D$ and $X$ and the fact that $X$ is de-meaned in the interaction term.\nFirst, the interaction term $D X$ allows us to control for different effects and/or distributions of $X$ across treatment and control group.\nSecond, de-meaning $X$ in the interaction term allows us to interpret the estimated coefficient $\\hat \\beta$ as the average treatment effect. In fact, assume we were estimating the following linear model, where $X$ is not de-meaned in the interaction term.\n$$ Y_i = \\alpha + \\beta D_i + \\gamma X_i + \\delta D_i X_i + u_i $$\nIn this case, the marginal effect of $D$ on $Y$ is $\\beta + \\delta X_i$ so that the average marginal effect is $\\beta + \\delta \\bar X$ which is different from $\\beta$.\nIf instead we use the de-meaned value of $X$ in the interaction term, the marginal effect of $D$ on $Y$ is $\\beta + \\delta (X_i - \\bar X)$ so that the average marginal effect is $\\beta + \\delta (\\bar X - \\bar X) = \\beta$. Now we can interpret $\\beta$ as the average marginal effect of $D$ on $Y$.\nSimulations In order to better understand both the problem and the solution, let\u0026rsquo;s run some simulations.\nWe run an estimator over different draws from the data generating process dgp_buttons(). This is only possible with synthetic data and we do not have this luxury in reality. For each sample, we record the estimated coefficient and the corresponding p-value.\ndef simulate(dgp, estimator, K=1000): # Initialize coefficients results = pd.DataFrame({'Coefficient': np.zeros(K), 'pvalue': np.zeros(K)}) # Compute coefficients for k in range(K): df = dgp.generate_data(seed=k) results.Coefficient[k] = estimator(df).params[1] results.pvalue[k] = estimator(df).pvalues[1] results['Significant'] = results['pvalue'] \u0026lt; 0.05 return results First, let\u0026rsquo;s try it with the old estimator that regresses revenue on both group and mobile dummy variables.\nols_estimator = lambda x: smf.ols('revenue ~ group + mobile', data=x).fit() results = simulate(dgp, ols_estimator) I plot the distribution of the coefficient estimates of button1 over 1000 simulations, highlighting the statistically significant ones at the 5% level. I also highlight the true value of the coefficient, zero, with a vertical dotted bar.\ndef plot_results(results): p_sig = sum(results['Significant']) / len(results) * 100 sns.histplot(data=results, x=\u0026quot;Coefficient\u0026quot;, hue=\u0026quot;Significant\u0026quot;, multiple=\u0026quot;stack\u0026quot;, palette = ['tab:red', 'tab:green']); plt.axvline(x=0, c='k', ls='--', label='truth') plt.title(rf\u0026quot;Estimated $\\beta_1$ ({p_sig:.0f}% significant)\u0026quot;); plot_results(results) As we can see, we reject the null hypothesis of no effect of button1 in 45% of the simulations. Since we set a confidence level of 5%, we would have expected at most around 5% of rejections. Our estimator is biased.\nAs we have seen above, the problem is that the estimator is not just a convex combination of the effect of button1 across mobile and desktop users (it\u0026rsquo;s zero for both), but it is contaminated by the effect of button2.\nLet\u0026rsquo;s now try the estimator proposed from Imbens and Wooldridge (2009). First, we need do de-mean our control variable, mobile. Then, we regress revenue on the interaction between group and the de-meaned control variable, mobile_res.\ndf['mobile_res'] = df['mobile'] - np.mean(df['mobile']) smf.ols('revenue ~ group * mobile_res', data=df).fit().summary().tables[1] coef std err t P\u003e|t| [0.025 0.975] Intercept 11.5773 0.067 172.864 0.000 11.446 11.709 group[T.button1] 0.0281 0.106 0.266 0.790 -0.180 0.236 group[T.button2] -1.5071 0.100 -15.112 0.000 -1.703 -1.311 mobile_res 2.9107 0.134 21.715 0.000 2.648 3.174 group[T.button1]:mobile_res 0.1605 0.211 0.760 0.448 -0.254 0.575 group[T.button2]:mobile_res 5.3771 0.200 26.905 0.000 4.985 5.769 The estimated coefficients are now close to their true values. The estimated coefficient for button1 is not significant, while the estimated coefficient for button2 is negative and significant.\nLet\u0026rsquo;s check whether this results holds across samples by running a simulation. We repeat the estimation procedure 1000 times and we plot the distribution of estimated coefficients for button1.\nnew_estimator = lambda x: smf.ols('revenue ~ group * mobile', data=x).fit() new_results = simulate(dgp, new_estimator) plot_results(new_results) Now the distribution of the estimated coefficient for button1 is centered around the true value of zero. Moreover, we reject the null hypothesis of no effect only in 1% of the simulations, consistently with the chosen confidence level of 95%.\nConclusion In this post, we have seen the dangers of running a factor regression model with multiple mutually exclusive treatment arms and treatment effect heterogeneity across a control variable. In this case, because the treatments are not independent, the regression coefficients are not a convex combination of the within-group average treatment effects, but also capture the treatment effects of the other treatments introducing contamination bias. The solution to the problem is both simple and elegant, requiring just a linear regression.\nHowever, the problem is more general than this setting and generally concerns every setting in which (all of the following)\nWe have multiple treatments that depend on each other We need to condition the analysis on a control variable The treatment effects are heterogeneous in the control variable Another popular example is the case of the Two-Way Fixed Effects (TWFE) estimator with staggered treatments.\nReferences [1] J. Angrist, Estimating the Labor Market Impact of Voluntary Military Service Using Social Security Data on Military Applicants (1998), Econometrica.\n[2] D. Rubin, Causal Inference Using Potential Outcomes (2005), Journal of the American Statistical Association.\n[3] G. Imbens, J. Wooldridge, Recent Developments in the Econometrics of Program Evaluation (2009), Journal of Economic Literature.\n[4] P. Goldsmith-Pinkham, P. Hull, M. Kolesár, Contamination Bias in Linear Regressions (2022), working paper.\nRelated Articles Understanding Omitted Variable Bias Understanding The Frisch-Waugh-Lovell Theorem DAGs and Control Variables Code You can find the original Jupyter Notebook here:\nhttps://github.com/matteocourthoud/Blog-Posts/blob/main/notebooks/cbias.ipynb\n","date":1688947200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1688947200,"objectID":"cf958f3cd5509ca67ac9bf63a0d61586","permalink":"https://matteocourthoud.github.io/post/contamination_bias/","publishdate":"2023-07-10T00:00:00Z","relpermalink":"/post/contamination_bias/","section":"post","summary":"Problems and solutions of linear regression with multiple treatments\nIn many causal inference settings, we might be interested in the effect of not just one treatment, but many mutually exclusive treatments.","tags":null,"title":"Understanding Contamination Bias","type":"post"},{"authors":null,"categories":null,"content":"An in depth guide to the state-of-the art variance reduction technique in A/B tests\nDuring my PhD, I spent a lot of time learning and applying causal inference methods to experimental and observational data. However, I was completely clueless when I first heard of CUPED (Controlled-Experiment using Pre-Experiment Data), a technique to increase the power of randomized controlled trials in A/B tests.\nWhat really amazed me was the popularity of the algorithm in the industry. CUPED was first introduced by Microsoft researchers Deng, Xu, Kohavi, Walker (2013) and has been widely used in companies such as Netflix, Booking, Meta, Airbnb, TripAdvisor, DoorDash, Faire, and many others. While digging deeper into it, I noticed a similarity with some causal inference methods I was familiar with, such as Difference-in-Differences. I was curious and decided to dig deeper.\nIn this post, I will present CUPED and try to compare it against other causal inference methods.\nExample Let\u0026rsquo;s assume we are a firm that is testing an ad campaign and we are interested in understanding whether it increases revenue or not. We randomly split a set of users into a treatment and control group and we show the ad campaign to the treatment group. Differently from the standard A/B test setting, assume we observe users also before the test.\nWe can now generate the simulated data, using the data generating process dgp_cuped() from src.dgp. I also import some plotting functions and libraries from src.utils.\n%matplotlib inline %config InlineBackend.figure_format = 'retina' from src.utils import * from src.dgp import dgp_cuped df = dgp_cuped().generate_data() df.head() i ad_campaign revenue0 revenue1 0 1 0 5.315635 8.359304 1 2 1 2.977799 7.751485 2 3 0 4.693796 9.025253 3 4 0 5.827975 8.540667 4 5 0 5.230095 8.910165 We have informations on 1000 individuals indexed by i for whom we observe the revenue generated pre and post treatment, revenue0 and revenue1 respectively, and whether they have been exposed to the ad_campaign.\nDifference in Means In randomized experiments or A/B tests, randomization allows us to estimate the average treatment effect using a simple difference in means. We can just compare the average outcome post-treatment $Y_1$ (revenue) across control and treated units and randomization guarantees that this difference is due to the treatment alone, in expectation.\n$$ \\widehat {ATE}^{simple} = \\bar Y_{t=1, d=1} - \\bar Y_{t=1, d=0} $$\nWhere the bar indicates the average over individuals. In our case, we compute the average revenue post ad campaign in the treatment group, minus the average revenue post ad campaign in the control group.\nnp.mean(df.loc[df.ad_campaign==True, 'revenue1']) - np.mean(df.loc[df.ad_campaign==False, 'revenue1']) 1.7914301325347406 The estimated treatment effect is 1.79, very close to the true value of 2. We can obtain the same estimate by regressing the post-treatment outcome revenue1 on the treatment indicator ad_campaign.\n$$ Y_{i, t=1} = \\alpha + \\beta D_i + \\varepsilon_i $$\nWhere $\\beta$ is the coefficient of interest.\nsmf.ols('revenue1 ~ ad_campaign', data=df).fit().summary().tables[1] coef std err t P\u003e|t| [0.025 0.975] Intercept 8.2995 0.211 39.398 0.000 7.881 8.718 ad_campaign 1.7914 0.301 5.953 0.000 1.194 2.389 This estimator is unbiased, which means it delivers the correct estimate, on average. However, it can still be improved: we could decrease its variance. Decreasing the variance of an estimator is extremely important since it allows us to\ndetect smaller effects detect the same effect, but with a smaller sample size In general, an estimator with a smaller variance allows us to run tests with higher power, i.e. ability to detect smaller effects.\nCan we improve the power of our AB test? Yes, with CUPED (among other methods).\nCUPED The idea of CUPED is the following. Suppose you are running an AB test and $Y$ is the outcome of interest (revenue in our example) and the binary variable $D$ indicates whether a single individual has been treated or not (ad_campaign in our example).\nSuppose you have access to another random variable $X$ which is not affected by the treatment and has known expectation $\\mathbb E[X]$. Then define\n$$ \\hat Y^{cuped}_{1} = \\bar Y_1 - \\theta \\bar X + \\theta \\mathbb E [X] $$\nwhere $\\theta$ is a scalar. This estimator is an unbiased estimator for $\\mathbb E[Y]$ since in expectation the two last terms cancel out. However, the variance of $\\hat Y^{cuped}_{1}$ is\n$$ \\begin{aligned} \\text{Var} \\left( \\hat Y^{cuped}_{1} \\right) \u0026amp;= \\text{Var} \\left( \\bar Y_1 - \\theta \\bar X + \\theta \\mathbb E [X] \\right) = \\newline \u0026amp;= \\text{Var} \\left( Y_1 - \\theta X \\right) / n = \\newline \u0026amp;= \\Big( \\text{Var} (Y_1) + \\theta^2 \\text{Var} (X) - 2 \\theta \\text{Cov} (X,Y) \\Big) / n \\end{aligned} $$\nNote that the variance of $\\hat Y^{cuped}_{1}$ is minimized for\n$$ \\theta^* = \\frac{\\text{Cov} (X,Y)}{\\text{Var} (X)} $$\nWhich is the OLS estimator of a linear regression of $Y$ on $X$. Substituting $\\theta^*$ into the formula of the variance of $\\hat Y^{cuped}_{1}$ we obtain\n$$ \\text{Var} \\left( \\hat Y^{cuped}_{1} \\right) = \\text{Var} (\\bar Y) (1 - \\rho^2) $$\nwhere $\\rho$ is the correlation between $Y$ and $X$. Therefore, the higher the correlation between $Y$ and $X$, the higher the variance reduction of CUPED.\nWe can then estimate the average treatment effectas the average difference in the transformed outcome between the control and treatment group.\n$$ \\begin{aligned} \\widehat {ATE}^{cuped} \u0026amp;= \\hat Y^{cuped}{1} (D=1) - \\hat Y^{cuped}{1}(D=0) = \\newline \u0026amp;= \\big( \\bar Y_1 - \\theta \\bar X + \\theta \\mathbb E [X] \\ \\big| \\ D = 1 \\big) - \\big( \\bar Y_1 - \\theta \\bar X + \\theta \\mathbb E [X] \\ \\big| \\ D = 0 \\big) = \\newline \u0026amp;= \\big( \\bar Y_1 - \\theta \\bar X \\ \\big| \\ D = 1 \\big) - \\big( \\bar Y_1 - \\theta \\bar X \\ \\big| \\ D = 0 \\big) \\end{aligned} $$\nNote that $\\mathbb E[X]$ cancels out when taking the difference. Therefore, it is sufficient to compute\n$$ \\hat Y_{cuped,1}\u0026rsquo; = \\bar Y_1 - \\theta \\bar X $$\nThis is not an unbiased estimator of $\\mathbb E[Y]$ but still delivers an unbiased estimator of the average treatment effect.\nOptimal X What is the optimal choice for the control variable $X$?\nWe know that $X$ should have the following properties:\nnot affected by the treatment as correlated with $Y_1$ as possible The authors of the paper suggest using the pre-treatment outcome $Y_{0}$ since it gives the most reduction in variance in practice.\nTherefore, in practice, we can compute the CUPED estimate of the average treatment effect as follows:\nRegress $Y_1$ on $Y_0$ and estimate $\\hat \\theta$ Compute $\\hat Y^{cuped}_{1} = \\bar Y_1 - \\hat \\theta \\bar Y_0$ Compute the difference of $\\hat Y^{cuped}_{1}$ between treatment and control group Equivalently, we can compute $\\hat Y^{cuped}_{1}$ at the individual level and then regress it on the treatment dummy variable $D$.\nBack To The Data Let\u0026rsquo;s compute the CUPED estimate for the treatment effect, one step at the time. First, let\u0026rsquo;s estimate $\\theta$.\ntheta = smf.ols('revenue1 ~ revenue0', data=df).fit().params[1] Now we can compute the transformed outcome $\\hat Y^{cuped}_{1}$.\ndf['revenue1_cuped'] = df['revenue1'] - theta * (df['revenue0'] - np.mean(df['revenue0'])) Lastly, we estimate the treatment effect as a difference in means, with the transformed outcome $\\hat Y^{cuped}_{1}$.\nsmf.ols('revenue1_cuped ~ ad_campaign', data=df).fit().summary().tables[1] coef std err t P\u003e|t| [0.025 0.975] Intercept 8.2259 0.143 57.677 0.000 7.943 8.509 ad_campaign 1.9415 0.204 9.529 0.000 1.537 2.346 The standard error is 33% smaller!\nEquivalent Formulation An alternative but algebraically equivalent way of obtaining the CUPED estimate is the folowing\nRegress $Y_1$ on $Y_0$ and compute the residuals $\\tilde Y_1$ Compute $\\hat Y^{cuped}_{1} = \\tilde Y_1 + \\bar Y_1$ Compute the difference of $\\hat Y^{cuped}_{1}$ between treatment and control group Step (3) is the same as before but (1) and (2) are different. This procedure is called partialling out and the algebraic equivalence is guaranteed by the Frisch-Waugh-Lowell Theorem.\nLet\u0026rsquo;s check if we indeed obtain the same result.\ndf['revenue1_tilde'] = smf.ols('revenue1 ~ revenue0', data=df).fit().resid + np.mean(df['revenue1']) smf.ols('revenue1_tilde ~ ad_campaign', data=df).fit().summary().tables[1] coef std err t P\u003e|t| [0.025 0.975] Intercept 8.2259 0.143 57.677 0.000 7.943 8.509 ad_campaign 1.9415 0.204 9.529 0.000 1.537 2.346 Yes! The regression table is exactly identical.\nCUPED vs Other CUPED seems to be a very powerful procedure but it is remindful of at least a couple of other methods.\nAutoregression or regression with control variables Difference-in-Differences Are these methods the same or is there a difference? Let\u0026rsquo;s check.\nAutoregression The first question that came to my mind when I first saw CUPED was \u0026ldquo;is CUPED just the simple difference with an additional control variable?\u0026rdquo;. Or equivalently, is CUPED equivalent to running the following regression\n$$ Y_{i, t=1} = \\alpha + \\beta D_i + \\gamma Y_{i, t=0} + \\varepsilon_i $$\nand estimating $\\gamma$ via least squares?\nsmf.ols('revenue1 ~ revenue0 + ad_campaign', data=df).fit().summary().tables[1] coef std err t P\u003e|t| [0.025 0.975] Intercept 1.9939 0.603 3.304 0.001 0.796 3.192 revenue0 1.2249 0.114 10.755 0.000 0.999 1.451 ad_campaign 1.9519 0.205 9.529 0.000 1.545 2.358 The estimated coefficient is very similar to the one we obtained with CUPED and also the standard error is very close. However, they are not exactly the same.\nIf you are familiar with the Frisch-Waugh-Lowell Theorem, you might wonder why the two procedures are not equivalent. The reason is that with CUPED we are partialling out only $Y$, while the FWL theorem holds when we are partialling out either X or both X and Y.\nDiff-in-Diffs The second question that came to my mind was \u0026ldquo;is CUPED just difference-in-differences?\u0026rdquo;. Difference-in-Differences (or diff-in-diffs, or DiD) is an estimator that computes the treatment effect as a double-difference instead of a single one: pre-post and treatment-control instead of just treatment-control.\n$$ \\widehat {ATE}^{DiD} = \\big( \\bar Y_{t=1, d=1} - \\bar Y_{t=1, d=0} \\big) - \\big( \\bar Y_{t=0, d=1} - \\bar Y_{t=0, d=0} \\big) $$\nThis method was initially introduced in the 19th century to estimate the causes of a Cholera epidemic in London. The main advantage of diff-in-diff is that it allows to estimate the average treatment effect also when randomization is not perfect and the treatment and control group are not comparable. The key assumption that is needed is that these difference between the treatment and control group is constant over time. By taking a double difference, we difference it out.\nLet\u0026rsquo;s check how diff-in-diff works empirically. The most common way to compute the diff-in-diff estiamtor is to first reshape the data in a long format or panel format (one observation is an individual $i$ at time period $t$) and then to regress the outcome $Y$ on the full interaction between the post-treatment dummy $T$ and the treatment dummy $D$.\n$$ Y_{i,t} = \\alpha + \\beta D_i + \\gamma \\mathbb I (t=1) + \\delta D_i * \\mathbb I (t=1) + \\varepsilon_{i,t} $$\nThe estimator of the average treatment effect is the coefficient of the interaction coefficient, $\\delta$.\ndf_long = pd.wide_to_long(df, stubnames='revenue', i='i', j='t').reset_index() df_long.head() i t revenue1_tilde ad_campaign revenue1_cuped revenue 0 1 0 8.093744 0 8.093744 5.315635 1 2 0 10.164644 1 10.164644 2.977799 2 3 0 9.472203 0 9.472203 4.693796 3 4 0 7.688063 0 7.688063 5.827975 4 5 0 8.742618 0 8.742618 5.230095 The long dataset is now indexed by individuals $i$ and time $t$. We can now run the diff-in-diffs regression.\nsmf.ols('revenue ~ t * ad_campaign', data=df_long).fit().summary().tables[1] coef std err t P\u003e|t| [0.025 0.975] Intercept 5.1481 0.174 29.608 0.000 4.805 5.491 t 3.1514 0.246 12.816 0.000 2.666 3.636 ad_campaign -0.1310 0.248 -0.527 0.599 -0.621 0.359 t:ad_campaign 1.9224 0.351 5.473 0.000 1.230 2.615 The estimated coefficient is close to the true value, 2, but the standard errors are bigger than the ones obtained with all other methods (0.35 \u0026raquo; 0.2). What did we miss? We didn\u0026rsquo;t cluster the standard errors!\nI won\u0026rsquo;t go in detail here on what standard error clustering means, but the intuition is the following. The statsmodels package by default computes the standard errors assuming that outcomes are independent across observations. This assumption is unlikely to be true in this setting where we observe individuals over time and we are trying to exploit this information. Clustering allows for correlation of the outcome variable within clusters. In our case, it makes sense (even without knowing the data generating process) to cluster the standard errors at the individual levels, allowing the outcome to be correlated over time for an individual $i$.\nsmf.ols('revenue ~ t * ad_campaign', data=df_long)\\ .fit(cov_type='cluster', cov_kwds={'groups': df_long['i']})\\ .summary().tables[1] coef std err z P\u003e|z| [0.025 0.975] Intercept 5.1481 0.139 37.056 0.000 4.876 5.420 t 3.1514 0.128 24.707 0.000 2.901 3.401 ad_campaign -0.1310 0.181 -0.724 0.469 -0.486 0.224 t:ad_campaign 1.9224 0.209 9.208 0.000 1.513 2.332 Clustering standard errors at the individual level we obtain standard errors that are comparable to the previous estimates ($\\sim 0.2$).\nNote that diff-in-diffs is equivalent to CUPED when we assume the CUPED coefficient $\\theta=1$.\ndf['revenue1_cuped2'] = df['revenue1'] - 1 * (df['revenue0'] - np.mean(df['revenue0'])) smf.ols('revenue1_cuped2 ~ ad_campaign', data=df).fit().summary().tables[1] coef std err t P\u003e|t| [0.025 0.975] Intercept 8.2353 0.145 56.756 0.000 7.947 8.523 ad_campaign 1.9224 0.207 9.274 0.000 1.511 2.334 Indeed, we obtain the same exact coefficient and almost identical standard errors!\nComparison Which method is better? From what we have seen so far, all methods seem to deliver an accurate estimate, but the simple difference has a larger standard deviation.\nLet\u0026rsquo;s now compare all the methods we have seen so far via simulation. We simulate the data generating process dgp_cuped() 1000 times and we save the estimated coefficient of the following methods:\nSimple difference Autoregression Diff-in-diffs CUPED def simulate(dgp, K=300, x='revenue0'): # Initialize coefficients results = pd.DataFrame(columns=['k', 'Estimator', 'Estimate']) # Compute coefficients for k in range(K): temp = pd.DataFrame({'k': [k] * 4, 'Estimator': ['1. Diff ', '2. Areg ', '3. DiD ', '4. CUPED'], 'Estimate': [0] * 4}) # Draw data df = dgp.generate_data(seed=k) # Single diff temp['Estimate'][0] = smf.ols('revenue1 ~ ad_campaign', data=df).fit().params[1] # Autoregression temp['Estimate'][1] = smf.ols(f'revenue1 ~ ad_campaign + {x}', data=df).fit().params[1] # Double diff df_long = pd.wide_to_long(df.dropna(), stubnames='revenue', i='i', j='t').reset_index() temp['Estimate'][2] = smf.ols('revenue ~ ad_campaign * t', data=df_long)\\ .fit(cov_type='cluster', cov_kwds={'groups': df_long['i']}).params[3] # Cuped df['revenue1_tilde'] = smf.ols(f'revenue1 ~ {x}', data=df).fit().resid + np.mean(df['revenue1']) temp['Estimate'][3] = smf.ols('revenue1_tilde ~ ad_campaign', data=df).fit().params[1] # Combine estimates results = pd.concat((results, temp)) return results.reset_index(drop=True) results = simulate(dgp=dgp_cuped()) Let\u0026rsquo;s plot the distribution of the estimated parameters.\nsns.kdeplot(data=results, x=\u0026quot;Estimate\u0026quot;, hue=\u0026quot;Estimator\u0026quot;); plt.axvline(x=2, c='k', ls='--'); plt.title('Simulated Distributions'); We can also tabulate the simulated mean and standard deviation of each estimator.\nresults.groupby('Estimator').agg(mean=(\u0026quot;Estimate\u0026quot;, \u0026quot;mean\u0026quot;), std=(\u0026quot;Estimate\u0026quot;, \u0026quot;std\u0026quot;)) mean std Estimator 1. Diff 1.999626 0.291497 2. Areg 2.034145 1.063968 3. DiD 1.993494 0.197638 4. CUPED 1.971853 0.198145 All estimators seem unbiased: the average values are all close to the true value of 2. Moreover, all estimators have a very similar standard deviation, apart from the single-difference estimator!\nAlways Identical? Are the estimators always identical, or is there some difference among them?\nWe could check many different departures from the original data generating process. For simplicity, I will consider only one here: imperfect randomization. Other tweaks of the data generating process that I considered are:\npre-treatment missing values additional covariates / control variables multiple pre-treatment periods heterogeneous treatment effects and combinations of them. However, I found imperfect randomization to be the most informative example.\nSuppose now that randomization was not perfect and two groups are not identical. In particular, if the data generating process is\n$$ Y_{i,t} = \\alpha + \\beta D_i + \\gamma \\mathbb I (t=1) + \\delta D_i * \\mathbb I (t=1) + u_i + \\varepsilon_{i,t} $$\nassume that $\\beta \\neq 0$.\nresults_beta1 = simulate(dgp=dgp_cuped(beta=1)) Let\u0026rsquo;s plot the distribution of the estimated parameters.\nsns.kdeplot(data=results_beta1, x=\u0026quot;Estimate\u0026quot;, hue=\u0026quot;Estimator\u0026quot;); plt.axvline(x=2, c='k', ls='--'); plt.title('Simulated Distributions'); results_beta1.groupby('Estimator').agg(mean=(\u0026quot;Estimate\u0026quot;, \u0026quot;mean\u0026quot;), std=(\u0026quot;Estimate\u0026quot;, \u0026quot;std\u0026quot;)) mean std Estimator 1. Diff 2.999626 0.291497 2. Areg 1.991508 0.227065 3. DiD 1.993494 0.197638 4. CUPED 1.577712 0.221448 With imperfect treatment assignment, both difference-in-differences and autoregression are unbiased for the true treatment effect, however diff-in-diffs is more efficient. Both CUPED and simple difference are biased instead. Why?\nDiff-in-diffs explicily controls for systematic differences between treatment and control group that are constant over time. This is exactly what this estimator was built for. Autoregression performs some sort of matching on the additional covariate, $Y_{t=0}$, effectively controlling for these systematic differences as well, but less efficiently (if you want to know more, I wrote related posts on control variables here and here). CUPED controls for persistent heterogeneity at the individual level, but not at the treatment assignment level. Lastly, the simple difference estimator does not control for anything.\nConclusion In this post, I have analyzed an estimator of average treatment effects in AB testing, very popular in the industry: CUPED. The key idea is that, by exploiting pre-treatment data, CUPED can achieve a lower variance by controlling for individual-level variation that is persistent over time. We have also seen that CUPED is closely related but not equivalent to autoregression and difference-in-differences. The differences among the methods clearly emerge when we have imperfect randomization.\nAn interesting avenue of future research is what happens when we have a lot of pre-treatment information, either in terms of time periods or observable characteristics. Scientists from Meta, Guo, Coey, Konutgan, Li, Schoener, Goldman (2021), have analyzed this problem in a very recent paper that exploits machine learning techniques to efficiently use this extra information. This approach is closely related to the Double/Debiased Machine Learning literature. If you are interested, I wrote two articles on the topic (part 1 and part 2) and I might write more in the future.\nReferences [1] A. Deng, Y. Xu, R. Kohavi, T. Walker, Improving the Sensitivity of Online Controlled Experiments by Utilizing Pre-Experiment Data (2013), WSDM.\n[2] H. Xir, J. Aurisset, Improving the sensitivity of online controlled experiments: Case studies at Netflix (2013), ACM SIGKDD.\n[3] Y. Guo, D. Coey, M. Konutgan, W. Li, C. Schoener, M. Goldman, Machine Learning for Variance Reduction in Online Experiments (2021), NeurIPS.\n[4] V. Chernozhukov, D. Chetverikov, M. Demirer, E. Duflo, C. Hansen, W. Newey, J. Robins, Double/debiased machine learning for treatment and structural parameters (2018), The Econometrics Journal.\n[5] M. Bertrand, E. Duflo, S. Mullainathan, How Much Should We Trust Differences-In-Differences Estimates? (2012), The Quarterly Journal of Economics.\nRelated Articles Double Debiased Machine Learning (part 1) Double Debiased Machine Learning (part 2) Understanding The Frisch-Waugh-Lovell Theorem Understanding Contamination Bias DAGs and Control Variables Code You can find the original Jupyter Notebook here:\nhttps://github.com/matteocourthoud/Blog-Posts/blob/main/notebooks/cuped.ipynb\n","date":1688947200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1688947200,"objectID":"88102c816a2f90fe53e405766df63eb5","permalink":"https://matteocourthoud.github.io/post/cuped/","publishdate":"2023-07-10T00:00:00Z","relpermalink":"/post/cuped/","section":"post","summary":"An in depth guide to the state-of-the art variance reduction technique in A/B tests\nDuring my PhD, I spent a lot of time learning and applying causal inference methods to experimental and observational data.","tags":null,"title":"Understanding CUPED","type":"post"},{"authors":null,"categories":null,"content":"How to use machine learning to estimate heterogeneous treatment effects\nIn many settings, we are not just interested in understanding a causal effect, but also whether this effect is different for different users. We might be interested in understanding if a drug has side effects that are different for people of different age. Or we might be interested in understanding if an ad campaign is particularly effective in certain geographical areas.\nThis knowledge is crucial because it allows us to target the treatment. If a drug has severe side effects for kids, we might want to restrict its distribution only to adults. Or if an ad campaign is effective only in English-speaking countries it is not worth showing it elsewhere.\nIn this blog post we are going to explore some approaches to uncover treatment effect heterogeneity. In particular, we are going to explore methods that try to leverage the flexibility of machine learning algorithms.\nExample Suppose we were a company interested in understanding how much a new premium feature increases revenue. In particular, we know that users of different age have different spending attitudes and we suspect that the impact of the premium feature could also be different depending on the age of the user.\nThis information might be very important, for example for advertisement targeting or discount design. If we discover that the premium feature increases revenue for a particular set of users, we might want to target advertisement towards that group, or offer them personalized discounts.\nTo understand the effect of the premium feature on revenue, the run an AB test in which we randomly give access to the premium feature to 10% of the users. The feature is expensive and we cannot afford to give it for free to more users. Hopefully a 10% treatment probability is enough.\nWe generate the simulated data using the data generating process dgp_premium() from src.dgp. I also import some plotting functions and libraries from src.utils.\n%matplotlib inline %config InlineBackend.figure_format = 'retina' from src.utils import * from src.dgp import dgp_premium dgp = dgp_premium() df = dgp.generate_data(seed=5) df.head() revenue premium age 0 10.62 True 27.32 1 10.35 True 54.57 2 10.13 False 26.68 3 9.97 False 56.58 4 10.16 False 38.51 We have data on 300 users, for whom we observe the revenue they generate and whether they were given the premium feature. Moreover, we also record the age of the users.\nTo understand whether randomization worked, we use the create_table_one function from Uber\u0026rsquo;s causalml package to produce a covariate balance table, containing the average value of our observable characteristics, across treatment and control groups. As the name suggests, this should always be the first table you present in causal inference analysis.\nfrom causalml.match import create_table_one create_table_one(df, 'premium', ['age', 'revenue']) Control Treatment SMD Variable n 269 31 age 39.01 (12.11) 38.43 (13.26) -0.0454 revenue 10.04 (0.16) 10.56 (0.23) 2.5905 Most users are in the control group and only 31 users have received the premium feature. Average age is comparable across groups (SMD\u0026lt;1), while it seems that the premium feature increases revenue by 2.6$ per user, on average.\nDoes the effect of the premium feature differ by age?\nOne simple approach could me to regress revenue on a full interaction of premium and age.\nlinear_model = smf.ols('revenue ~ premium * age', data=df).fit() linear_model.summary().tables[1] coef std err t P\u003e|t| [0.025 0.975] Intercept 10.0244 0.034 292.716 0.000 9.957 10.092 premium[T.True] 0.5948 0.099 6.007 0.000 0.400 0.790 age 0.0005 0.001 0.570 0.569 -0.001 0.002 premium[T.True]:age -0.0021 0.002 -0.863 0.389 -0.007 0.003 The interaction coefficient is close to zero and not significant. It seems that there is not a different effect of premium by age. But is it true? The interaction coefficient only captures linear relationships. What if the relationship is non-linear?\nWe can check it by directly plotting the raw data. We plot revenue by age, splitting the data between premium users and non-premium users.\nsns.scatterplot(data=df, x='age', y='revenue', hue='premium', s=40); From the raw data, it looks like revenue is generally higher for people between 30 and 40 and premium has a particularly strong effect for people between 35 and 45/50.\nWe can visualize the estimated revenue by age with and without treatment.\ndef plot_TE(df, true_te=False): sns.scatterplot(data=df, x='age', y='revenue', hue='premium', s=40, legend=True) sns.lineplot(df['age'], df['mu0_hat'], label='$\\mu_0$') sns.lineplot(df['age'], df['mu1_hat'], label='$\\mu_1$') if true_te: plt.fill_between(df['age'], df['y0'], df['y0'] + df['y1'], color='grey', alpha=0.2, label=\u0026quot;True TE\u0026quot;) plt.title('Distribution of revenue by age and premium status') plt.legend(title='Treated') We first compute the predicted revenue with ($\\mu_1$) and without premium subscription ($\\mu_0$) and we plot them together with the raw data.\ndf['mu0_hat'] = linear_model.predict(df.assign(premium=0)) df['mu1_hat'] = linear_model.predict(df.assign(premium=1)) plot_TE(df) As we can see, the orange line is higher than the blue line, suggesting a positive effect of premium on revenue. However, the two lines are essentially parallel, suggesting no heterogeneity in treatment effects.\nCan we be more precise? Is there a way to estimate this treatment heterogeneity in a flexible way, without assuming functional forms?\nThe answer is yes! We can use machine learning methods to flexibly estimate heterogeneous treatment effects. In particular, in this blog post we are going to inspect three and popular methods that were introduced by Künzel, Sekhon, Bickel, Yu, (2019):\nS-learner T-learner X-learner Setting We assume that for a set of i.i.d. subjects $i = 1, \u0026hellip;, n$ we observed a tuple $(X_i, D_i, Y_i)$ comprised of\na treatment assignment $T_i \\in \\lbrace 0, 1 \\rbrace$ (premium) a response $Y_i \\in \\mathbb R$ (revenue) a feature vector $X_i \\in \\mathbb R^n$ (age) We are interested in estimating the average treatment effect.\n$$ \\tau = \\mathbb E \\Big[ Y_i^{(1)} - Y_i^{(0)} \\Big] $$\nWhere $Y_i^{(d)}$ indicates the potential outcome of individual $i$ under treatment status $d$. We also make the following assumptions.\nAssumption 1 : unconfoundedness (or ignorability, or selection on observables)\n$$ \\big \\lbrace Y_i^{(1)} , Y_i^{(0)} \\big \\rbrace \\ \\perp \\ D_i \\ | \\ X_i $$\ni.e. conditional on observable characteristics $X$, the treatment assignment $D$ is as good as random. What we are effectively assuming is that there is no other characteristics that we do not observe that could impact both whether a user selects the dark_mode and their read_time. This is a strong assumption that is more likely to be satisfied the more individual characteristics we observe.\nAssumption 2: stable unit treatment value (SUTVA)\n$$ Y^{(d)} \\perp D $$\ni.e. the potential outcome does not depend on the treatment status. In our case, we are ruling out the fact that another user selecting the premium feature might affect my effect of premium on revenue. The most common setting where SUTVA is violated is in presence of network effects: if a friend of mine uses a social network increases my utility from using it.\nS-Learner The simplest meta-algorithm is the single learner or S-learner. To build the S-learner estimator, we fit a single model for all observations.\n$$ \\mu(z) = \\mathbb E \\left[ Y_i \\ \\big | \\ (X_i, D_i) = z \\right] $$\nthe estimator is given by the difference between the predicted values evaluated with and without the treatment, $d=1$ and $d=0$.\n$$ \\hat \\tau_{S} (x) = \\hat \\mu(x,1) - \\hat \\mu(x,0) $$\ndef S_learner(dgp, model, y, D, X): temp = dgp.generate_data(true_te=True).sort_values(X) mu = model.fit(temp[X + [D]], temp[y]) temp['mu0_hat'] = mu.predict(temp[X + [D]].assign(premium=0)) temp['mu1_hat'] = mu.predict(temp[X + [D]].assign(premium=1)) plot_TE(temp, true_te=True) Let\u0026rsquo;s use a decision tree regression model to build the the S-learner, using the DecisionTreeRegressor function from the sklearn package. I won\u0026rsquo;t go into details about decision trees here, but I will just say that it\u0026rsquo;s a non-parametric estimator that uses the training data to split the state space (premium and age in our case) into blocks and predicts the outcome (revenue in our case) as its average value within block.\nfrom sklearn.tree import DecisionTreeRegressor model = DecisionTreeRegressor(min_impurity_decrease=0.001) S_learner(dgp, model, y=\u0026quot;revenue\u0026quot;, D=\u0026quot;premium\u0026quot;, X=[\u0026quot;age\u0026quot;]) The plot depicts the data together with the response functions $\\hat \\mu(x,1)$ and $\\hat \\mu(x,0)$. I have also plotted in grey the area between the true response functions: the true treatment effects.\nAs we can see, the S-learner is flexible enough to understand that there is a difference in levels between treatment and control group (we have two separate lines). It also captures well the response function for the control group, $\\hat \\mu(x,0)$, but not so well the control function for the treatment group, $\\hat \\mu(x,1)$.\nThe problem with the S-learner is that it is learning a single model so we have to hope that the model uncovers heterogeneity in the treatment $D$, but it might not be the case. Moreover, if the model is heavily regularized because of the high dimensionality of $X$, it might not recover any treatment effect. For example, with decision trees, we might not split on the treatment $D$.\nT-learner To build the two-learner or T-learner estimator, we fit two different models, one for treated units and one for control units.\n$$ \\mu^{(1)}(x) = \\mathbb E \\left[ Y_i \\ \\big | \\ X_i = x, T_i = 1 \\right] \\qquad ; \\qquad \\mu^{(0)}(x) = \\mathbb E \\left[ Y_i \\ \\big | \\ X_i = x, T_i = 0 \\right] $$\nthe estimator is given by the difference between the predicted values of the two algorithms.\n$$ \\hat \\tau_{T} (x) = \\hat \\mu^{(1)}(x) - \\hat \\mu^{(0)}(x) $$\ndef T_learner(df, model, y, D, X): temp = dgp.generate_data(true_te=True).sort_values(X) mu0 = model.fit(temp.loc[temp[D]==0, X], temp.loc[temp[D]==0, y]) temp['mu0_hat'] = mu0.predict(temp[X]) mu1 = model.fit(temp.loc[temp[D]==1, X], temp.loc[temp[D]==1, y]) temp['mu1_hat'] = mu1.predict(temp[X]) plot_TE(temp, true_te=True) We use a decision tree regression model as before but, this time, we fit two separate decision trees for the treatment and control group.\nT_learner(dgp, model, y=\u0026quot;revenue\u0026quot;, D=\u0026quot;premium\u0026quot;, X=[\u0026quot;age\u0026quot;]) As we can see, the T-learner is much more flexible than the S-learner because it fits two separate models. The response function for the control group, $\\hat \\mu(x,0)$, is still very accurate and the response function for the treatment group, $\\hat \\mu(x,1)$, is more flexible than before.\nThe problem now is that we are using just a fraction of the data for each prediction problem, while the S-learner was using all the data. By fitting two separate models we are losing some information. Moreover, by using two different models we might get heterogeneity where there is none. For example, with decision trees, we will probably get different splits with different samples even if the data generating process is the same.\nX-learner The cross-learner or X-learner estimator is an extension of the T-learner estimator. It is built in the following way:\nAs for the T-learner, compute separate models for $\\mu^{(1)}(x)$ and $\\mu^{(0)}(x)$ using the treated and control units, respectively\nCompute the estimated treatment effects as\n$$ \\Delta_i (x) = \\begin{cases} Y_i - \\hat \\mu^{(0)}(x) \u0026amp;\\quad \\text{ if } D_i = 1 \\newline \\hat \\mu^{(1)}(x) - Y_i \u0026amp;\\quad \\text{ if } D_i = 0 \\end{cases} $$\nPredicting $\\Delta$ from $X$, compute $\\hat \\tau^{(0)}(x)$ from treated units and $\\hat \\tau^{(1)}(x)$ from control units\nEstimate the propensity score\n$$ e(x) = \\mathbb E \\left[ D_i = 1 \\ \\big | \\ X_i = x \\right] $$\nCompute the treatment effects $$ \\hat \\tau_X(x) = \\hat \\tau^{(0)}(x) \\hat e(x) + \\hat \\tau^{(1)}(x) (1 - \\hat e(x)) $$\nCan we still recover pseudo response functions? Yes!\nWhich we can rewrite the treatment effects as\n$$ \\begin{align} \\hat \\tau_X(x) \u0026amp; = \\hat \\tau^{(0)}(x) \\hat e(x) + \\hat \\tau^{(1)}(x) (1 - \\hat e(x)) = \\newline \u0026amp;= \\hat e(x) \\left[ \\hat \\mu^{(1)}(x) - Y_i^{(0)} \\right] + (1 - \\hat e(x)) \\left[ Y_i^{(1)} - \\hat \\mu^{(0)}(x) \\right] = \\newline \u0026amp;= \\left[ \\hat e(x) \\hat \\mu^{(1)}(x) + (1 - \\hat e(x)) Y_i^{(1)} \\right] - \\left[ \\hat e(x) Y_i^{(0)} + (1 - \\hat e(x)) \\hat \\mu^{(0)}(x) \\right] \\end{align} $$\nSo that the pseudo response functions estimated by the X-learner are\n$$ \\begin{align} \\tilde \\mu_i^{(1)} (x) \u0026amp;= \\hat e(x) \\hat \\mu^{(1)}(x) + (1 - \\hat e(x)) Y_i^{(1)} \\newline \\tilde \\mu_i^{(0)} (x) \u0026amp;= \\hat e(x) Y_i^{(0)} + (1 - \\hat e(x)) \\hat \\mu^{(0)}(x) \\end{align} $$\nAs we can see, the X-learner combines the true values $Y_i^{(d)}$ with the estimated ones $\\mu_i^{(d)} (x)$ weighting by the propensity scores $e_i(x)$, i.e. the estimated treatment probabilities.\nWhat does it mean? It means that if we have many more observations for one group (in our case the control group), the control response function $\\hat \\mu^{(0)}(x) $ will get most of the weight. Instead, for the other group (the treatment group in our case), the actual observations $Y_i^{(1)}$ will get most of the weight.\nTo illustrate the method, I am going to build pseudo response functions by approximating $Y_i^{(d)}$ using the nearest observation, using the KNeighborsRegressor function. I estimate the propensity scores via logistic regression using the LogisticRegressionCV function.\nfrom sklearn.neighbors import KNeighborsRegressor from sklearn.linear_model import LogisticRegressionCV def X_learner(df, model, y, D, X): temp = dgp.generate_data(true_te=True).sort_values(X) # Mu mu0 = model.fit(temp.loc[temp[D]==0, X], temp.loc[temp[D]==0, y]) temp['mu0_hat_'] = mu0.predict(temp[X]) mu1 = model.fit(temp.loc[temp[D]==1, X], temp.loc[temp[D]==1, y]) temp['mu1_hat_'] = mu1.predict(temp[X]) # Y y0 = KNeighborsRegressor(n_neighbors=1).fit(temp.loc[temp[D]==0, X], temp.loc[temp[D]==0, y]) temp['y0_hat'] = y0.predict(temp[X]) y1 = KNeighborsRegressor(n_neighbors=1).fit(temp.loc[temp[D]==1, X], temp.loc[temp[D]==1, y]) temp['y1_hat'] = y1.predict(temp[X]) # Weight e = LogisticRegressionCV().fit(y=temp[D], X=temp[X]).predict_proba(temp[X])[:,1] temp['mu0_hat'] = e * temp['y0_hat'] + (1-e) * temp['mu0_hat_'] temp['mu1_hat'] = (1-e) * temp['y1_hat'] + e * temp['mu1_hat_'] # Plot plot_TE(temp, true_te=True) X_learner(df, model, y=\u0026quot;revenue\u0026quot;, D=\u0026quot;premium\u0026quot;, X=[\u0026quot;age\u0026quot;]) As we can clearly see from this graph, the main advantage of X-learners is that it adapts the flexibility of the response functions to the context. In areas of the state space where we have a lot of data, it mostly uses the estimated response function, in areas of the state space with few data, it uses the observation themselves.\nConclusion In this post, we have seen different estimators introduced by Künzel, Sekhon, Bickel, Yu, (2019) that leverage flexible machine learning algorithms to estimate heterogeneous treatment effects. The estimators differ for their degree of sophistication: the S-learner fits a single estimator including the treatment indicator as a covariate. The T-learner fits two separate estimators for the treatment and control group. Lastly, the X-learner is an extension of the T-learner that allows for different degrees of flexibility depending on the amount of data available across treatment and control groups.\nEstimation of heterogeneous treatment effect is extremely important for treatment targeting. Indeed, there is now a growing literature that exploits machine learning methods to get flexible estimates without imposing functional form assumptions. Among the many, it\u0026rsquo;s important to mention the R-learner procedure of Nie and Wager (2021) and the causal trees and forests of Athey and Wager (2018). I might write more about these procedures in the future so, stay tuned ☺️\nReferences [1] S. Künzel, J. Sekhon, P. Bickel, B. Yu, Metalearners for estimating heterogeneous treatment effects using machine learning (2019), PNAS.\n[2] X. Nie, S. Wager, Quasi-oracle estimation of heterogeneous treatment effects (2021), Biometrika.\n[3] S. Athey, S. Wager, Estimation and Inference of Heterogeneous Treatment Effects using Random Forests (2018), Journal of the American Statistical Association.\nRelated Articles Matching, Weighting, or Regression? DAGs and Control Variables Code You can find the original Jupyter Notebook here:\nhttps://github.com/matteocourthoud/Blog-Posts/blob/main/notebooks/meta.ipynb\n","date":1688947200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1688947200,"objectID":"23b98ec03768fc1a97168ce2e40c689a","permalink":"https://matteocourthoud.github.io/post/meta_learners/","publishdate":"2023-07-10T00:00:00Z","relpermalink":"/post/meta_learners/","section":"post","summary":"How to use machine learning to estimate heterogeneous treatment effects\nIn many settings, we are not just interested in understanding a causal effect, but also whether this effect is different for different users.","tags":null,"title":"Understanding Meta Learners","type":"post"},{"authors":null,"categories":null,"content":"Understanding and comparing different methods for conditional causal inference analysis\nAB tests or randomized controlled trials are the gold standard in causal inference. By randomly exposing units to a treatment we make sure that individuals in both groups are comparable, on average, and any difference we observe can be attributed to the treatment effect alone.\nHowever, often the treatment and control groups are not perfectly comparable. This could be due to the fact that randomization was not perfect or available. Not always we can randomize a treatment, for ethical or practical reasons. And even when we can, sometimes we do not have enough individuals or units so that differences between groups are seizable. This happens often, for example, when randomization is not done at the individual level, but at a higher level of aggregation, for example zipcodes, counties or even states.\nIn these settings, we can still recover a causal estimate of the treatment effect if we have enough information about individuals, by making the treatment and control group comparable, ex-post. In this blog post, we are going to introduce and compare different procedures to estimate causal effects in presence of imbalances between treatment and control groups that are fully observable. In particular we are going to analyze weighting, matching and regression procedures.\nExample Assume we had blog on statistics and causal inference 😇. To improve user experience, we are considering releasing a dark mode, and we would like to understand whether this new feature increases the time users spend on our blog.\nWe are not a sophisticated company, therefore we do not run an AB test but we simply release the dark mode and we observe whether users select it or not and the time they spend on the blog. We know that there might be selection: users that prefer the dark mode could have different reading preferences and this might complicate our causal analysis.\nWe can represent the data generating process with the following Directed Acyclic Graph (DAG).\nflowchart TB classDef included fill:#DCDCDC,stroke:#000000,stroke-width:2px; classDef excluded fill:#ffffff,stroke:#000000,stroke-width:2px; classDef unobserved fill:#ffffff,stroke:#000000,stroke-width:2px,stroke-dasharray: 5 5; X1((gender)) X2((age)) X3((hours)) D((dark mode)) Y((read time)) D --\u0026gt; Y X1 --\u0026gt; Y X1 --\u0026gt; D X2 --\u0026gt; D X3 --\u0026gt; Y class D,Y included; class X1,X2,X3 excluded; We generate the simulated data using the data generating process dgp_darkmode() from src.dgp. I also import some plotting functions and libraries from src.utils.\n%matplotlib inline %config InlineBackend.figure_format = 'retina' from src.utils import * from src.dgp import dgp_darkmode df = dgp_darkmode().generate_data() df.head() read_time dark_mode male age hours 0 14.4 False 0 43.0 65.6 1 15.4 False 1 55.0 125.4 2 20.9 True 0 23.0 642.6 3 20.0 False 0 41.0 129.1 4 21.5 True 0 29.0 190.2 We have informations on 300 users for whom we observe whether they select the dark_mode (the treatment), their weekly read_time (the outcome of interest) and some characteristics like gender, age and total hours previously spend on the blog.\nWe would like to estimate the effect of the new dark_mode on users\u0026rsquo; read_time. If we were runnig an AB test or randomized control trial, we could just compare users with and without the dark mode and we could attribute the difference in average reading time to the dark_mode. Let\u0026rsquo;s check what number we would get.\nnp.mean(df.loc[df.dark_mode==True, 'read_time']) - np.mean(df.loc[df.dark_mode==False, 'read_time']) -0.4446330948042103 Individuals that select the dark_mode spend on average 1.37 hours less on the blog, per week. Should we conclude that dark_mode is a bad idea? Is this a causal effect?\nWe did not randomize the dark_mode so that users that selected it might not be directly comparable with users that didn\u0026rsquo;t. Can we verify this concern? Partially. We can only check characteristics that we observe, gender, age and total hours in our setting. We cannot check if users differ along other dimensions that we don\u0026rsquo;t observe.\nLet\u0026rsquo;s use the create_table_one function from Uber\u0026rsquo;s causalml package to produce a covariate balance table, containing the average value of our observable characteristics, across treatment and control groups. As the name suggests, this should always be the first table you present in causal inference analysis.\nfrom causalml.match import create_table_one X = ['male', 'age', 'hours'] table1 = create_table_one(df, 'dark_mode', X) table1 Control Treatment SMD Variable n 151 149 age 46.01 (9.79) 39.09 (11.53) -0.6469 hours 337.78 (464.00) 328.57 (442.12) -0.0203 male 0.34 (0.47) 0.66 (0.48) 0.6732 There seems to be some difference between treatment (dark_mode) and control group. In particular, users that select the dark_mode are older, have spent less hours on the blog and they are more likely to be males.\nAnother way to visually observe all the differences at once is with a paired violinplot. The advantage of the paired violinplot is that it allows us to observe the full distribution of the variable (approximated via kernel density estimation).\ndef plot_distributions(df, X, d): df_long = df.copy()[X + [d]] df_long[X] =(df_long[X] - df_long[X].mean()) / df_long[X].std() df_long = pd.melt(df_long, id_vars=d, value_name='value') sns.violinplot(y=\u0026quot;variable\u0026quot;, x=\u0026quot;value\u0026quot;, hue=d, data=df_long, split=True).\\ set(xlabel=\u0026quot;\u0026quot;, ylabel=\u0026quot;\u0026quot;, title=\u0026quot;Normalized Variable Distribution\u0026quot;); plot_distributions(df, X, \u0026quot;dark_mode\u0026quot;) The insight of the violinplot is very similar: it seems that users that select the dark_mode are different from users that don\u0026rsquo;t.\nWhy do we care?\nIf we do not control for the observable characteristics, we are unable to estimate the true treatment effect. In short, we cannot be certain that the difference in outcome, read_time, can be attributed to the treatment, dark_mode, instead of other characteristics. For example, it could be that males read less and also prefer the dark_mode, therefore we observe a negative correlation even though dark_mode has no effect on read_time (or even positive).\nIn terms of Dyrected Acyclic Graphs, this means that we have several backdoor paths that we need to block in order for our analysis to be causal.\nflowchart TB classDef included fill:#DCDCDC,stroke:#000000,stroke-width:2px; classDef excluded fill:#ffffff,stroke:#000000,stroke-width:2px; classDef unobserved fill:#ffffff,stroke:#000000,stroke-width:2px,stroke-dasharray: 5 5; X1((gender)) X2((age)) X3((hours)) D((dark mode)) Y((read time)) D --\u0026gt; Y X1 --\u0026gt; Y X1 --\u0026gt; D X2 --\u0026gt; D X3 --\u0026gt; Y linkStyle 0 stroke:#00ff00,stroke-width:4px; linkStyle 1,2 stroke:#ff0000,stroke-width:4px; class D,Y included; class X1,X2,X3 excluded; How do we block backdoor paths? By conditioning the analysis on those intermediate variables. The conditional analysis allows us to recover the average treatment effect of the dark_mode on read_time.\nflowchart TB classDef included fill:#DCDCDC,stroke:#000000,stroke-width:2px; classDef excluded fill:#ffffff,stroke:#000000,stroke-width:2px; classDef unobserved fill:#ffffff,stroke:#000000,stroke-width:2px,stroke-dasharray: 5 5; X1((gender)) X2((age)) X3((hours)) D((dark mode)) Y((read time)) D --\u0026gt; Y X1 -.-\u0026gt; Y X1 -.-\u0026gt; D X2 --\u0026gt; D X3 --\u0026gt; Y linkStyle 0 stroke:#00ff00,stroke-width:4px; class D,Y,X1,X2,X3 included; How do we condition the analysis on gender, age and hours? We have some options:\nMatching Propensity score weighting Regression with control variables Let\u0026rsquo;s explore and compare them!\nConditional Analysis We assume that for a set of subjects $i = 1, \u0026hellip;, n$ we observed a tuple $(D_i, Y_i, X_i)$ comprised of\na treatment assignment $D_i \\in \\lbrace 0, 1 \\rbrace$ (dark_mode) a response $Y_i \\in \\mathbb R$ (read_time) a feature vector $X_i \\in \\mathbb R^n$ (gender, age and hours) Assumption 1 : unconfoundedness (or ignorability, or selection on observables)\n$$ \\big \\lbrace Y_i^{(1)} , Y_i^{(0)} \\big \\rbrace \\ \\perp \\ D_i \\ | \\ X_i $$\ni.e. conditional on observable characteristics $X$, the treatment assignment $D$ is as good as random. What we are effectively assuming is that there is no other characteristics that we do not observe that could impact both whether a user selects the dark_mode and their read_time. This is a strong assumption that is more likely to be satisfied the more individual characteristics we observe.\nAssumption 2: overlap (or common support)\n$$ \\exists \\eta \u0026gt; 0 \\ : \\ \\eta \\leq \\mathbb E \\left[ T_i = 1 \\ \\big | \\ X_i = x \\right] \\leq 1-\\eta $$\ni.e. no observation is deterministically assigned to the treatment or control group. This is a more technical assumption that basically means that for any level of gender, age or hours, there could exist an individual that select the dark_mode and one that doesn\u0026rsquo;t. Differently from the unconfoundedness assumption, the overal assumption is testable.\nMatching The first and most intuitive method to perform conditional analysis is matching.\nThe idea of matching is very simple. Since we are not sure whether, for example, male and female users are directly comparable, we do the analysis within gender. Instead of comparing read_time across dark_mode in the whole sample, we do it separately for male and female users.\ndf_gender = pd.pivot_table(df, values='read_time', index='male', columns='dark_mode', aggfunc=np.mean) df_gender['diff'] = df_gender[1] - df_gender[0] df_gender dark_mode False True diff male 0 20.318000 22.24902 1.931020 1 16.933333 16.89898 -0.034354 Now the effect of dark_mode seems reversed: it is negative for male users (-0.79) but bigger and positive for female users (+1.38), suggesting a positive aggregate effect, 1.38 - 0.79 = 0.59 (assuming equal proportion of genders)! This sign reversal is a very classical example of the Simpson\u0026rsquo;s Paradox.\nThis comparison was easy to perform for gender, since it is a binary variable. With multiple variables, potentially continuous, matching becomes much more difficult. One common strategy is to match users in the treatment group with the most similar user in the control group, using some sort of nearest neighbor algorithm. I won\u0026rsquo;t go into the algorithm details here, but we can perform the matching with the NearestNeighborMatch function from the causalml package.\nThe NearestNeighborMatch function generates a new dataset where users in the treatment group have been matched 1:1 (option ratio=1) to users in the control group.\nfrom causalml.match import NearestNeighborMatch psm = NearestNeighborMatch(replace=True, ratio=1, random_state=1) df_matched = psm.match(data=df, treatment_col=\u0026quot;dark_mode\u0026quot;, score_cols=X) Are the two groups more comparable now? We can produce a new version of the balance table.\ntable1_matched = create_table_one(df_matched, \u0026quot;dark_mode\u0026quot;, X) table1_matched Control Treatment SMD Variable n 104 104 age 41.93 (10.05) 41.85 (10.02) -0.0086 hours 206.92 (309.62) 209.48 (321.79) 0.0081 male 0.62 (0.49) 0.62 (0.49) 0.0 Now the average differences between the two groups have shrunk by at least a couple of orders of magnitude. However, note how the sample size has slightly decreased (300 $\\to$ 246) since (1) we only match treated users and (2) we are not able to find a good match for all of them.\nWe can visually inspect distributional differences with the paired violinplot.\nplot_distributions(df_matched, X, \u0026quot;dark_mode\u0026quot;) A popular way to visualize pre- and post-matching covariate balance is the balance plot that essentially displays the standardized mean differences before and after matching, for each control variable.\ndef plot_balance(t1, t2, X): df_smd = pd.DataFrame({\u0026quot;Variable\u0026quot;: X + X, \u0026quot;Sample\u0026quot;: [\u0026quot;Unadjusted\u0026quot; for _ in range(len(X))] + [\u0026quot;Adjusted\u0026quot; for _ in range(len(X))], \u0026quot;Standardized Mean Difference\u0026quot;: t1[\u0026quot;SMD\u0026quot;][1:].to_list() + t2[\u0026quot;SMD\u0026quot;][1:].to_list()}) sns.scatterplot(x=\u0026quot;Standardized Mean Difference\u0026quot;, y=\u0026quot;Variable\u0026quot;, hue=\u0026quot;Sample\u0026quot;, data=df_smd).\\ set(title=\u0026quot;Balance Plot\u0026quot;) plt.axvline(x=0, color='k', ls='--', zorder=-1, alpha=0.3); plot_balance(table1, table1_matched, X) As we can see, now all differences in observable characteristics between the two groups are essentially zero. We could also compare the distributions using other metrics or test statistics, such as the Kolmogorov-Smirnov test statistic.\nHow do we estimate the average treatment effect? We can simply do a difference in means. An equivalent way that automatically provides standard errors is to run a linear regression of the outcome, read_time, on the treatment, dark_mode.\nNote that, since we have performed the matching for each treated user, the treatment effect we are estimating is the average treatment effect on the treated (ATT), which can be different from the average treatment effect if the treated sample differs from the overall population (which is likely to be the case, since we are doing matching in the first place).\nsmf.ols(\u0026quot;read_time ~ dark_mode\u0026quot;, data=df_matched).fit().summary().tables[1] coef std err t P\u003e|t| [0.025 0.975] Intercept 17.0365 0.469 36.363 0.000 16.113 17.960 dark_mode[T.True] 1.4490 0.663 2.187 0.030 0.143 2.755 The effect is now positive, but not statistically significant.\nNote that we might have matched multiple treated users with the same untreated user, violating the independence assumption across observations and, in turn, distorting inference.\nWe have two solutions:\ncluster standard errors at the matched individual level compute standard errors via bootstrap We implement the first and cluster the standard errors by the original individual identifiers (the dataframe index).\nsmf.ols(\u0026quot;read_time ~ dark_mode\u0026quot;, data=df_matched)\\ .fit(cov_type='cluster', cov_kwds={'groups': df_matched.index})\\ .summary().tables[1] coef std err z P\u003e|z| [0.025 0.975] Intercept 17.0365 0.650 26.217 0.000 15.763 18.310 dark_mode[T.True] 1.4490 0.821 1.765 0.078 -0.160 3.058 The effect is even less statistically significant.\nPropensity Score Rosenbaum and Rubin (1983) proved a very powerful result: if the strong ignorability assumption holds, it is sufficient to condition the analysis on the probability ot treatment, the propensity score, in order to have conditional independence.\n$$ \\big \\lbrace Y_i^{(1)} , Y_i^{(0)} \\big \\rbrace \\ \\perp \\ D_i \\ | \\ X_i \\quad \\leftrightarrow \\quad \\big \\lbrace Y_i^{(1)} , Y_i^{(0)} \\big \\rbrace \\ \\perp \\ D_i \\ | \\ e(X_i) $$\nWhere $e(X_i)$ is the probability of treatment of individual $i$, given the observable characteristics $X_i$.\n$$ e(x) = \\Pr \\left( D_i = 1 \\ \\big | \\ X_i = x \\right) $$\nNote that in an AB test the propensity score is constant across individuals.\nThe result from Rosenbaum and Rubin (1983) is incredibly powerful and practical, since the propensity score is a one dimensional variable, while $X$ might be very high dimensional.\nUnder the unconfoundedness assumption introduced above, we can rewrite the average treatment effect as\n$$ \\tau(x) = \\mathbb E \\left[ Y^{(1)} - Y^{(0)} \\ \\big| \\ X = x \\right] = \\mathbb E \\left[ \\frac{D_i Y_i}{e(X_i)} - \\frac{(1-D_i) Y_i}{1-e(X_i)} \\right] $$\nNote that this formulation of the average treatment effect does not depend on the potential outcomes $Y_i^{(1)}$ and $Y_i^{(0)}$, but only on the observed outcomes $Y_i$.\nThis formulation of the average treatment effect implies the Inverse Propensity Weighted (IPW) estimator which is an unbiased estimator for the average treatment effect $\\tau$.\n$$ \\hat \\tau^{IPW} = \\frac{1}{n} \\sum _ {i=1}^{n} \\left( \\frac{D_i Y_i}{e(X_i)} - \\frac{(1-D_i) Y_i}{1-e(X_i)} \\right) $$\nThis estimator is unfeasible since we do not observe the propensity scores $e(X_i)$. However, we can estimate them. Actually, Imbens, Hirano, Ridder (2003) show that you should use the estimated propensity scores even if you knew the true values (for example because you know the sampling procedure). The idea is that if the estimated propensity scores are different from the true ones, this can be informative in the estimation.\nThere are several possible ways to estimate a probability, the simplest and most common one being logistic regression.\nfrom sklearn.linear_model import LogisticRegressionCV df[\u0026quot;pscore\u0026quot;] = LogisticRegressionCV().fit(y=df[\u0026quot;dark_mode\u0026quot;], X=df[X]).predict_proba(df[X])[:,1] It is best practice, whenever we fit a prediction model, to fit the model on a different sample with respect to the one that we use for inference. This practice is usually called cross-validation or cross-fitting. One of the best (but computationally expensive) cross-validation procedures is leave-one-out (LOO) cross-fitting: when predicting the value of observation $i$ we use all observations except for $i$. We implement the LOO cross-fitting procedure using the cross_val_predict and LeaveOneOut functions from the sklearn package.\nfrom sklearn.model_selection import cross_val_predict, LeaveOneOut df['pscore'] = cross_val_predict(estimator=LogisticRegressionCV(), X=df[X], y=df[\u0026quot;dark_mode\u0026quot;], cv=LeaveOneOut(), method='predict_proba', n_jobs=-1)[:,1] An important check to perform after estimating propensity scores is plotting them, across the treatment and control groups. First of all, we can then observe whether the two groups are balanced or not, depending on how close the two distributions are. Moreover, we can also check how likely it is that the overlap assumption is satisfied. Ideally both distributions should span the same interval.\nsns.histplot(data=df, x='pscore', hue='dark_mode', bins=30, stat='density', common_norm=False).\\ set(ylabel=\u0026quot;\u0026quot;, title=\u0026quot;Distribution of Propensity Scores\u0026quot;); As expected, the distribution of propensity scores between the treatment and control group is significantly different, suggesting that the two groups are hardly comparable. However, there is significant overlap in the support of the distributions, suggesting that the overlap assumption is likely to be satisfied.\nHow do we estimate the average treatment effect?\nOnce we have computed the propensity scores, we just need to re-weight observations by their respective propensity score. We can then either compute a difference between the weighted read_time averages, or run a weighted regression of read_time on dark_mode.\nw = 1 / (df[\u0026quot;pscore\u0026quot;] * df[\u0026quot;dark_mode\u0026quot;] + (1-df[\u0026quot;pscore\u0026quot;]) * (1-df[\u0026quot;dark_mode\u0026quot;])) smf.wls(\u0026quot;read_time ~ dark_mode\u0026quot;, weights=w, data=df).fit().summary().tables[1] coef std err t P\u003e|t| [0.025 0.975] Intercept 18.5859 0.412 45.110 0.000 17.775 19.397 dark_mode[T.True] 1.1303 0.582 1.942 0.053 -0.015 2.276 The effect of the dark_mode is now positive and almost statistically significant, at the 5% level.\nNote that the wls function automatically normalizes weights so that they sum to 1, which greatly improves the stability of the estimator. In fact, the unnormalized IPW estimator can be very unstable when the propensity scores approach zero or one.\nAlso note that the standard errors are not correct, since they do not take into account the extra uncertainty introduced in the estimation of the propensity score. This issue was noted by Abadie and Imbens (2016).\nRegression with Control Variables The last method we are going to review today is linear regression with control variables. This estimator is extremely easy to implement, since we just need to add the user characteristics - gender, age and hours - to the regression of read_time on dark_mode.\nsmf.ols(\u0026quot;read_time ~ dark_mode + male + age + hours\u0026quot;, data=df).fit().summary().tables[1] coef std err t P\u003e|t| [0.025 0.975] Intercept 16.8591 1.082 15.577 0.000 14.729 18.989 dark_mode[T.True] 1.3858 0.524 2.646 0.009 0.355 2.417 male -4.4855 0.499 -8.990 0.000 -5.468 -3.504 age 0.0513 0.022 2.311 0.022 0.008 0.095 hours 0.0043 0.001 8.427 0.000 0.003 0.005 The average treatment effect is again positive and statistically significant at the 1% level!\nComparison How do the different methods compare to each other?\nIPW and Regression There is a tight connection between the IPW estimator and linear regression with covariates. This is particularly evident when we have a one-dimensional, discrete covariate $X$.\nIn this case, the estimand of IPW (i.e. the quantity that IPW estimates) is given by\n$$ \\tau^{IPW} = \\frac{ \\sum_x \\color{red}{\\tau_x} \\color{blue}{\\Pr(D_i | X_i = x)} \\Pr(X_i = x)}{\\sum_x \\color{blue}{\\Pr(D_i | X_i = x)} \\Pr(X_i = x)} $$\nThe IPW estimand is a weighted average of the treatment effects $\\tau_x$, where the weights are given by the treatment probabilities.\nOn the other hand, the estimand of linear regression with control variables is\n$$ \\tau^{OLS} = \\frac{ \\sum_x \\color{red}{\\tau_x} \\color{blue}{\\Pr(D_i | X_i = x)(1 - \\Pr(D_i | X_i = x)) } \\Pr(X_i = x)}{\\sum_x \\color{blue}{\\Pr(D_i | X_i = x)(1 - \\Pr(D_i | X_i = x)) } \\Pr(X_i = x)} $$\nThe OLS estimand is a weighted average of the treatment effects $\\tau_x$, where the weights are given by the variances of the treatment probabilities. This means that linear regression is a weighted estimator, that gives more weight to observations that have characteristics for which we observe more treatment variability. Since a binary random variable has the highest variance when its expected value is 0.5, OLS gives the most weight to observations that have characteristics for which we observe a 50/50 split between treatment and control group. On the other hand, if for some characteristics we only observe treated or untreated individuals, those observations are going to receive zero weight. I recommend Chapter 3 of Angrist and Pischke (2009) for more details.\nIPW and Matching As we have seen in the IPW section, Rosenbaum and Rubin (1983) result tells us that we do not need to perform the analysis conditional on all the covariates $X$, but it is sufficient to condition on the propensity score $e(X)$.\nWe have seed how this result implies a weighted estimator but it also extends to matching: we do not need to match observations on all the covariates $X$, but it is sufficient to match them on the propensity score $e(X)$. This method is called propensity score matching.\npsm = NearestNeighborMatch(replace=False, random_state=1) df_ipwmatched = psm.match(data=df, treatment_col=\u0026quot;dark_mode\u0026quot;, score_cols=['pscore']) As before, after matching, we can simply compute the estimate as a difference in means, remembering that observations are not independent and therefore we need to be cautious when doing inference.\nsmf.ols(\u0026quot;read_time ~ dark_mode\u0026quot;, data=df_ipwmatched)\\ .fit(cov_type='cluster', cov_kwds={'groups': df_ipwmatched.index})\\ .summary().tables[1] coef std err z P\u003e|z| [0.025 0.975] Intercept 18.4633 0.505 36.576 0.000 17.474 19.453 dark_mode[T.True] 1.1888 0.703 1.692 0.091 -0.188 2.566 The estimated effect of dark_mode is positive, significant at the 1% level and very close to the true value of 2!\nConclusion In this blog post, we have seen how to perform conditional analysis using different approached. Matching directly matches most similar units in the treatment and control group. Weighting simply assigns different weight to different observations depending on their probability of receiving the treatment. Regression instead weights observations depending on the conditional treatment variances, giving more weight to observations that have characteristics common to both the treatment and control group.\nThese procedures are extremely helpful because they can either allow us to estimate causal effects from (very rich) observational data or correct experimental estimates when randomization was not perfect or we have a small sample.\nLast but not least, if you want to know more, I strongly recommend this video lecture on propensity scores from Paul Goldsmith-Pinkham that is freely available online.\nThe whole course is a gem and it is an incredible privilege to have such high quality material available online for free!\nReferences [1] P. Rosenbaum, D. Rubin, The central role of the propensity score in observational studies for causal effects (1983), Biometrika.\n[2] G. Imbens, K. Hirano, G. Ridder, Efficient Estimation of Average Treatment Effects Using the Estimated Propensity Score (2003), Econometrica.\n[3] J. Angrist, J. S. Pischke, Mostly harmless econometrics: An Empiricist\u0026rsquo;s Companion (2009), Princeton University Press.\nRelated Articles Understanding The Frisch-Waugh-Lovell Theorem How to Compare Two or More Distributions DAGs and Control Variables Code You can find the original Jupyter Notebook here:\nhttps://github.com/matteocourthoud/Blog-Posts/blob/main/notebooks/ipw.ipynb\n","date":1688947200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1688947200,"objectID":"ae435059a64a781cf23c8547c70087e0","permalink":"https://matteocourthoud.github.io/post/weighting_matching/","publishdate":"2023-07-10T00:00:00Z","relpermalink":"/post/weighting_matching/","section":"post","summary":"Understanding and comparing different methods for conditional causal inference analysis\nAB tests or randomized controlled trials are the gold standard in causal inference. By randomly exposing units to a treatment we make sure that individuals in both groups are comparable, on average, and any difference we observe can be attributed to the treatment effect alone.","tags":null,"title":"Weighting, Matching, or Regression?","type":"post"},{"authors":null,"categories":null,"content":"An introduction to the Bayesian approach to randomized experiments.\nRandomized experiments, a.k.a. AB tests, are now the established gold standard in the industry to estimate causal effects. Randomly assigning the treatment (new product, feature, UI, \u0026hellip;) to a subset of the population (users, patients, customers, \u0026hellip;) we ensure that, on average, the difference in outcomes (revenue, visits, clicks, \u0026hellip;) can be attributed to the treatment. Established companies like Booking.com report constantly running thousands of AB tests at the same time. And newer growing companies like Duolingo attribute a large chunk of their success to their culture of experimentation at scale.\nWith so many experiments, one question comes natural: in one specific experiment, can you leverage information from previous tests? How? In this post, I will try to answer these questions by introducing the Bayesian approach to AB testing. The Bayesian framework is well suited for this type of task because it naturally allows for the updating of existing knowledge (the prior) using new data. However, the method is particularly sensitive to functional form assumptions and apparently innocuous model choices can translate in sensible differences in the estimates, especially when the data is very skewed.\nSearch and Infinite Scrolling For the rest of the article, we are going to use a toy example, loosely inspired by Azavedo et al. (2019): a search engine that wants to increase its ad revenue, without sacrificing search quality. We are a company with an established experimentation culture and we continuously test new ideas on how to rank results, how to select the most relevant ads for consumers, and the user interface (UI) of the results page. Suppose that, in this specific case, we came up with a new brilliant idea: infinite scrolling! Instead of having a discrete sequence of pages, we allow users to keep scrolling down if they want to see more results.\nTo understand whether infinite scrolling works, we ran an AB test: we randomize users into a treatment and a control group. We implement infinite scrolling only for users in the treatment group. I import the data generating process dgp_infinite_scroll() from src.dgp. With respect to previous articles, I generated a new DGP parent class that handles randomization and data generation, while its children classes contain the specific use-cases. I also import some plotting functions and libraries from src.utils. To include not only code but also data and tables, I use Deepnote, a Jupyter-like web-based collaborative notebook environment.\n%matplotlib inline %config InlineBackend.figure_format = 'retina' from src.utils import * from src.dgp import DGP, dgp_infinite_scroll dgp = dgp_infinite_scroll(n=10_000) df = dgp.generate_data(true_effect=0.14) df.head() past_revenue infinite_scroll ad_revenue 0 3.76 0 3.56 1 2.40 1 1.71 2 2.98 0 4.71 3 4.24 0 4.43 4 3.87 0 3.69 We have information on $10.000$ website visitors for which we observe the monthly ad_revenue they generated, whether they were assigned to the treatment group and were using the infinite_scroll, and also the average monthly past_revenue.\nThe random treatment assignment makes the difference-in-means estimator unbiased: we expect the treatment and control group to be comparable on average, so we can causal attribute the average observed difference in outcomes to the treatment effect. We estimate the treatment effect by linear regression. We can interpret the coefficient of infinite_scroll as the estimated treatment effect.\nsmf.ols('ad_revenue ~ infinite_scroll', df).fit().summary().tables[1] coef std err t P\u003e|t| [0.025 0.975] Intercept 1.9906 0.020 100.783 0.000 1.952 2.029 infinite_scroll 0.1441 0.028 5.163 0.000 0.089 0.199 It seems that the infinite_scroll was indeed a good idea and it increase the average monthly revenue by $0.1524$$. Moreover, the effect is significantly different from zero at the 1% confidence level.\nWe could further improve the precision of the estimator by controlling for past_revenue in the regression. We do not expect a sensible change in the estimated coefficient, but the precision should improve (if you want to know more on out control variables, check my other articles on CUPED and DAGs).\nreg = smf.ols('ad_revenue ~ infinite_scroll + past_revenue', df).fit() reg.summary().tables[1] coef std err t P\u003e|t| [0.025 0.975] Intercept 0.0170 0.024 0.696 0.487 -0.031 0.065 infinite_scroll 0.1588 0.020 7.992 0.000 0.120 0.198 past_revenue 0.9923 0.010 98.659 0.000 0.973 1.012 Indeed, past_revenue is highly predictive of current ad_revenue and the precision of the estimated coefficient for infinite_scroll decreases by one-third.\nSo far, everything has been very standard. However, as we said at the beginning, suppose this is not the only experiment we ran trying to improve our browser (and ultimately ad revenue). The infinite scroll is just one idea among thousands of others that we have tested in the past. Is there a way to efficiently use this additional information?\nBayesian Statistics One of the main advantages of Bayesian statistics over the frequentist approach is that it easily allows to incorporate additional information into a model. The idea directly follows from the main results behind all Bayesian statistics: Bayes Theorem. Bayes theorem, allows you to do inference on a model by inverting the inference problem: from the probability of the model given the data, to the probability of the data given the model, a much easier object to deal with.\n$$ \\underbrace{ \\Pr \\big( \\text{model} \\ \\big| \\ \\text{data} \\big) }{\\text{posterior}} = \\underbrace{ \\Pr(\\text{model}) }{\\text{prior}} \\ \\underbrace{ \\frac{ \\Pr \\big( \\text{data} \\ \\big| \\ \\text{model} \\big) }{ \\Pr(\\text{data}) } }_{\\text{likelihood}} $$\nWe can split the right-hand side of Bayes Theorem (or Rule) into two components: the prior and the likelihood. The likelihood is the information about the model that comes from the data, the prior instead is any additional information about the model.\nFirst of all, let\u0026rsquo;s map Bayes theorem into our context. What is the data, what is the model and what is our object of interest?\nthe data which consists in our outcome variable ad_revenue, $y$, the treatment infinite_scroll, $D$ and the other variables, past_revenue and a constant, which we jointly denote as $X$ the model is the distribution of ad_revenue, given past_revenue and the infinite_scroll feature, $y | D, X$ our object of interest is the posterior $\\Pr \\big( \\text{model} \\ \\big| \\ \\text{data} \\big)$, in particular the relationship between ad_revenue and infinite_scroll X = sm.add_constant(df[['past_revenue']].values) D = df['infinite_scroll'].values y = df['ad_revenue'].values How do we use prior information in the context of AB testing, potentially including additional covariates?\nBayesian Regression Let\u0026rsquo;s use a linear model to make it directly comparable with the frequentist approach:\n$$ y_i = \\beta X_i + \\tau D_i + \\varepsilon_i \\qquad \\text{where} \\quad \\varepsilon_i \\sim N \\big( 0, \\sigma^2 \\big) $$\nThis is a parametric model with two sets of parameters: the linear coefficients $\\beta$ and $\\tau$, and the variance of the residuals $\\sigma$. An equivalent, but more Bayesian, way to write the model is:\n$$ y \\ | \\ X, D; \\beta, \\tau, \\sigma \\sim N \\Big( \\beta X + \\tau D \\ , \\sigma^2 \\Big) , $$\nwhere the semi-column separates the data from the model parameters. Differently from the frequentist approach, in Bayesian regressions we do not rely on the central limit theorem to approximate the conditional distribution of $y$, but we directly assume it is normal. Is it just a formality? Not really, but a proper comparison between the frequentist and Bayesian approach is beyond the scope of this article.\nWe are interested in doing inference on the model parameters, $\\beta$, $\\tau$, and $\\sigma$. Another core difference between the frequentist and the Bayesian approach is that the the first assumes that the model parameters are fixed (scalars), while the latter allows them to be stochastic (random variables).\nThis assumption has a very practical implication: you can easily incorporate previous information about the model parameters in the form of prior distributions. As the name says, priors contain information that was available even before looking at the data. This leads to one of the most relevant questions in Bayesian statistics: how do you chose a prior?\nPriors When choosing a prior, one analytically appealing restriction is to have a prior distribution such that the posterior belongs to the same family. These priors are called conjugate priors. For example, before seeing the data, I assume my treatment effect is normally distributed and I would like it to be normally distributed also after incorporating the information contained in the data.\nIn the case of Bayesian linear regression, the conjugate priors for $\\beta$ and $\\sigma$ are normally and inverse-gamma distributed. Let\u0026rsquo;s start a bit blindly, by taking a standard normal and inverse gamma distribution as prior.\n$$ \\beta_i \\sim N(\\boldsymbol 0, \\boldsymbol 1) \\ \\tau_i \\sim N(0,1) \\ \\sigma^2 \\sim \\Gamma^{-1} (1, 1) $$\nWe use the package PyMC to do inference. First we need to specify the model: what are the distributions of the different parameters (priors) and what is the likelihood of the data.\nimport pymc as pm with pm.Model() as baseline_model: # Priors beta = pm.MvNormal('beta', mu=np.ones(np.shape(X)[1]), cov=np.eye(np.shape(X)[1])) tau = pm.Normal('tau', mu=0, sigma=1) sigma = pm.InverseGamma('sigma', mu=1, sigma=1, initval=1) # Likelihood Ylikelihood = pm.Normal('y', mu=(X@beta + D@tau).flatten(), sigma=sigma, observed=y) PyMC has an extremely nice function that allows us to visualize the model as a graph, model_to_graphviz.\npm.model_to_graphviz(baseline_model) From the graphical representation, we can see the various model components, their distributions, and how they interact with each other.\nWe are now ready to compute the model posterior. How does it work? In short, we sample realizations of model parameters, we compute the likelihood of the data given those values and the compute the corresponding posterior.\nidata = pm.sample(model=baseline_model, draws=1000) Auto-assigning NUTS sampler... Initializing NUTS using jitter+adapt_diag... Multiprocess sampling (4 chains in 4 jobs) NUTS: [beta, tau, sigma] 100.00% [8000/8000 00:03\u0026lt;00:00 Sampling 4 chains, 0 divergences] Sampling 4 chains for 1_000 tune and 1_000 draw iterations (4_000 + 4_000 draws total) took 15 seconds. The fact that Bayesian inference requires sampling, has been historically one of the main bottlenecks of Bayesian statistics, since it makes it sensibly slower than the frequentist approach. However, this is less and less of a problem with the increased computational power of model computers.\nWe are now ready to print out results. First, with the summary() method, we can print a model summary very similar to those produced by the statsmodels package.\npm.summary(idata, hdi_prob=0.95).round(4) mean sd hdi_2.5% hdi_97.5% mcse_mean mcse_sd ess_bulk ess_tail r_hat beta[0] 0.017 0.025 -0.029 0.067 0.001 0.0 1765.0 2233.0 1.0 beta[1] 0.992 0.010 0.972 1.012 0.000 0.0 1810.0 1964.0 1.0 tau 0.159 0.020 0.120 0.200 0.000 0.0 2885.0 1792.0 1.0 sigma 0.993 0.007 0.980 1.008 0.000 0.0 3537.0 2692.0 1.0 The estimated parameters are extremely close to the ones we got with the frequentist approach, with an estimated effect of the infinite_scroll equal to $0.157$.\nIf sampling had the disadvantage of being slow, it has the advantage of being very transparent. We can directly plot the distribution of the posterior. Let\u0026rsquo;s do it for the treatment effect $\\tau$. The PyMC function plot_posterior plots the distribution of the posterior, with a black bar for the Bayesian equivalent of a 95% confidence interval.\npm.plot_posterior(idata, kind=\u0026quot;hist\u0026quot;, var_names=('tau'), hdi_prob=0.95, figsize=(6, 3), bins=30); As expected, since we chose conjugate priors, the posterior distribution looks gaussian.\nSo far we have chosen the prior without much guidance. However, suppose we had access to past experiments. How do we incorporate this specific information?\nPast Experiments Suppose the idea of the infinite scroll, was just one among a ton of other ones that we tried and tested in the past. For each idea we have the data for the corresponding experiment, with the corresponding estimated coefficient. Suppose we had a thousand of them.\npast_experiments = [dgp.generate_data(seed_data=i) for i in range(1000)] taus = [smf.ols('ad_revenue ~ infinite_scroll + past_revenue', pe).fit().params.values for pe in past_experiments] How do we use this additional information?\nNormal Prior The first idea could be to calibrate our prior to reflect the data distribution in the past. Keeping the normality assumption, we use the estimated average and standard deviations of the estimates from past experiments.\ntaus_mean = np.mean(taus, axis=0)[1] taus_mean 0.0047987091716528915 On average, had practically no effect on ad_revenue, with a average effect of $0.0009$.\ntaus_std = np.sqrt(np.cov(taus, rowvar=0)[1,1]) taus_std 0.15153398725701195 However, there was sensible variation across experiments, with a standard deviation of $0.029$.\nLet\u0026rsquo;s now estimate the\nwith pm.Model() as model_normal_prior: k = np.shape(X)[1] beta = pm.MvNormal('beta', mu=np.ones(k), cov=np.eye(k)) tau = pm.Normal('tau', mu=taus_mean, sigma=taus_std) sigma = pm.InverseGamma('sigma', mu=1, sigma=1, initval=1) Ylikelihood = pm.Normal('y', mu=(X@beta + D@tau).flatten(), sigma=sigma, observed=y) Let\u0026rsquo;s sample from the model.\nidata_normal_prior = pm.sample(model=model_normal_prior, draws=1000) Auto-assigning NUTS sampler... Initializing NUTS using jitter+adapt_diag... Multiprocess sampling (4 chains in 4 jobs) NUTS: [beta, tau, sigma] 100.00% [8000/8000 00:03\u0026lt;00:00 Sampling 4 chains, 0 divergences] Sampling 4 chains for 1_000 tune and 1_000 draw iterations (4_000 + 4_000 draws total) took 14 seconds. The acceptance probability does not match the target. It is 0.9025, but should be close to 0.8. Try to increase the number of tuning steps. And plot the sample posterior distribution of the treatment effect parameter $\\tau$.\npm.plot_posterior(idata_normal_prior, kind=\u0026quot;hist\u0026quot;, var_names=('tau'), hdi_prob=0.95, figsize=(6, 3), bins=30); The estimated coefficient is sensibly smaller: $0.08$ instead of the previous estimate of $0.12$. Why is it the case?\nThe fact is that the previous coefficient of $0.12$ is extremely unlikey, given our prior. We can compute the probability of getting the same or a more extreme value, given the prior.\n1 - sp.stats.norm(taus_mean, taus_std).cdf(0.12) 0.22355735943737898 The probability of such value is almost zero. Therefore, the estimated coefficient has moved towards the prior mean of $0.0009$.\nStudent t Prior So far, we have assumed a normal distribution for all linear coefficients. Is it appropriate? Let\u0026rsquo;s check it visually (check here for other methods on how to compare distributions).\nsns.histplot([tau[0] for tau in taus]).set(title=r'Distribution of $\\hat{\\beta}_0$ in past experiments'); The distribution seems pretty normal. What the treatment effect paramenter $\\tau$?\nfig, ax = plt.subplots() sns.histplot([tau[1] for tau in taus], label='past experiments'); ax.axvline(reg.params['infinite_scroll'], lw=2, c='C3', ls='--', label='current experiment') plt.legend(); plt.title(r'Distribution of $\\hat{\\tau}$ in past experiments'); The distribution very heavy tailed! While at the center it looks like a normal distributions, the tails are much \u0026ldquo;fatter\u0026rdquo; and we have a couple of very extreme values. excluding the case of measurement error, this is a setting that happens often in the industry, where most ideas have extremely small or null effects and very rarely an idea is actually a breakthrough.\nOne way to model this distribution is a student-t distribution. In particular, we use a t-student with mean $0.0009$, variance $0.003$ and $1.3$ degrees of freedom to match the moments of the empirical distributions of past estimates.\nwith pm.Model() as model_studentt_prior: # Priors k = np.shape(X)[1] beta = pm.MvNormal('beta', mu=np.ones(k), cov=np.eye(k)) tau = pm.StudentT('tau', mu=taus_mean, sigma=0.003, nu=1.3) sigma = pm.InverseGamma('sigma', mu=1, sigma=1, initval=1) # Likelihood Ylikelihood = pm.Normal('y', mu=(X@beta + D@tau).flatten(), sigma=sigma, observed=y) Let\u0026rsquo;s sample from the model.\nidata_studentt_priors = pm.sample(model=model_studentt_prior, draws=1000) Auto-assigning NUTS sampler... Initializing NUTS using jitter+adapt_diag... Multiprocess sampling (4 chains in 4 jobs) NUTS: [beta, tau, sigma] 100.00% [8000/8000 00:03\u0026lt;00:00 Sampling 4 chains, 0 divergences] Sampling 4 chains for 1_000 tune and 1_000 draw iterations (4_000 + 4_000 draws total) took 15 seconds. And plot the sample posterior distribution of the treatment effect parameter $\\tau$.\npm.plot_posterior(idata_studentt_priors, kind=\u0026quot;hist\u0026quot;, var_names=('tau'), hdi_prob=0.95, figsize=(6, 3), bins=30); The estimated coefficient is now again similar to the one we got with the standard normal prior, $0.11$. However, the estimate is more precise since the confidence interval has shrunk from $[0.077, 0.016]$ to $[0.065, 0.015]$.\nWhat has happened?\nShrinking The answer lies in the shape of the different prior distributions that we have used:\nstandard normal, $N(0,1)$ normal with matched moments, $N(0, 0.03)$ t-student with matched moments, $t_{1.3}$(0, 0.003)$ t_hats = np.linspace(-0.3, 0.3, 1_000) distributions = { 'N(0,1)': sp.stats.norm(0, 1).pdf(t_hats), 'N(0, 0.03)': sp.stats.norm(0, 0.03).pdf(t_hats), '$t_{1.3}$(0, 0.003)': sp.stats.t(df=1.3).pdf(t_hats / 0.003)*300, } Let\u0026rsquo;s plot all of them together.\nfor i, (label, y) in enumerate(distributions.items()): sns.lineplot(x=t_hats, y=y, color=f'C{i}', label=label); plt.legend(); plt.title('Prior Distributions'); As we can see, all distributions are centered on zero, but they have very different shapes. The standard normal distribution is essentially flat over the $[-0.15, 0.15]$ interval. Every value has basically the same probability. The last two instead, even though they have the same mean and variance, have very different shapes.\nHow does it translate into our estimation? We can plot the implied posterior for different estimates, for each prior distribution.\ndef compute_posterior(b, prior): likelihood = sp.stats.norm(b, taus_std).pdf(t_hats) return np.average(t_hats, weights=prior*likelihood) fig, ax = plt.subplots(figsize=(7,6)) ax.axvline(0, lw=1.5, c='k'); ax.axhline(0, lw=1.5, c='k'); ax.axvline(reg.params['infinite_scroll'], lw=2, ls='--', c='darkgray'); for i, (label, y) in enumerate(distributions.items()): sns.lineplot(x=t_hats, y=[compute_posterior(t, y) for t in t_hats] , color=f'C{i}', label=label); ax.set_xlim(-0.17, 0.17); ax.set_ylim(-0.17, 0.17); plt.legend(); ax.set_xlabel('Experiment Estimate'); ax.set_ylabel('Posterior'); As we can see, the different priors transform the experimental estimates in very different ways. The standard normal prior essentially has no effect for estimates in the $[-0.15, 0.15]$ interval. The normal prior with matched moments instead shrinks each estimate by approximately 2/3. The effect of the t-student prior is instead non-linear: it shrinks small estimates towards zero, while it keeps large estimates as they are.\nMy intuition is the following. A prior distribution very skewed or with \u0026ldquo;fat tails\u0026rdquo; means that large values are rare but not impossible. In practice, it means accepting that breakthrough improvements are possible. On the other hand, for the same variance, the distribution is more concentrated around zero than a standard normal so that small values are shrunk even more.\nConclusion In this article we have seen how to extend the analysis of AB test to incorporate information from past experiments. In particular, we have seen the importance of choosing a prior. Selecting the distribution function is just as important as tuning its parameters. The shape of the prior distribution can drastically affect our inference, especially in a world with skewed distributions.\nDespite the length of the article, this was just a glimpse in the world of AB testing and Bayesian statistics. While being computationally more intensive and requiring additional assumptions, the Bayesian approach is often more natural, powerful and flexible than the frequentist one. Knowing pros and cons of both approaches is crucial to get the best of both worlds, picking the approach that work best or combining them efficiently.\nReferences E. Azevedo, A. Deng, J. Olea, G. Weyl, Empirical Bayes Estimation of Treatment Effects with Many A/B Tests: An Overview (2019). AEA Papers and Proceedings.\nA. Deng, Objective Bayesian Two Sample Hypothesis Testing for Online Controlled Experiments (2018), WWW15.\nRelated Articles The Bayesian Bootstrap\nUnderstanding CUPED\nDAGs and Control Variables\nCode You can find the original Jupyter Notebook here:\nhttps://github.com/matteocourthoud/Blog-Posts/blob/main/notebooks/bayes_ab.ipynb\n","date":1675209600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1675209600,"objectID":"090c9550ee6367bbd3cdbf7ace391b05","permalink":"https://matteocourthoud.github.io/post/bayes_ab/","publishdate":"2023-02-01T00:00:00Z","relpermalink":"/post/bayes_ab/","section":"post","summary":"An introduction to the Bayesian approach to randomized experiments.\nRandomized experiments, a.k.a. AB tests, are now the established gold standard in the industry to estimate causal effects. Randomly assigning the treatment (new product, feature, UI, \u0026hellip;) to a subset of the population (users, patients, customers, \u0026hellip;) we ensure that, on average, the difference in outcomes (revenue, visits, clicks, \u0026hellip;) can be attributed to the treatment.","tags":null,"title":"Bayesian AB Testing","type":"post"},{"authors":null,"categories":null,"content":"An introduction to the Bayesian approach to randomized experiments.\nAB testing and experimentation is now the established gold standard in the industry to estimate causal effects. Randomly assigning the treatment (new product, feature, UI, \u0026hellip;) to a subset of the population (users, patients, customers, \u0026hellip;) we ensure that, on average, the difference in outcomes (revenue, visits, clicks, \u0026hellip;) can be attributed to the treatment. Many companies, especially in tech, before implementing any major change test them to back up their decisions with numbers. Established companies like Booking.com report constantly running thousands of AB tests at the same time. And newer growing companies like Duolingo attribute a large chunk of their success to their culture of experimentation at scale.\nWith so many experiments, one question comes natural: in one specific experiment, can you leverage information from previous tests? In this post, I will try to answer this question by introducing the Bayesian approach to AB testing. The Bayesian framework is particularly well suited for this type of task because it naturally allows for the updating of existing knowledge (the prior) using new data. However, the method is particularly sensitive to functional form assumptions and apparently innocuous model choices can translate in sensible differences in the estimates, especially when the data is very skewed.\nSearch and Infinite Scrolling For the rest of the article, we are going to use a toy example, loosely inspired by Azavedo et al. (2019): a search engine that wants to increase its ad revenue, without sacrificing search quality. We are a company with an established experimentation culture and we continuously test new ideas on how to rank results, how to select the most relevant ads for consumers, and the user interface (UI) of the results page. Suppose that, in this specific case, we came up with a new brilliant idea: infinite scrolling! Instead of having a discrete sequence of pages, we allow users to keep scrolling down if they want to see more results.\nTo understand whether infinite scrolling works, we ran an AB test: we randomize users into a treatment and a control group. We implement infinite scrolling only for users in the treatment group. I import the data generating process dgp_infinite_scroll() from src.dgp. With respect to previous articles, I generated a new DGP parent class that handles randomization and data generation, while its children classes contain the specific use-cases. I also import some plotting functions and libraries from src.utils. To include not only code but also data and tables, I use Deepnote, a Jupyter-like web-based collaborative notebook environment.\n%matplotlib inline %config InlineBackend.figure_format = 'retina' from src.utils import * from src.dgp import DGP, dgp_infinite_scroll dgp = dgp_infinite_scroll(n=10_000) df = dgp.generate_data(true_effect=0.14) df.head() past_revenue infinite_scroll ad_revenue 0 3.76 1 3.70 1 2.40 1 1.71 2 2.98 1 4.85 3 4.24 1 4.57 4 3.87 0 3.69 We have information on $10.000$ website visitors for which we observe the monthly ad_revenue they generated, whether they were assigned to the treatment group and were using the infinite_scroll, and also the average monthly past_revenue.\nThe random treatment assignment makes the difference-in-means estimator unbiased: we expect the treatment and control group to be comparable on average, so we can causal attribute the average observed difference in outcomes to the treatment effect. We estimate the treatment effect by linear regression. We can interpret the coefficient of infinite_scroll as the estimated treatment effect.\nsmf.ols('ad_revenue ~ infinite_scroll', df).fit().summary().tables[1] coef std err t P\u003e|t| [0.025 0.975] Intercept 1.9865 0.020 101.320 0.000 1.948 2.025 infinite_scroll 0.1524 0.028 5.461 0.000 0.098 0.207 It seems that the infinite_scroll was indeed a good idea and it increase the average monthly revenue by $0.1524$$. Moreover, the effect is significantly different from zero at the 1% confidence level.\nWe could further improve the precision of the estimator by controlling for past_revenue in the regression. We do not expect a sensible change in the estimated coefficient, but the precision should improve (if you want to know more on out control variables, check my other articles on CUPED and DAGs).\nreg = smf.ols('ad_revenue ~ infinite_scroll + past_revenue', df).fit() reg.summary().tables[1] coef std err t P\u003e|t| [0.025 0.975] Intercept 0.0181 0.024 0.741 0.459 -0.030 0.066 infinite_scroll 0.1571 0.020 7.910 0.000 0.118 0.196 past_revenue 0.9922 0.010 98.655 0.000 0.972 1.012 Indeed, past_revenue is highly predictive of current ad_revenue and the precision of the estimated coefficient for infinite_scroll decreases by one-third.\nSo far, everything has been very standard. However, as we said at the beginning, suppose this is not the only experiment we ran trying to improve our browser (and ultimately ad revenue). The infinite scroll is just one idea among thousands of others that we have tested in the past. Is there a way to efficiently use this additional information?\nBayesian Statistics One of the main advantages of Bayesian statistics over the frequentist approach is that it easily allows to incorporate additional information into a model. The idea directly follows from the main results behind all Bayesian statistics: Bayes Theorem. Bayes theorem, allows you to do inference on a model by inverting the inference problem: from the probability of the model given the data, to the probability of the data given the model, a much easier object to deal with.\n$$ \\underbrace{ \\Pr \\big( \\text{model} \\ \\big| \\ \\text{data} \\big) }{\\text{posterior}} = \\underbrace{ \\Pr(\\text{model}) }{\\text{prior}} \\ \\underbrace{ \\frac{ \\Pr \\big( \\text{data} \\ \\big| \\ \\text{model} \\big) }{ \\Pr(\\text{data}) } }_{\\text{likelihood}} $$\nWe can split the right-hand side of Bayes Theorem (or Rule) into two components: the prior and the likelihood. The likelihood is the information about the model that comes from the data, the prior instead is any additional information about the model.\nFirst of all, let\u0026rsquo;s map Bayes theorem into our context. What is the data, what is the model and what is our object of interest?\nthe data which consists in our outcome variable ad_revenue, $y$, the treatment infinite_scroll, $D$ and the other variables, past_revenue and a constant, which we jointly denote as $X$ the model is the distribution of ad_revenue, given past_revenue and the infinite_scroll feature, $y | D, X$ our object of interest is the posterior $\\Pr \\big( \\text{model} \\ \\big| \\ \\text{data} \\big)$, in particular the relationship between ad_revenue and infinite_scroll X = sm.add_constant(df[['past_revenue']].values) D = df['infinite_scroll'].values y = df['ad_revenue'].values How do we use prior information in the context of AB testing, potentially including additional covariates?\nBayesian Regression Let\u0026rsquo;s use a linear model to make it directly comparable with the frequentist approach:\n$$ y_i = \\beta X_i + \\tau D_i + \\varepsilon_i \\qquad \\text{where} \\quad \\varepsilon_i \\sim N \\big( 0, \\sigma^2 \\big) $$\nThis is a parametric model with two sets of parameters: the linear coefficients $\\beta$ and $\\tau$, and the variance of the residuals $\\sigma$. An equivalent, but more Bayesian, way to write the model is:\n$$ y \\ | \\ X, D; \\beta, \\tau, \\sigma \\sim N \\Big( \\beta X + \\tau D \\ , \\sigma^2 \\Big) , $$\nwhere the semi-column separates the data from the model parameters. Differently from the frequentist approach, in Bayesian regressions we do not rely on the central limit theorem to approximate the conditional distribution of $y$, but we directly assume it is normal. Is it just a formality? Not really, but a proper comparison between the frequentist and Bayesian approach is beyond the scope of this article.\nWe are interested in doing inference on the model parameters, $\\beta$, $\\tau$, and $\\sigma$. Another core difference between the frequentist and the Bayesian approach is that the the first assumes that the model parameters are fixed (scalars), while the latter allows them to be stochastic (random variables).\nThis assumption has a very practical implication: you can easily incorporate previous information about the model parameters in the form of prior distributions. As the name says, priors contain information that was available even before looking at the data. This leads to one of the most relevant questions in Bayesian statistics: how do you chose a prior?\nPriors When choosing a prior, one analytically appealing restriction is to have a prior distribution such that the posterior belongs to the same family. These priors are called conjugate priors. For example, before seeing the data, I assume my treatment effect is normally distributed and I would like it to be normally distributed also after incorporating the information contained in the data.\nIn the case of Bayesian linear regression, the conjugate priors for $\\beta$ and $\\sigma$ are normally and inverse-gamma distributed. Let\u0026rsquo;s start a bit blindly, by taking a standard normal and inverse gamma distribution as prior.\n$$ \\beta_i \\sim N(\\boldsymbol 0, \\boldsymbol 1) \\ \\tau_i \\sim N(0,1) \\ \\sigma^2 \\sim \\Gamma^{-1} (1, 1) $$\nWe use the package PyMC to do inference. First we need to specify the model: what are the distributions of the different parameters (priors) and what is the likelihood of the data.\nimport pymc as pm with pm.Model() as baseline_model: # Priors beta = pm.MvNormal('beta', mu=np.ones(np.shape(X)[1]), cov=np.eye(np.shape(X)[1])) tau = pm.Normal('tau', mu=0, sigma=1) sigma = pm.InverseGamma('sigma', mu=1, sigma=1, initval=1) # Likelihood Ylikelihood = pm.Normal('y', mu=(X@beta + D@tau).flatten(), sigma=sigma, observed=y) PyMC has an extremely nice function that allows us to visualize the model as a graph, model_to_graphviz.\npm.model_to_graphviz(baseline_model) From the graphical representation, we can see the various model components, their distributions, and how they interact with each other.\nWe are now ready to compute the model posterior. How does it work? In short, we sample realizations of model parameters, we compute the likelihood of the data given those values and the compute the corresponding posterior.\nidata = pm.sample(model=baseline_model, draws=1000) Auto-assigning NUTS sampler... Initializing NUTS using jitter+adapt_diag... Multiprocess sampling (4 chains in 4 jobs) NUTS: [beta, tau, sigma] 100.00% [8000/8000 00:03\u0026lt;00:00 Sampling 4 chains, 0 divergences] Sampling 4 chains for 1_000 tune and 1_000 draw iterations (4_000 + 4_000 draws total) took 15 seconds. The fact that Bayesian inference requires sampling, has been historically one of the main bottlenecks of Bayesian statistics, since it makes it sensibly slower than the frequentist approach. However, this is less and less of a problem with the increased computational power of model computers.\nWe are now ready to print out results. First, with the summary() method, we can print a model summary very similar to those produced by the statsmodels package.\npm.summary(idata, hdi_prob=0.95).round(4) mean sd hdi_2.5% hdi_97.5% mcse_mean mcse_sd ess_bulk ess_tail r_hat beta[0] 0.019 0.025 -0.031 0.068 0.001 0.0 1943.0 1866.0 1.0 beta[1] 0.992 0.010 0.970 1.011 0.000 0.0 2239.0 1721.0 1.0 tau 0.157 0.021 0.117 0.197 0.000 0.0 2770.0 2248.0 1.0 sigma 0.993 0.007 0.980 1.007 0.000 0.0 3473.0 2525.0 1.0 The estimated parameters are extremely close to the ones we got with the frequentist approach, with an estimated effect of the infinite_scroll equal to $0.157$.\nIf sampling had the disadvantage of being slow, it has the advantage of being very transparent. We can directly plot the distribution of the posterior. Let\u0026rsquo;s do it for the treatment effect $\\tau$. The PyMC function plot_posterior plots the distribution of the posterior, with a black bar for the Bayesian equivalent of a 95% confidence interval.\npm.plot_posterior(idata, kind=\u0026quot;hist\u0026quot;, var_names=('tau'), hdi_prob=0.95, figsize=(6, 3), bins=30); As expected, since we chose conjugate priors, the posterior distribution looks gaussian.\nSo far we have chosen the prior without much guidance. However, suppose we had access to past experiments. How do we incorporate this specific information?\nPast Experiments Suppose the idea of the infinite scroll, was just one among a ton of other ones that we tried and tested in the past. For each idea we have the data for the corresponding experiment, with the corresponding estimated coefficient. Suppose we had a thousand of them.\npast_experiments = [dgp.generate_data(seed_data=i) for i in range(1000)] taus = [smf.ols('ad_revenue ~ infinite_scroll + past_revenue', pe).fit().params.values for pe in past_experiments] How do we use this additional information?\nNormal Prior The first idea could be to calibrate our prior to reflect the data distribution in the past. Keeping the normality assumption, we use the estimated average and standard deviations of the estimates from past experiments.\ntaus_mean = np.mean(taus, axis=0)[1] taus_mean 0.0009094486420266667 On average, had practically no effect on ad_revenue, with a average effect of $0.0009$.\ntaus_std = np.sqrt(np.cov(taus, rowvar=0)[1,1]) taus_std 0.029014447772168384 However, there was sensible variation across experiments, with a standard deviation of $0.029$.\nLet\u0026rsquo;s now estimate the\nwith pm.Model() as model_normal_prior: k = np.shape(X)[1] beta = pm.MvNormal('beta', mu=np.ones(k), cov=np.eye(k)) tau = pm.Normal('tau', mu=taus_mean, sigma=taus_std) sigma = pm.InverseGamma('sigma', mu=1, sigma=1, initval=1) Ylikelihood = pm.Normal('y', mu=(X@beta + D@tau).flatten(), sigma=sigma, observed=y) Let\u0026rsquo;s sample from the model.\nidata_normal_prior = pm.sample(model=model_normal_prior, draws=1000) Auto-assigning NUTS sampler... Initializing NUTS using jitter+adapt_diag... Multiprocess sampling (4 chains in 4 jobs) NUTS: [beta, tau, sigma] 100.00% [8000/8000 00:04\u0026lt;00:00 Sampling 4 chains, 0 divergences] Sampling 4 chains for 1_000 tune and 1_000 draw iterations (4_000 + 4_000 draws total) took 14 seconds. And plot the sample posterior distribution of the treatment effect parameter $\\tau$.\npm.plot_posterior(idata_normal_prior, kind=\u0026quot;hist\u0026quot;, var_names=('tau'), hdi_prob=0.95, figsize=(6, 3), bins=30); The estimated coefficient is sensibly smaller: $0.08$ instead of the previous estimate of $0.12$. Why is it the case?\nThe fact is that the previous coefficient of $0.12$ is extremely unlikey, given our prior. We can compute the probability of getting the same or a more extreme value, given the prior.\n1 - sp.stats.norm(taus_mean, taus_std).cdf(0.12) 2.025724712373389e-05 The probability of such value is almost zero. Therefore, the estimated coefficient has moved towards the prior mean of $0.0009$.\nStudent t Prior So far, we have assumed a normal distribution for all linear coefficients. Is it appropriate? Let\u0026rsquo;s check it visually (check here for other methods on how to compare distributions).\nsns.histplot([tau[0] for tau in taus]).set(title=r'Distribution of $\\hat{\\beta}_0$ in past experiments'); The distribution seems pretty normal. What the treatment effect paramenter $\\tau$?\nfig, ax = plt.subplots() sns.histplot([tau[1] for tau in taus], label='past experiments'); ax.axvline(reg.params['infinite_scroll'], lw=2, c='C3', ls='--', label='current experiment') plt.legend(); plt.title(r'Distribution of $\\hat{\\tau}$ in past experiments'); The distribution very heavy tailed! While at the center it looks like a normal distributions, the tails are much \u0026ldquo;fatter\u0026rdquo; and we have a couple of very extreme values. excluding the case of measurement error, this is a setting that happens often in the industry, where most ideas have extremely small or null effects and very rarely an idea is actually a breakthrough.\nOne way to model this distribution is a student-t distribution. In particular, we use a t-student with mean $0.0009$, variance $0.003$ and $1.3$ degrees of freedom to match the moments of the empirical distributions of past estimates.\nwith pm.Model() as model_studentt_prior: # Priors k = np.shape(X)[1] beta = pm.MvNormal('beta', mu=np.ones(k), cov=np.eye(k)) tau = pm.StudentT('tau', mu=taus_mean, sigma=0.003, nu=1.3) sigma = pm.InverseGamma('sigma', mu=1, sigma=1, initval=1) # Likelihood Ylikelihood = pm.Normal('y', mu=(X@beta + D@tau).flatten(), sigma=sigma, observed=y) Let\u0026rsquo;s sample from the model.\nidata_studentt_priors = pm.sample(model=model_studentt_prior, draws=1000) Auto-assigning NUTS sampler... Initializing NUTS using jitter+adapt_diag... Multiprocess sampling (4 chains in 4 jobs) NUTS: [beta, tau, sigma] 100.00% [8000/8000 00:03\u0026lt;00:00 Sampling 4 chains, 0 divergences] Sampling 4 chains for 1_000 tune and 1_000 draw iterations (4_000 + 4_000 draws total) took 15 seconds. And plot the sample posterior distribution of the treatment effect parameter $\\tau$.\npm.plot_posterior(idata_studentt_priors, kind=\u0026quot;hist\u0026quot;, var_names=('tau'), hdi_prob=0.95, figsize=(6, 3), bins=30); The estimated coefficient is now again similar to the one we got with the standard normal prior, $0.11$. However, the estimate is more precise since the confidence interval has shrunk from $[0.077, 0.016]$ to $[0.065, 0.015]$.\nWhat has happened?\nShrinking The answer lies in the shape of the different prior distributions that we have used:\nstandard normal, $N(0,1)$ normal with matched moments, $N(0, 0.03)$ t-student with matched moments, $t_{1.3}$(0, 0.003)$ t_hats = np.linspace(-0.3, 0.3, 1_000) distributions = { 'N(0,1)': sp.stats.norm(0, 1).pdf(t_hats), 'N(0, 0.03)': sp.stats.norm(0, 0.03).pdf(t_hats), '$t_{1.3}$(0, 0.003)': sp.stats.t(df=1.3).pdf(t_hats / 0.003)*300, } Let\u0026rsquo;s plot all of them together.\nfor i, (label, y) in enumerate(distributions.items()): sns.lineplot(x=t_hats, y=y, color=f'C{i}', label=label); plt.xlim(-0.15, 0.15); plt.legend(); plt.title('Prior Distributions'); As we can see, all distributions are centered on zero, but they have very different shapes. The standard normal distribution is essentially flat over the $[-0.15, 0.15]$ interval. Every value has basically the same probability. The last two instead, even though they have the same mean and variance, have very different shapes.\nHow does it translate into our estimation? We can plot the implied posterior for different estimates, for each prior distribution.\ndef compute_posterior(b, prior): likelihood = sp.stats.norm(b, taus_std).pdf(t_hats) return np.average(t_hats, weights=prior*likelihood) fig, ax = plt.subplots(figsize=(7,6)) ax.axvline(0, lw=1.5, c='k'); ax.axhline(0, lw=1.5, c='k'); ax.axvline(0.12, lw=2, ls='--', c='darkgray'); for i, (label, y) in enumerate(distributions.items()): sns.lineplot(x=t_hats, y=[compute_posterior(t, y) for t in t_hats] , color=f'C{i}', label=label); ax.set_xlim(-0.16, 0.16); ax.set_ylim(-0.16, 0.16); plt.legend(); ax.set_xlabel('Experiment Estimate'); ax.set_ylabel('Posterior'); As we can see, the different priors transform the experimental estimates in very different ways. The standard normal prior essentially has no effect for estimates in the $[-0.15, 0.15]$ interval. The normal prior with matched moments instead shrinks each estimate by approximately 2/3. The effect of the t-student prior is instead non-linear: it shrinks small estimates towards zero, while it keeps large estimates as they are.\nMy intuition is the following. A prior distribution very skewed or with \u0026ldquo;fat tails\u0026rdquo; means that large values are rare but not impossible. In practice, it means accepting that breakthrough improvements are possible. On the other hand, for the same variance, the distribution is more concentrated around zero than a standard normal so that small values are shrunk even more.\nConclusion In this article we have seen how to extend the analysis of AB test to incorporate information from past experiments. In particular, we have seen the importance of choosing a prior. Selecting the distribution function is just as important as tuning its parameters. The shape of the prior distribution can drastically affect our inference, especially in a world with skewed distributions.\nDespite the length of the article, this was just a glimpse in the world of AB testing and Bayesian statistics. While being computationally more intensive and requiring additional assumptions, the Bayesian approach is often more natural, powerful and flexible than the frequentist one. Knowing pros and cons of both approaches is crucial to get the best of both worlds, picking the approach that work best or combining them efficiently.\nReferences E. Azevedo, A. Deng, J. Olea, G. Weyl, Empirical Bayes Estimation of Treatment Effects with Many A/B Tests: An Overview (2019). AEA Papers and Proceedings.\nA. Deng, Objective Bayesian Two Sample Hypothesis Testing for Online Controlled Experiments (2018), WWW15.\nRelated Articles The Bayesian Bootstrap\nUnderstanding CUPED\nDAGs and Control Variables\nCode You can find the original Jupyter Notebook here:\nhttps://github.com/matteocourthoud/Blog-Posts/blob/main/notebooks/delta.ipynb\n","date":1673222400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1673222400,"objectID":"dfeaad011d9a9eaebd1b71292285840d","permalink":"https://matteocourthoud.github.io/post/bayes_reg/","publishdate":"2023-01-09T00:00:00Z","relpermalink":"/post/bayes_reg/","section":"post","summary":"An introduction to the Bayesian approach to randomized experiments.\nAB testing and experimentation is now the established gold standard in the industry to estimate causal effects. Randomly assigning the treatment (new product, feature, UI, \u0026hellip;) to a subset of the population (users, patients, customers, \u0026hellip;) we ensure that, on average, the difference in outcomes (revenue, visits, clicks, \u0026hellip;) can be attributed to the treatment.","tags":null,"title":"Bayesian AB Testing","type":"post"},{"authors":null,"categories":null,"content":"An introduction to the delta method for inference on ratio metrics.\nWhen we run an experiment, we are often not only interested in the effect of a treatment (new product, new feature, new interface, \u0026hellip;) on revenue, but in it\u0026rsquo;s cost-effectiveness. In other words, is the investment worth the cost? Common examples include investments in computing resources, returns on advertisement, but also click-through rates and other ratio metrics.\nWhen we investigate causal effects, the gold standard is randomized control trials, a.k.a. AB tests. Randomly assigning the treatment to a subset of the population (users, patients, customers, \u0026hellip;) we ensure that, on average, the difference in outcomes can be attributed to the treatment. However, when the object of interest is cost-effectiveness, AB tests present some additional problems since we are not just interested in one treatment effect, but in the ratio of two treatment effects, the outcome of the investment over its cost.\nIn this post we are going to see how to analyze randomized experiments when the object of interest is the return on investment (ROI). We are going to explore alternative metrics to measure whether an investment paid off. We will also introduce a very powerful tool for inference with complex metrics: the delta method. While the algebra can be intense, the result is simple: we can compute the confidence interval for our ratio estimator using a simple linear regression.\nInvesting in Cloud Computing To better illustrate the concepts, we are going to use a toy example throughout the article: suppose we were an online marketplace and we wanted to invest in cloud computing: we want to increase the computing power behind our internal search engine, by switching to a higher tier server. The idea is that the faster search will improve the user experience, potentially leading to higher sales. Therefore, the question is: is the investment worth the cost? The object of interest is the return on investment (ROI).\nDifferently from usual AB tests or randomized experiments, we are not interested in a single causal effect, but in the ratio of two metrics: the effect on revenue and the effect on cost. We will still use a randomized control trial or AB test to estimate the ROI: we randomly assign groups of users to either the treatment or the control group. The treated users will benefit from the faster cloud machines, while the control users will use the old slower machines. Randomization ensures that we can estimate the impact of the new machines on either cost or revenue by comparing users in the treatment and control group: the difference in their average is an unbiased estimator of the average treatment effect. However, things are more complicated for their ratio.\nI import the data generating process dgp_cloud() from src.dgp. With respect to previous articles, I generated a new DGP parent class that handles randomization and data generation, while its children classes contain the specific use-cases. I also import some plotting functions and libraries from src.utils. To include not only code but also data and tables, I use Deepnote, a Jupyter-like web-based collaborative notebook environment.\n%matplotlib inline %config InlineBackend.figure_format = 'retina' from src.utils import * from src.dgp import dgp_cloud, DGP dgp = dgp_cloud(n=10_000) df = dgp.generate_data(seed_assignment=6) df.head() new_machine cost revenue 0 1 3.14 20.90 1 0 3.77 33.57 2 1 3.16 24.31 3 0 2.36 20.35 4 0 1.65 12.60 The data contains information on the total cost and revenue for a set of $10.000$ users over a period of a month. We also have information on the treatment: whether the search engine was running on the old or new machines. As it often happens with business metrics, both distributions of cost and revenues are very skewed. Moreover, most people do not buy anything and therefore generate zero revenue, even though they still use the platform, generating positive costs.\nfix, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5)) sns.histplot(df.cost, ax=ax1, color='C0').set(title='Distribution of Cost') sns.histplot(df.revenue, ax=ax2, color='C1').set(title='Distribution of Revenue'); We can compute the difference-in-means estimate for cost and revenue by regressing the outcome on the treatment indicator.\nsmf.ols('cost ~ new_machine', data=df).fit().summary().tables[1] coef std err t P\u003e|t| [0.025 0.975] Intercept 2.9617 0.043 69.034 0.000 2.878 3.046 new_machine 0.5152 0.060 8.563 0.000 0.397 0.633 The average cost has increased by $0.5152$$ per user. What about revenue?\nsmf.ols('revenue ~ new_machine', data=df).fit().summary().tables[1] coef std err t P\u003e|t| [0.025 0.975] Intercept 25.9172 0.425 60.950 0.000 25.084 26.751 new_machine 1.0664 0.596 1.788 0.074 -0.103 2.235 The average revenue per user has also increased, by $1.0664$$. So, was the investment profitable?\nTo answer this question, we first have to decide which metric to use as our outcome metric. In case of ratio metrics, this is not trivial.\nAverage Return or Return of the Average? It is very tempting to approach this problem saying: it is true that we have two variables, by we can just compute their ratio, and then analyze everything as usual, using a single variable: the individual level return.\n$$ \\rho_i = \\frac{\\text{individual revenue}}{\\text{individual cost}} = \\frac{R_i}{C_i} $$\nWhat happens if we analyze the experiment using this single metric?\ndf[\u0026quot;rho\u0026quot;] = df[\u0026quot;revenue\u0026quot;] / df[\u0026quot;cost\u0026quot;] smf.ols(\u0026quot;rho ~ new_machine\u0026quot;, df).fit().summary().tables[1] coef std err t P\u003e|t| [0.025 0.975] Intercept 6.6898 0.044 150.832 0.000 6.603 6.777 new_machine -0.7392 0.062 -11.893 0.000 -0.861 -0.617 The estimated effect is negative and significant, $-0.7392$! It seems like the the new machines were not a good investment, and the returns have decreased by $74%$.\nThis result seems to contradict our previous estimates. We have seen before that the revenue has increased on average more than the cost ($0.9505$ vs $0.5076$). Why is it the case? The problem is that we are giving the same weight to heavy users and light users. Let\u0026rsquo;s use a simple example with two users. The first one (blue) is a light user and before was costing $1$ $ and returning $10$ $, while now is costing $4$ $ and returning $20$ $. The other user (violet) is a heavy user and before was costing $10$ $ and returning $100$ $ and now is costing $20$ $ and returning $220$ $.\nThe average return is -3x: on average the return per user has decreased by $300%$. However, the total return per user is $1000%$: the increase in cost of $13$$ has generated $130$$ in revenue! The results are wildly different and entirely driven by the weight of the two users: the effect of the heavy user is low in relative terms but high in absolute terms, while it\u0026rsquo;s the opposite for the light user. The average relative effect is therefore mostly driven by the light user, while the relative average effect is mostly driven by the heavy user.\nWhich metric is more relevant in our setting? We talking about return on investment, we are usually interested in understanding whether we got a return on the money we spend. Therefore, the total return is more interesting than the average return.\nFrom now on, the object of interest will be the return on investment (ROI), given by the expected increase in revenue over the expected increase in cost, and we will denote it with the greek letter rho, $\\rho$.\n$$ \\rho = \\frac{\\text{incremental revenue}}{\\text{incremental cost}} = \\frac{\\mathbb E [\\Delta R]}{\\mathbb E [\\Delta C]} $$\nWe can estimate the ROI as the ratio of the two previous estimates: the average difference in revenue between the treatment and control group, over the average difference in cost between the treatment and control group.\n$$ \\hat{\\rho} = \\frac{\\mathbb E_n [\\Delta R]}{\\mathbb E_n [\\Delta C]} $$\nNote a subtle but crucial difference with respect to the previous formula: we have replaced the expected values $\\mathbb E$ with the empirical expectation operators $\\mathbb E_n$, also known as the sample average. The difference in notation is minimal, but the conceptual difference is huge. The first, $\\mathbb E$, is a theoretical concept, while the second, $\\mathbb E_n$, is empirical: it is a number that depends on the actual data. I personally like the notation since it highlights the close link between the two concepts (the second is the empirical counterpart of the first), while also making it clear that the second crucially depends on the sample size $n$.\ndef estimate_roi(df): Delta_C = df.loc[df.new_machine==1, \u0026quot;cost\u0026quot;].mean() - df.loc[df.new_machine==0, \u0026quot;cost\u0026quot;].mean() Delta_R = df.loc[df.new_machine==1, \u0026quot;revenue\u0026quot;].mean() - df.loc[df.new_machine==0, \u0026quot;revenue\u0026quot;].mean() return Delta_R / Delta_C estimate_roi(df) 2.0698235970047887 The estimate is $2.0698$: each additional dollar spent in the new machines translated in $2.0698$ extra dollars in revenue. Sounds great!\nBut how much should we trust this number? Is it significantly different form one, or it is just driven by noise?\nInference To answer this question, we would like to compute a confidence interval for our estimate. How do we compute a confidence interval for a ratio metric? The first step is to compute the standard deviation of the estimator. One method that is always available is the bootstrap: resample the data with replacement multiple times and use the distribution of the estimates over samples to compute the standard deviation of the estimator.\nLet\u0026rsquo;s try it in our case. I compute the standard deviation over $10.000$ bootstrapped samples, using the function pd.DataFrame().sample() with the options frac=1 to obtain a dataset of the same size and replace=True to sample with replacement.\nboot_estimates = [estimate_roi(df.sample(frac=1, replace=True, random_state=i)) for i in range(10_000)] np.std(boot_estimates) 0.9790730538161984 The bootstrap estimate of the standard deviation is equal to $0.979$. How good is it?\nSince we fully control the data generating process, we can simulate the \u0026ldquo;true\u0026rdquo; distribution of the estimator. We do that for $10.000$ simulations and we compute the resulting standard deviation of the estimator.\nnp.std(dgp.evaluate_f_redrawing_outcomes(estimate_roi, 10_000)) 1.0547776958025372 The estimated variance of the estimator using the \u0026ldquo;true\u0026rdquo; data generating process is slightly higher but very similar, around $1.055$.\nThe issue with the bootstrap is that it is very computational intense since it requires repeating the estimating procedure thousands of times. We are now going to explore another extremely powerful alternative that requires a single estimation step, the delta method. The delta method generally allows us to do inference on functions of random variable, therefore its applications are broader than ratios.\n⚠️ Warning: the next section is going to be algebra-intense. If you want, you can skip it and go straight to the last section.\nThe Delta Method What is the delta method? In short, it is an incredibly powerful asymptotic inference method for functions of random variables, that exploits Taylor expansions. In short, the delta method requires four ingredients\nOne or more random variables A function The Central Limit Theorem Taylor expansions I will assume some basic knowledge of all four concepts. Suppose we had a set of realizations $X_1$, \u0026hellip;, $X_n$ of a random variable that satisfy the requirements for the Central Limit Theorem (CLT): independence, identically distributions with expected value $\\mu$, and finite variance $\\sigma^2$. Under these conditions, the CLT tells us that the sample average $\\mathbb E_n[X]$ converges in distribution to a normal distribution, or more precisely\n$$ \\sqrt{n} \\ \\frac{ \\mathbb E_n[X] - \\mu}{\\sigma} \\ \\overset{D}{\\to} \\ N(0, 1) $$\nWhat does the equation mean? It reads \u0026ldquo;the normalized sample average, scaled by a factor $\\sqrt{n}$, converges in distribution to a standard normal distribution, i.e. it is approximately Gaussian for a sufficiently large sample.\nNow, suppose we were interested in a function of the sample average $f\\big(\\mathbb E_n[X]\\big)$. Note that this is different from the sample average of the function $\\mathbb E_n\\big[f(X)\\big]$. The delta method tells us what the function of the sample average converges to.\n$$ \\sqrt{n} \\ \\frac{ f\\big(\\mathbb E_n[X]\\big) - f(\\mu)}{\\sigma} \\ \\overset{D}{\\to} \\ N \\big(0, f\u0026rsquo;(\\mu)^2 \\big) $$\n, where $f\u0026rsquo;(\\mu)^2$ is the derivative of the function $f$, evaluated at $\\mu$.\nWhat is the intuition behind this formula? We now have a new term inside the expression of the variance, the squared first derivative $f\u0026rsquo;(\\mu)^2$ ($\\neq$ second derivative). If the derivative of the function is low, the variance decreases since different inputs translate into similar outputs. On the contrary, if the derivative of the function is high, the variance of the distribution is amplified, since different inputs translate into even more different outputs.\nThe result directly follows from the Taylor approximation of $f \\big(\\mathbb E_n[X]\\big)$\n$$ f\\big(\\mathbb E_n[X]\\big) = f(\\mu) + f\u0026rsquo;(\\mu) (\\mathbb E_n[X] - \\mu) + \\text{residual} $$\nImportantly, asymptotically, the last term disappears and the linear approximation holds exactly!\nHow is this connected to the ratio estimator? We need a bit more math and to switch from a single dimension to two dimensions in order to understand that. In our case, we have a bivariate function of two random variables, $\\Delta R$ and $\\Delta C$, which returns their ratio. In the case of a multivariate function $f$, the asymptotic variance of the estimator is given by\n$$ \\text{AVar} \\big( \\hat{\\rho} \\big) = \\nabla \\hat{\\rho}\u0026rsquo; \\Sigma_n \\nabla \\hat{\\rho} $$\nwhere, $\\nabla$ indicates the gradient of the function, i.e. the vector of directional derivatives, and $\\Sigma_n$ is the empirical variance-covariance matrix of $X$. In our case, they correspond to\n$$ \\nabla \\hat{\\rho} = \\begin{bmatrix} \\frac{1}{\\mathbb E_n [\\Delta C]} \\newline - \\frac{\\mathbb E_n [\\Delta R]}{\\mathbb E_n [\\Delta C]^2} \\end{bmatrix} $$\nand\n$$ \\Sigma_n = \\begin{bmatrix} \\text{Var}_n (\\Delta R) \u0026amp; \\text{Cov}_n (\\Delta R, \\Delta C) \\newline \\text{Cov}_n (\\Delta R, \\Delta C) \u0026amp; \\text{Var}_n (\\Delta C) \\newline \\end{bmatrix} $$\n, where the subscripts $n$ indicate the empirical counterparts, as for the expected value.\nCombining the previous three equations together with a little matrix algebra, we get the formula of the asymptotic variance of the return on investment estimator.\n$$ \\begin{align*} \\text{AVar} \\big( \\hat{\\rho} \\big) \u0026amp;= \\frac{1}{\\mathbb E_n[\\Delta C]^2} \\text{Var}_n(\\Delta R) - 2 \\frac{\\mathbb E_n[\\Delta R]}{\\mathbb E_n[\\Delta C]^3} \\text{Cov}_n(\\Delta R, \\Delta C) + \\frac{\\mathbb E_n[\\Delta R]^2}{\\mathbb E_n[\\Delta C]^4} \\text{Var}_n(\\Delta C) \\end{align*} $$\nSince the estimator is given by $\\hat{\\rho} = \\frac{\\mathbb E_n[\\Delta R]}{\\mathbb E_n[\\Delta C]}$, we can rewrite the asymptotic variance as\n$$ \\begin{align*} \\text{AVar} \\big( \\hat{\\rho} \\big) = \\frac{1}{\\mathbb E_n[\\Delta C]^2} \\text{Var}_n \\Big( \\Delta R - \\hat{\\rho} \\Delta C \\Big) \\end{align*} $$\nThe last expression is very interesting because it suggests that we can rewrite the asymptotic variance of our estimator as the variance of a difference-in-means estimator for a new auxiliary variable. In fact, we can rewrite the above expression as\n$$ \\begin{align*} \\text{AVar} \\big( \\hat{\\rho} \\big) = \\text{Var}_n \\Big( \\Delta \\tilde R \\Big) \\qquad \\text{where} \\quad \\tilde R = \\frac{R - \\hat{\\rho} \\ C}{| \\mathbb E [\\Delta C] |} \\end{align*} $$\nThis expression is incredibly useful because it gives us intuition and allows us to estimate the standard deviation of our estimator by linear regression.\nInference with Linear Regression Did you skip the previous section? No problem!\nAfter some algebra, we concluded that we can estimate the variance of a difference-in-means estimator for an auxiliary variable defined as\n$$ \\tilde R = \\frac{R - \\hat{\\rho} \\ C}{| \\mathbb E_n [\\Delta C] |} $$\nThis expression might seem obscure at first, but it is incredibly useful. In fact, it gives us (1) an intuitive interpretation of the variance of the estimator and (2) a practical way to estimate it.\nInterpretation first! How should we read the above expression? We can estimate the variance of the empirical estimator as the variance of a difference-in-means estimator, for a new variable $\\tilde R$ that we can easily compute from the data. We just need to take the revenue $R$, subtract the cost $C$ multiplied by the estimated ROI $\\rho$ and scale it down by the expected cost difference $|\\mathbb E_n[\\Delta C]|$. We can interpret this variable as the baseline revenue, i.e. the revenue not affected by the investment. The fact that it is scaled by the expected cost difference tells us that its variance will be decreasing in the total investment: the more we spend, the more precisely we can estimate the return on that expenditure.\nNow, let\u0026rsquo;s estimate the variance of the ROI estimator, in four steps.\nWe need to estimate the return on investment $\\hat \\rho$. rho_hat = estimate_roi(df) The term $| \\mathbb E_n[\\Delta C] |$ is the absolute difference in average cost between the treatment and control group. abs_Delta_C = np.abs(df.loc[df.new_machine==1, \u0026quot;cost\u0026quot;].mean() - df.loc[df.new_machine==0, \u0026quot;cost\u0026quot;].mean()) We now have all the ingredients to generate the auxiliary variable $\\tilde R$. df['revenue_tilde'] = (df['revenue'] - rho_hat * df['cost']) / abs_Delta_C The variance of the treatment-control difference $\\Delta \\tilde R$ can be directly computed by linear regression, as in randomized controlled trials for difference-in-means estimators (see Agrist and Pischke, 2008). smf.ols('revenue_tilde ~ new_machine', df).fit().summary().tables[1] coef std err t P\u003e|t| [0.025 0.975] Intercept 38.4067 0.653 58.771 0.000 37.126 39.688 new_machine -2.01e-14 0.917 -2.19e-14 1.000 -1.797 1.797 The estimated standard error of the ROI is $0.917$, very close to the bootstrap estimate of $0.979$ and the simulated value of $1.055$. However, with respect to bootstrapping, the delta method allowed us to compute it in a single step, making it sensibly faster (around $1000$ times on my local machine).\nNote that this estimated standard deviation implies a 95% confidence interval of $2.0698 +- 1.96 \\times 0.917$, equal to $[-0.2735, 3.8671]$. This might seem like good news since the confidence interval does not cover zero. However, note that in this case, a more interesting null hypothesis is that the ROI is equal to 1: we are breaking even. A value larger than 1 implies profits, while a value lower than 1 implies losses. In our case, we cannot reject the null hypothesis that the investment in new machines was not profitable.\nConclusion In this article, we have explored a very common causal inference problem: assessing the return on investment. Whether it\u0026rsquo;s a physical investment in new hardware, a virtual cost, or advertisement expenditure, we are interested in understanding whether this incremental cost has paid off. The additional complications come from the fact that we are studying not one, but two causal quantities, intertwined.\nWe have first explored and compared different outcome metrics to assess whether the investment paid off. Then, we have introduced an incredibly powerful method to do inference with complex random variables: the delta method. In the particular case of ratios, the delta method delivers a very insightful and practical functional form for the asymptotic variance of the estimator that can be estimated with a simple linear regression.\nReferences [1] A. Deng, U. Knoblich, J. Lu, Applying the Delta Method in Metric Analytics: A Practical Guide with Novel Ideas (2018).\n[2] R. Budylin, A. Drutsa, I. Katsev, V. Tsoy, Consistent Transformation of Ratio Metrics for Efficient Online Controlled Experiments (2018). ACM.\n[3] J. Angrist, J. Pischke, Mostly harmless econometrics: An empiricist\u0026rsquo;s companion (2009). Princeton university press.\nRelated Articles The Bayesian Bootstrap\nOutliers, Leverage, Residuals, and Influential Observations\nA/B Tests, Privacy, and Online Regression\nCode You can find the original Jupyter Notebook here:\nhttps://github.com/matteocourthoud/Blog-Posts/blob/main/notebooks/delta.ipynb\n","date":1672099200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1672099200,"objectID":"778d1b1ce0a2ca288360e5d99c84092a","permalink":"https://matteocourthoud.github.io/post/delta/","publishdate":"2022-12-27T00:00:00Z","relpermalink":"/post/delta/","section":"post","summary":"An introduction to the delta method for inference on ratio metrics.\nWhen we run an experiment, we are often not only interested in the effect of a treatment (new product, new feature, new interface, \u0026hellip;) on revenue, but in it\u0026rsquo;s cost-effectiveness.","tags":null,"title":"Experiments on Returns on Investment","type":"post"},{"authors":null,"categories":null,"content":"An introduction to quantile regression.\nIn A/B tests, a.k.a. randomized controlled trials, we usually estimate the average treatment effect (ATE): effect of a treatment (a drug, ad, product, \u0026hellip;) on an outcome of interest (a disease, firm revenue, customer satisfaction, \u0026hellip;), where the \u0026ldquo;average\u0026rdquo; is taken over the test subjects (patients, users, customers, \u0026hellip;). The ATE is a very useful quantity since it tells us the effect that we can expect if we were to treat a new subject with the same treatment.\nHowever, sometimes we might be interested in quantities different from the average, such as the median. The median is an alternative measure of central tendency that is more robust to outliers and is often more informative with skewed distributions. More generally, we might want to estimate the effect for different quantiles of the outcome distribution. A common use-case is studying the impact of a UI change on the loading time of a website: a slightly heavier website might translate in an imperceptible change for most users, but a big change for a few users with very slow connections. Another common use-case is studying the impact of a product change on a product that is bought by few people: do existing customers buy it more or are we attracting new customers?\nThese questions are hard to answer with linear regression that estimates the average treatment effect. A more suitable tool is quantile regression that can instead estimate the median treatment effect. In this article we are going to cover a brief introduction to quantile regression and the estimation of quantile treatment effects.\nLoyalty Cards and Spending Suppose we were an online store and we wanted to increase sales. We decide to offer our customers a loyalty card that grants them discounts as they increase their spend in the store. We would like to assess if the loyalty card is effective in increasing sales so we run an A/B test: we offer the loyalty card only to a subset of our customers, at random.\nI import the data generating process dgp_loyalty() from src.dgp. I also import some plotting functions and libraries from src.utils. To include not only code but also data and tables, I use Deepnote, a Jupyter-like web-based collaborative notebook environment.\n%matplotlib inline %config InlineBackend.figure_format = 'retina' from src.utils import * from src.dgp import dgp_loyalty Now, let\u0026rsquo;s have a look at the data. We have information on $10.000$ customers, for whom we observe their spend and whether they were offered the loyalty card. We also observe some demographics, like age and gender.\ndf = dgp_loyalty().generate_data() df.head() loyalty spend age gender 0 1 0.0 30 Male 1 0 0.0 26 Male 2 1 0.0 27 Male 3 1 0.0 29 Male 4 1 0.0 23 Female Interestingly, we notice that the outcome of interest, spend, seems to have a lot of zeros. Let\u0026rsquo;s dig deeper.\nMean vs Median Before analyzing our experiment, let\u0026rsquo;s have a look at our outcome variable, spend. We first inspect it using centrality measures. We have two main options: the mean and the median.\nFirst of all, what are they? The mean captures the average value, while the median captures the value in the middle of the distribution. In general, the mean is mathematically more tractable and easier to interpret, while the median is more robust to outliers. You can find plenty of articles online comparing the two measures and suggesting which one is more appropriate and when. Let\u0026rsquo;s have a look at the mean and median spend.\nnp.mean(df['spend']) 28.136224 np.median(df['spend']) 0.0 How do we interpret these two numbers? People spend on average 28\\$ on our store. However, more than 50% of people don\u0026rsquo;t spend anything. As we can see, both measures are very informative and, to a certain extent, complementary. We can better understand the distribution of spend by plotting its histogram.\nsns.histplot(data=df, x=\u0026quot;spend\u0026quot;).set(ylabel='', title='Spending Distribution'); As previewed by the values of the mean and the median, the distribution of spend is very skewed, with more than 5000 customers (out of 10000) not spending anything.\nOne natural question then is: are we interested in the effect of the loyalty card on average spend or on median spend? The first would tell us if customers spend more on average, while the second would tell us if the average customer spends more.\nLinear regression can tell us the effect of the loyalty card on average spend. However, what can we do if we were interested in the effect of the loyalty card on median spend (or other quantiles)? The answer is quantile regression.\nQuantile Regression With linear regression, we try to estimate the conditional expectation function of an outcome variable $Y$ (spend in our example) with respect to one or more explanatory variables $X$ (loyalty in our example).\n$$ \\mathbb E \\big[ Y \\big| X \\big] $$\nIn other words, we want to find a function $f$ such that $f(X) = \\mathbb E[Y|X]$. We do so, by solving the following minimization problem:\n$$ \\hat f(X) = \\arg \\min_{f} \\mathbb E \\big[ Y - f(X) \\big]^2 $$\nIt can be shown that the function $f$ that solves this minimization is indeed the conditional expectation of $Y$, with respect to $X$.\nSince $f(X)$ can be infinite dimensional, we usually estimate a parametric form of $f(X)$. The most common one is the linear form $f(X) = \\beta X$, where $\\beta$ is estimated by solving the corresponding minimization problem:\n$$ \\hat \\beta = \\arg \\min_{\\beta} \\mathbb E \\big[ Y - \\beta X \\big]^2 $$\nThe linear form is not just convenient, but it can be interpreted as the best local approximation of $f(X)$, referring to Taylor\u0026rsquo;s expansion.\nWith quantile regression, we do the same. The only difference is that, instead of estimating the conditional expectation of $Y$ with respect to $X$, we want to estimate the $q$-quantile of $Y$ with respect to $X$.\n$$ \\mathbb Q_q \\big[ Y \\big| X \\big] $$\nFirst of all, what is a quantile? The Wikipedia definition says\n\u0026ldquo;In statistics and probability, quantiles are cut points dividing the range of a probability distribution into continuous intervals with equal probabilities, or dividing the observations in a sample in the same way. Common quantiles have special names, such as quartiles (four groups), deciles (ten groups), and percentiles (100 groups).\u0026rdquo;\nFor example, the 0.1-quantile represents the value that sits on the right of 10% of the mass of the distribution. The median is the 0.5-quantile (or, equivalently, the $50^{th}$ percentile or the $5^{th}$ decile) and corresponds with the value in the center of the distribution. Let\u0026rsquo;s see a simple example with a log-normal distribution. I plot the three quartiles that divide the data in four equally sized bins.\ndata = np.random.lognormal(0, 1, 1_000_000); sns.histplot(data).set(title='Lognormal Distribution', xlim=(0,10)) plt.axvline(x=np.percentile(data, 25), c='C8', label='25th percentile') plt.axvline(x=np.median(data), c='C1', label='Median (50th pct)') plt.axvline(x=np.percentile(data, 75), c='C3', label='75th percentile') plt.legend(); As we can see, the three quartiles divide the data into four bins, of equal size.\nSo, what is the objective of quantile regression? The objective is to find a function $f$ such that $f(X) = F^{-1}(y_q)$, where $F$ is the cumulative distribution function of $Y$ and $y_q$ is the $q$-quantile of the distribution of $Y$.\nHow do we do this? It can be shown with a little linear algebra that we can obtain the conditional quantile as the solution of the following minimization problem:\n$$ \\hat f(X) = \\arg \\min_{f} \\mathbb E \\big[ \\rho_q (Y - f(X)) \\big] = \\arg \\min_{f} \\ (1-q) \\int_{-\\infty}^{f(x)} (y - f(x)) \\text{d} F(y) + q \\int_{f(x)}^{\\infty} (y - f(x)) \\text{d} F(y) $$\nWhere $\\rho_q$ is an auxiliary weighting function with the following shape.\nWhat is the intuition behind the objective function?\nThe idea is that we can interpret the equation as follows\n$$ \\mathbb E \\big[ \\rho_q (Y - f(X)) \\big] = (1-q) \\underset{\\color{red}{\\text{mass of distribution below }f(x)}}{\\int_{-\\infty}^{f(x)} (y - f(x)) \\text{d} F(y)} + q \\underset{\\color{red}{\\text{mass of distribution above }f(x)}}{\\int_{f(x)}^{\\infty} (y - f(x)) \\text{d} F(y)} \\overset{\\color{blue}{\\text{if } f(x) = y_q}}{=} - (1-q) q + q (1-q) = 0 $$\nSo that, when $f(X)$ corresponds with the quantile $y_q$, the value of the objective function is zero.\nExactly as before, we can estimate a parametric form of $f$ and, exactly as before, we can interpret it as a best local approximation (not trivially though, see Angrist, Chernozhukov, and Fernández-Val (2006)).\n$$ \\hat \\beta_q = \\arg \\min_{\\beta} \\mathbb E \\big[ \\rho_q (Y - \\beta X ) \\big] $$\nWe wrote $\\hat \\beta_q$ to indicate that this is the coefficient for the best linear approximation of the conditional $q$-quantile function.\nHow do we estimate a quantile regression?\nEstimation The statsmodels package allows us to estimate quantile regression with the the quantreg() function. We just need to specify the quantile $q$ when we fit the model. Let\u0026rsquo;s use $q=0.5$, which corresponds with the median.\nsmf.quantreg(\u0026quot;spend ~ loyalty\u0026quot;, data=df).fit(q=0.5).summary().tables[1] coef std err t P\u003e|t| [0.025 0.975] Intercept 8.668e-07 0.153 5.66e-06 1.000 -0.300 0.300 loyalty 3.4000 0.217 15.649 0.000 2.974 3.826 Quantile regression estimates a positive coefficient for loyalty. How does this estimate compare with linear regression?\nsmf.ols(\u0026quot;spend ~ loyalty\u0026quot;, data=df).fit().summary().tables[1] coef std err t P\u003e|t| [0.025 0.975] Intercept 25.6583 0.564 45.465 0.000 24.552 26.765 loyalty 4.9887 0.801 6.230 0.000 3.419 6.558 The estimated coefficient with linear regression is higher. What does it mean? We will spend more time on the interpretation of quantile regression coefficients later.\nCan we condition the analysis on other variables? We suspect that spend is also affected by other variables and we want to increase the precision of our estimate by also conditioning the analysis on age and gender. We can just add the variables to the quantreg() model.\nsmf.quantreg(\u0026quot;spend ~ loyalty + age + gender\u0026quot;, data=df).fit(q=0.5).summary().tables[1] coef std err t P\u003e|t| [0.025 0.975] Intercept -50.5353 1.053 -47.977 0.000 -52.600 -48.471 gender[T.Male] -20.2963 0.557 -36.410 0.000 -21.389 -19.204 loyalty 4.5747 0.546 8.374 0.000 3.504 5.646 age 2.3663 0.026 92.293 0.000 2.316 2.417 The coefficient of loyalty increases when we condition the analysis on age and gender. This is true also for linear regresssion.\nsmf.ols(\u0026quot;spend ~ loyalty + age + gender\u0026quot;, data=df).fit().summary().tables[1] coef std err t P\u003e|t| [0.025 0.975] Intercept -57.4466 0.911 -63.028 0.000 -59.233 -55.660 gender[T.Male] -26.3170 0.482 -54.559 0.000 -27.262 -25.371 loyalty 3.9101 0.473 8.272 0.000 2.983 4.837 age 2.7688 0.022 124.800 0.000 2.725 2.812 There are a couple of things that we haven\u0026rsquo;t mentioned yet. The first one is inference. How do we compute confidence intervals and p-values for our estimates in quantile regression?\nInference The asymptotic variance of the estimate $a$ of the quantile $q$ of a distribution $F$ is given by\n$$ AVar(y) = q(1-q) f^{-2}(y) $$\nwhere $f$ is the density function of $F$. This expression can be decomposed into two components: $q(1-q)$ and $f^{-2}(y)$.\nThe first component, $q(1-q)$, basically tells us that the variance of a quantile is higher the more the quantile is closer to the center of the distribution. Why is that so? First, we need to think about when the quantile of a point changes in response to a change in the value of a second point. The quantile changes when the second point swaps from left to right (or viceversa) of the first point. This is intuitively very easy if the first point lies in the middle of the distribution, but very hard if it lies at the extreme.\nThe second component, $f^{-2}(a)$, instead tells us that this side swapping is more likely if the first point is surrounded by a lot of points.\nImportantly, estimating the variance of a quantile requires an estimate of the whole distribution of $Y$. This is done via approximation and it can be computationally very intensive. However, alternative procedures like the bootstrap or the bayesian bootstrap are always available.\nThe second thing that we haven\u0026rsquo;t talked about yet is the interpretation of the estimated coefficients. We got a lower coefficient of loyalty on spend with median regression. What does it mean?\nInterpretation The interpretation of linear regression coefficients is straightforward: each coefficient is the derivative of the conditional expectation function $\\mathbb E[Y|X]$ with respect to one dimension of $X$. In our case, we can interpret the regression coefficient of loyalty as the average spend increase from being offered a loyalty card. Crucially, here \u0026ldquo;average\u0026rdquo; means that this holds true for each customer, on average.\nHowever, the interpretation of quantile regression coefficients is tricky. Before, we were tempted to say that the loyalty card increases the spend of the median customer by 3.4\\$. But what does it mean? Is it the same median customer that spends more or do we have a different median customer? This might seem like a philosophical question but it has important implications on reporting of quantile regression results. In the first case, we are making a statement that, as for the interpretation of linear regression coefficients, applies to a single individual. In the second case, we are making a statement about the distribution.\nChernozhukov and Hansen (2005) show that a strong but helpful assumption is rank invariance: assuming that the treatment does not shift the relative composition of the distribution. In other words, if we rank people by spend before the experiment, we assume that this ranking is not affected by the introduction of the loyalty card. If I was spending less than you before, I might spend more afterwards, but still less than you (for any two people).\nUnder this assumption, we can interpret the quantile coefficients as marginal effects for single individuals sitting at different points of the outcome distribution, as in the first interpretation provided above. Moreover, we can report the treatment effect for many quantiles and interpret each one of them as a local effect for a different individual. Let\u0026rsquo;s plot the distribution of treatment effects, for different quantiles of spend.\ndef plot_quantile_TE(df, formula, q, varname): df_results = pd.DataFrame() for q in np.arange(q, 1-q, q): qreg = smf.quantreg(formula, data=df).fit(q=q) temp = pd.DataFrame({'q': [q], 'coeff': [qreg.params[varname]], 'std': [qreg.bse[varname]], 'ci_lower': [qreg.conf_int()[0][varname]], 'ci_upper': [qreg.conf_int()[1][varname]]}) df_results = pd.concat([df_results, temp]).reset_index(drop=True) # Plot fig, ax = plt.subplots() sns.lineplot(data=df_results, x='q', y='coeff') ax.fill_between(data=df_results, x='q', y1='ci_lower', y2='ci_upper', alpha=0.1); plt.axhline(y=0, c=\u0026quot;k\u0026quot;, lw=2, zorder=1) ols_coeff = smf.ols(formula, data=df).fit().params[varname] plt.axhline(y=ols_coeff, ls=\u0026quot;--\u0026quot;, c=\u0026quot;C1\u0026quot;, label=\u0026quot;OLS coefficient\u0026quot;, zorder=1) plt.legend() plt.title(\u0026quot;Estimated coefficient, by quantile\u0026quot;) plot_quantile_TE(df, formula=\u0026quot;spend ~ loyalty\u0026quot;, varname='loyalty', q=0.05) This plot is extremely insightful: for almost half of the customers, the loyalty card has no effect. On the other hand, customers that were already spending something end up spending even more (around 10/12\\$ more). This is a very powerful insight that we would have missed with linear regression that estimated an average effect of 5\\$.\nWe can repeat the same exercise, conditioning the analysis on gender and age.\nplot_quantile_TE(df, formula=\u0026quot;spend ~ loyalty + age + gender\u0026quot;, varname='loyalty', q=0.05) Conditioning on other covariates removes a lot of the heterogeneity in treatment effects. The loyalty card increases spending for most people, it\u0026rsquo;s demographic characteristics that are responsible for no spending to begin with.\nConclusion In this article, we have explored a different causal estimand: median treatment effects. How does it compare with the average treatment effect that we usually estimate? The pros and cons are closely related to the pros and cons of the median with respect to the mean as a measure of central tendency. Median treatment effects are more informative on what is the effect on the average subject and are more robust to outliers. However, they are much more computationally intensive and they require strong assumptions for identification, such as rank invariance.\nReferences [1] R. Koenker, Quantile Regression (1996), Cambridge University Press.\n[1] R. Koenker, K. Hallock, Quantile Regression, (2001), Journal of Economic Perspectives.\n[2] V. Chernozhukov, C. Hansen, An IV Model of Quantile Treatment Effects (2005), Econometrica.\n[3] J. Angrist, V. Chernozhukov, I. Fernández-Val, Quantile Regression under Misspecification, with an Application to the U.S. Wage Structure (2006), Econometrica.\nRelated Articles DAGs and Control Variables The Bayesian Bootstrap Goodbye Scatterplot, Welcome Binned Scatterplot Code You can find the original Jupyter Notebook here:\nhttps://github.com/matteocourthoud/Blog-Posts/blob/main/notebooks/quantile_reg.ipynb\n","date":1665360000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1665360000,"objectID":"e12c0dd097f8f050f5b470527f36d1ec","permalink":"https://matteocourthoud.github.io/post/quantile_reg/","publishdate":"2022-10-10T00:00:00Z","relpermalink":"/post/quantile_reg/","section":"post","summary":"An introduction to quantile regression.\nIn A/B tests, a.k.a. randomized controlled trials, we usually estimate the average treatment effect (ATE): effect of a treatment (a drug, ad, product, \u0026hellip;) on an outcome of interest (a disease, firm revenue, customer satisfaction, \u0026hellip;), where the \u0026ldquo;average\u0026rdquo; is taken over the test subjects (patients, users, customers, \u0026hellip;).","tags":null,"title":"Mean vs Median Causal Effect","type":"post"},{"authors":null,"categories":null,"content":"How to run experiments without storing individual data\nAB tests, a.k.a. randomized controlled trials, are widely recognized as the gold standard technique to compute the causal effect of a treatment (a drug, ad, product, \u0026hellip;) on an outcome of interest (a disease, firm revenue, customer satisfaction, \u0026hellip;). We randomly split a set of subjects (patients, users, customers, \u0026hellip;) into a treatment and a control group and give the treatment to the treatment group. This procedure ensures that ex-ante, the only expected difference between the two groups is caused by the treatment.\nOne potential privacy concern is that one needs to store data about many users for the whole duration of the experiment in order to estimate the effect of the treatment. This is not a problem if we can run the experiment instantaneusly, but can become an issue when the experiment duration is long. In this post, we are going to explore one solution to this problem: online regression. We will see how to estimate (conditional) average treatment effects and how to do inference, using both asymptotic approximations and bootstrapping.\n⚠️ Some parts are algebra-intense, but you can skip them if you are only interested in the intuition.\nCredit Cards and Donations Suppose, for example, that we were a fin-tech company. We have designed a new user interface (UI) for our mobile application and we would like to understand whether it slows down our transaction. In order to estimate the causal effect of the new UI on transaction speed, we plan to run an A/B test or randomized controlled trial.\nWe have one major problem: we should not store user-level information for privacy reasons.\nFirst, let\u0026rsquo;s have a look at the data. I import the data generating process dgp_credit() from src.dgp and some plotting functions and libraries from src.utils. I include code snippets from Deepnote, a Jupyter-like web-based collaborative notebook environment. For our purpose, Deepnote is very handy because it allows me not only to include code but also output, like data and tables.\n%matplotlib inline %config InlineBackend.figure_format = 'retina' from src.utils import * from src.dgp import dgp_credit I first generate the whole dataset in one-shot. We will then investigate how to perform the experimental analysis in case the data was arriving dynamically.\ndef generate_data(self, N=100, seed=0): np.random.seed(seed) # Connection speed connection = np.random.lognormal(3, 1, N) # Treatment assignment treated = np.random.binomial(1, 0.5, N) # Transfer speed #spend = np.minimum(np.random.lognormal(1 + treated + 0.1*np.sqrt(balance), 2, N), balance) speed = np.minimum(np.random.exponential(10 + 4*treated - 0.5*np.sqrt(connection), N), connection) # Generate the dataframe df = pd.DataFrame({'c': [1]*N, 'treated': treated, 'connection': np.round(connection,2), 'speed': np.round(speed,2)}) return df N = 100 df = generate_data(N) df.head() c treated connection speed 0 1 0 117.22 0.94 1 1 1 29.97 29.97 2 1 0 53.45 7.38 3 1 0 188.84 0.76 4 1 1 130.00 24.44 We have information on 100 users, for whom we observe\u0026hellip;\nFirst, let\u0026rsquo;s have a look\nmodel = smf.ols('speed ~ treated + connection', data=df).fit() model.summary().tables[1] coef std err t P\u003e|t| [0.025 0.975] Intercept 6.0740 1.079 5.630 0.000 3.933 8.215 treated 1.3939 1.297 1.075 0.285 -1.180 3.968 connection -0.0033 0.017 -0.197 0.844 -0.037 0.030 In order to understand how we can make linear regression one data point at the time, we first need a brief linear algebra recap.\nFirst of all, let\u0026rsquo;s define $y$ the dependent variable, spend in our case, and $X$ the explanatory variable, the treated indicator, the account balance and a constant.\ndef xy_from_df(df, r0, r1): return df.iloc[r0:r1,:3].to_numpy(), df.iloc[r0:r1,3].to_numpy() The estimator is given by $$ \\hat{\\beta}_{OLS} = (X\u0026rsquo;X)^{-1}X\u0026rsquo;y $$\nfrom numpy.linalg import inv X, Y = xy_from_df(df, 0, 100) inv(X.T @ X) @ X.T @ Y array([ 6.07404291e+00, 1.39385101e+00, -3.33599131e-03]) We get indeed the same exact number as with the smf.ols command!\nCan we compute $\\beta$ one observation at the time?\nThe answer is yes! Assume we had $n$ observations and we just received the $n+1$th observation: the pair $(x_{n+1}, y_{n+1})$. In order to compute $\\hat{\\beta}_{n+1}$ we need to have stored only two objects in memory\n$\\hat{\\beta}_{n}$, the previous estimate of $\\beta$ $(X_n\u0026rsquo; X_n)^{-1}$, the previous value of $(X\u0026rsquo; X)^{-1}$ First of all, how do we update $(X\u0026rsquo; X)^{-1}$? $$ \\begin{align*} (X_{n+1}\u0026rsquo; X_{n+1})^{-1} = (X_n\u0026rsquo; X_n)^{-1} - \\frac{(X_n\u0026rsquo; X_n)^{-1} x_{n+1} x_{n+1}\u0026rsquo; (X_n\u0026rsquo; X_n)^{-1}}{1 + x_{n+1}\u0026rsquo; (X_n\u0026rsquo; X_n)^{-1} x_{n+1}} \\end{align*} $$\nAfter having updated $(X\u0026rsquo; X)^{-1}$, we can update $\\hat{\\beta}$. $$ \\hat{\\beta}{n+1} = \\hat{\\beta}{n} + (X_n\u0026rsquo; X_n)^{-1} x_{n} (y_n - x_n\u0026rsquo; \\hat{\\beta}_{n}) $$\nNote that this procedure is not only privacy friendly but also memory-friendly. Our dataset is a $100 \\times 4$ matrix while $(X\u0026rsquo; X)^{-1}$ is $3 \\times 3$ matrix and $\\beta$ is a $3 \\times 1$ matrix. We are storing only 12 numbers instead of up to 400!\ndef update_xb(XiX, beta, x, y): XiX -= (XiX @ x.T @ x @ XiX) / (1 + x @ XiX @ x.T ) beta += XiX @ x.T @ (y - x @ beta) return XiX, beta We are now ready to estimate our OLS coefficient, one data point at the time. However, we cannot really start from the first observation, because we would be unable to invert the matrix $X\u0026rsquo;X$. We need at least $k+1$ observations, where $k$ is the number of variables in $X$.\nLet\u0026rsquo;s use a warm start of 10 observations to be safe.\n# Initialize XiX and beta from first 10 observations x, y = xy_from_df(df, 0, 10) XiX = inv(x.T @ x) beta = XiX @ x.T @ y # Update estimate live for n in range(10, N): x, y = xy_from_df(df, n, n+1) XiX, beta = update_xb(XiX, beta, x, y) # Print result print(beta) [ 6.07404291e+00 1.39385101e+00 -3.33599131e-03] We got exactly the same coefficient! Nice!\nHow did we get there? We can plot the evolution of out estimate $\\hat{\\beta}$ as we accumulate data. The dynamic plotting function is a bit more cumbersome, but you can find it in src.figures.\nfrom src.figures import online_regression online_regression(df, \u0026quot;fig/online_reg1.gif\u0026quot;) As we can see, as the number of data points increases, the estimate seems to less and less volatile.\nBut is it really the case? As usual, we are not just interested in the point estimate of the effect of the coupon on spending, we would also like to understand how precise this estimate is.\nInference We have seen how to estimate the treatment effect \u0026ldquo;online\u0026rdquo;: one observation at the time. Can we also compute the variance of the estimator in the same manner?\nFirst of all, let\u0026rsquo;s review what the variance of the OLS estimator looks like. Under baseline assumptions, the variance of the OLS estimator is given by: $$ \\text{Var}(\\hat{\\beta}_{OLS}) = (X\u0026rsquo;X)^{-1} \\hat{\\sigma}^2 $$\nwhere $\\hat{\\sigma}^2$ is the variance of the residuals $e = (y - X\u0026rsquo;\\hat{\\beta})$.\nThe regression table reports the standard errors of the coefficients, which are the squared elements on the diagonal of $\\text{Var}(\\hat{\\beta})$.\nmodel.summary().tables[1] coef std err t P\u003e|t| [0.025 0.975] Intercept 6.0740 1.079 5.630 0.000 3.933 8.215 treated 1.3939 1.297 1.075 0.285 -1.180 3.968 connection -0.0033 0.017 -0.197 0.844 -0.037 0.030 Let\u0026rsquo;s check that we would obtain the same numbers using matrix algebra.\nbeta = inv(X.T @ X) @ X.T @ Y np.sqrt(np.diag(inv(X.T @ X) * np.var(Y - X @ beta))) array([1.06261376, 1.27718352, 0.01669716]) Indeed, we get exactly the same numbers!\nWe already have a method to part of $\\text{Var}(\\hat{\\beta}{OLS})$ online: $(X\u0026rsquo;X)^{-1}$ update the matrix $(X\u0026rsquo;X)^{-1}$ online. How do we update $\\hat{\\sigma}^2$? This is the formula to update the sum of squared residuals $S$. $$ S{n+1} = S_{n} + \\frac{(y_{n+1} - x_{n+1}\\hat{\\beta}n)}{1 + x{n+1}\u0026rsquo; (X_n\u0026rsquo; X_n)^{-1} x_{n+1}} $$\ndef update_xbs(XiX, beta, S, x, y): S += (y - x @ beta)**2 / (1 + x @ XiX @ x.T ) XiX -= (XiX @ x.T @ x @ XiX) / (1 + x @ XiX @ x.T ) beta += XiX @ x.T @ (y - x @ beta) return XiX, beta, S[0,0] Note, the order here is very important!\n# Inizialize XiX, beta, and sigma from the first 10 observations x, y = xy_from_df(df, 0, 10) XiX = inv(x.T @ x) beta = XiX @ x.T @ y S = np.sum((y - x @ beta)**2) # Update XiX, beta, and sigma online for n in range(10, N): x, y = xy_from_df(df, n, n+1) XiX, beta, S = update_xbs(XiX, beta, S, x, y) # Print result print(np.sqrt(np.diag(XiX * S / (N - 3)))) [1.0789208 1.29678338 0.0169534 ] We indeed got the same result! Note that to get from the sum of squared residuals $S$ to the residuals variance $\\hat{\\sigma}^2$ we need to divide by the degrees of freedom: $n - k = 100 - 3$.\nAs before we have plotted the evolution of the estimate of the OLS coefficient over time, we can now augment that plot with a confidence band of +- one standard deviation.\nonline_regression(df, \u0026quot;fig/online_reg2.gif\u0026quot;, ci=True) As we can see, the estimated variance of the OLS estimator indeed decreases as the sample size increases.\nBootstrap So far we have used the asymptotic assumptions behind the Central Limit Theorem to compute the standard errors of the estimator. However, we have a particularly small sample. We further check the empirical distribution of the model residuals.\nsns.histplot(model.resid, bins=30); The residuals seem to be particularly skewed! This might be a problem in such a small sample.\nOne alternative is the bootstrap. Instead of relying on asymptotics, we approximate the distribution of our estimator by resampling our dataset with replacement. Can we bootstrap online?\nThe answer is once again yes! They key is to weight each observation with an integer weight drawn from a Poisson distribution with mean (and variance) equal to 1. We repeat this process multiple times, in parallel and then we\nThe updating rules for $(X\u0026rsquo;X)^{-1}$ and $\\hat{beta}$ become the following. $$ \\begin{align*} (X_{n+1}\u0026rsquo; X_{n+1})^{-1} = (X_n\u0026rsquo; X_n)^{-1} - \\frac{w (X_n\u0026rsquo; X_n)^{-1} x_{n+1} x_{n+1}\u0026rsquo; (X_n\u0026rsquo; X_n)^{-1}}{1 + w x_{n+1}\u0026rsquo; (X_n\u0026rsquo; X_n)^{-1} x_{n+1}} \\end{align*} $$\nand $$ \\hat{\\beta}{n+1} = \\hat{\\beta}{n} + w (X_n\u0026rsquo; X_n)^{-1} x_{n} (y_n - x_n\u0026rsquo; \\hat{\\beta}_{n}) $$\nwhere $w$ are Poisson weights. First, let\u0026rsquo;s update the updating function for $(X\u0026rsquo;X)^{-1}$ and $\\hat{beta}$.\ndef update_xbw(XiX, beta, w, x, y): XiX -= (w * XiX @ x.T @ x @ XiX) / (1 + w * x @ XiX @ x.T ) beta += w * XiX @ x.T @ (y - x @ beta) return XiX, beta We can now run the online estimation. We bootstrap 1000 different estimates of $\\hat{\\beta}$.\n# Inizialize a vector of XiXs and betas np.random.seed(0) K = 1000 x, y = xy_from_df(df, 0, 10) XiXs = [inv(x.T @ x) for k in range(K)] betas = [xix @ x.T @ y for xix in XiXs] # Update the vector of XiXs and betas online for n in range(10, N): x, y = xy_from_df(df, n, n+1) for k in range(K): w = np.random.poisson(1) XiXs[k], betas[k] = update_xbw(XiXs[k], betas[k], w, x, y) We can compute the estimated standard deviation of the treatment effect, simply by computing the standard deviation of the vector of bootstrapped coefficients.\nnp.std(betas, axis=0) array([0.95301002, 1.14186364, 0.01207962]) The estimated standard errors are slightly different from the previous values of $[1.275, 1.532, 0.020]$, but not very far apart.\nLastly, some of you might have wondered \u0026ldquo;why sampling discrete weights and not continuous ones?\u0026rdquo;. Indeed, we can. This procedure is called the Bayesian Bootstrap and you can find a more detailed explanation here.\nConclusion In this post, we have seen hot to run an experiment without storing individual-level data. How are we able to do it? In order to compute the average treatment effect, we do not need every single observation but it\u0026rsquo;s sufficient to store just a more compact representation of it.\nReferences [1] W. Chou, Randomized Controlled Trials without Data Retention (2021), Working Paper.\nRelated Articles Experiments, Peeking, and Optimal Stopping The Bayesian Bootstrap Code You can find the original Jupyter Notebook here:\nhttps://github.com/matteocourthoud/Blog-Posts/blob/main/notebooks/online_reg.ipynb\n","date":1662249600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1662249600,"objectID":"2fc5b3cb2df3b8d042d06cc9686debfd","permalink":"https://matteocourthoud.github.io/post/online_reg/","publishdate":"2022-09-04T00:00:00Z","relpermalink":"/post/online_reg/","section":"post","summary":"How to run experiments without storing individual data\nAB tests, a.k.a. randomized controlled trials, are widely recognized as the gold standard technique to compute the causal effect of a treatment (a drug, ad, product, \u0026hellip;) on an outcome of interest (a disease, firm revenue, customer satisfaction, \u0026hellip;).","tags":null,"title":"A/B Tests, Privacy and Online Regression","type":"post"},{"authors":null,"categories":null,"content":"What makes an observation \u0026ldquo;unusual\u0026rdquo;?\nIn data science, one common task is outlier detection. This is a broad term that is often misused or misunderstood. More broadly, we are often interested in understanding any observation is \u0026ldquo;unusual\u0026rdquo;. First of all, what does it mean to be unusual? In this article we are going to inspect three different ways in which an observation can be unusual: it can be unusual characteristics, it might not fit the model or it might be particularly influential in fitting the model. We will see that in linear regression the latter characteristics is a byproduct of the first two.\nImportantly, being unusual is not necessarily bad. Observations that have different characteristics from all others usually carry more information. We also expect some observations not to fit the model well, otherwise the model is likely biased (overfitting). However, \u0026ldquo;unusual\u0026rdquo; observations are also more likely to be generated by a different process. Extreme cases include measurement error or fraud, but differences can be more nuanced. Domain knowledge is always kind and dropping observations only for for statistical reasons is never wise.\nThat said, let\u0026rsquo;s have a look at some different ways in which observations can be \u0026ldquo;unusual\u0026rdquo;.\nExample Suppose we are an peer-to-peer online platform and we are interested in understanding if there is anything suspicious going on with our business. We have information about how much time our customers spend on the platform and the total value of their transactions.\nFirst, let\u0026rsquo;s have a look at the data. I import the data generating process dgp_p2p() from src.dgp and some plotting functions and libraries from src.utils. I include code snippets from Deepnote, a Jupyter-like web-based collaborative notebook environment. For our purpose, Deepnote is very handy because it allows me not only to include code but also output, like data and tables.\n%matplotlib inline %config InlineBackend.figure_format = 'retina' from src.utils import * from src.dgp import dgp_p2p df = dgp_p2p().generate_data() df.head() hours transactions 0 2.6 8.30 1 2.0 8.00 2 7.0 21.00 3 6.7 18.00 4 1.2 3.82 We have information on 50 clients for which we observe hours spent on the website and total transactions amount. Since we only have two variables we can easily inspect them using a scatterplot.\nsns.scatterplot(data=df, x='hours', y='transactions').set(title='Data Scatterplot'); The relationship between hours and transactions seems to follow a clear linear relationship. If we fit a linear model, we observe a particularly tight fit.\nsmf.ols('hours ~ transactions', data=df).fit().summary().tables[1] coef std err t P\u003e|t| [0.025 0.975] Intercept -0.0975 0.084 -1.157 0.253 -0.267 0.072 transactions 0.3452 0.009 39.660 0.000 0.328 0.363 Does any data point look suspiciously different from the others? How?\nLeverage The first metric that we are going to use to evaluate \u0026ldquo;unusual\u0026rdquo; observations is the leverage, which was first introduced by Cook (1980). The objective of the leverage is to capture how much a single point is different with respect to other data points. These data points are often called outliers and there exist a nearly amount of algorithms and rules of thumb to flag them.However the idea is the same: flagging observations that are unusual in terms of features.\nThe leverage of an observation $i$ is defined as\n$$ h_{ii} := x_i\u0026rsquo; (X\u0026rsquo;X)^{-1} x_i $$\nOne interpretation of the leverage is as a measure of distance where individual observations are compared against the average of all observations.\nAnother interpretation of the leverage is as the influence of the outcome of observation $i$, $y_i$, on the corresponding fitted value $\\hat{y_i}$.\n$$ h_{ii} = \\frac{\\partial \\hat{y}_i}{\\partial y_i} $$\nAlgebraically, the leverage of observation $i$ is the $i^{th}$ element of the design matrix $X\u0026rsquo; (X\u0026rsquo;X)^{-1} X$. Among the many properties of the leverages, is the fact that they are non-negative and their values sum to 1.\nLet\u0026rsquo;s compute the leverage of the observations in our dataset. We also flag observations that have unusual leverages (which we arbitrarily define as more than two standard deviations away from the average leverage).\nX = np.reshape(df['hours'].values, (-1, 1)) Y = np.reshape(df['transactions'].values, (-1, 1)) df['leverage'] = np.diagonal(X @ np.linalg.inv(X.T @ X) @ X.T) df['high_leverage'] = df['leverage'] \u0026gt; (np.mean(df['leverage']) + 2*np.std(df['leverage'])) Let\u0026rsquo;s plot the distribution of leverage values in our data.\nfix, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6)) sns.histplot(data=df, x='leverage', hue='high_leverage', alpha=1, bins=30, ax=ax1).set(title='Distribution of Leverages'); sns.scatterplot(data=df, x='hours', y='transactions', hue='high_leverage', ax=ax2).set(title='Data Scatterplot'); As we can see, the distribution is skewed with two observations having a unusually high leverage. Indeed, in the scatterplot these two observations are slightly separated from the rest of the distribution.\nIs this bad news? It depends. Outliers are not a problem per se. Actually, if they are genuine observations, they might carry much more information than other observations. On the other hand, they are also more likely not to be genuine observations (e.g. fraud, measurement error, \u0026hellip;) or to be inherently different from the other ones (e.g. professional users vs amateurs). In any case, we might want to investigate further and use as much context-specific information as we can.\nImportantly, the fact that an observation has a high leverage tells us information about the features of the model but nothing about the model itself. Are these users just different observations or they also behave differently?\nResiduals So far we have only talked about unusual features, but what about unusual behavior? This is what regression residuals measure.\nRegression residuals are the difference between the predicted outcome values and the observed outcome values. In a sense, they capture what the model cannot explain: the higher the residual of one observation the more it is unusual in the sense that the model cannot explain it.\nIn the case of linear regression, residuals can be written as\n$$ \\hat{e} = y - \\hat{y} = y - \\hat \\beta X $$\nIn our case, since $X$ is one dimensional (hours), we can easily visualize them.\nY_hat = X @ np.linalg.inv(X.T @ X) @ X.T @ Y plt.scatter(X, Y, s=50, label='data') plt.plot(X, Y_hat, c='k', lw=2, label='prediction') plt.vlines(X, np.minimum(Y, Y_hat), np.maximum(Y, Y_hat), color='r', lw=3, label=\u0026quot;residuals\u0026quot;); plt.legend() plt.title(f\u0026quot;Regression prediction and residuals\u0026quot;); Do some observations have unusually high residuals? Let\u0026rsquo;s plot their distribution.\ndf['residual'] = np.abs(Y - X @ np.linalg.inv(X.T @ X) @ X.T @ Y) df['high_residual'] = df['residual'] \u0026gt; (np.mean(df['residual']) + 2*np.std(df['residual'])) fix, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6)) sns.histplot(data=df, x='residual', hue='high_residual', alpha=1, bins=30, ax=ax1).set(title='Distribution of Residuals'); sns.scatterplot(data=df, x='hours', y='transactions', hue='high_residual', ax=ax2).set(title='Data Scatterplot'); Two observations have particularly high residuals. This means that for these observations, the model is not good at predicting the observed outcomes.\nIs this bad news? Not necessarily. A model that fits the observations too well is likely to be biased. However, it might still be important to understand why some users have a different relationship between hours spent and total transactions. As usual, information on the specific context is key.\nSo far we have looked at observations with \u0026ldquo;unusual\u0026rdquo; characteristics and \u0026ldquo;unusual\u0026rdquo; model fit, but what is the observation itself is distorting the model? How much our model is driven by a handful of observations?\nInfluence The concept of influence and influence functions was developed precisely to answer this question: what are influential observations? This questions were very popular in the 80\u0026rsquo;s and lost appeal for a long time until the recent need of explaining complex machine learning and AI models.\nThe general idea is to define an observation as influential if removing it significantly changes the estimated model. In linear regression, we define the influence of observation $i$ as:\n$$ \\hat{\\beta} - \\hat{\\beta}_{-i} = (X\u0026rsquo;X)^{-1} x_i e_i $$\nWhere $\\hat{\\beta}_{-i}$ is the OLS coefficient estimated omitting observation $i$.\nAs you can see, there is a tight connection to both leverage $h_{ii}$ and residuals $e_i$: influence is almost the product of the two. Indeed, in linear regression, observations with high leverage are observations that are both outliers and have high residuals. None of the two conditions alone is sufficient for an observation to have an influence on the model.\nWe can see it best in the data.\ndf['influence'] = (np.linalg.inv(X.T @ X) @ X.T).T * np.abs(Y - Y_hat) df['high_influence'] = df['influence'] \u0026gt; (np.mean(df['influence']) + 2*np.std(df['influence'])) fix, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6)) sns.histplot(data=df, x='influence', hue='high_influence', alpha=1, bins=30, ax=ax1).set(title='Distribution of Influences'); sns.scatterplot(data=df, x='hours', y='transactions', hue='high_influence', ax=ax2).set(title='Data Scatterplot'); In our dataset, there is only one observation with high influence, and it is disproportionally larger than the influence of all other observations.\nWe can now plot all \u0026ldquo;unusual\u0026rdquo; points in the same plot. I also report residuals and leverage of each point in a separate plot.\ndef plot_leverage_residuals(df): # Hue df['type'] = 'Normal' df.loc[df['high_residual'], 'type'] = 'High Residual' df.loc[df['high_leverage'], 'type'] = 'High Leverage' df.loc[df['high_influence'], 'type'] = 'High Influence' # Init figure fig, (ax1,ax2) = plt.subplots(1,2, figsize=(12,5)) ax1.plot(X, Y_hat, lw=1, c='grey', zorder=0.5) sns.scatterplot(data=df, x='hours', y='transactions', ax=ax1, hue='type').set(title='Data') sns.scatterplot(data=df, x='residual', y='leverage', hue='type', ax=ax2).set(title='Metrics') ax1.get_legend().remove() sns.move_legend(ax2, \u0026quot;upper left\u0026quot;, bbox_to_anchor=(1.05, 0.8)); plot_leverage_residuals(df) As we can see, we have one point with high residual and low leverage, one with high leverage and low residual and only one point with both high leverage and high residual: the only influential point.\nFrom the plot it is also clear why none of the two conditions alone is sufficient for an observation to rive the model. The orange point has high residual but it lies right in the middle of the distribution and therefore cannot tilt the line of best fit. The green point instead has high leverage and lies far from the center of the distribution but its perfectly aligned with the line of fit. Removing it would not change anything. The red dot instead is different from the others in terms of both characteristics and behavior and therefore tilts the fit line towards itself.\nConclusion In this post, we have seen some different ways in which observations can be \u0026ldquo;unusual\u0026rdquo;: they can have either unusual characteristics or unusual behavior. In linear regression, when an observation has both it is also influential: it tilts the model towards itself.\nIn the example of the article, we concentrated on a univariate linear regression. However, research on influence functions has recently become a hot topic because of the need to make black-box machine learning algorithms understandable. With models with millions of parameters, billions of observations and wild non-linearities, it can be very hard to establish whether a single observation is influential and how.\nReferences [1] D. Cook, Detection of Influential Observation in Linear Regression (1980), Technometrics.\n[2] D. Cook, S. Weisberg, Characterizations of an Empirical Influence Function for Detecting Influential Cases in Regression (1980), Technometrics.\n[2] P. W. Koh, P. Liang, Understanding Black-box Predictions via Influence Functions (2017), ICML Proceedings.\nCode You can find the original Jupyter Notebook here:\nhttps://github.com/matteocourthoud/Blog-Posts/blob/main/notebooks/outliers.ipynb\n","date":1660694400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1660694400,"objectID":"ad8cb00ec55acaf296aecb15acdf4385","permalink":"https://matteocourthoud.github.io/post/outliers/","publishdate":"2022-08-17T00:00:00Z","relpermalink":"/post/outliers/","section":"post","summary":"What makes an observation \u0026ldquo;unusual\u0026rdquo;?\nIn data science, one common task is outlier detection. This is a broad term that is often misused or misunderstood. More broadly, we are often interested in understanding any observation is \u0026ldquo;unusual\u0026rdquo;.","tags":null,"title":"Outliers, Leverage, and Influential Observations","type":"post"},{"authors":null,"categories":null,"content":"A short guide to a simple and powerful alternative to the bootstrap\nIn causal inference we do not want just to compute treatment effect, we also want to do inference (duh!). In some cases, it\u0026rsquo;s very easy to compute the asymptotic difference of an estimator, thanks to the central limit theorem. This is the case of computing the average treatment effect in AB tests or randomized controlled trials, for example. However, in other settings, inference is more complicated. The most frequent setting is the computation of quantities that are not sums or averages, such as the median treatment effect, for example. In these cases, we cannot rely on the central limit theorem. What can we do then?\nThe bootstrap is the answer! It is a very powerful procedure to compute the distribution of an estimator, without needing any knowledge of the data generating process. It is also very intuitive and simple to implement: just re-sample your data with replacement a lot of times and compute your estimator on the re-computed sample.\nCan we do better? The answer is yes! The Bayesian Bootstrap is a powerful procedure that in a lot of setting performs better than the bootstrap. In particular, it\u0026rsquo;s usually faster, can give tighter confidence intervals and prevents a lot of corner cases of the bootstrap. In this article we are going to explore this simple but powerful procedure more in detail.\nThe Bootstrap Bootstrap is a procedure to compute properties of an estimator by random re-sampling with replacement from the data. It was first introduced by Efron (1979). The procedure is very simple and consists in the following steps.\nSuppose you have access to an i.i.d. sample $\\lbrace X_i \\rbrace_{i=1}^n$ and you want to compute a statistic $\\theta$ using an estimator $\\hat \\theta(X)$. You can approximate the distribution of $\\hat \\theta$ by\nSample $n$ observations with replacement from your sample $\\lbrace \\tilde X_i \\rbrace_{i=1}^n$ Compute the estimator $\\hat \\theta_{bootstrap}(\\tilde X)$ Repeat steps 1 and 2 a large number of times The distribution of $\\hat \\theta_{bootstrap}$ is a good approximation of the distribution of $\\hat \\theta$.\nWhy is the bootstrap so powerful?\nFirst of all, it\u0026rsquo;s easy to implement. It does not require you to do anything more than what you were already doing: estimating $\\theta$. You just need to do it a lot of times. Indeed, the main disadvantage of the bootstrap is its computational speed. If estimating $\\theta$ once is slow, bootstrapping it is prohibitive.\nSecond, the bootstrap makes no distributional assumption. It only assumes a representative sample from your population, where observations are independent from each other. This assumption might be violated when observations are tightly connected with each other, such when studying social networks.\nIs bootstrap just weighting?\nIn the end, what we are doing is assigning integer weights to our observations, such that their sum adds up to $n$. Such distribution is the multinomial distribution.\nLet\u0026rsquo;s have a look at what a multinomial distribution look like by drawing a sample of size 10.000.\n%matplotlib inline %config InlineBackend.figure_format = 'retina' from src.utils import * N = 10_000 np.random.seed(1) bootstrap_weights = np.random.multinomial(N, np.ones(N)/N) np.sum(bootstrap_weights) 10000 First of all, we check that indeed the weights sum up to 1000, or equivalently, we generated a re-sample of the same size of the data.\nWe can now plot the distribution of weights.\nsns.countplot(bootstrap_weights, color='C0').set(title='Bootstrap Weights'); As we can see, around 3600 observations got zero weight, however a couple of observations got a weights of 6. Or equivalently, around 3600 observations did not get re-sampled while a couple of observations got samples as many as 6 times.\nNow you might have a spontaneous question: why not use continuous weights instead of discrete ones?\nVery good question! The Bayesian Bootstrap is the answer.\nThe Bayesian Bootstrap The Bayesian bootstrap was introduced by Rubin (1981) and it\u0026rsquo;s based on a very simple idea: why not draw a smoother distribution of weights? The continuous equivalent of the multinomial distribution is the Dirichelet distribution. Below I plot the probability distribution of Multinomial and Dirichelet weights for a single observation (they are Poisson and Gamma distributed, respectively).\nfrom scipy.stats import gamma, poisson x1 = np.arange(0, 8, 0.001) x2 = np.arange(0, 8, 1) sns.barplot(x2, poisson.pmf(x2, mu=1), color='C0', label='Multinomial Weights'); plt.plot(x1, gamma.pdf(x1, a=1.0001), color='C1', label='Dirichlet Weights'); plt.legend() plt.title('Distribution of Bootstrap Weights'); The Bayesian Bootstrap has many advantages.\nThe first and most intuitive one is that it delivers estimates that are much more smooth than the normal bootstrap, because of its continuous weighting scheme. Moreover, the continuous weighting scheme prevents corner cases from emerging, since no observation will ever receive zero weight. For example, in linear regression, no problem of collinearity emerges, if there wasn\u0026rsquo;t one in the original sample. Lastly, being a Bayesian method, we gain interpretation: the estimated distribution of the estimator can be interpreted as the posterior distribution with an uninformative prior. Let\u0026rsquo;s now draw a set a Dirichlet weights.\nbayesian_weights = np.random.dirichlet(alpha=np.ones(N), size=1)[0] * N np.sum(bayesian_weights) 10000.000000000005 The weights naturally sum to (approximately) 1, so we have to scale them by a factor N.\nAs before, we can plot the distribution of weights, with the difference that now we have continuous weights, so we have to approximate the distribution.\nsns.histplot(bayesian_weights, color='C1').set(title='Dirichlet Weights'); As you might have noticed, the Dirichelet distirbution has a parameter $\\alpha$ that we have set to 1 for all observations. What does it do?\nThe $\\alpha$ parameter essentially governs both the absolute and relative probability of being samples. Increasing $\\alpha$ for all observations makes the distribution less skewed so that all observations have a more similar weight. For $\\alpha \\to \\infty$, all observations receiver the same weight and we are back to the original sample.\nHow should we pick $\\alpha$? Shao and Tu (1995) suggest the following.\nThe distribution of the random weight vector does not have to be restricted to the Diri(l, \u0026hellip; , 1). Later investigations found that the weights having a scaled Diri(4, \u0026hellip; ,4) distribution give better approximations (Tu and Zheng, 1987)\nLet\u0026rsquo;s have a look at how a Dirichelet distribution with $\\alpha = 4$ for all observations compare to our previous distribution with $\\alpha = 1$ for all observations.\nbayesian_weights2 = np.random.dirichlet(np.ones(N) * 4, 1)[0] * N sns.histplot(bayesian_weights, color='C1') sns.histplot(bayesian_weights2, color='C2').set(title='Comparing Dirichlet Weights'); plt.legend([r'$\\alpha = 1$', r'$\\alpha = 4$']); The new distribution is much less skewed and more concentrated around the average value of 1.\nExamples Let\u0026rsquo;s have a look at a couple of examples, where we compare both inference procedures.\nMean of a Skewed Distribution First, let\u0026rsquo;s have a look at one of the simplest and most common estimators: the sample mean.\nnp.random.seed(2) X = pd.Series(np.random.pareto(2, 100)) sns.histplot(X).set(title='Sample from Pareto Distribution'); def classic_boot(df, estimator, seed=1): df_boot = df.sample(n=len(df), replace=True, random_state=seed) estimate = estimator(df_boot) return estimate classic_boot(X, np.mean) 0.7079805545831946 def bayes_boot(df, estimator, seed=1): np.random.seed(seed) w = np.random.dirichlet(np.ones(len(df)), 1)[0] result = estimator(df, weights=w) return result bayes_boot(X, np.average) 1.0378495251293498 from joblib import Parallel, delayed def bootstrap(boot_method, df, estimator, K): r = Parallel(n_jobs=8)(delayed(boot_method)(df, estimator, seed=i) for i in range(K)) return r def compare_boot(df, boot1, boot2, estimator, title, K=1000): s1 = bootstrap(boot1, df, estimator, K) s2 = bootstrap(boot2, df, estimator, K) df = pd.DataFrame({'Estimate': s1 + s2, 'Estimator': ['Classic']*K + ['Bayes']*K}) sns.histplot(data=df, x='Estimate', hue='Estimator') plt.legend([f'Bayes: {np.mean(s2):.2f} ({np.std(s2):.2f})', f'Classic: {np.mean(s1):.2f} ({np.std(s1):.2f})']) plt.title(f'Bootstrap Estimates of {title}') compare_boot(X, classic_boot, bayes_boot, np.average, 'Sample Mean') In this setting, both procedures give a very similar answer.\nWhich one is faster?\nimport time def compare_time(df, boot1, boot2, estimator, K=1000): t1, t2 = np.zeros(K), np.zeros(K) for k in range(K): # Classic bootstrap start = time.time() boot1(df, estimator) t1[k] = time.time() - start # Bayesian bootstrap start = time.time() boot2(df, estimator) t2[k] = time.time() - start print(f\u0026quot;Bayes wins {np.mean(t1 \u0026gt; t2)*100}% of the time (by {np.mean((t1 - t2)/t1*100):.2f}%)\u0026quot;) compare_time(X, classic_boot, bayes_boot, np.average) Bayes wins 99.8% of the time (by 82.89%) The Bayesian bootstrap is faster than the classical bootstrap 100% of the simulations, and by an impressive 83%!\nNo Weighting? No Problem What if we have an estimator that does not accept weights, such as the median? We can do two-level sampling.\ndef twolv_boot(df, estimator, seed=1): np.random.seed(seed) w = np.random.dirichlet(np.ones(len(df))*4, 1)[0] df_boot = df.sample(n=len(df)*10, replace=True, weights=w, random_state=seed) result = estimator(df_boot) return result np.random.seed(1) X = pd.Series(np.random.normal(0, 10, 1000)) compare_boot(X, classic_boot, twolv_boot, np.median, 'Sample Median') In this setting, the Bayesian Bootstrap is also more precise than the classical bootstrap.\nLogistic Regression with Rare Outcome Let\u0026rsquo;s now explore the first of two settings in which the classical bootstrap might fall into corner cases. Suppose we observed a feature $x$, normally distributed, and a binary outcome $y$. We are interested in the relationship between the two variables.\nN = 100 np.random.seed(1) x = np.random.normal(0, 1, N) y = np.rint(np.random.normal(x, 1, N) \u0026gt; 2) df = pd.DataFrame({'x': x, 'y': y}) df.head() x y 0 1.624345 0.0 1 -0.611756 0.0 2 -0.528172 0.0 3 -1.072969 0.0 4 0.865408 0.0 In this case, we observe a positive outcome only in 10 observations out of 100.\nnp.sum(df['y']) 10.0 Since the outcome is binary, we fit a logistic regression model.\nsmf.logit('y ~ x', data=df).fit(disp=False).summary().tables[1] coef std err z P\u003e|z| [0.025 0.975] Intercept -4.0955 0.887 -4.618 0.000 -5.834 -2.357 x 2.7664 0.752 3.677 0.000 1.292 4.241 Can we bootstrap the distribution of our estimator? Let\u0026rsquo;s try to compute the logistic regression coefficient over 1000 bootstrap samples.\nestimate_logit = lambda df: smf.logit('y ~ x', data=df).fit(disp=False).params[1] for i in range(1000): try: classic_boot(df, estimate_logit, seed=i) except Exception as e: print(f'Error for bootstrap number {i}: {e}') Error for bootstrap number 92: Perfect separation detected, results not available Error for bootstrap number 521: Perfect separation detected, results not available Error for bootstrap number 545: Perfect separation detected, results not available Error for bootstrap number 721: Perfect separation detected, results not available Error for bootstrap number 835: Perfect separation detected, results not available For 5 samples out of 1000, we are unable to compute the estimate. This would not have happened with then bayesian bootstrap.\nThis might seem like an innocuous issue in this case: we can just drop those observations. Let\u0026rsquo;s conclude with a much more dangerous example.\nSuppose we observed a binary feature $x$ and a continuous outcome $y$. We are again interested in the relationship between the two variables.\nN = 100 np.random.seed(1) x = np.random.binomial(1, 5/N, N) y = np.random.normal(1 + 2*x, 1, N) df = pd.DataFrame({'x': x, 'y': y}) df.head() x y 0 0 1.315635 1 0 -1.022201 2 0 0.693796 3 0 1.827975 4 0 1.230095 Let\u0026rsquo;s compare the two bootstrap estimators of the regression coefficient of $y$ on $x$.\nestimate_beta = lambda df, **kwargs: smf.wls('y ~ x', data=df, **kwargs).fit().params[1] compare_boot(df, classic_boot, bayes_boot, estimate_beta, 'beta') The classic bootstrap procedure estimates a 50% larger variance of our estimator. Why? If we look more closely, we seen that in almost 20 re-samples, we get a very unusual estimate of zero!\nThe problem is that in some samples we might not have have any observations with $x=1$. Therefore, in these re-samples, the estimated coefficient is zero. This does not happen with the Bayesian bootstrap, since it does not drop any observation.\nThe problematic part here is that we are not getting any error message or warning. This bias is very sneaky and could easily go unnoticed!\nConclusion The article was inspired by the following tweet by Brown University professor Peter Hull\nOk, so I come bearing good news for ~93% of you: esp. those bootstraping complex models (e.g. w/many FEs)\nInstead of resampling, which can be seen as reweighting by a random integer W that may be zero, you can reweight by a random non-zero non-integer W https://t.co/Rpm1GmomHg\n\u0026mdash; Peter Hull (@instrumenthull) January 29, 2022 Indeed, besides being a simple and intuitive procedure, the Bayesian Bootstrap is not part of the standard econometrics curriculum in economic graduate schools.\nReferences [1] B. Efron Bootstrap Methods: Another Look at the Jackknife (1979), The Annals of Statistics.\n[2] D. Rubin, The Bayesian Bootstrap (1981), The Annals of Statistics.\n[3] A. Lo, A Large Sample Study of the Bayesian Bootstrap (1987), The Annals of Statistics.\n[4] J. Shao, D. Tu, Jacknife and Bootstrap (1995), Springer.\nCode You can find the original Jupyter Notebook here:\nhttps://github.com/matteocourthoud/Blog-Posts/blob/main/notebooks/bayes_boot.ipynb\n","date":1660694400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1660694400,"objectID":"f44533cada8c0549787d29781ec04e23","permalink":"https://matteocourthoud.github.io/post/bayes_boot/","publishdate":"2022-08-17T00:00:00Z","relpermalink":"/post/bayes_boot/","section":"post","summary":"A short guide to a simple and powerful alternative to the bootstrap\nIn causal inference we do not want just to compute treatment effect, we also want to do inference (duh!","tags":null,"title":"The Bayesian Bootstrap","type":"post"},{"authors":null,"categories":null,"content":"A detailed introduction to one of the most popular causal inference techniques in the industry\nIt is now widely accepted that the gold standard technique to compute the causal effect of a treatment (a drug, ad, product, \u0026hellip;) on an outcome of interest (a disease, firm revenue, customer satisfaction, \u0026hellip;) is AB testing, a.k.a. randomized experiments. We randomly split a set of subjects (patients, users, customers, \u0026hellip;) into a treatment and a control group and give the treatment to the treatment group. This procedure ensures that, ex-ante, the only expected difference between the two groups is caused by the treatment.\nOne of the key assumptions in AB testing is that there is no contamination between treatment and control group. Giving a drug to one patient in the treatment group does not affect the health of patients in the control group. This might not be the case for example if we are twying to cure a contageous disease and the two groups are not isolated. In the industry, frequent violations of the contamination assumption are network effects - my utility of using a social network increases as the number of friends on the network increases - and general equilibrium effects - if I improve one product, it might decrease the sales of another similar product.\nBecause of this reason, often experiments are carried out at a sufficiently large scale so that there is no contamination across groups, such as cities, states or even countries. Then another problem arises because of the larger scale: the treatment becomes more expensive. Giving a drug to 50% of patients in a hospital is much less expensive than giving a drug to 50% of cities in a country. Therefore, often only few units are treated but often over a longer period of time.\nIn these settings, a very powerful method emerged around 10 years age: Synthetic Control. The idea of synthetic control is to exploit the temporal variation in the data instead of the cross-sectional one (across time instead of across units). This method is extremely popular in the industry - e.g. in companies like Google, Uber, Facebook, Microsoft, Amazon - because it is easy to interpret and deals with a setting that emerges often at large scales. In this post we are going to explore this technique by means of example: we will investigate the effectiveness of self-driving cars for a ride-sharing platform.\nSelf-Driving Cars Suppose you were a ride-sharing platform and you wanted to test the effect of self-driving cars in your fleet.\nAs you can imagine, there are many limitations to running an AB/test for this type of feature. First of all, it\u0026rsquo;s complicated to randomize individual rides. Second, it\u0026rsquo;s a very expensive intervention. Third, and statistically most important, you cannot run this intervention at the ride level. The problem is that there are spillover effects from treated to control units: if indeed self-driving cars are more efficient, it means that they can serve more customers in the same amount of time, reducing the customers available to normal drivers (the control group). This spillover contaminates the experiment and prevents a causal interpretation of the results.\nFor all these reasons, we select only one city. Given the synthetic vibe of the article we cannot but select\u0026hellip; (drum roll)\u0026hellip; Miami!\nI generate a simulated dataset in which we observe a panel of U.S. cities over time. The revenue data is made up, while the socio-economic variables are taken from the OECD database. I import the data generating process dgp_selfdriving() from src.dgp. I also import some plotting functions and libraries from src.utils.\n%matplotlib inline %config InlineBackend.figure_format = 'retina' from src.utils import * from src.dgp import dgp_selfdriving treatment_year = 2013 treated_city = 'Miami' df = dgp_selfdriving().generate_data(year=treatment_year, city=treated_city) df.head() city year density employment gdp population treated post revenue 0 Atlanta 2003 290 0.629761 6.4523 4.267538 False False 25.713947 1 Atlanta 2004 295 0.635595 6.5836 4.349712 False False 23.852279 2 Atlanta 2005 302 0.645614 6.6998 4.455273 False False 24.332397 3 Atlanta 2006 313 0.648573 6.5653 4.609096 False False 23.816017 4 Atlanta 2007 321 0.650976 6.4184 4.737037 False False 25.786902 We have information on the largest 46 U.S. cities for the period 2002-2019. The panel is balanced, which means that we observe all cities for all time periods. Self-driving cars were introduced in 2013.\nIs the treated unit, Miami, comparable to the rest of the sample? Let\u0026rsquo;s use the create_table_one function from Uber\u0026rsquo;s causalml package to produce a covariate balance table, containing the average value of our observable characteristics, across treatment and control groups. As the name suggests, this should always be the first table you present in causal inference analysis.\nfrom causalml.match import create_table_one create_table_one(df, 'treated', ['density', 'employment', 'gdp', 'population', 'revenue']) Control Treatment SMD Variable n 765 17 density 256.63 (172.90) 364.94 (19.61) 0.8802 employment 0.63 (0.05) 0.60 (0.04) -0.5266 gdp 6.07 (1.16) 5.12 (0.29) -1.1124 population 3.53 (3.81) 5.85 (0.31) 0.861 revenue 25.25 (2.45) 23.86 (2.39) -0.5737 As expected, the groups are not balanced: Miami is more densely populated, poorer, larger and has lower employment rate than the other cities in the US in our sample.\nWe are interested in understanding the impact of the introduction of self-driving cars on revenue.\nOne initial idea could be to analyze the data as we would in an A/B test, comparing control and treatment group. We can estimate the treatment effect as a difference in means in revenue between the treatment and control group, after the introduction of self-driving cars.\nsmf.ols('revenue ~ treated', data=df[df['post']==True]).fit().summary().tables[1] coef std err t P\u003e|t| [0.025 0.975] Intercept 26.6006 0.127 210.061 0.000 26.351 26.850 treated[T.True] -0.7156 0.859 -0.833 0.405 -2.405 0.974 The effect of self-driving cars seems to be negative but not significant.\nThe main problem here is that treatment was not randomly assigned. We have a single treated unit, Miami, and it\u0026rsquo;s hardly comparable to other cities.\nOne alternative procedure, is to compare revenue before and after the treatment, within the city of Miami.\nsmf.ols('revenue ~ post', data=df[df['city']==treated_city]).fit().summary().tables[1] coef std err t P\u003e|t| [0.025 0.975] Intercept 22.4485 0.534 42.044 0.000 21.310 23.587 post[T.True] 3.4364 0.832 4.130 0.001 1.663 5.210 The effect of self-driving cars seems to be positive and statistically significant.\nHowever, the problem of this procedure is that there might have been many other things happening after 2013. It\u0026rsquo;s quite a stretch to attribute all differences to self-driving cars.\nWe can better understand this concern if we plot the time trend of revenue over cities. First, we need to reshape the data into a wide format, with one column per city and one row per year.\ndf = df.pivot(index='year', columns='city', values='revenue').reset_index() Now, let\u0026rsquo;s plot the revenue over time for Miami and for the other cities.\ncities = [c for c in df.columns if c!='year'] df['Other Cities'] = df[[c for c in cities if c != treated_city]].mean(axis=1) def plot_lines(df, line1, line2, year, hline=True): sns.lineplot(x=df['year'], y=df[line1].values, label=line1) sns.lineplot(x=df['year'], y=df[line2].values, label=line2) plt.axvline(x=year, ls=\u0026quot;:\u0026quot;, color='C2', label='Self-Driving Cars', zorder=1) plt.legend(); plt.title(\u0026quot;Average revenue per day (in M$)\u0026quot;); Since we are talking about Miami, let\u0026rsquo;s use an appropriate color palette.\nsns.set_palette(sns.color_palette(['#f14db3', '#0dc3e2', '#443a84'])) plot_lines(df, treated_city, 'Other Cities', treatment_year) As we can see, revenue seems to be increasing after the treatment in Miami. But it\u0026rsquo;s a very volatile time series. And revenue was increasing also in the rest of the country. It\u0026rsquo;s very hard from this plot to attribute the change to self-driving case.\nCan we do better?\nSynthetic Control The answer is yes! Synthetic control allow us to do causal inference when we have as few as one treated unit and many control units and we observe them over time. The idea is simple: combine untreated units so that they mimic the behavior of the treated unit as closely as possible, without the treatment. Then use this \u0026ldquo;synthetic unit\u0026rdquo; as a control. The method first introduced by Abadie, Diamond and Hainmueller (2010) and has been called \u0026ldquo;the most important innovation in the policy evaluation literature in the last few years\u0026rdquo;. Moreover, it is widely used in the industry because of its simplicity and interpretability.\nSetting We assume that for a panel of i.i.d. subjects $i = 1, \u0026hellip;, n$ over time $t=1, \u0026hellip;,T$ we observed a set of variables $(X_{it}, D_i, Y_{it})$ that includes\na treatment assignment $D_i \\in \\lbrace 0, 1 \\rbrace$ (treated) a response $Y_{i,t} \\in \\mathbb R$ (revenue) a feature vector $X_{i,t} \\in \\mathbb R^n$ (population, density, employment and GDP) Moreover, one unit (Miami in our case) is treated at time $t^*$ (2013 in our case). We distinguish time periods before treatment and time periods after treatment.\nCrucially, treatment $D_i$ is not randomly assigned, therefore a difference in means between the treated unit(s) and the control group is not an unbiased estimator of the average treatment effect.\nThe Problem The problem is that, as usual, we do not observe the counterfactual outcome for treated units, i.e. we do not know what would have happened to them, if they had not been treated. This is known as the fundamental problem of causal inference.\nThe simplest approach, would be just to compare pre and post periods. This is called the event study approach.\nHowever, we can do better than this. In fact, even though treatment was not randomly assigned, we still have access to some units that were not treated.\nFor the outcome variable we observe the following values\n$$ Y = \\begin{bmatrix} Y^{(1)} _ {t, post} \\ \u0026amp; Y^{(0)} _ {c, post} \\newline Y^{(0)} _ {t, pre} \\ \u0026amp; Y^{(0)} _ {c, pre} \\end{bmatrix} $$\nWhere $Y^{(d)} _ {a, t}$ is the outcome of an observation at time $t$, given treatment assignment $a$ and treatment status $d$. We basically have a missing data problem since we do not observe $Y^{(0)} _ {t, post}$: what would have happened to treated units ($a=t$) without treatment ($d=0$).\nThe Solution Following Doudchenko and Inbens (2018), we can formulate an estimate of the counterfactual outcome for the treated unit as a linear combination of the observed outcomes for the control units.\n$$ \\hat Y^{(0)} _ {t, post} = \\alpha + \\sum_{i \\in c} \\beta_{i} Y^{(0)} _ {i, post} $$\nwhere\nthe constant $\\alpha$ allows for different averages between the two groups the weights $\\beta_i$ are allowed to vary across control units $i$ (otherwise, it would be a difference-in-differences) How should we choose which weights to use? We want our synthetic control to approximate the outcome as closely as possible, before the treatment. The first approach could be to define the weights as\n$$ \\hat \\beta = \\arg \\min_{\\beta} || \\boldsymbol X_{t, pre} - \\boldsymbol \\beta \\boldsymbol X_{c, pre} || = \\sqrt{ \\sum_{p} \\left( X_{t, p, pre} - \\sum_{i \\in c} \\beta_{p} X_{c, p, pre} \\right)^2 } $$\nI.e. the weights are such that they minimize the distance between observable characteristics of control units $X_c$ and the treated unit $X_t$ before the treatment.\nYou might notice a very close similarity to linear regression. Indeed, we are doing something very similar.\nIn linear regression, we usually have many units (observations), few exogenous features and one endogenous feature and we try to express the endogenous feature as a linear combination of the endogenous features, for each unit.\nWith synthetic control, we instead have many time periods (features), few control units and a single treated unit and we try to express the treated unit as a linear combination of the control units, for each time period.\nTo perform the same operation, we essentially need to transpose the data.\nAfter the swap, we compute the synthetic control weights, exactly as we would compute regression coefficients. However now one observation is a time period and one feature is a unit.\nNotice that this swap is not innocent. In linear regression we assume that the relationship between the exogenous features and the endogenous feature is the same across units, instead in synthetic control we assume that the relationship between the treated units and the control unit is the same over time.\nBack to self-driving cars Let\u0026rsquo;s go back to the data now! First, we write a synth_predict function that takes as input a model that is trained on control cities and tries to predict the outcome of the treated city, Miami, before the introduction of self-driving cars.\ndef synth_predict(df, model, city, year): other_cities = [c for c in cities if c not in ['year', city]] y = df.loc[df['year'] \u0026lt;= year, city] X = df.loc[df['year'] \u0026lt;= year, other_cities] df[f'Synthetic {city}'] = model.fit(X, y).predict(df[other_cities]) return model Let\u0026rsquo;s estimate the model via linear regression.\nfrom sklearn.linear_model import LinearRegression coef = synth_predict(df, LinearRegression(), treated_city, treatment_year).coef_ How well did we match pre-self-driving cars revenue in Miami? What is the implied effect of self-driving cars?\nWe can visually answer both questions by plotting the actual revenue in Miami against the predicted one.\nplot_lines(df, treated_city, f'Synthetic {treated_city}', treatment_year) It looks like self-driving cars had a sensible positive effect on revenue in Miami: the predicted trend is lower than the actual data and diverges right after the introduction of self-driving cars.\nOn the other hand, we are clearly overfitting: the pre-treatment predicted revenue line is perfectly overlapping with the actual data. Given the high variability of revenue in Miami, this is suspicious, to say the least.\nAnother problem concerns the weights. Let\u0026rsquo;s plot them.\ndf_states = pd.DataFrame({'city': [c for c in cities if c!=treated_city], 'ols_coef': coef}) plt.figure(figsize=(10, 9)) sns.barplot(data=df_states, x='ols_coef', y='city'); We have many negative weights, which do not make much sense from a causal inference perspective. I can understand that Miami can be expressed as a combination of 0.2 St. Louis, 0.15 Oklahoma and 0.15 Hartford. But what does it mean that Miami is -0.15 Milwaukee?\nSince we would like to interpret our synthetic control as a weighted average of untreated states, all weights should be positive and they should sum to one.\nTo address both concerns (weighting and overfitting), we need to impose some restrictions on the weights.\nExtensions Weights To solve the problems of overweighting and negative weights, Abadie, Diamond and Hainmueller (2010) propose the following weights:\n$$ \\hat \\beta = \\arg \\min_{\\beta} || \\boldsymbol X_t - \\boldsymbol \\beta \\boldsymbol X_c || = \\sqrt{ \\sum_{p} \\left( X_{t, p} - \\sum_{i \\in c} \\beta_{p} X_{c, p} \\right)^2 } \\quad \\text{s.t.} \\quad \\sum_{p} \\beta_p = 1 \\quad \\text{and} \\quad \\beta_p \\geq 0 \\quad \\forall p $$\nWhich means, a set of weights $\\beta$ such that\nweighted observable characteristics of the control group $X_c$, match the observable characteristics of the treatment group $X_t$, before the treatment\nthey sum to 1\nand are not negative.\nWith this approach we get an interpretable counterfactual as a weighted avarage of untreated units.\nLet\u0026rsquo;s write now our own objective function. I create a new class SyntheticControl() which has both a loss function, as described above, a method to fit it and predict the values for the treated unit.\nfrom toolz import partial from scipy.optimize import fmin_slsqp class SyntheticControl(): # Loss function def loss(self, W, X, y) -\u0026gt; float: return np.sqrt(np.mean((y - X.dot(W))**2)) # Fit model def fit(self, X, y): w_start = [1/X.shape[1]]*X.shape[1] self.coef_ = fmin_slsqp(partial(self.loss, X=X, y=y), np.array(w_start), f_eqcons=lambda x: np.sum(x) - 1, bounds=[(0.0, 1.0)]*len(w_start), disp=False) self.mse = self.loss(W=self.coef_, X=X, y=y) return self # Predict def predict(self, X): return X.dot(self.coef_) We can now repeat the same procedure as before, but using the SyntheticControl method instead of the simple, unconstrained LinearRegression.\ndf_states['coef_synth'] = synth_predict(df, SyntheticControl(), treated_city, treatment_year).coef_ plot_lines(df, treated_city, f'Synthetic {treated_city}', treatment_year) As we can see, now we are not overfitting anymore. The actual and predicted revenue pre-treatment are close but not identical. The reason is that the non-negativity constraint is constraining most coefficients to be zero (as Lasso does).\nIt looks like the effect is again negative. However, let\u0026rsquo;s plot the difference between the two lines to better visualize the magnitude.\ndef plot_difference(df, city, year, vline=True, hline=True, **kwargs): sns.lineplot(x=df['year'], y=df[city] - df[f'Synthetic {city}'], **kwargs) if vline: plt.axvline(x=year, ls=\u0026quot;:\u0026quot;, color='C2', label='Self-driving cars', lw=3, zorder=100) plt.legend() if hline: sns.lineplot(x=df['year'], y=0, lw=3, color='k', zorder=1) plt.title(\u0026quot;Estimated effect of self-driving cars\u0026quot;); plot_difference(df, treated_city, treatment_year) The difference is clearly positive and slightly increasing over time.\nWe can also visualize the weights to interpret the estimated counterfactual (what would have happened in Miami, without self-driving cars).\nplt.figure(figsize=(10, 9)) sns.barplot(data=df_states, x='coef_synth', y='city'); As we can see, now we are expressing revenue in Miami as a linear combination of just a couple of cities: Tampa, St. Louis and, to a lower extent, Las Vegas. This makes the whole procedure very transparent.\nInference What about inference? Is the estimate significantly different from zero? Or, more practically, \u0026ldquo;how unusual is this estimate under the null hypothesis of no policy effect?\u0026rdquo;.\nWe are going to perform a randomization/permutation test in order to answer this question. The idea is that if the policy has no effect, the effect we observe for Miami should not be significantly different from the effect we observe for any other city.\nTherefore, we are going to replicate the procedure above, but for all other cities and compare them with the estimate for Miami.\nfrom matplotlib.offsetbox import OffsetImage, AnnotationBbox fig, ax = plt.subplots() for city in cities: synth_predict(df, SyntheticControl(), city, treatment_year) plot_difference(df, city, treatment_year, vline=False, alpha=0.2, color='C1', lw=3) plot_difference(df, treated_city, treatment_year) ax.add_artist(AnnotationBbox(OffsetImage(plt.imread('fig/miami.png'), zoom=0.25), (2015, 2.7), frameon=False)); From the graph we notice two things. First, the effect for Miami is quite extreme and therefore likely not to be driven by random noise.\nSecond, we also notice that there are a couple of cities for which we cannot fit the pre-trend very well. In particular, there is a line that is sensibly lower than all others. This is expected since, for each city, we are building the counterfactual trend as a convex combination of all other cities. Cities that are quite extreme in terms of revenue are very useful to build the counterfactuals of other cities, but it\u0026rsquo;s hard to build a counterfactual for them.\nNot to bias the analysis, let\u0026rsquo;s exclude states for which we cannot build a \u0026ldquo;good enough\u0026rdquo; counterfectual, in terms of pre-treatment MSE.\n$$ MSE_{pre} = \\frac{1}{n} \\sum_{t \\in \\text{pre}} \\left( Y_t - \\hat Y_t \\right)^2 $$\nAs a rule of thumb, Abadie, Diamond and Hainmueller (2010) suggest to exclude units for which the prediction MSE is larger than twice the MSE of the treated unit.\n# Reference mse mse_treated = synth_predict(df, SyntheticControl(), treated_city, treatment_year).mse # Other mse fig, ax = plt.subplots() for city in cities: mse = synth_predict(df, SyntheticControl(), city, treatment_year).mse if mse \u0026lt; 2 * mse_treated: plot_difference(df, city, treatment_year, vline=False, alpha=0.2, color='C1', lw=3) plot_difference(df, treated_city, treatment_year) ax.add_artist(AnnotationBbox(OffsetImage(plt.imread('fig/miami.png'), zoom=0.25), (2015, 2.7), frameon=False)); After exluding extreme observations, it looks like the effect for Miami is very unusual.\nOne statistic that Abadie, Diamond and Hainmueller (2010) suggest to perform a randomization test is the ratio between pre-treatment MSE and post-treatment MSE.\n$$ \\lambda = \\frac{MSE_{post}}{MSE_{pre}} = \\frac{\\frac{1}{n} \\sum_{t \\in \\text{post}} \\left( Y_t - \\hat Y_t \\right)^2 }{\\frac{1}{n} \\sum_{t \\in \\text{pre}} \\left( Y_t - \\hat Y_t \\right)^2 } $$\nWe can compute a p-value as the number of observations with higher ratio.\nlambdas = {} for city in cities: mse_pre = synth_predict(df, SyntheticControl(), city, treatment_year).mse mse_tot = np.mean((df[f'Synthetic {city}'] - df[city])**2) lambdas[city] = (mse_tot - mse_pre) / mse_pre print(f\u0026quot;p-value: {np.mean(np.fromiter(lambdas.values(), dtype='float') \u0026gt; lambdas[treated_city]):.4}\u0026quot;) p-value: 0.04348 It seems that only $4.3%$ of the cities had a larger MSE ratio than Miami, implying a p-value of 0.043. We can visualize the distribution of the statistic under permutation with a histogram.\nfig, ax = plt.subplots() _, bins, _ = plt.hist(lambdas.values(), bins=20, color=\u0026quot;C1\u0026quot;); plt.hist([lambdas[treated_city]], bins=bins) plt.title('Ratio of $MSE_{post}$ and $MSE_{pre}$ across cities'); ax.add_artist(AnnotationBbox(OffsetImage(plt.imread('fig/miami.png'), zoom=0.25), (2.7, 1.7), frameon=False)); Indeed, the statistic for Miami is quite extreme.\nConclusion In this article, we have explored a very popoular method for causal inference when we have few treated units, but many time periods. This setting emerges often in industry settings when the treatment has to be assigned at the aggregate level and randomization might not be possible. The key idea of synthetic control is to combinate control units into one syntetic control unit to use as counterfactual to estimate the causal effect of the treatment.\nOne of the main advantages of synthetic control is that, as long as we use positive weights that are constrained to sum to one, the method avoids extrapolation: we will never go out of the support of the data. Moreover, synthetic control studies can be \u0026ldquo;pre-registered\u0026rdquo;: you can specify the weights before the study to avoid p-hacking and cherry picking. Another reason why this method is so popular in the industry is that weights make the counterfactual analysis explicit: one can look at the weights and understand which comparison we are making.\nThis method is relatively young and many extensions are appearing every year. Some notable ones are the generalyzed synthetic control by Xu (2017), the synthetic difference-in-differences by Doudchenko and Imbens (2017), the penalyzed synthetic control of Abadie e L\u0026rsquo;Hour (2020) and the matrix completion methods of Athey et al. (2021). Last but not least, if you want to have the method explained by one of its inventors, there is this great lecture by Alberto Abadie at the NBER Summer Institute freely available on Youtube.\nReferences [1] A. Abadie, A. Diamond and J. Hainmueller, Synthetic Control Methods for Comparative Case Studies: Estimating the Effect of California’s Tobacco Control Program (2010), Journal of the American Statistical Association.\n[2] A. Abadie, Using Synthetic Controls: Feasibility, Data Requirements, and Methodological Aspects (2021), Journal of Economic Perspectives.\n[3] N. Doudchenko, G. Imbens, Balancing, Regression, Difference-In-Differences and Synthetic Control Methods: A Synthesis (2017), working paper.\n[4] Y. Xu, Generalized Synthetic Control Method: Causal Inference with Interactive Fixed Effects Models (2018), Political Analysis.\n[5] A. Abadie, J. L\u0026rsquo;Hour, A Penalized Synthetic Control Estimator for Disaggregated Data (2020), Journal of the American Statistical Association.\n[6] S. Athey, M. Bayati, N. Doudchenko, G. Imbens, K. Khosravi, Matrix Completion Methods for Causal Panel Data Models (2021), Journal of the American Statistical Association.\nRelated Articles DAGs and Control Variables Matching, Weighting, or Regression? Understanding Meta Learners Code You can find the original Jupyter Notebook here:\nhttps://github.com/matteocourthoud/Blog-Posts/blob/main/notebooks/synth.ipynb\n","date":1658880000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1658880000,"objectID":"2ae39374104666b438678d37d3498d42","permalink":"https://matteocourthoud.github.io/post/synth/","publishdate":"2022-07-27T00:00:00Z","relpermalink":"/post/synth/","section":"post","summary":"A detailed introduction to one of the most popular causal inference techniques in the industry\nIt is now widely accepted that the gold standard technique to compute the causal effect of a treatment (a drug, ad, product, \u0026hellip;) on an outcome of interest (a disease, firm revenue, customer satisfaction, \u0026hellip;) is AB testing, a.","tags":null,"title":"Understanding Synthetic Control Methods","type":"post"},{"authors":null,"categories":null,"content":"Understanding and comparing different methods for conditional causal inference analysis\nAB tests or randomized controlled trials are the gold standard in causal inference. By randomly exposing units to a treatment we make sure that individuals in both groups are comparable, on average, and any difference we observe can be attributed to the treatment effect alone.\nHowever, often the treatment and control groups are not perfectly comparable. This could be due to the fact that randomization was not perfect or available. Not always we can randomize a treatment, for ethical or practical reasons. And even when we can, sometimes we do not have enough individuals or units so that differences between groups are seizable. This happens often, for example, when randomization is not done at the individual level, but at a higher level of aggregation, for example zipcodes, counties or even states.\nIn these settings, we can still recover a causal estimate of the treatment effect if we have enough information about individuals, by making the treatment and control group comparable, ex-post. In this blog post, we are going to introduce and compare different procedures to estimate causal effects in presence of imbalances between treatment and control groups that are fully observable. In particular we are going to analyze weighting, matching and regression procedures.\nExample Assume we had blog on statistics and causal inference 😇. To improve user experience, we are considering releasing a dark mode, and we would like to understand whether this new feature increases the time users spend on our blog.\nWe are not a sophisticated company, therefore we do not run an AB test but we simply release the dark mode and we observe whether users select it or not and the time they spend on the blog. We know that there might be selection: users that prefer the dark mode could have different reading preferences and this might complicate our causal analysis.\nWe can represent the data generating process with the following Directed Acyclic Graph (DAG).\nflowchart TB classDef included fill:#DCDCDC,stroke:#000000,stroke-width:2px; classDef excluded fill:#ffffff,stroke:#000000,stroke-width:2px; classDef unobserved fill:#ffffff,stroke:#000000,stroke-width:2px,stroke-dasharray: 5 5; X1((gender)) X2((age)) X3((hours)) D((dark mode)) Y((read time)) D --\u0026gt; Y X1 --\u0026gt; Y X1 --\u0026gt; D X2 --\u0026gt; D X3 --\u0026gt; Y class D,Y included; class X1,X2,X3 excluded; We generate the simulated data using the data generating process dgp_darkmode() from src.dgp. I also import some plotting functions and libraries from src.utils.\n%matplotlib inline %config InlineBackend.figure_format = 'retina' from src.utils import * from src.dgp import dgp_darkmode df = dgp_darkmode().generate_data() df.head() read_time dark_mode male age hours 0 14.4 False 0 43.0 65.6 1 15.4 False 1 55.0 125.4 2 20.9 True 0 23.0 642.6 3 20.0 False 0 41.0 129.1 4 21.5 True 0 29.0 190.2 We have informations on 300 users for whom we observe whether they select the dark_mode (the treatment), their weekly read_time (the outcome of interest) and some characteristics like gender, age and total hours previously spend on the blog.\nWe would like to estimate the effect of the new dark_mode on users\u0026rsquo; read_time. If we were runnig an AB test or randomized control trial, we could just compare users with and without the dark mode and we could attribute the difference in average reading time to the dark_mode. Let\u0026rsquo;s check what number we would get.\nnp.mean(df.loc[df.dark_mode==True, 'read_time']) - np.mean(df.loc[df.dark_mode==False, 'read_time']) -0.4446330948042103 Individuals that select the dark_mode spend on average 1.37 hours less on the blog, per week. Should we conclude that dark_mode is a bad idea? Is this a causal effect?\nWe did not randomize the dark_mode so that users that selected it might not be directly comparable with users that didn\u0026rsquo;t. Can we verify this concern? Partially. We can only check characteristics that we observe, gender, age and total hours in our setting. We cannot check if users differ along other dimensions that we don\u0026rsquo;t observe.\nLet\u0026rsquo;s use the create_table_one function from Uber\u0026rsquo;s causalml package to produce a covariate balance table, containing the average value of our observable characteristics, across treatment and control groups. As the name suggests, this should always be the first table you present in causal inference analysis.\nfrom causalml.match import create_table_one X = ['male', 'age', 'hours'] table1 = create_table_one(df, 'dark_mode', X) table1 Control Treatment SMD Variable n 151 149 age 46.01 (9.79) 39.09 (11.53) -0.6469 hours 337.78 (464.00) 328.57 (442.12) -0.0203 male 0.34 (0.47) 0.66 (0.48) 0.6732 There seems to be some difference between treatment (dark_mode) and control group. In particular, users that select the dark_mode are older, have spent less hours on the blog and they are more likely to be males.\nAnother way to visually observe all the differences at once is with a paired violinplot. The advantage of the paired violinplot is that it allows us to observe the full distribution of the variable (approximated via kernel density estimation).\ndef plot_distributions(df, X, d): df_long = df.copy()[X + [d]] df_long[X] =(df_long[X] - df_long[X].mean()) / df_long[X].std() df_long = pd.melt(df_long, id_vars=d, value_name='value') sns.violinplot(y=\u0026quot;variable\u0026quot;, x=\u0026quot;value\u0026quot;, hue=d, data=df_long, split=True).\\ set(xlabel=\u0026quot;\u0026quot;, ylabel=\u0026quot;\u0026quot;, title=\u0026quot;Normalized Variable Distribution\u0026quot;); plot_distributions(df, X, \u0026quot;dark_mode\u0026quot;) The insight of the violinplot is very similar: it seems that users that select the dark_mode are different from users that don\u0026rsquo;t.\nWhy do we care?\nIf we do not control for the observable characteristics, we are unable to estimate the true treatment effect. In short, we cannot be certain that the difference in outcome, read_time, can be attributed to the treatment, dark_mode, instead of other characteristics. For example, it could be that males read less and also prefer the dark_mode, therefore we observe a negative correlation even though dark_mode has no effect on read_time (or even positive).\nIn terms of Dyrected Acyclic Graphs, this means that we have several backdoor paths that we need to block in order for our analysis to be causal.\nflowchart TB classDef included fill:#DCDCDC,stroke:#000000,stroke-width:2px; classDef excluded fill:#ffffff,stroke:#000000,stroke-width:2px; classDef unobserved fill:#ffffff,stroke:#000000,stroke-width:2px,stroke-dasharray: 5 5; X1((gender)) X2((age)) X3((hours)) D((dark mode)) Y((read time)) D --\u0026gt; Y X1 --\u0026gt; Y X1 --\u0026gt; D X2 --\u0026gt; D X3 --\u0026gt; Y linkStyle 0 stroke:#00ff00,stroke-width:4px; linkStyle 1,2 stroke:#ff0000,stroke-width:4px; class D,Y included; class X1,X2,X3 excluded; How do we block backdoor paths? By conditioning the analysis on those intermediate variables. The conditional analysis allows us to recover the average treatment effect of the dark_mode on read_time.\nflowchart TB classDef included fill:#DCDCDC,stroke:#000000,stroke-width:2px; classDef excluded fill:#ffffff,stroke:#000000,stroke-width:2px; classDef unobserved fill:#ffffff,stroke:#000000,stroke-width:2px,stroke-dasharray: 5 5; X1((gender)) X2((age)) X3((hours)) D((dark mode)) Y((read time)) D --\u0026gt; Y X1 -.-\u0026gt; Y X1 -.-\u0026gt; D X2 --\u0026gt; D X3 --\u0026gt; Y linkStyle 0 stroke:#00ff00,stroke-width:4px; class D,Y,X1,X2,X3 included; How do we condition the analysis on gender, age and hours? We have some options:\nMatching Propensity score weighting Regression with control variables Let\u0026rsquo;s explore and compare them!\nConditional Analysis We assume that for a set of subjects $i = 1, \u0026hellip;, n$ we observed a tuple $(D_i, Y_i, X_i)$ comprised of\na treatment assignment $D_i \\in \\lbrace 0, 1 \\rbrace$ (dark_mode) a response $Y_i \\in \\mathbb R$ (read_time) a feature vector $X_i \\in \\mathbb R^n$ (gender, age and hours) Assumption 1 : unconfoundedness (or ignorability, or selection on observables)\n$$ \\big \\lbrace Y_i^{(1)} , Y_i^{(0)} \\big \\rbrace \\ \\perp \\ D_i \\ | \\ X_i $$\ni.e. conditional on observable characteristics $X$, the treatment assignment $D$ is as good as random. What we are effectively assuming is that there is no other characteristics that we do not observe that could impact both whether a user selects the dark_mode and their read_time. This is a strong assumption that is more likely to be satisfied the more individual characteristics we observe.\nAssumption 2: overlap (or common support)\n$$ \\exists \\eta \u0026gt; 0 \\ : \\ \\eta \\leq \\mathbb E \\left[ T_i = 1 \\ \\big | \\ X_i = x \\right] \\leq 1-\\eta $$\ni.e. no observation is deterministically assigned to the treatment or control group. This is a more technical assumption that basically means that for any level of gender, age or hours, there could exist an individual that select the dark_mode and one that doesn\u0026rsquo;t. Differently from the unconfoundedness assumption, the overal assumption is testable.\nMatching The first and most intuitive method to perform conditional analysis is matching.\nThe idea of matching is very simple. Since we are not sure whether, for example, male and female users are directly comparable, we do the analysis within gender. Instead of comparing read_time across dark_mode in the whole sample, we do it separately for male and female users.\ndf_gender = pd.pivot_table(df, values='read_time', index='male', columns='dark_mode', aggfunc=np.mean) df_gender['diff'] = df_gender[1] - df_gender[0] df_gender dark_mode False True diff male 0 20.318000 22.24902 1.931020 1 16.933333 16.89898 -0.034354 Now the effect of dark_mode seems reversed: it is negative for male users (-0.79) but bigger and positive for female users (+1.38), suggesting a positive aggregate effect, 1.38 - 0.79 = 0.59 (assuming equal proportion of genders)! This sign reversal is a very classical example of the Simpson\u0026rsquo;s Paradox.\nThis comparison was easy to perform for gender, since it is a binary variable. With multiple variables, potentially continuous, matching becomes much more difficult. One common strategy is to match users in the treatment group with the most similar user in the control group, using some sort of nearest neighbor algorithm. I won\u0026rsquo;t go into the algorithm details here, but we can perform the matching with the NearestNeighborMatch function from the causalml package.\nThe NearestNeighborMatch function generates a new dataset where users in the treatment group have been matched 1:1 (option ratio=1) to users in the control group.\nfrom causalml.match import NearestNeighborMatch psm = NearestNeighborMatch(replace=True, ratio=1, random_state=1) df_matched = psm.match(data=df, treatment_col=\u0026quot;dark_mode\u0026quot;, score_cols=X) Are the two groups more comparable now? We can produce a new version of the balance table.\ntable1_matched = create_table_one(df_matched, \u0026quot;dark_mode\u0026quot;, X) table1_matched Control Treatment SMD Variable n 104 104 age 41.93 (10.05) 41.85 (10.02) -0.0086 hours 206.92 (309.62) 209.48 (321.79) 0.0081 male 0.62 (0.49) 0.62 (0.49) 0.0 Now the average differences between the two groups have shrunk by at least a couple of orders of magnitude. However, note how the sample size has slightly decreased (300 $\\to$ 246) since (1) we only match treated users and (2) we are not able to find a good match for all of them.\nWe can visually inspect distributional differences with the paired violinplot.\nplot_distributions(df_matched, X, \u0026quot;dark_mode\u0026quot;) A popular way to visualize pre- and post-matching covariate balance is the balance plot that essentially displays the standardized mean differences before and after matching, for each control variable.\ndef plot_balance(t1, t2, X): df_smd = pd.DataFrame({\u0026quot;Variable\u0026quot;: X + X, \u0026quot;Sample\u0026quot;: [\u0026quot;Unadjusted\u0026quot; for _ in range(len(X))] + [\u0026quot;Adjusted\u0026quot; for _ in range(len(X))], \u0026quot;Standardized Mean Difference\u0026quot;: t1[\u0026quot;SMD\u0026quot;][1:].to_list() + t2[\u0026quot;SMD\u0026quot;][1:].to_list()}) sns.scatterplot(x=\u0026quot;Standardized Mean Difference\u0026quot;, y=\u0026quot;Variable\u0026quot;, hue=\u0026quot;Sample\u0026quot;, data=df_smd).\\ set(title=\u0026quot;Balance Plot\u0026quot;) plt.axvline(x=0, color='k', ls='--', zorder=-1, alpha=0.3); plot_balance(table1, table1_matched, X) As we can see, now all differences in observable characteristics between the two groups are essentially zero. We could also compare the distributions using other metrics or test statistics, such as the Kolmogorov-Smirnov test statistic.\nHow do we estimate the average treatment effect? We can simply do a difference in means. An equivalent way that automatically provides standard errors is to run a linear regression of the outcome, read_time, on the treatment, dark_mode.\nNote that, since we have performed the matching for each treated user, the treatment effect we are estimating is the average treatment effect on the treated (ATT), which can be different from the average treatment effect if the treated sample differs from the overall population (which is likely to be the case, since we are doing matching in the first place).\nsmf.ols(\u0026quot;read_time ~ dark_mode\u0026quot;, data=df_matched).fit().summary().tables[1] coef std err t P\u003e|t| [0.025 0.975] Intercept 17.0365 0.469 36.363 0.000 16.113 17.960 dark_mode[T.True] 1.4490 0.663 2.187 0.030 0.143 2.755 The effect is now positive, but not statistically significant.\nNote that we might have matched multiple treated users with the same untreated user, violating the independence assumption across observations and, in turn, distorting inference.\nWe have two solutions:\ncluster standard errors at the matched individual level compute standard errors via bootstrap We implement the first and cluster the standard errors by the original individual identifiers (the dataframe index).\nsmf.ols(\u0026quot;read_time ~ dark_mode\u0026quot;, data=df_matched)\\ .fit(cov_type='cluster', cov_kwds={'groups': df_matched.index})\\ .summary().tables[1] coef std err z P\u003e|z| [0.025 0.975] Intercept 17.0365 0.650 26.217 0.000 15.763 18.310 dark_mode[T.True] 1.4490 0.821 1.765 0.078 -0.160 3.058 The effect is even less statistically significant.\nPropensity Score Rosenbaum and Rubin (1983) proved a very powerful result: if the strong ignorability assumption holds, it is sufficient to condition the analysis on the probability ot treatment, the propensity score, in order to have conditional independence.\n$$ \\big \\lbrace Y_i^{(1)} , Y_i^{(0)} \\big \\rbrace \\ \\perp \\ D_i \\ | \\ X_i \\quad \\leftrightarrow \\quad \\big \\lbrace Y_i^{(1)} , Y_i^{(0)} \\big \\rbrace \\ \\perp \\ D_i \\ | \\ e(X_i) $$\nWhere $e(X_i)$ is the probability of treatment of individual $i$, given the observable characteristics $X_i$.\n$$ e(x) = \\Pr \\left( D_i = 1 \\ \\big | \\ X_i = x \\right) $$\nNote that in an AB test the propensity score is constant across individuals.\nThe result from Rosenbaum and Rubin (1983) is incredibly powerful and practical, since the propensity score is a one dimensional variable, while $X$ might be very high dimensional.\nUnder the unconfoundedness assumption introduced above, we can rewrite the average treatment effect as\n$$ \\tau(x) = \\mathbb E \\left[ Y^{(1)} - Y^{(0)} \\ \\big| \\ X = x \\right] = \\mathbb E \\left[ \\frac{D_i Y_i}{e(X_i)} - \\frac{(1-D_i) Y_i}{1-e(X_i)} \\right] $$\nNote that this formulation of the average treatment effect does not depend on the potential outcomes $Y_i^{(1)}$ and $Y_i^{(0)}$, but only on the observed outcomes $Y_i$.\nThis formulation of the average treatment effect implies the Inverse Propensity Weighted (IPW) estimator which is an unbiased estimator for the average treatment effect $\\tau$.\n$$ \\hat \\tau^{IPW} = \\frac{1}{n} \\sum _ {i=1}^{n} \\left( \\frac{D_i Y_i}{e(X_i)} - \\frac{(1-D_i) Y_i}{1-e(X_i)} \\right) $$\nThis estimator is unfeasible since we do not observe the propensity scores $e(X_i)$. However, we can estimate them. Actually, Imbens, Hirano, Ridder (2003) show that you should use the estimated propensity scores even if you knew the true values (for example because you know the sampling procedure). The idea is that if the estimated propensity scores are different from the true ones, this can be informative in the estimation.\nThere are several possible ways to estimate a probability, the simplest and most common one being logistic regression.\nfrom sklearn.linear_model import LogisticRegressionCV df[\u0026quot;pscore\u0026quot;] = LogisticRegressionCV().fit(y=df[\u0026quot;dark_mode\u0026quot;], X=df[X]).predict_proba(df[X])[:,1] It is best practice, whenever we fit a prediction model, to fit the model on a different sample with respect to the one that we use for inference. This practice is usually called cross-validation or cross-fitting. One of the best (but computationally expensive) cross-validation procedures is leave-one-out (LOO) cross-fitting: when predicting the value of observation $i$ we use all observations except for $i$. We implement the LOO cross-fitting procedure using the cross_val_predict and LeaveOneOut functions from the sklearn package.\nfrom sklearn.model_selection import cross_val_predict, LeaveOneOut df['pscore'] = cross_val_predict(estimator=LogisticRegressionCV(), X=df[X], y=df[\u0026quot;dark_mode\u0026quot;], cv=LeaveOneOut(), method='predict_proba', n_jobs=-1)[:,1] An important check to perform after estimating propensity scores is plotting them, across the treatment and control groups. First of all, we can then observe whether the two groups are balanced or not, depending on how close the two distributions are. Moreover, we can also check how likely it is that the overlap assumption is satisfied. Ideally both distributions should span the same interval.\nsns.histplot(data=df, x='pscore', hue='dark_mode', bins=30, stat='density', common_norm=False).\\ set(ylabel=\u0026quot;\u0026quot;, title=\u0026quot;Distribution of Propensity Scores\u0026quot;); As expected, the distribution of propensity scores between the treatment and control group is significantly different, suggesting that the two groups are hardly comparable. However, there is significant overlap in the support of the distributions, suggesting that the overlap assumption is likely to be satisfied.\nHow do we estimate the average treatment effect?\nOnce we have computed the propensity scores, we just need to re-weight observations by their respective propensity score. We can then either compute a difference between the weighted read_time averages, or run a weighted regression of read_time on dark_mode.\nw = 1 / (df[\u0026quot;pscore\u0026quot;] * df[\u0026quot;dark_mode\u0026quot;] + (1-df[\u0026quot;pscore\u0026quot;]) * (1-df[\u0026quot;dark_mode\u0026quot;])) smf.wls(\u0026quot;read_time ~ dark_mode\u0026quot;, weights=w, data=df).fit().summary().tables[1] coef std err t P\u003e|t| [0.025 0.975] Intercept 18.5859 0.412 45.110 0.000 17.775 19.397 dark_mode[T.True] 1.1303 0.582 1.942 0.053 -0.015 2.276 The effect of the dark_mode is now positive and almost statistically significant, at the 5% level.\nNote that the wls function automatically normalizes weights so that they sum to 1, which greatly improves the stability of the estimator. In fact, the unnormalized IPW estimator can be very unstable when the propensity scores approach zero or one.\nAlso note that the standard errors are not correct, since they do not take into account the extra uncertainty introduced in the estimation of the propensity score. This issue was noted by Abadie and Imbens (2016).\nRegression with Control Variables The last method we are going to review today is linear regression with control variables. This estimator is extremely easy to implement, since we just need to add the user characteristics - gender, age and hours - to the regression of read_time on dark_mode.\nsmf.ols(\u0026quot;read_time ~ dark_mode + male + age + hours\u0026quot;, data=df).fit().summary().tables[1] coef std err t P\u003e|t| [0.025 0.975] Intercept 16.8591 1.082 15.577 0.000 14.729 18.989 dark_mode[T.True] 1.3858 0.524 2.646 0.009 0.355 2.417 male -4.4855 0.499 -8.990 0.000 -5.468 -3.504 age 0.0513 0.022 2.311 0.022 0.008 0.095 hours 0.0043 0.001 8.427 0.000 0.003 0.005 The average treatment effect is again positive and statistically significant at the 1% level!\nComparison How do the different methods compare to each other?\nIPW and Regression There is a tight connection between the IPW estimator and linear regression with covariates. This is particularly evident when we have a one-dimensional, discrete covariate $X$.\nIn this case, the estimand of IPW (i.e. the quantity that IPW estimates) is given by\n$$ \\tau^{IPW} = \\frac{ \\sum_x \\color{red}{\\tau_x} \\color{blue}{\\Pr(D_i | X_i = x)} \\Pr(X_i = x)}{\\sum_x \\color{blue}{\\Pr(D_i | X_i = x)} \\Pr(X_i = x)} $$\nThe IPW estimand is a weighted average of the treatment effects $\\tau_x$, where the weights are given by the treatment probabilities.\nOn the other hand, the estimand of linear regression with control variables is\n$$ \\tau^{OLS} = \\frac{ \\sum_x \\color{red}{\\tau_x} \\color{blue}{\\Pr(D_i | X_i = x)(1 - \\Pr(D_i | X_i = x)) } \\Pr(X_i = x)}{\\sum_x \\color{blue}{\\Pr(D_i | X_i = x)(1 - \\Pr(D_i | X_i = x)) } \\Pr(X_i = x)} $$\nThe OLS estimand is a weighted average of the treatment effects $\\tau_x$, where the weights are given by the variances of the treatment probabilities. This means that linear regression is a weighted estimator, that gives more weight to observations that have characteristics for which we observe more treatment variability. Since a binary random variable has the highest variance when its expected value is 0.5, OLS gives the most weight to observations that have characteristics for which we observe a 50/50 split between treatment and control group. On the other hand, if for some characteristics we only observe treated or untreated individuals, those observations are going to receive zero weight. I recommend Chapter 3 of Angrist and Pischke (2009) for more details.\nIPW and Matching As we have seen in the IPW section, Rosenbaum and Rubin (1983) result tells us that we do not need to perform the analysis conditional on all the covariates $X$, but it is sufficient to condition on the propensity score $e(X)$.\nWe have seed how this result implies a weighted estimator but it also extends to matching: we do not need to match observations on all the covariates $X$, but it is sufficient to match them on the propensity score $e(X)$. This method is called propensity score matching.\npsm = NearestNeighborMatch(replace=False, random_state=1) df_ipwmatched = psm.match(data=df, treatment_col=\u0026quot;dark_mode\u0026quot;, score_cols=['pscore']) As before, after matching, we can simply compute the estimate as a difference in means, remembering that observations are not independent and therefore we need to be cautious when doing inference.\nsmf.ols(\u0026quot;read_time ~ dark_mode\u0026quot;, data=df_ipwmatched)\\ .fit(cov_type='cluster', cov_kwds={'groups': df_ipwmatched.index})\\ .summary().tables[1] coef std err z P\u003e|z| [0.025 0.975] Intercept 18.4633 0.505 36.576 0.000 17.474 19.453 dark_mode[T.True] 1.1888 0.703 1.692 0.091 -0.188 2.566 The estimated effect of dark_mode is positive, significant at the 1% level and very close to the true value of 2!\nConclusion In this blog post, we have seen how to perform conditional analysis using different approached. Matching directly matches most similar units in the treatment and control group. Weighting simply assigns different weight to different observations depending on their probability of receiving the treatment. Regression instead weights observations depending on the conditional treatment variances, giving more weight to observations that have characteristics common to both the treatment and control group.\nThese procedures are extremely helpful because they can either allow us to estimate causal effects from (very rich) observational data or correct experimental estimates when randomization was not perfect or we have a small sample.\nLast but not least, if you want to know more, I strongly recommend this video lecture on propensity scores from Paul Goldsmith-Pinkham that is freely available online.\nThe whole course is a gem and it is an incredible privilege to have such high quality material available online for free!\nReferences [1] P. Rosenbaum, D. Rubin, The central role of the propensity score in observational studies for causal effects (1983), Biometrika.\n[2] G. Imbens, K. Hirano, G. Ridder, Efficient Estimation of Average Treatment Effects Using the Estimated Propensity Score (2003), Econometrica.\n[3] J. Angrist, J. S. Pischke, Mostly harmless econometrics: An Empiricist\u0026rsquo;s Companion (2009), Princeton University Press.\nRelated Articles Understanding The Frisch-Waugh-Lovell Theorem How to Compare Two or More Distributions DAGs and Control Variables Code You can find the original Jupyter Notebook here:\nhttps://github.com/matteocourthoud/Blog-Posts/blob/main/notebooks/ipw.ipynb\n","date":1658102400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1658102400,"objectID":"ba2c687b6581266e5183062fa3b09eff","permalink":"https://matteocourthoud.github.io/post/ipw/","publishdate":"2022-07-18T00:00:00Z","relpermalink":"/post/ipw/","section":"post","summary":"Understanding and comparing different methods for conditional causal inference analysis\nAB tests or randomized controlled trials are the gold standard in causal inference. By randomly exposing units to a treatment we make sure that individuals in both groups are comparable, on average, and any difference we observe can be attributed to the treatment effect alone.","tags":null,"title":"Weighting, Matching, or Regression?","type":"post"},{"authors":null,"categories":null,"content":"How to use machine learning to estimate heterogeneous treatment effects\nIn many settings, we are not just interested in understanding a causal effect, but also whether this effect is different for different users. We might be interested in understanding if a drug has side effects that are different for people of different age. Or we might be interested in understanding if an ad campaign is particularly effective in certain geographical areas.\nThis knowledge is crucial because it allows us to target the treatment. If a drug has severe side effects for kids, we might want to restrict its distribution only to adults. Or if an ad campaign is effective only in English-speaking countries it is not worth showing it elsewhere.\nIn this blog post we are going to explore some approaches to uncover treatment effect heterogeneity. In particular, we are going to explore methods that try to leverage the flexibility of machine learning algorithms.\nExample Suppose we were a company interested in understanding how much a new premium feature increases revenue. In particular, we know that users of different age have different spending attitudes and we suspect that the impact of the premium feature could also be different depending on the age of the user.\nThis information might be very important, for example for advertisement targeting or discount design. If we discover that the premium feature increases revenue for a particular set of users, we might want to target advertisement towards that group, or offer them personalized discounts.\nTo understand the effect of the premium feature on revenue, the run an AB test in which we randomly give access to the premium feature to 10% of the users. The feature is expensive and we cannot afford to give it for free to more users. Hopefully a 10% treatment probability is enough.\nWe generate the simulated data using the data generating process dgp_premium() from src.dgp. I also import some plotting functions and libraries from src.utils.\n%matplotlib inline %config InlineBackend.figure_format = 'retina' from src.utils import * from src.dgp import dgp_premium dgp = dgp_premium() df = dgp.generate_data(seed=5) df.head() revenue premium age 0 10.62 True 27.32 1 10.35 True 54.57 2 10.13 False 26.68 3 9.97 False 56.58 4 10.16 False 38.51 We have data on 300 users, for whom we observe the revenue they generate and whether they were given the premium feature. Moreover, we also record the age of the users.\nTo understand whether randomization worked, we use the create_table_one function from Uber\u0026rsquo;s causalml package to produce a covariate balance table, containing the average value of our observable characteristics, across treatment and control groups. As the name suggests, this should always be the first table you present in causal inference analysis.\nfrom causalml.match import create_table_one create_table_one(df, 'premium', ['age', 'revenue']) Control Treatment SMD Variable n 269 31 age 39.01 (12.11) 38.43 (13.26) -0.0454 revenue 10.04 (0.16) 10.56 (0.23) 2.5905 Most users are in the control group and only 31 users have received the premium feature. Average age is comparable across groups (SMD\u0026lt;1), while it seems that the premium feature increases revenue by 2.6$ per user, on average.\nDoes the effect of the premium feature differ by age?\nOne simple approach could me to regress revenue on a full interaction of premium and age.\nlinear_model = smf.ols('revenue ~ premium * age', data=df).fit() linear_model.summary().tables[1] coef std err t P\u003e|t| [0.025 0.975] Intercept 10.0244 0.034 292.716 0.000 9.957 10.092 premium[T.True] 0.5948 0.099 6.007 0.000 0.400 0.790 age 0.0005 0.001 0.570 0.569 -0.001 0.002 premium[T.True]:age -0.0021 0.002 -0.863 0.389 -0.007 0.003 The interaction coefficient is close to zero and not significant. It seems that there is not a different effect of premium by age. But is it true? The interaction coefficient only captures linear relationships. What if the relationship is non-linear?\nWe can check it by directly plotting the raw data. We plot revenue by age, splitting the data between premium users and non-premium users.\nsns.scatterplot(data=df, x='age', y='revenue', hue='premium', s=40); From the raw data, it looks like revenue is generally higher for people between 30 and 40 and premium has a particularly strong effect for people between 35 and 45/50.\nWe can visualize the estimated revenue by age with and without treatment.\ndef plot_TE(df, true_te=False): sns.scatterplot(data=df, x='age', y='revenue', hue='premium', s=40, legend=True) sns.lineplot(df['age'], df['mu0_hat'], label='$\\mu_0$') sns.lineplot(df['age'], df['mu1_hat'], label='$\\mu_1$') if true_te: plt.fill_between(df['age'], df['y0'], df['y0'] + df['y1'], color='grey', alpha=0.2, label=\u0026quot;True TE\u0026quot;) plt.title('Distribution of revenue by age and premium status') plt.legend(title='Treated') We first compute the predicted revenue with ($\\mu_1$) and without premium subscription ($\\mu_0$) and we plot them together with the raw data.\ndf['mu0_hat'] = linear_model.predict(df.assign(premium=0)) df['mu1_hat'] = linear_model.predict(df.assign(premium=1)) plot_TE(df) As we can see, the orange line is higher than the blue line, suggesting a positive effect of premium on revenue. However, the two lines are essentially parallel, suggesting no heterogeneity in treatment effects.\nCan we be more precise? Is there a way to estimate this treatment heterogeneity in a flexible way, without assuming functional forms?\nThe answer is yes! We can use machine learning methods to flexibly estimate heterogeneous treatment effects. In particular, in this blog post we are going to inspect three and popular methods that were introduced by Künzel, Sekhon, Bickel, Yu, (2019):\nS-learner T-learner X-learner Setting We assume that for a set of i.i.d. subjects $i = 1, \u0026hellip;, n$ we observed a tuple $(X_i, D_i, Y_i)$ comprised of\na treatment assignment $T_i \\in \\lbrace 0, 1 \\rbrace$ (premium) a response $Y_i \\in \\mathbb R$ (revenue) a feature vector $X_i \\in \\mathbb R^n$ (age) We are interested in estimating the average treatment effect.\n$$ \\tau = \\mathbb E \\Big[ Y_i^{(1)} - Y_i^{(0)} \\Big] $$\nWhere $Y_i^{(d)}$ indicates the potential outcome of individual $i$ under treatment status $d$. We also make the following assumptions.\nAssumption 1 : unconfoundedness (or ignorability, or selection on observables)\n$$ \\big \\lbrace Y_i^{(1)} , Y_i^{(0)} \\big \\rbrace \\ \\perp \\ D_i \\ | \\ X_i $$\ni.e. conditional on observable characteristics $X$, the treatment assignment $D$ is as good as random. What we are effectively assuming is that there is no other characteristics that we do not observe that could impact both whether a user selects the dark_mode and their read_time. This is a strong assumption that is more likely to be satisfied the more individual characteristics we observe.\nAssumption 2: stable unit treatment value (SUTVA)\n$$ Y^{(d)} \\perp D $$\ni.e. the potential outcome does not depend on the treatment status. In our case, we are ruling out the fact that another user selecting the premium feature might affect my effect of premium on revenue. The most common setting where SUTVA is violated is in presence of network effects: if a friend of mine uses a social network increases my utility from using it.\nS-Learner The simplest meta-algorithm is the single learner or S-learner. To build the S-learner estimator, we fit a single model for all observations.\n$$ \\mu(z) = \\mathbb E \\left[ Y_i \\ \\big | \\ (X_i, D_i) = z \\right] $$\nthe estimator is given by the difference between the predicted values evaluated with and without the treatment, $d=1$ and $d=0$.\n$$ \\hat \\tau_{S} (x) = \\hat \\mu(x,1) - \\hat \\mu(x,0) $$\ndef S_learner(dgp, model, y, D, X): temp = dgp.generate_data(true_te=True).sort_values(X) mu = model.fit(temp[X + [D]], temp[y]) temp['mu0_hat'] = mu.predict(temp[X + [D]].assign(premium=0)) temp['mu1_hat'] = mu.predict(temp[X + [D]].assign(premium=1)) plot_TE(temp, true_te=True) Let\u0026rsquo;s use a decision tree regression model to build the the S-learner, using the DecisionTreeRegressor function from the sklearn package. I won\u0026rsquo;t go into details about decision trees here, but I will just say that it\u0026rsquo;s a non-parametric estimator that uses the training data to split the state space (premium and age in our case) into blocks and predicts the outcome (revenue in our case) as its average value within block.\nfrom sklearn.tree import DecisionTreeRegressor model = DecisionTreeRegressor(min_impurity_decrease=0.001) S_learner(dgp, model, y=\u0026quot;revenue\u0026quot;, D=\u0026quot;premium\u0026quot;, X=[\u0026quot;age\u0026quot;]) The plot depicts the data together with the response functions $\\hat \\mu(x,1)$ and $\\hat \\mu(x,0)$. I have also plotted in grey the area between the true response functions: the true treatment effects.\nAs we can see, the S-learner is flexible enough to understand that there is a difference in levels between treatment and control group (we have two separate lines). It also captures well the response function for the control group, $\\hat \\mu(x,0)$, but not so well the control function for the treatment group, $\\hat \\mu(x,1)$.\nThe problem with the S-learner is that it is learning a single model so we have to hope that the model uncovers heterogeneity in the treatment $D$, but it might not be the case. Moreover, if the model is heavily regularized because of the high dimensionality of $X$, it might not recover any treatment effect. For example, with decision trees, we might not split on the treatment $D$.\nT-learner To build the two-learner or T-learner estimator, we fit two different models, one for treated units and one for control units.\n$$ \\mu^{(1)}(x) = \\mathbb E \\left[ Y_i \\ \\big | \\ X_i = x, T_i = 1 \\right] \\qquad ; \\qquad \\mu^{(0)}(x) = \\mathbb E \\left[ Y_i \\ \\big | \\ X_i = x, T_i = 0 \\right] $$\nthe estimator is given by the difference between the predicted values of the two algorithms.\n$$ \\hat \\tau_{T} (x) = \\hat \\mu^{(1)}(x) - \\hat \\mu^{(0)}(x) $$\ndef T_learner(df, model, y, D, X): temp = dgp.generate_data(true_te=True).sort_values(X) mu0 = model.fit(temp.loc[temp[D]==0, X], temp.loc[temp[D]==0, y]) temp['mu0_hat'] = mu0.predict(temp[X]) mu1 = model.fit(temp.loc[temp[D]==1, X], temp.loc[temp[D]==1, y]) temp['mu1_hat'] = mu1.predict(temp[X]) plot_TE(temp, true_te=True) We use a decision tree regression model as before but, this time, we fit two separate decision trees for the treatment and control group.\nT_learner(dgp, model, y=\u0026quot;revenue\u0026quot;, D=\u0026quot;premium\u0026quot;, X=[\u0026quot;age\u0026quot;]) As we can see, the T-learner is much more flexible than the S-learner because it fits two separate models. The response function for the control group, $\\hat \\mu(x,0)$, is still very accurate and the response function for the treatment group, $\\hat \\mu(x,1)$, is more flexible than before.\nThe problem now is that we are using just a fraction of the data for each prediction problem, while the S-learner was using all the data. By fitting two separate models we are losing some information. Moreover, by using two different models we might get heterogeneity where there is none. For example, with decision trees, we will probably get different splits with different samples even if the data generating process is the same.\nX-learner The cross-learner or X-learner estimator is an extension of the T-learner estimator. It is built in the following way:\nAs for the T-learner, compute separate models for $\\mu^{(1)}(x)$ and $\\mu^{(0)}(x)$ using the treated and control units, respectively\nCompute the estimated treatment effects as\n$$ \\Delta_i (x) = \\begin{cases} Y_i - \\hat \\mu^{(0)}(x) \u0026amp;\\quad \\text{ if } D_i = 1 \\newline \\hat \\mu^{(1)}(x) - Y_i \u0026amp;\\quad \\text{ if } D_i = 0 \\end{cases} $$\nPredicting $\\Delta$ from $X$, compute $\\hat \\tau^{(0)}(x)$ from treated units and $\\hat \\tau^{(1)}(x)$ from control units\nEstimate the propensity score\n$$ e(x) = \\mathbb E \\left[ D_i = 1 \\ \\big | \\ X_i = x \\right] $$\nCompute the treatment effects $$ \\hat \\tau_X(x) = \\hat \\tau^{(0)}(x) \\hat e(x) + \\hat \\tau^{(1)}(x) (1 - \\hat e(x)) $$\nCan we still recover pseudo response functions? Yes!\nWhich we can rewrite the treatment effects as\n$$ \\begin{align} \\hat \\tau_X(x) \u0026amp; = \\hat \\tau^{(0)}(x) \\hat e(x) + \\hat \\tau^{(1)}(x) (1 - \\hat e(x)) = \\newline \u0026amp;= \\hat e(x) \\left[ \\hat \\mu^{(1)}(x) - Y_i^{(0)} \\right] + (1 - \\hat e(x)) \\left[ Y_i^{(1)} - \\hat \\mu^{(0)}(x) \\right] = \\newline \u0026amp;= \\left[ \\hat e(x) \\hat \\mu^{(1)}(x) + (1 - \\hat e(x)) Y_i^{(1)} \\right] - \\left[ \\hat e(x) Y_i^{(0)} + (1 - \\hat e(x)) \\hat \\mu^{(0)}(x) \\right] \\end{align} $$\nSo that the pseudo response functions estimated by the X-learner are\n$$ \\begin{align} \\tilde \\mu_i^{(1)} (x) \u0026amp;= \\hat e(x) \\hat \\mu^{(1)}(x) + (1 - \\hat e(x)) Y_i^{(1)} \\newline \\tilde \\mu_i^{(0)} (x) \u0026amp;= \\hat e(x) Y_i^{(0)} + (1 - \\hat e(x)) \\hat \\mu^{(0)}(x) \\end{align} $$\nAs we can see, the X-learner combines the true values $Y_i^{(d)}$ with the estimated ones $\\mu_i^{(d)} (x)$ weighting by the propensity scores $e_i(x)$, i.e. the estimated treatment probabilities.\nWhat does it mean? It means that if we have many more observations for one group (in our case the control group), the control response function $\\hat \\mu^{(0)}(x) $ will get most of the weight. Instead, for the other group (the treatment group in our case), the actual observations $Y_i^{(1)}$ will get most of the weight.\nTo illustrate the method, I am going to build pseudo response functions by approximating $Y_i^{(d)}$ using the nearest observation, using the KNeighborsRegressor function. I estimate the propensity scores via logistic regression using the LogisticRegressionCV function.\nfrom sklearn.neighbors import KNeighborsRegressor from sklearn.linear_model import LogisticRegressionCV def X_learner(df, model, y, D, X): temp = dgp.generate_data(true_te=True).sort_values(X) # Mu mu0 = model.fit(temp.loc[temp[D]==0, X], temp.loc[temp[D]==0, y]) temp['mu0_hat_'] = mu0.predict(temp[X]) mu1 = model.fit(temp.loc[temp[D]==1, X], temp.loc[temp[D]==1, y]) temp['mu1_hat_'] = mu1.predict(temp[X]) # Y y0 = KNeighborsRegressor(n_neighbors=1).fit(temp.loc[temp[D]==0, X], temp.loc[temp[D]==0, y]) temp['y0_hat'] = y0.predict(temp[X]) y1 = KNeighborsRegressor(n_neighbors=1).fit(temp.loc[temp[D]==1, X], temp.loc[temp[D]==1, y]) temp['y1_hat'] = y1.predict(temp[X]) # Weight e = LogisticRegressionCV().fit(y=temp[D], X=temp[X]).predict_proba(temp[X])[:,1] temp['mu0_hat'] = e * temp['y0_hat'] + (1-e) * temp['mu0_hat_'] temp['mu1_hat'] = (1-e) * temp['y1_hat'] + e * temp['mu1_hat_'] # Plot plot_TE(temp, true_te=True) X_learner(df, model, y=\u0026quot;revenue\u0026quot;, D=\u0026quot;premium\u0026quot;, X=[\u0026quot;age\u0026quot;]) As we can clearly see from this graph, the main advantage of X-learners is that it adapts the flexibility of the response functions to the context. In areas of the state space where we have a lot of data, it mostly uses the estimated response function, in areas of the state space with few data, it uses the observation themselves.\nConclusion In this post, we have seen different estimators introduced by Künzel, Sekhon, Bickel, Yu, (2019) that leverage flexible machine learning algorithms to estimate heterogeneous treatment effects. The estimators differ for their degree of sophistication: the S-learner fits a single estimator including the treatment indicator as a covariate. The T-learner fits two separate estimators for the treatment and control group. Lastly, the X-learner is an extension of the T-learner that allows for different degrees of flexibility depending on the amount of data available across treatment and control groups.\nEstimation of heterogeneous treatment effect is extremely important for treatment targeting. Indeed, there is now a growing literature that exploits machine learning methods to get flexible estimates without imposing functional form assumptions. Among the many, it\u0026rsquo;s important to mention the R-learner procedure of Nie and Wager (2021) and the causal trees and forests of Athey and Wager (2018). I might write more about these procedures in the future so, stay tuned ☺️\nReferences [1] S. Künzel, J. Sekhon, P. Bickel, B. Yu, Metalearners for estimating heterogeneous treatment effects using machine learning (2019), PNAS.\n[2] X. Nie, S. Wager, Quasi-oracle estimation of heterogeneous treatment effects (2021), Biometrika.\n[3] S. Athey, S. Wager, Estimation and Inference of Heterogeneous Treatment Effects using Random Forests (2018), Journal of the American Statistical Association.\nRelated Articles Matching, Weighting, or Regression? DAGs and Control Variables Code You can find the original Jupyter Notebook here:\nhttps://github.com/matteocourthoud/Blog-Posts/blob/main/notebooks/meta.ipynb\n","date":1657843200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1657843200,"objectID":"4b468992bbd9752fb359bb79c0adfa35","permalink":"https://matteocourthoud.github.io/post/meta/","publishdate":"2022-07-15T00:00:00Z","relpermalink":"/post/meta/","section":"post","summary":"How to use machine learning to estimate heterogeneous treatment effects\nIn many settings, we are not just interested in understanding a causal effect, but also whether this effect is different for different users.","tags":null,"title":"Understanding Meta Learners","type":"post"},{"authors":null,"categories":null,"content":"%matplotlib inline %config InlineBackend.figure_format = 'retina' from src.utils import * from src.dgp import dgp_compare df = dgp_compare().generate_data(include_beta=True) df.head() outcome treated male age income beta 0 38.68 False 0 28.0 1514.0 2.726425 1 37.81 False 1 47.0 1524.0 2.777192 2 27.70 False 0 60.0 2683.0 0.247312 3 29.56 False 0 29.0 3021.0 2.632797 4 36.83 False 0 25.0 1859.0 1.724331 np.mean(df['beta']) 2.0 Simple difference smf.ols('outcome ~ treated', data=df).fit().summary().tables[1] coef std err t P\u003e|t| [0.025 0.975] Intercept 30.8496 0.091 340.817 0.000 30.672 31.027 treated[T.True] -0.0060 0.118 -0.051 0.959 -0.238 0.226 Regression with control smf.ols('outcome ~ treated + male + age + income', data=df).fit().summary().tables[1] coef std err t P\u003e|t| [0.025 0.975] Intercept 29.5761 0.272 108.833 0.000 29.043 30.109 treated[T.True] 2.2682 0.129 17.623 0.000 2.016 2.520 male 4.8568 0.104 46.688 0.000 4.653 5.061 age -0.1215 0.006 -21.752 0.000 -0.132 -0.111 income 0.0014 9e-05 16.032 0.000 0.001 0.002 Matching from causalml.match import NearestNeighborMatch, create_table_one X = ['male', 'age', 'income'] psm = NearestNeighborMatch(replace=True, ratio=1, random_state=42) df_matched = psm.match(data=df, treatment_col=\u0026quot;treated\u0026quot;, score_cols=X) smf.ols('outcome ~ treated', data=df_matched).fit().summary().tables[1] coef std err t P\u003e|t| [0.025 0.975] Intercept 29.5347 0.078 380.105 0.000 29.382 29.687 treated[T.True] 1.6918 0.110 15.396 0.000 1.476 1.907 IPW df[\u0026quot;pscore\u0026quot;] = smf.logit(\u0026quot;np.rint(treated) ~ male + age + income\u0026quot;, data=df).fit(disp=False).predict() w = 1 / (df[\u0026quot;pscore\u0026quot;] * df[\u0026quot;treated\u0026quot;] + (1-df[\u0026quot;pscore\u0026quot;]) * (1-df[\u0026quot;treated\u0026quot;])) smf.wls(\u0026quot;outcome ~ treated\u0026quot;, weights=w, data=df).fit().summary().tables[1] coef std err t P\u003e|t| [0.025 0.975] Intercept 30.2572 0.083 362.783 0.000 30.094 30.421 treated[T.True] 1.6806 0.116 14.429 0.000 1.452 1.909 R Learner from causalml.inference.meta import BaseRLearner from lightgbm import LGBMRegressor BaseRLearner(learner=LGBMRegressor()).estimate_ate(X=df[X], treatment=df['treated'], y=df['outcome']) (array([1.6529302]), array([1.65076231]), array([1.65509809])) S Learner from causalml.inference.meta import BaseSLearner BaseSLearner(learner=LGBMRegressor()).estimate_ate(X=df[X], treatment=df['treated'], y=df['outcome']) array([2.08090912]) T Learner from causalml.inference.meta import BaseTLearner BaseTLearner(learner=LGBMRegressor()).estimate_ate(X=df[X], treatment=df['treated'], y=df['outcome']) (array([2.05252873]), array([1.86063479]), array([2.24442267])) X Learner from causalml.inference.meta import BaseXLearner BaseXLearner(learner=LGBMRegressor()).estimate_ate(X=df[X], treatment=df['treated'], y=df['outcome']) (array([2.0255193]), array([1.83456903]), array([2.21646956])) DR Learner from causalml.inference.meta import BaseDRLearner BaseDRLearner(learner=LGBMRegressor()).estimate_ate(X=df[X], treatment=df['treated'], y=df['outcome']) (array([2.08923293]), array([1.8921804]), array([2.28628547])) Compare All def simulate(dgp, K=100): # Initialize coefficients results = pd.DataFrame(columns=['k', 'Estimator', 'Estimate']) names = ['1. Diff ', '2. Reg ', '3. Match', '4. IPW ', '5. R lrn', '6. S lrn', '7. T lrn', '8. X lrn', '9. DRlrn'] # Compute coefficients for k in range(K): print(f\u0026quot;Simulation {k}/{K}\u0026quot;, end=\u0026quot;\\r\u0026quot;) temp = pd.DataFrame({'k': [k] * len(names), 'Estimator': names, 'Estimate': [0] * len(names)}) # Draw data df = dgp.generate_data(seed=k) # Single diff temp['Estimate'][0] = smf.ols('outcome ~ treated', data=df).fit().params[1] # Regression with controls temp['Estimate'][1] = smf.ols(f'outcome ~ treated + male + age + income', data=df).fit().params[1] # Matching psm = NearestNeighborMatch(replace=True, ratio=1) df_matched = psm.match(data=df, treatment_col=\u0026quot;treated\u0026quot;, score_cols=X) temp['Estimate'][2] = smf.ols('outcome ~ treated', data=df_matched).fit().params[1] # IPW df[\u0026quot;pscore\u0026quot;] = smf.logit(\u0026quot;np.rint(treated) ~ male + age + income\u0026quot;, data=df).fit(disp=False).predict() w = 1 / (df[\u0026quot;pscore\u0026quot;] * df[\u0026quot;treated\u0026quot;] + (1-df[\u0026quot;pscore\u0026quot;]) * (1-df[\u0026quot;treated\u0026quot;])) temp['Estimate'][3] = smf.wls(\u0026quot;outcome ~ treated\u0026quot;, weights=w, data=df).fit().params[1] # R Learner temp['Estimate'][4] = BaseRLearner(learner=LGBMRegressor()).estimate_ate(X=df[X], treatment=df['treated'], y=df['outcome'])[0][0] # S Learner temp['Estimate'][5] = BaseSLearner(learner=LGBMRegressor()).estimate_ate(X=df[X], treatment=df['treated'], y=df['outcome'])[0] # T Learner temp['Estimate'][6] = BaseTLearner(learner=LGBMRegressor()).estimate_ate(X=df[X], treatment=df['treated'], y=df['outcome'])[0][0] # X Learner temp['Estimate'][7] = BaseXLearner(learner=LGBMRegressor()).estimate_ate(X=df[X], treatment=df['treated'], y=df['outcome'])[0][0] # DR Learner temp['Estimate'][8] = BaseDRLearner(learner=LGBMRegressor()).estimate_ate(X=df[X], treatment=df['treated'], y=df['outcome'])[0][0] # Combine estimates results = pd.concat((results, temp)) return results.reset_index(drop=True) results = simulate(dgp=dgp_compare()) Simulation 99/100 Let\u0026rsquo;s plot the distribution of the estimated parameters.\np = sns.kdeplot(data=results, x=\u0026quot;Estimate\u0026quot;, hue=\u0026quot;Estimator\u0026quot;, legend=True); sns.move_legend(p, \u0026quot;upper left\u0026quot;, bbox_to_anchor=(1.05, 0.8)) plt.axvline(x=2, c='k', ls='--'); plt.title('Simulated Distributions'); We can also tabulate the simulated mean and standard deviation of each estimator.\nresults.groupby('Estimator').agg(mean=(\u0026quot;Estimate\u0026quot;, \u0026quot;mean\u0026quot;), std=(\u0026quot;Estimate\u0026quot;, \u0026quot;std\u0026quot;)) mean std Estimator 1. Diff -0.054010 0.120718 2. Reg 2.190934 0.126232 3. Match 1.575239 0.211772 4. IPW 1.596031 0.119129 5. R lrn 1.649703 0.263321 6. S lrn 1.963797 0.129638 7. T lrn 1.891307 0.160480 8. X lrn 1.975699 0.161004 9. DRlrn 1.876465 0.173318 The only unbiased estimators seems to be the S-learner and the X-learner, followed by T-learner, DR-learner and linear regression.\nhe S-learner however is more efficient.\n","date":1656633600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1656633600,"objectID":"aeaec79edb73d88e38b0d4cf934a1ae6","permalink":"https://matteocourthoud.github.io/post/compare/","publishdate":"2022-07-01T00:00:00Z","relpermalink":"/post/compare/","section":"post","summary":"%matplotlib inline %config InlineBackend.figure_format = 'retina' from src.utils import * from src.dgp import dgp_compare df = dgp_compare().generate_data(include_beta=True) df.head() outcome treated male age income beta 0 38.68 False 0 28.0 1514.0 2.","tags":null,"title":"Comparing ATE Estimators","type":"post"},{"authors":null,"categories":null,"content":"A complete guide to comparing distributions, from visualization to statistical tests\nComparing the empirical distribution of a variable across different groups is a common problem in data science. In particular, in causal inference the problem often arises when we have to assess the quality of randomization.\nWhen we want to assess the causal effect of a policy (or UX feature, ad campaign, drug, \u0026hellip;), the golden standard in causal inference are randomized control trials, also known as A/B tests. In practice, we select a sample for the study and we randomly split it into a control and a treatment group, and we compare the outcomes between the two groups. Randomization ensures that only difference between the two groups is the treatment, on average, so that we can attribute outcome differences to the treatment effect.\nThe problem is that, despite randomization, the two groups are never identical. However, sometimes, they are not even \u0026ldquo;similar\u0026rdquo;. For example, we might have more males in one group, or older people, etc.. (we usually call these characteristics, covariates or control variables). When it happens, we cannot be certain anymore that the difference in the outcome is only due to the treatment and cannot be attributed to the inbalanced covariates instead. Therefore, it is always important, after randomization, to check whether all observed variables are balanced across groups and whether there are no systematic differences. Another option, to be certain ex-ante that certain covariates are balanced, is stratified sampling.\nIn this blog post, we are going to see different ways to compare two (or more) distributions and assess the magnitude and significance of their difference. We are going to consider two different approaches, visual and statistical. The two approaches generally trade-off intuition with rigor: from plots we can quickly assess and explore differences, but it\u0026rsquo;s hard to tell whether these differences are systematic or due to noise.\nExample Let\u0026rsquo;s assume we need to perform an experiment on a group of individuals and we have randomized them into a treatment and control group. We would like them to be as comparable as possible, in order to attribute any difference between the two groups to the treatment effect alone. We also have divided the treatment group in different arms for testing different treatments (e.g. slight variations of the same drug).\nFor this example, I have simulated a dataset of 1000 individuals, for whom we observe a set of characteristics. I import the data generating process dgp_rnd_assignment() from src.dgp and some plotting functions and libraries from src.utils.\n%matplotlib inline %config InlineBackend.figure_format = 'retina' from src.utils import * from src.dgp import dgp_rnd_assignment df = dgp_rnd_assignment().generate_data() df.head() Group Arm Gender Age Income 0 control NaN 0 29.0 568.44 1 control NaN 1 32.0 596.45 2 treatment arm 3 0 29.0 380.86 3 control NaN 0 25.0 476.38 4 treatment arm 4 1 32.0 628.28 We have information on $1000$ individuals, for which we observe gender, age and weekly income. Each individual is assigned either to the treatment or control group and treated individuals are distributed across four treatment arms.\nTwo Groups - Plots Let\u0026rsquo;s start with the simplest setting: we want to compare the distribution of income across the treatment and control group. We first explore visual approaches and the statistical approaches. The advantage of the first is intuition while the advantage of the second is rigor.\nFor most visualizations I am going to use Python\u0026rsquo;s seaborn library.\nBoxplot A first visual approach is the boxplot. The boxplot is a good trade-off between summary statistics and data visualization. The center of the box represents the median while the borders represent the first (Q1) and third quartile (Q3), respectively. The whiskers instead, extend to the first data points that are more than 1.5 times the interquartile range (Q3 - Q1) outside the box. The points that fall outside of the whiskers are plotted individually and are usually considered outliers.\nTherefore, the boxplot provides both summary statistics (the box and the whiskers) and direct data visualization (the outliers).\nsns.boxplot(data=df, x='Group', y='Income'); plt.title(\u0026quot;Boxplot\u0026quot;); It seems that the income distribution in the treatment group is slightly more dispersed: the orange box is larger and its whiskers cover a wider range. However, the issue with the boxplot is that it hides the shape of the data, telling us some summary statistics but not showing us the actual data distribution.\nHistogram The most intuitive way to plot a distribution is the histogram. The histogram groups the data into equally wide bins and plots the number of observations within each bin.\nsns.histplot(data=df, x='Income', hue='Group', bins=50); plt.title(\u0026quot;Histogram\u0026quot;); There are multiple issues with this plot:\nSince the two groups have a different number of observations, the two histograms are not comparable The number of bins is arbitrary We can solve the first issue using the stat option to plot the density instead of the count and setting the common_norm option to False to use the same normalization.\nsns.histplot(data=df, x='Income', hue='Group', bins=50, stat='density', common_norm=False); plt.title(\u0026quot;Density Histogram\u0026quot;); Now the two histograms are comparable!\nHowever, an important issue remains: the size of the bins is arbitrary. In the extreme, if we bunch the data less, we end up with bins with at most one observation, if we bunch the data more, we end up with a single bin. In both cases, if we exaggerate, the plot loses informativeness. This is a classical bias-variance trade-off.\nKernel Density One possible solution is to use a kernel density function that tries to approximate the histogram with a continuous function, using kernel density estimation (KDE).\nsns.kdeplot(x='Income', data=df, hue='Group', common_norm=False); plt.title(\u0026quot;Kernel Density Function\u0026quot;); From the plot, it seems that the estimated kernel density of income has \u0026ldquo;fatter tails\u0026rdquo; (i.e. higher variance) in the treatment group, while the average seems similar across groups.\nThe issue with kernel density estimation is that it is a bit of a black-box and might mask relevant features of the data.\nCumulative Distribution A more transparent representation of the two distribution is their cumulative distribution function. At each point of the x axis (income) we plot the percentage of data points that have an equal or lower value. The main advantages of the cumulative distribution function are that\nwe do not need to make any arbitrary choice (e.g. number of bins) we do not need to perform any approximation (e.g. with KDE), but we represent all data points sns.histplot(x='Income', data=df, hue='Group', bins=len(df), stat=\u0026quot;density\u0026quot;, element=\u0026quot;step\u0026quot;, fill=False, cumulative=True, common_norm=False); plt.title(\u0026quot;Cumulative distribution function\u0026quot;); How should we interpret the graph?\nSince the two lines cross more or less at 0.5 (y axis), it means that their median is similar\nSince the orange line is above the blue line on the left and below the blue line on the left, it means that the distribution of the treatment group as fatter tails\nQQ Plot A related method is the qq-plot, where q stands for quantile. The qq-plot plots the quantiles of the two distributions against each other. If the distributions are the same, we should get a 45 degree line.\nThere is no native qq-plot function in Python and, while the statsmodels package provides a qqplot function, it is quite cumbersome. Therefore, we will do it by hand.\nFirst, we need to compute the quartiles of the two groups, using the percentile function.\nincome = df['Income'].values income_t = df.loc[df.Group=='treatment', 'Income'].values income_c = df.loc[df.Group=='control', 'Income'].values df_pct = pd.DataFrame() df_pct['q_treatment'] = np.percentile(income_t, range(100)) df_pct['q_control'] = np.percentile(income_c, range(100)) Now we can plot the two quantile distributions against each other, plus the 45-degree line, representing the benchmark perfect fit.\nplt.figure(figsize=(8, 8)) plt.scatter(x='q_control', y='q_treatment', data=df_pct, label='Actual fit'); sns.lineplot(x='q_control', y='q_control', data=df_pct, color='r', label='Line of perfect fit'); plt.xlabel('Quantile of income, control group') plt.ylabel('Quantile of income, treatment group') plt.legend() plt.title(\u0026quot;QQ plot\u0026quot;); The qq-plot delivers a very similar insight with respect to the cumulative distribution plot: income in the treatment group has the same median (lines cross in the center) but wider tails (dots are below the line on the left end and above on the right end).\nTwo Groups - Tests So far, we have seen different ways to visualize differences between distributions. The main advantage of visualization is intuition: we can eyeball the differences and intuitively assess them.\nHowever, we might want to be more rigorous and try to assess the statistical significance of the difference between the distributions, i.e. answer the question \u0026ldquo;is the observed difference systematic or due to sampling noise?\u0026rdquo;.\nWe are now going to analyze different tests to discern two distributions from each other.\nT-test The first and most common test is the student t-test. T-tests are generally used to compare means. In this case, we want to test whether the means of the income distribution is the same across the two groups. The test statistic for the two-means comparison test is given by:\n$$ stat = \\frac{|\\bar x_1 - \\bar x_2|}{\\sqrt{s^2 / n }} $$\nWhere $\\bar x$ is the sample mean and $s$ is the sample standard deviation. Under mild conditions, the test statistic is asymptotically distributed as a student t distribution.\nWe use the ttest_ind function from scipy to perform the t-test. The function returns both the test statistic and the implied p-value.\nfrom scipy.stats import ttest_ind stat, p_value = ttest_ind(income_c, income_t) print(f\u0026quot;t-test: statistic={stat:.4f}, p-value={p_value:.4f}\u0026quot;) t-test: statistic=-1.5549, p-value=0.1203 The p-value of the test is $0.12$, therefore we do not reject the null hypothesis of no difference in means across treatment and control groups.\nNote: the t-test assumes that the variance in the two samples is the same so that its estimate is computed on the joint sample. Welch’s t-test allows for unequal variances in the two samples.\nStandardized Mean Difference (SMD) In general, it is good practice to always perform a test for difference in means on all variables across the treatment and control group, when we are running a randomized control trial or A/B test.\nHowever, since the denominator of the t-test statistic depends on the sample size, the t-test has been criticized for making p-values hard to compare across studies. In fact, we may obtain a significant result in an experiment with very small magnitude of difference but large sample size while we may obtain a non-significant result in an experiment with large magnitude of difference but small sample size.\nOne solution that has been proposed is the standardized mean difference (SMD). As the name suggests, this is not a proper test statistic, but just a standardized difference, which can be computed as:\n$$ SMD = \\frac{|\\bar x_1 - \\bar x_2|}{\\sqrt{(s^2_1 + s^2_2) / 2}} $$\nUsually a value below $0.1$ is considered a \u0026ldquo;small\u0026rdquo; difference.\nIt is good practice to collect average values of all variables across treatment and control group and a measure of distance between the two — either the t-test or the SMD — into a table that is called balance table. We can use the create_table_one function from the causalml library to generate it. As the name of the function suggests, the balance table should always be the first table you present when performing an A/B test.\nfrom causalml.match import create_table_one df['treatment'] = df['Group']=='treatment' create_table_one(df, 'treatment', ['Gender', 'Age', 'Income']) Control Treatment SMD Variable n 704 296 Age 32.40 (8.54) 36.42 (7.76) 0.4928 Gender 0.51 (0.50) 0.58 (0.49) 0.1419 Income 524.59 (117.35) 538.75 (160.15) 0.1009 In the first two columns, we can see the average of the different variables across the treatment and control groups, with standard errors in parenthesis. In the last column, the values of the SMD indicate a standardized difference of more than $0.1$ for all variables, suggesting that the two groups are probably different.\nMann–Whitney U Test An alternative test is the Mann–Whitney U test. The null hypothesis for this test is that the two groups have the same distribution, while the alternative hypothesis is that one group has larger (or smaller) values than the other.\nDifferently from the other tests we have seen so far, the Mann–Whitney U test is agnostic to outliers and concentrates on the center of the distribution.\nThe test procedure is the following.\nCombine all data points and rank them (in increasing or decreasing order)\nCompute $U_1 = R_1 - n_1(n_1 + 1)/2$, where $R_1$ is the sum of the ranks for data points in the first group and $n_1$ is the number of points in the first group.\nCompute $U_2$ similarly for the second group.\nThe test statistic is given by $stat = min(U_1, U_2)$.\nUnder the null hypothesis of no systematic rank differences between the two distributions (i.e. same median), the test statistic is asymptotically normally distributed with known mean and variance.\nThe intuition behind the computation of $R$ and $U$ is the following: if the values in the first sample were all bigger than the values in the second sample, then $R_1 = n_1(n_1 + 1)/2$ and, as a consequence, $U_1$ would then be zero (minimum attainable value). Otherwise, if the two samples were similar, $U_1$ and $U_2$ would be very close to $n_1 n_2 / 2$ (maximum attainable value).\nWe perform the test using the mannwhitneyu function from scipy.\nfrom scipy.stats import mannwhitneyu stat, p_value = mannwhitneyu(income_t, income_c) print(f\u0026quot; Mann–Whitney U Test: statistic={stat:.4f}, p-value={p_value:.4f}\u0026quot;) Mann–Whitney U Test: statistic=106371.5000, p-value=0.6012 We get a p-value of 0.6 which implies that we do not reject the null hypothesis of no difference between the two distributions.\nNote: as for the t-test, there exists a version of the Mann–Whitney U test for unequal variances in the two samples, the Brunner-Munzel test.\nPermutation Tests A non-parametric alternative is permutation testing. The idea is that, under the null hypothesis, the two distributions should be the same, therefore shuffling the group labels should not significantly alter any statistic.\nWe can chose any statistic and check how its value in the original sample compares with its distribution across group label permutations. For example, let\u0026rsquo;s use as a test statistic the difference of sample means between the treatment and control group.\nsample_stat = np.mean(income_t) - np.mean(income_c) stats = np.zeros(1000) for k in range(1000): labels = np.random.permutation((df['Group'] == 'treatment').values) stats[k] = np.mean(income[labels]) - np.mean(income[labels==False]) p_value = np.mean(stats \u0026gt; sample_stat) print(f\u0026quot;Permutation test: p-value={p_value:.4f}\u0026quot;) Permutation test: p-value=0.0530 The permutation test gives us a p-value of $0.056$, implying a weak non-rejection of the null hypothesis at the 5% level.\nHow do we interpret the p-value? It means that the difference in means in the data is larger than $1 - 0.0560 = 94.4%$ of the differences in means across the permuted samples.\nWe can visualize the test, by plotting the distribution of the test statistic across permutations against its sample value.\nplt.hist(stats, label='Permutation Statistics', bins=30); plt.axvline(x=sample_stat, c='r', ls='--', label='Sample Statistic'); plt.legend(); plt.xlabel('Income difference between treatment and control group') plt.title('Permutation Test'); As we can see, the sample statistic is quite extreme with respect to the values in the permuted samples, but not excessively.\nChi-Squared Test The chi-squared test is a very powerful test that is mostly used to test differences in frequencies.\nOne of the least known applications of the chi-squared test, is testing the similarity between two distributions. The idea is to bin the observations of the two groups. If the two distributions were the same, we would expect the same frequency of observations in each bin. Importantly, we need enough observations in each bin, in order for the test to be valid.\nI generate bins corresponding to deciles of the distribution of income in the control group and then I compute the expected number of observations in each bin in the treatment group, if the two distributions were the same.\n# Init dataframe df_bins = pd.DataFrame() # Generate bins from control group _, bins = pd.qcut(income_c, q=10, retbins=True) df_bins['bin'] = pd.cut(income_c, bins=bins).value_counts().index # Apply bins to both groups df_bins['income_c_observed'] = pd.cut(income_c, bins=bins).value_counts().values df_bins['income_t_observed'] = pd.cut(income_t, bins=bins).value_counts().values # Compute expected frequency in the treatment group df_bins['income_t_expected'] = df_bins['income_c_observed'] / np.sum(df_bins['income_c_observed']) * np.sum(df_bins['income_t_observed']) df_bins bin income_c_observed income_t_observed income_t_expected 0 (232.26, 380.496] 70 46 29.075391 1 (380.496, 425.324] 70 30 29.075391 2 (425.324, 456.795] 70 24 29.075391 3 (456.795, 488.83] 71 26 29.490754 4 (488.83, 513.66] 70 18 29.075391 5 (513.66, 540.048] 70 19 29.075391 6 (540.048, 576.664] 71 21 29.490754 7 (576.664, 621.022] 70 25 29.075391 8 (621.022, 682.003] 70 42 29.075391 9 (682.003, 973.46] 71 41 29.490754 We can now perform the test by comparing the expected (E) and observed (O) number of observations in the treatment group, across bins. The test statistic is given by\n$$ stat = \\sum _{i=1}^{n} \\frac{(O_i - E_i)^{2}}{E_i} $$\nwhere the bins are indexed by $i$ and $O$ is the observed number of data points in bin $i$ and $E$ is the expected number of data points in bin $i$. Since we generated the bins using deciles of the distribution of income in the control group, we expect the number of observations per bin in the treatment group to be the same across bins. The test statistic is asymptocally distributed as a chi-squared distribution.\nTo compute the test statistic and the p-value of the test, we use the chisquare function from scipy.\nfrom scipy.stats import chisquare stat, p_value = chisquare(df_bins['income_t_observed'], df_bins['income_t_expected']) print(f\u0026quot;Chi-squared Test: statistic={stat:.4f}, p-value={p_value:.4f}\u0026quot;) Chi-squared Test: statistic=32.1432, p-value=0.0002 Differently from all other tests so far, the chi-squared test strongly rejects the null hypothesis that the two distributions are the same. Why?\nThe reason lies in the fact that the two distributions have a similar center but different tails and the chi-squared test tests the similarity along the whole distribution and not only in the center, as we were doing with the previous tests.\nThis result tells a cautionary tale: it is very important to understand what you are actually testing before drawing blind conclusions from a p-value!\nKolmogorov-Smirnov Test The idea of the Kolmogorov-Smirnov test, is to compare the cumulative distributions of the two groups. In particular, the Kolmogorov-Smirnov test statistic is the maximum absolute difference between the two cumulative distributions.\n$$ stat = \\sup _{x} \\ \\Big| \\ F_1(x) - F_2(x) \\ \\Big| $$\nWhere $F_1$ and $F_2$ are the two cumulative distribution functions and $x$ are the values of the underlying variable. The asymptotic distribution of the Kolmogorov-Smirnov test statistic is Kolmogorov distributed.\nTo better understand the test, let\u0026rsquo;s plot the cumulative distribution functions and the test statistic. First, we compute the cumulative distribution functions.\ndf_ks = pd.DataFrame() df_ks['Income'] = np.sort(df['Income'].unique()) df_ks['F_control'] = df_ks['Income'].apply(lambda x: np.mean(income_c\u0026lt;=x)) df_ks['F_treatment'] = df_ks['Income'].apply(lambda x: np.mean(income_t\u0026lt;=x)) df_ks.head() Income F_control F_treatment 0 216.36 0.000000 0.003378 1 232.26 0.001420 0.003378 2 243.15 0.001420 0.006757 3 259.88 0.002841 0.006757 4 262.82 0.002841 0.010135 We now need to find the point where the absolute distance between the cumulative distribution functions is largest.\nk = np.argmax( np.abs(df_ks['F_control'] - df_ks['F_treatment'])) ks_stat = np.abs(df_ks['F_treatment'][k] - df_ks['F_control'][k]) We can visualize the value of the test statistic, by plotting the two cumulative distribution functions and the value of the test statistic.\ny = (df_ks['F_treatment'][k] + df_ks['F_control'][k])/2 plt.plot('Income', 'F_control', data=df_ks, label='Control') plt.plot('Income', 'F_treatment', data=df_ks, label='Treatment') plt.errorbar(x=df_ks['Income'][k], y=y, yerr=ks_stat/2, color='k', capsize=5, mew=3, label=f\u0026quot;Test statistic: {ks_stat:.4f}\u0026quot;) plt.legend(loc='center right'); plt.title(\u0026quot;Kolmogorov-Smirnov Test\u0026quot;); From the plot, we can see that the value of the test statistic corresponds to the distance between the two cumulative distributions at income~650. For that value of income, we have the largest imbalance between the two groups.\nWe can now perform the actual test using the kstest function from scipy.\nfrom scipy.stats import kstest stat, p_value = kstest(income_t, income_c) print(f\u0026quot; Kolmogorov-Smirnov Test: statistic={stat:.4f}, p-value={p_value:.4f}\u0026quot;) Kolmogorov-Smirnov Test: statistic=0.0974, p-value=0.0355 The p-value is below 5%: we reject the null hypothesis that the two distributions are the same, with 95% confidence.\nNote 1: The KS test is too conservative and rejects the null hypothesis too rarely. Lilliefors test corrects this bias using a different distribution for the test statistic, the Lilliefors distribution.\nNote 2: the KS test uses very little information since it only compares the two cumulative distributions at one point: the one of maximum distance. The Anderson-Darling test and the Cramér-von Mises test instead compare the two distributions along the whole domain, by integration (the difference between the two lies in the weighting of the squared distances).\nMultiple Groups - Plots So far we have only considered the case of two groups: treatment and control. But that if we had multiple groups? Some of the methods we have seen above scale well, while others don\u0026rsquo;t.\nAs a working example, we are now going to check whether the distribution of income is the same across treatment arms.\nBoxplot The boxplot scales very well, when we have a number of groups in the single-digits, since we can put the different boxes side-by-side.\nsns.boxplot(x='Arm', y='Income', data=df.sort_values('Arm')); plt.title(\u0026quot;Boxplot, multiple groups\u0026quot;); From the plot, it looks like the distribution of income is different across treatment arms, with higher numbered arms having a higher average income.\nViolin Plot A very nice extension of the boxplot that combines summary statistics and kernel density estimation is the violinplot. The violinplot plots separate densities along the y axis so that they don\u0026rsquo;t overlap. By default, it also adds a miniature boxplot inside.\nsns.violinplot(x='Arm', y='Income', data=df.sort_values('Arm')); plt.title(\u0026quot;Violin Plot, multiple groups\u0026quot;); As for the boxplot, the violin plot suggests that income is different across treatment arms.\nRidgeline Plot Lastly, the ridgeline plot plots multiple kernel density distributions along the x-axis, making them more intuitive than the violin plot but partially overlapping them. Unfortunately, there is no default ridgeline plot neither in matplotlib nor in seaborn. We need to import it from joypy.\nfrom joypy import joyplot joyplot(df, by='Arm', column='Income', colormap=sns.color_palette(\u0026quot;crest\u0026quot;, as_cmap=True)); plt.xlabel('Income'); plt.title(\u0026quot;Ridgeline Plot, multiple groups\u0026quot;); Again, the ridgeline plot suggests that higher numbered treatment arms have higher income. From this plot it is also easier to appreciate the different shapes of the distributions.\nMultiple Groups - Tests Lastly, let\u0026rsquo;s consider hypothesis tests to compare multiple groups. For simplicity, we will concentrate on the most popular one: the F-test.\nF-test With multiple groups, the most popular test is the F-test. The F-test compares the variance of a variable across different groups. This analysis is also called analysis of variance, or ANOVA.\nIn practice, the F-test statistic is\n$$ \\text{stat} = \\frac{\\text{between-group variance}}{\\text{within-group variance}} = \\frac{\\sum_{g} \\big( \\bar x_g - \\bar x \\big) / (G-1)}{\\sum_{g} \\sum_{i \\in g} \\big( \\bar x_i - \\bar x_g \\big) / (N-G)} $$\nWhere $G$ is the number of groups, $N$ is the number of observations, $\\bar x$ is the overall mean and $\\bar x_g$ is the mean within group $g$. Under the null hypothesis of group independence, the f-statistic is F-distributed.\nfrom scipy.stats import f_oneway income_groups = [df.loc[df['Arm']==arm, 'Income'].values for arm in df['Arm'].dropna().unique()] stat, p_value = f_oneway(*income_groups) print(f\u0026quot;F Test: statistic={stat:.4f}, p-value={p_value:.4f}\u0026quot;) F Test: statistic=9.0911, p-value=0.0000 The test p-value is basically zero, implying a strong rejection of the null hypothesis of no differences in the income distribution across treatment arms.\nConclusion In this post we have see a ton of different ways to compare two or more distributions, both visually and statistically. This is a primary concern in many applications, but especially in causal inference where we use randomization to make treatment and control group as comparable as possible.\nWe have also seen how different methods might be better suited for different situations. Visual methods are great to build intuition, but statistical methods are essential for decision-making, since we need to be able to assess the magnitude and statistical significance of the differences.\nReferences [1] Student, The Probable Error of a Mean (1908), Biometrika.\n[2] F. Wilcoxon, Individual Comparisons by Ranking Methods (1945), Biometrics Bulletin.\n[3] B. L. Welch, The generalization of \u0026ldquo;Student\u0026rsquo;s\u0026rdquo; problem when several different population variances are involved (1947), Biometrika.\n[4] H. B. Mann, D. R. Whitney, On a Test of Whether one of Two Random Variables is Stochastically Larger than the Other (1947), The Annals of Mathematical Statistics.\n[5] E. Brunner, U. Munzen, The Nonparametric Behrens-Fisher Problem: Asymptotic Theory and a Small-Sample Approximation (2000), Biometrical Journal.\n[6] A. N. Kolmogorov, Sulla determinazione empirica di una legge di distribuzione (1933), Giorn. Ist. Ital. Attuar..\n[7] H. Cramér, On the composition of elementary errors (1928), Scandinavian Actuarial Journal.\n[8] R. von Mises, Wahrscheinlichkeit statistik und wahrheit (1936), Bulletin of the American Mathematical Society.\n[9] T. W. Anderson, D. A. Darling, Asymptotic Theory of Certain \u0026ldquo;Goodness of Fit\u0026rdquo; Criteria Based on Stochastic Processes (1953), The Annals of Mathematical Statistics.\nRelated Articles Goodbye Scatterplot, Welcome Binned Scatterplot Code You can find the original Jupyter Notebook here:\nhttps://github.com/matteocourthoud/Blog-Posts/blob/main/notebooks/distr.ipynb\n","date":1655856000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1655856000,"objectID":"e99f68079316990ae82671527ce10872","permalink":"https://matteocourthoud.github.io/post/distr/","publishdate":"2022-06-22T00:00:00Z","relpermalink":"/post/distr/","section":"post","summary":"A complete guide to comparing distributions, from visualization to statistical tests\nComparing the empirical distribution of a variable across different groups is a common problem in data science. In particular, in causal inference the problem often arises when we have to assess the quality of randomization.","tags":null,"title":"How to Compare Two or More Distributions","type":"post"},{"authors":null,"categories":null,"content":"Problems and solutions of linear regression with multiple treatments\nIn many causal inference settings, we might be interested in the effect of not just one treatment, but many mutually exclusive treatments. For example, we might want to test alternative UX designs, or drugs, or policies. Depending on the context, there might be many reasons why we want to test different treatments at the same time, but generally it can help reducing the sample size, as we need just a single control group. A simple way to recover the different treatment effects is a linear regression of the outcome of interest on the different treatment indicators.\nHowever, in causal inference, we often condition the analysis on other observable variables (often called control variables), either to increase power or, especially in quasi-experimental settings, to identify a causal parameter instead of a simple correlation. There are cases in which adding control variables can backfire, but otherwise, we usually think that the regression framework is still able to recover the average treatment effect.\nIn a breakthrough paper, Goldsmith-Pinkham, Hull and Kolesár (2022) have recently shown that in case of multiple and mutually-exclusive treatments with control variables, the regression coefficients do not identify a causal effect. However, not everything is lost: the authors propose a simple solution to this problem that still makes use of linear regression.\nIn this blog post, I am going to go through a simple example illustrating the nature of the problem and the solution proposed by the authors.\nMultiple Treatments Example Suppose we are an online store and we are not satisfied with our current checkout page. In particular, we would like to change our checkout button to increase the probability of a purchase. Our UX designer comes up with two alternative checkout buttons, which are displayed below.\nIn order to understand which button to use, we run an A/B test, or randomized control trial. In particular, when people arrive at the checkout page, we show them one of the three options, at random. Then, for each user, we record the revenue generated which is our outcome of interest.\nI generate a synthetic dataset using dgp_buttons() from src.dgp as data generating process. I also import plotting functions and standard libraries from src.utils.\n%matplotlib inline %config InlineBackend.figure_format = 'retina' from src.utils import * from src.dgp import dgp_buttons dgp = dgp_buttons() df = dgp.generate_data() df.head() group revenue mobile 0 button1 8.927335 0 1 default 13.613456 1 2 button2 4.777628 0 3 default 8.909049 0 4 default 10.160347 0 We have information on 2000 users, for which we observe their checkout button (default, button1 or button2), the revenue they generate and whether they connected from desktop or mobile.\nWe notice too late that we have a problem with randomization. We showed button1 more frequently to desktop users and button2 more frequently to mobile users. The control group that sees the default button instead is balanced.\nWhat should we do? What happens if we simply compare revenue across groups? Let\u0026rsquo;s do it by regressing revenue on group dummy variables.\nsmf.ols('revenue ~ group', data=df).fit().summary().tables[1] coef std err t P\u003e|t| [0.025 0.975] Intercept 11.6553 0.149 78.250 0.000 11.363 11.948 group[T.button1] -0.5802 0.227 -2.556 0.011 -1.026 -0.135 group[T.button2] -0.5958 0.218 -2.727 0.006 -1.024 -0.167 From the regression results we estimate a negative and significant effect for both buttons. Should we believe these estimates? Are they causal?\nIt is unlikely that what we have estimated are the true treatment effects. In fact, there might be substantial differences in purchase attitudes between desktop and mobile users. Since we do not have a comparable number of mobile and desktop users across treatment arms, it might be that the observed differences in revenue are due to the device used and not the button design.\nBecause of this, we decide to condition our analysis on the device used and we include the mobile dummy variable in the regression.\nsmf.ols('revenue ~ group + mobile', data=df).fit().summary().tables[1] coef std err t P\u003e|t| [0.025 0.975] Intercept 9.1414 0.110 82.905 0.000 8.925 9.358 group[T.button1] 0.3609 0.141 2.558 0.011 0.084 0.638 group[T.button2] -1.0326 0.134 -7.684 0.000 -1.296 -0.769 mobile 4.7181 0.116 40.691 0.000 4.491 4.946 Now the coefficient of button1 is positive and significant. Should we recommend its implementation?\nThe answer is surprisingly no. Goldsmith-Pinkham, Hull, Kolesár (2022) show that this type of regression does not identify the average treatment effect when:\nthere are mutually exclusive treatment arms (in our case, groups) we are controlling for some variable $X$ (in our case, mobile) there treatment effects are heterogeneous in $X$ This is true even if the treatment is \u0026ldquo;as good as random\u0026rdquo; once we condition on $X$.\nIndeed, in our case, the true treatment effects are the ones reported in the following table.\nThe first button has no effect on revenue, irrespectively of the device, while the second button has a positive effect for mobile users and a negative effect for desktop users. Our (wrong) regression specification instead estimates a positive effect of the first button.\nLet\u0026rsquo;s now dig more in detail into the math, to understand why this is happening.\nTheory This section borrows heavily from Goldsmith-Pinkham, Hull, Kolesár (2022). For a great summary of the paper, I recommend this excellent Twitter thread by one of the authors, Paul Goldsmith-Pinkham.\nEconomists love using linear regression to estimate treatment effects — it turns out that there are perils to this method, but also amazing perks\nCome with me in this 🧵 if you want to learn… https://t.co/eDsRLkZFZe\n\u0026mdash; Paul Goldsmith-Pinkham (@paulgp) June 7, 2022 Single Treatment Arm Suppose we are interested in the effect of a treatment $D$ on an outcome $Y$. First, let\u0026rsquo;s consider the standard case of a single treatment arm so that the treatment variable is binary, $D \\in \\lbrace 0 , 1 \\rbrace$. Also consider a single binary control variable $X \\in \\lbrace 0 , 1 \\rbrace$. We also assume that treatment assignment is as good as random, conditionally on $X$. This means that there might be systematic differences between the treatment and control group, however, these differences are fully accounted for by $X$. Formally we write\n$$ \\left( Y_i^{(0)}, Y_i^{(1)} \\right) \\ \\perp \\ D_i \\ | \\ X_i $$\nWhere $Y_i^{(d)}$ denotes the potential outcome of individual $i$ when its treatment status is $d$. For example, $Y_i^{(0)}$ indicates the outcome of individual $i$ in case it is not treated. This notation comes from Rubin\u0026rsquo;s potential outcomes framework. We can write the individual treatment effect of individual $i$ as\n$$ \\tau_i = Y_i^{(1)} - Y_i^{(0)} $$\nIn this setting, the regression of interest is\n$$ Y_i = \\alpha + \\beta D_i + \\gamma X_i + u_i $$\nThe coefficient of interest is $\\beta$.\nAngrist (1998) shows that the regression coefficient $\\beta$ identifies the average treatment effect. In particular, $\\beta$ identifies a weighted average of the within-group $x$ average treatment effect $\\tau (x)$ with convex weights. In this particular setting, we can write it as\n$$ \\beta = \\lambda \\tau(0) + (1 - \\lambda) \\tau(1) \\qquad \\text{where} \\qquad \\tau (x) = \\mathbb E \\big[ Y_i^{(1)} - Y_i^{(0)} \\ \\big| \\ X_i = x \\big] $$\nThe weights $\\lambda$ and $(1-\\lambda)$ are given by the within-group treatment variance. Hence, the OLS estimator gives less weight to groups where we have less treatment variance, i.e., where treatment is more imbalanced. Groups where treatment is distributed 50-50 get the most weight.\n$$ \\lambda = \\frac{ \\text{Var} \\big(D_i \\ \\big| \\ X_i = 0 \\big) \\Pr \\big(X_i=0 \\big)}{\\sum_{x \\in \\lbrace 0 , 1 \\rbrace} \\text{Var} \\big(D_i \\ \\big| \\ X_i = x \\big) \\Pr \\big( X_i=x \\big)} \\in [0, 1] $$\nThe weights can be derived using the Frisch-Waugh-Lowell theorem to express $\\beta_1$ as the OLS coefficient of a univariate regression of $Y$ on $D_{i, \\perp X}$, where $D_{i, \\perp X}$ are the residuals from regressing $D$ on $X$. If you are not familiar with the Frisch-Waugh-Lowell theorem, I wrote an introductory blog post here.\n$$ \\beta_1 = \\frac{ \\mathbb E \\big[ D_{i, \\perp X} Y_i \\big] }{ \\mathbb E \\big[ D_{i, \\perp X}^2 \\big] } = \\underbrace{ \\frac{\\mathbb E \\big[ D_{i, \\perp X} Y_i(0) \\big]}{\\mathbb E \\big[ D_{i, \\perp X}^2 \\big]} } _ {=0} + \\frac{\\mathbb E \\big[ D_{i, \\perp X} D_i \\tau_i \\big]}{\\mathbb E \\big[ D_{i, \\perp X}^2 \\big]} = \\frac{\\mathbb E \\big[ \\text{Var} (D_i | X_i) \\ \\tau(X_i) \\big]}{\\mathbb E \\big[ \\text{Var}(D_i | X_i) \\big]} $$\nThe second term of the central expression disappears because the residual $D_{i, \\perp X}$ is by construction mean independent of the control variable $X_i$, i.e.\n$$ \\mathbb E \\big[ D_{i, \\perp X} | X_i \\big] = 0 $$\nThis mean independence property is crucial to obtain an unbiased estimate and its failure in the multiple-treatment case is the source of the contamination bias.\nMultiple Treatment Arms Let\u0026rsquo;s now consider the case of multiple treatment arms, $D \\in \\lbrace 0, 1, 2 \\rbrace$, where $1$ and $2$ indicate two mutually-exclusive treatments. We still assume conditional ignorability, i.e., treatment assignment is as good as random, conditional on $X$.\n$$ \\left( Y_i^{(0)}, Y_i^{(1)}, Y_i^{(2)} \\right) \\ \\perp \\ D_i \\ | \\ X_i $$\nIn this case, we have two different individual treatment effects, one per treatment.\n$$ \\tau_{i1} = Y_i^{(1)} - Y_i^{(0)} \\qquad \\text{and} \\qquad \\tau_{i2} = Y_i^{(2)} - Y_i^{(0)} $$\nThe regression of interest is\n$$ Y_i = \\alpha + \\beta_1 D_{i1} + \\beta_2 D_{i2} + \\gamma X_i + u_i $$\nDoes the OLS estimator of $\\beta_1$ and $\\beta_2$ identify an average treatment effect?\nIt would be very tempting to say yes. In fact, it looks like not much has changed with respect to the previous setup. We just have one extra treatment, but the potential outcomes are still conditionally independent of it. Where is the issue?\nLet\u0026rsquo;s concentrate on $\\beta_1$ (the same applies to $\\beta_2$). As before, can rewrite $\\beta_1$ using the Frisch-Waugh-Lowell theorem as the OLS coefficient of a univariate regression of $Y_i$ on $D_{i1, \\perp X, D_2}$, where $D_{i1, \\perp X, D_2}$ are the residuals from regressing $D_1$ on $D_2$ and $X$.\n$$ \\beta_1 = \\frac{ \\mathbb E \\big[D_{i1, \\perp X, D_2} Y_i \\big] }{ \\mathbb E \\big[ D_{i1, \\perp X, D_2}^2 \\big]} = \\underbrace{ \\frac{ \\mathbb E \\big[ D_{i1, \\perp X, D_2} Y_i(0) \\big] }{\\mathbb E \\big[ D_{i1, \\perp X, D_2}^2 \\big]} } _ {=0} + \\frac{ \\mathbb E \\big[ D_{i1, \\perp X, D_2} D_{i1} \\tau_{i1} \\big] }{ \\mathbb E \\big[ D_{i1, \\perp X, D_2}^2 \\big]} + \\color{red}{ \\underbrace{ \\color{black}{ \\frac{ \\mathbb E \\big[ D_{i1, \\perp X, D_2} D_{i2} \\tau_{i2} \\big] }{ \\mathbb E \\big[ D_{i1, \\perp X, D_2}^2 \\big]}} } _ { \\neq 0} } $$\nThe problem is the last term. Without the last term, we could still write $\\beta_1$ as a convex combination of the individual treatment effects. However, the last term biases the estimator by adding a component that depends on the treatment effect of $D_2$, $\\tau_2$. Why does this term not disappear?\nThe problem is that $D_{i1, \\perp X, D_2}$ is not mean independent of $D_{i2}$, i.e.\n$$ \\mathbb E \\big[ D_{i1, \\perp X, D_2} D_{i2} \\ \\big| \\ X_i \\big] \\neq 0 $$\nThe reason lies in the fact that the treatments are mutually exclusive. This implies that when $D_{i1}=1$, $D_{i2}$ must be zero, regardless of the value of $X_i$. Therefore, the last term does not cancel out and it introduces a contamination bias.\nSolution Goldsmith-Pinkham, Hull, Kolesár (2022) show that a simple estimator, first proposed by Imbens and Wooldridge (2009), is able to remove the bias. The procedure is the following.\nDe-mean the control variable: $\\tilde X = X - \\bar X$ Regress $Y$ on the interaction between the treatment indicators $D$ and the demeaned control variable $\\tilde X$ The OLS estimators of $\\beta_1$ and $\\beta_2$ are unbiased estimators of the average treatment effects. It also just requires a linear regression. Moreover, this estimator is unbiased also for continuous control variables $X$, not only for a binary one as we have considered so far.\nWhy was this estimator initially proposed by Imbens and Wooldridge (2009)? Let\u0026rsquo;s analyze two parts separately: the interaction term between $D$ and $X$ and the fact that $X$ is de-meaned in the interaction term.\nFirst, the interaction term $D X$ allows us to control for different effects and/or distributions of $X$ across treatment and control group.\nSecond, de-meaning $X$ in the interaction term allows us to interpret the estimated coefficient $\\hat \\beta$ as the average treatment effect. In fact, assume we were estimating the following linear model, where $X$ is not de-meaned in the interaction term.\n$$ Y_i = \\alpha + \\beta D_i + \\gamma X_i + \\delta D_i X_i + u_i $$\nIn this case, the marginal effect of $D$ on $Y$ is $\\beta + \\delta X_i$ so that the average marginal effect is $\\beta + \\delta \\bar X$ which is different from $\\beta$.\nIf instead we use the de-meaned value of $X$ in the interaction term, the marginal effect of $D$ on $Y$ is $\\beta + \\delta (X_i - \\bar X)$ so that the average marginal effect is $\\beta + \\delta (\\bar X - \\bar X) = \\beta$. Now we can interpret $\\beta$ as the average marginal effect of $D$ on $Y$.\nSimulations In order to better understand both the problem and the solution, let\u0026rsquo;s run some simulations.\nWe run an estimator over different draws from the data generating process dgp_buttons(). This is only possible with synthetic data and we do not have this luxury in reality. For each sample, we record the estimated coefficient and the corresponding p-value.\ndef simulate(dgp, estimator, K=1000): # Initialize coefficients results = pd.DataFrame({'Coefficient': np.zeros(K), 'pvalue': np.zeros(K)}) # Compute coefficients for k in range(K): df = dgp.generate_data(seed=k) results.Coefficient[k] = estimator(df).params[1] results.pvalue[k] = estimator(df).pvalues[1] results['Significant'] = results['pvalue'] \u0026lt; 0.05 return results First, let\u0026rsquo;s try it with the old estimator that regresses revenue on both group and mobile dummy variables.\nols_estimator = lambda x: smf.ols('revenue ~ group + mobile', data=x).fit() results = simulate(dgp, ols_estimator) I plot the distribution of the coefficient estimates of button1 over 1000 simulations, highlighting the statistically significant ones at the 5% level. I also highlight the true value of the coefficient, zero, with a vertical dotted bar.\ndef plot_results(results): p_sig = sum(results['Significant']) / len(results) * 100 sns.histplot(data=results, x=\u0026quot;Coefficient\u0026quot;, hue=\u0026quot;Significant\u0026quot;, multiple=\u0026quot;stack\u0026quot;, palette = ['tab:red', 'tab:green']); plt.axvline(x=0, c='k', ls='--', label='truth') plt.title(rf\u0026quot;Estimated $\\beta_1$ ({p_sig:.0f}% significant)\u0026quot;); plot_results(results) As we can see, we reject the null hypothesis of no effect of button1 in 45% of the simulations. Since we set a confidence level of 5%, we would have expected at most around 5% of rejections. Our estimator is biased.\nAs we have seen above, the problem is that the estimator is not just a convex combination of the effect of button1 across mobile and desktop users (it\u0026rsquo;s zero for both), but it is contaminated by the effect of button2.\nLet\u0026rsquo;s now try the estimator proposed from Imbens and Wooldridge (2009). First, we need do de-mean our control variable, mobile. Then, we regress revenue on the interaction between group and the de-meaned control variable, res_mobile.\ndf['mobile_res'] = df['mobile'] - np.mean(df['mobile']) smf.ols('revenue ~ group * mobile_res', data=df).fit().summary().tables[1] coef std err t P\u003e|t| [0.025 0.975] Intercept 11.5773 0.067 172.864 0.000 11.446 11.709 group[T.button1] 0.0281 0.106 0.266 0.790 -0.180 0.236 group[T.button2] -1.5071 0.100 -15.112 0.000 -1.703 -1.311 mobile_res 2.9107 0.134 21.715 0.000 2.648 3.174 group[T.button1]:mobile_res 0.1605 0.211 0.760 0.448 -0.254 0.575 group[T.button2]:mobile_res 5.3771 0.200 26.905 0.000 4.985 5.769 The estimated coefficients are now close to their true values. The estimated coefficient for button1 is not significant, while the estimated coefficient for button2 is negative and significant.\nLet\u0026rsquo;s check whether this results holds across samples by running a simulation. We repeat the estimation procedure 1000 times and we plot the distribution of estimated coefficients for button1.\nnew_estimator = lambda x: smf.ols('revenue ~ group * mobile', data=x).fit() new_results = simulate(dgp, new_estimator) plot_results(new_results) Now the distribution of the estimated coefficient for button1 is centered around the true value of zero. Moreover, we reject the null hypothesis of no effect only in 1% of the simulations, consistently with the chosen confidence level of 95%.\nConclusion In this post, we have seen the dangers of running a factor regression model with multiple mutually exclusive treatment arms and treatment effect heterogeneity across a control variable. In this case, because the treatments are not independent, the regression coefficients are not a convex combination of the within-group average treatment effects, but also capture the treatment effects of the other treatments introducing contamination bias. The solution to the problem is both simple and elegant, requiring just a linear regression.\nHowever, the problem is more general than this setting and generally concerns every setting in which (all of the following)\nWe have multiple treatments that depend on each other We need to condition the analysis on a control variable The treatment effects are heterogeneous in the control variable Another popular example is the case of the Two-Way Fixed Effects (TWFE) estimator with staggered treatments.\nReferences [1] J. Angrist, Estimating the Labor Market Impact of Voluntary Military Service Using Social Security Data on Military Applicants (1998), Econometrica.\n[2] D. Rubin, Causal Inference Using Potential Outcomes (2005), Journal of the American Statistical Association.\n[3] G. Imbens, J. Wooldridge, Recent Developments in the Econometrics of Program Evaluation (2009), Journal of Economic Literature.\n[4] P. Goldsmith-Pinkham, P. Hull, M. Kolesár, Contamination Bias in Linear Regressions (2022), working paper.\nRelated Articles Understanding Omitted Variable Bias Understanding The Frisch-Waugh-Lovell Theorem DAGs and Control Variables Code You can find the original Jupyter Notebook here:\nhttps://github.com/matteocourthoud/Blog-Posts/blob/main/cbias.ipynb\n","date":1655251200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1655251200,"objectID":"df36ee61c9e4f4cb7fa0bfee0ae319c9","permalink":"https://matteocourthoud.github.io/post/cbias/","publishdate":"2022-06-15T00:00:00Z","relpermalink":"/post/cbias/","section":"post","summary":"Problems and solutions of linear regression with multiple treatments\nIn many causal inference settings, we might be interested in the effect of not just one treatment, but many mutually exclusive treatments.","tags":null,"title":"Understanding Contamination Bias","type":"post"},{"authors":null,"categories":null,"content":"How many times did you face questions starting with \u0026ldquo;Suppose you have an urn with three red balls and five blue balls, \u0026hellip;\u0026rdquo;? The answer for me is, not often since high-school, but recently they popped up again in data science interviews.\nDespite countless classes in statistics, I still take a deep breath and hope I won\u0026rsquo;t embarrass myself too much. My main problem is that I get crazy confused with binary labels, especially if the label itself is not too intuitive.\nSome non-intuitive (for me) binary labels include:\nsine and cosine concavity and convexity precision and recall type 1 and type 2 error rate And last but not least, combinations and permutations.\nSo this blog post is an attempt to clarify once and for all the difference between the two and get some practice.\nThe Math Permutations vs Combinations The main difference between combinations and permutations is order. In particular:\nPermutations: order matters Combinations: order does not matter What does it mean in practice?\nOne rule of thumb is to check whether the individual objects are identifiable. For example:\nUrn with blue and red balls: the individual ball is not identifiable, therefore we are talking about combinations Deck of cards: individual cards are identifiable, therefore it could be either way Also, note that for any problem, the number of permutations is always weakly larger than the number of combinations. The intuition is simple: since in permutations order matters, AB and BA are two different outcomes, while with combinations they are grouped into a single one.\nExample\nFor this section, we are going to use a simple example in which we have to order a two-scoops ice cream cone and there are three possible flavors: amarena, chocolate and pistacchio.\nPermutations Let\u0026rsquo;s start with permutations since they are mathematically simpler. We have seen that in combinations order matters. In our example, suppose we care about which flavor is on top on the ice cream. Now we are going to further distinguish between two cases:\nWith replacement Without replacement Where replacement means that after I draw an object, I can draw it again (e.g. I put it back in the pool).\nReplacement\nIn the first case, we can order both scoops of the same flavor. Therefore, for each scoop (2) we have 3 options (the flavors).\nThe number of overall events is\n$$ \\text{from 3 permute 2, with replacement} = 3 * 3 = 3^2 = 9 $$\nIn general, the number of possible permutations of $n$ objects in $k$ draws with replacement is $n^k$.\nNo Replacement\nAlternatively, we can only order different flavors in each scoop. In this case, for the first scoop we can pick any flavor, but for the second scoop we can only pick one of the two remaining flavors.\nThe number of overall events is\n$$ \\text{from 3 permute 2, without replacement} = 3 * 2 = \\frac{3!}{(3-2)!} = \\frac{3 * 2 * 1}{1} = 6 $$\nwhere ! denotes the factorial operation which can be recursively defined as $n! = n \\times (n-1)!$ with $0! = 1$.\nIn general, the number of possible permutations of $n$ objects in $k$ draws without replacement is $\\frac{n!}{(n-k)!}$.\nCombinations Combinations are usually more common since in a lot of scenarios we do not care about the order or the identity of the objects. In our example, let\u0026rsquo;s assume we don\u0026rsquo;t care which flavor gets on top. As before, we are going to further distinguish between two cases:\nWith replacement Without replacement No Replacement\nIn this case, we cannot have two scoops of the same flavor. Therefore, for each scoop (2) we have 3 options (the flavors). However, the order of the flavors does not matter, i.e. we are indifferent between getting chocolate on top or on the bottom, as long as we get it.\nThe number of overall events therefore is the number of permutations of 3 flavors among 2 scoops, divided by the permutations of 2 out of 2 scoops.\n$$ \\text{from 3 choose 2, without replacement} = \\frac{\\text{from 3 permute 2, without replacement}}{\\text{from 2 permute 2, without replacement}} = \\frac{\\frac{3!}{(3-2)!}}{\\frac{2!}{(2-2)!}} = \\frac{3!}{2!(3-2)!} = \\frac{3 * 2 * 1}{2 * 1 * 1} = 3 $$\nIn general, we define the mathematical operation \u0026ldquo;from $n$ choose $k$\u0026rdquo; as\n$$ \\text{from n choose k} := {n \\choose k} = \\frac{n!}{k!(n-k)!} $$\nWhich corresponds to the number of possible combinations of $n$ objects in $k$ draws without replacement.\nReplacement\nAlternatively, we can be allowed to order the same flavor for both scoops.\nIn this case, the number of overall events is\n$$ \\text{from 3 choose 2, with replacement} = {3 + 2 - 1 \\choose 2} = \\frac{(3 + 2 - 1)!}{2!(3-1)!} = \\frac{4 * 3 * 2 * 1}{2 * 1 * 2 * 1} = 6 $$\nIn general, the number of possible combinations of $n$ objects in $k$ draws with replacement is ${n + k - 1 \\choose k}$.\nSummary We can summarize all the possible scenarios in a simple table.\nWith Replacement Without Replacement Permutations (order matters): $n^k$ $\\frac{n!}{(n-k)!} $ Combinations (order doesn\u0026rsquo;t matter): $ {n + k - 1 \\choose k} \\text{=}\\frac{(n+k-1)!}{(n-1)!k!} $ $ {n \\choose k} = \\frac{n!}{(n-k)!k!} $ Where $n$ is the number of objects and $k$ is the number of draws.\nExample Let\u0026rsquo;s explore together a more complex example to see how we can apply permutations and combinations to compute probabilities.\nThere are four people in an elevator, four floors in the building, and each person exits at random. Find the probability that:\nall people exit at different floors all people exit at the same floor two get off at one floor and two get off at another I use the factorial and comb functions from the math library.\nfrom math import factorial, comb Practical Advice Before we start, some practical advice. What worked best for me is to approach the question in the following way:\nwhat are the overall events that we are considering? what are the positive events that we are considering? And for both questions, I ask myself:\ndoes order matter? is there replacement? Moreover, it is also very useful to restate the problem in terms of objects and draws. For example, in this case, I can restate the problem as: \u0026ldquo;I am drawing a floor for each person\u0026rdquo;. This makes it clear whether or not there is replacement, i.e. whether or not I can draw the same floor for different persons.\nQuestion 1 What is the probability that they all get off at different floors?\nTotal events: (from floors 4 permute 4, with replacement)\nPositive events: (from floors 4 permute 4, without replacement)\nfactorial(4) / 4**4 0.09375 Question 2 What is the probability that they all exit at the same floor?\nTotal events: (from 4 floors permute 4, with replacement)\nPositive events: (from floors 4 choose 1)\n4 / 4**4 0.015625 Question 3 What is the probability that two get off at one floor and two at another?\nTotal events: (from floors 4 permute 4, with replacement)\nPositive events: (from 4 people choose 2, without replacement) * (from 4 floors choose 2, without replacement)\ncomb(4, 2) * comb(4, 2) / 4**4 0.140625 Practice Questions Now it\u0026rsquo;s your time to shine! Here are some practice questions with solutions\nProblem 1 Suppose that you randomly draw 4 cards from a deck of 52 cards. What is the probability of getting 2 spades and 2 clubs?\nTotal events: (from 52 cards choose 4, without replacement)\nPositive events: (from 13 cards choose 2) * (from 13 cards choose 2)\ncomb(13, 2) * comb(13, 2) / comb(52, 4) 0.02247298919567827 Problem 2 Suppose you draw 5 cards without replacement from a standard deck of 52 playing cards. What is the probability of guessing all 5 cards in any order?\nTotal events: (from 52 cards choose 5, without replacement)\nPositive events: 1\n1 / comb(52, 5) 3.8476929233231754e-07 Problem 3 Suppose you draw 3 cards without replacement from a standard deck of 52 playing cards. What is the probability of guessing all 3 cards in the correct order?\nTotal events: (from 52 cards permute 3, without replacement)\nPositive events: 1\n1 / (factorial(52) / factorial(3)) 7.43879958514289e-68 Problem 4 Suppose you draw 5 cards without replacement from a standard deck of 52 playing cards. What is the probability of guessing 3 of them (out of 3 guesses), in any order?\nTotal events: (from 52 cards permute 3, without replacement)\nPositive events: (from 5 cards permute 3, without replacement)\n(factorial(5) / factorial(3)) / (factorial(52) / factorial(5)) 2.9755198340571564e-65 Problem 5 A 4 digit PIN is selected. What is the probability that there are no repeated digits?\nTotal events: (from 10 permute 4, with replacement)\nPositive events: (from 10 permute 4, without replacement)\n(factorial(10) / factorial(6)) / 10**4 0.504 Problem 6 In a certain state’s lottery, 48 balls numbered 1 through 48 are placed in a machine and 6 of them are drawn at random. If the 6 numbers drawn match the numbers that a player had chosen, the player wins 1,000,000. In this lottery, the order the numbers are drawn in doesn’t matter. Compute the probability that you win the million-dollar prize if you purchase a single lottery ticket.\nTotal events: (from 48 choose 6, without replacement)\nPositive events: 1\n1 / comb(48, 6) 8.148955075788542e-08 Problem 7 In a certain state’s lottery, 48 balls numbered 1 through 48 are placed in a machine and 6 of them are drawn at random. If five of the six numbers drawn match the numbers that a player has chosen, the player wins a second prize of 1,000. Compute the probability that you win the second prize if you purchase a single lottery ticket.\nTotal events: (from 48 choose 6, without replacement)\nPositive events: (from 6 choose 5, without replacement)\ncomb(6, 5) / comb(48, 5) 3.504050682589073e-06 Problem 8 Compute the probability of randomly drawing five cards from a deck and getting exactly one Ace.\nTotal events: (from 52 choose 5, without replacement)\nPositive events: (from 4 aces choose 1) * (from 48 cards that are not aces choose 4)\n4 * comb(48, 4) / comb(52, 5) 0.2994736356080894 Problem 9 Compute the probability of randomly drawing five cards from a deck and getting exactly two Aces.\nTotal events: (from 52 choose 5, without replacement)\nPositive events: (from 4 aces choose 2) * (from 48 cards that are not aces choose 3)\ncomb(4,2) * comb(48, 3) / comb(52, 5) 0.03992981808107859 Problem 10 Suppose you have 3 people in a room. What is the probability that there is at least one shared birthday?\nTotal events: (from 365 days permute 3, with replacement)\nNegative events: (from 365 days permute 3, without replacement)\n1 - (365 * 364 * 363) / (365**3) 0.008204165884781345 Problem 11 Given a class of 12 girls and 10 boys, what is the probability that a committee of five, chosen at random from the class, consists only of girls?\nTotal events: (from 22 choose 5, without replacement)\nPositive events: (from 12 choose 5, without replacement)\ncomb(12, 5) / comb(22, 5) 0.03007518796992481 Conclusion Permutation and combination questions are a classic in data science questions (unfortunately).\nCode You can find the original Jupyter Notebook here:\nhttps://github.com/matteocourthoud/Blog-Posts/blob/main/combperm.ipynb\n","date":1654819200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1654819200,"objectID":"721084b2131e724ea18c536b6b3ae59c","permalink":"https://matteocourthoud.github.io/post/combperm/","publishdate":"2022-06-10T00:00:00Z","relpermalink":"/post/combperm/","section":"post","summary":"How many times did you face questions starting with \u0026ldquo;Suppose you have an urn with three red balls and five blue balls, \u0026hellip;\u0026rdquo;? The answer for me is, not often since high-school, but recently they popped up again in data science interviews.","tags":null,"title":"Understanding Combinations and Permutations","type":"post"},{"authors":null,"categories":null,"content":"In the previous part of this blog post, we have seen how pre-testing can distort inference, i.e., how selecting control variables depending on their statistical significance results in wrong confidence intervals for the variable of interest. This bias is generally called regularization bias and also emerges in machine learning algorithms.\nIn blog post, we are going to explore a solution to the simple selection example, post-double selection, and a more general approach when we have many control variables and we do not want to assume linearity, double-debiased machine learning.\nRecap To better understand the source of the bias, in the first part of this post, we have explored the example of a firm that is interested in testing the effectiveness of an a campaign. The firm has information on its current ad spending and on the level of sales. The problem arises because the firm is uncertain on whether it should condition its analysis on the level of past sales.\nThe following Directed Acyclic Graph (DAG) summarizes the data generating process.\nflowchart LR classDef included fill:#DCDCDC,stroke:#000000,stroke-width:2px; classDef excluded fill:#ffffff,stroke:#000000,stroke-width:2px; classDef unobserved fill:#ffffff,stroke:#000000,stroke-width:2px,stroke-dasharray: 5 5; D((ad spend)) Z((past sales)) Y((sales)) D --\u0026gt; Y Z -- ??? --\u0026gt; Y Z --\u0026gt; D class D,Y included; class Z excluded; linkStyle 0 stroke:#00ff00,stroke-width:4px; I import the data generating process dgp_tbd() from src.dgp and some plotting functions and libraries from src.utils.\n%matplotlib inline %config InlineBackend.figure_format = 'retina' from src.utils import * from src.dgp import dgp_pretest df = dgp_pretest().generate_data() df.head() ads sales past_sales 0 16.719800 19.196620 6.624345 1 7.732222 9.287491 4.388244 2 10.923469 11.816906 4.471828 3 8.457062 9.024376 3.927031 4 13.085146 12.814823 5.865408 We have data on $1000$ different markets, in which we observe current sales, the amount spent in advertisement and past sales.\nWe want to understand ads spending is effective in increasing sales. One possibility is to regress the latter on the former, using the following regression model, also called the short model.\n$$ \\text{sales} = \\alpha \\cdot \\text{ads} + \\varepsilon $$\nShould we also include past sales in the regression? Then the regression model would be the following, also called long model.\n$$ \\text{sales} = \\alpha \\cdot \\text{ads} + \\beta \\cdot \\text{past sales} + \\varepsilon $$\nOne naive approach would be to let the data decide: we could run the second regression and, if the effect of past sales, $\\beta$, is statistically significant, we are good with the long model, otherwise we run the short model. This procedure is called pre-testing.\nThe problem with this procedure is that it introduces a bias that is called regularization or pre-test bias. Pre-testing ensures that this bias is small enough not to distort the estimated coefficient. However, it does not ensure that it is small enough not to distort the confidence intervals around the estimated coefficient.\nIs there a solution? Yes!\nPost-Double Selection The solution is called post-double selection. The method was first introduced in Belloni, Chernozhukov, Hansen (2014) and later expanded in a variety of papers.\nThe authors assume the following data generating process:\n$$ y = \\alpha D + \\beta X + u \\newline D = \\delta X + v $$\nIn our example, $Y$ corresponds to sales, $D$ corresponds to ads, $X$ corresponds to past_sales and the effect of interest is $\\alpha$. In our example, $X$ is 1-dimensional for simplicity, but generally we are interested in cases where X is high-dimensional, potentially even having more dimensions than the number of observations. In that case, variable selection is essential in linear regression since we cannot have more features than variables (the OLS coefficients are not uniquely determined anymore).\nPost-double selection consists in the following procedure.\nReduced Form selection: lasso $Y$ on $X$. Select the statistically significant variables in the set $S_{RF} \\subseteq X$ First Stage selection: regress $D$ on $X$. Select the statistically significant variables in the set $S_{FS} \\subseteq X$ Regress $Y$ on $D$ and the union of the selected variables in the first two steps, $S_{FS} \\cup S_{RF}$ The authors show that this procedure produces confidence intervals for the coefficient of interest $\\alpha$ that have the correct coverage, i.e. the correct probability of type 1 error.\nNote (1): this procedure is always less parsimonious, in terms of variable selection, than pre-testing. In fact, we still select all the variables we would have selected with pre-testing but, in the first stage, we might select additional variables.\nNote (2): the terms first stage and reduced form come from the intrumental variables literature in econometrics. Indeed, the first application of post-double selection was to select instrumental variables in Belloni, Chen, Chernozhukov, Hansen (2012).\nNote (3): the name post-double selection comes from the fact that now we are not performing variable selection once but twice.\nIntuition The idea behind post-double selection is: bound the omitted variables bias. In case you are not familiar with it, I wrote a separate blog post on omitted variable bias.\nIn our setting, we can express the omitted variable bias as\n$$ \\text{OVB} = \\beta \\delta \\qquad \\text{ where } \\qquad \\beta := \\frac{Cov(X, Y)}{Var(X)}, \\quad \\delta := \\frac{Cov(D, X)}{Var(D)} $$\nAs we can see, the omitted variable bias comes from the product of two quantities related to the omitted variable $X$:\nIts partial correlation with the outcome $Y$, $\\beta$ Its partial correlation with the variable of interest $D$, $\\delta$ With pre-testing, we ensure that the partial correlation between $X$ the outcome $Y$, $\\beta$, is small. In fact, we omit $Z$ when we shouldn\u0026rsquo;t (i.e. we commit a type 2 error) rarely. What do small and rarely mean?\nWhen we are selecting a variable because of its significance, we ensure that it dimension is smaller than $\\frac{c}{\\sqrt{n}}$ for some number $c$, where $n$ is the sample size.\nTherefore, with pre-testing, we ensure that, no matter what the value of $\\delta$ is, the dimension of the bias is smaller than $\\frac{c}{\\sqrt{n}}$ which means that it converges to zero for sufficiently large $n$. This is why the pre-testing estimator is still consistent.\nHowever, in order for our confidence intervals to have the right coverage, this is not enough. In practice, we need the bias to converge to zero faster than $\\frac{1}{\\sqrt{n}}$. Why?\nTo get an intuition for this result, we need to turn to the Central Limit Theorem. The CLT tells us that for large $n$ the distribution of the sample average of a random variable $X$ converges to a normal distribution with mean $\\mu$ and standard deviation $\\frac{\\sigma}{\\sqrt{n}}$, where $\\mu$ and $\\sigma$ are the mean and standard deviation of $X$. To do inference, we usually apply the Central Limit Theorem to our estimator to get its asymptotic distribution, which in turn allows us to build confidence intervals (using the mean and the standard deviation). Therefore, if the bias is not sensibly smaller than the standard deviation of the estimator, the confidence intervals are going to be wrong. Therefore, we need the bias to converge to zero faster than the standard deviation, i.e. faster than $\\frac{1}{\\sqrt{n}}$.\nIn our setting, the omitted variable bias is $\\beta \\gamma$ and we want it to converge to zero faster than $\\frac{1}{\\sqrt{n}}$. Post-double selection guarantees that\nReduced form selection (pre-testing): any \u0026ldquo;missing\u0026rdquo; variable $j$ has $|\\beta_j| \\leq \\frac{c}{\\sqrt{n}}$ First stage selection (additional): any \u0026ldquo;missing\u0026rdquo; variable $j$ has $|\\delta_j| \\leq \\frac{c}{\\sqrt{n}}$ As a consequence, as long as the number of omitted variables is finite, the omitted variable bias is going to converge to zero at a rate $\\frac{1}{n}$, which is faster than $\\frac{1}{\\sqrt{n}}$. Problem solved!\nApplication Let\u0026rsquo;s now go back to our example and test the post-double selection procedure. In practice, we want to do the following:\nFirst Stage selection: regress ads on past_sales. Check if past_sales is statistically significant Reduced Form selection: regress sales on past_sales. Check if past_sales is statistically significant Regress sales on ads and include past_sales only if it was significant in either one of the two previous regressions I update the pre_test function from the first part of the post to compute also the post-double selection estimator.\ndef pre_test(d='ads', y='sales', x='past_sales', K=1000, **kwargs): # Init alphas = pd.DataFrame({'Long': np.zeros(K), 'Short': np.zeros(K), 'Pre-test': np.zeros(K), 'Post-double': np.zeros(K)}) # Loop over simulations for k in range(K): # Generate data df = dgp_pretest().generate_data(seed=k, **kwargs) # Compute coefficients alphas['Long'][k] = smf.ols(f'{y} ~ {d} + {x}', df).fit().params[1] alphas['Short'][k] = smf.ols(f'{y} ~ {d}', df).fit().params[1] # Compute significance of beta and gamma p_value_ydx = smf.ols(f'{y} ~ {d} + {x}', df).fit().pvalues[2] p_value_yx = smf.ols(f'{y} ~ {x}', df).fit().pvalues[1] p_value_dx = smf.ols(f'{d} ~ {x}', df).fit().pvalues[1] # Select pre-test specification based on regression of y on d and x if p_value_ydx\u0026lt;0.05: alphas['Pre-test'][k] = alphas['Long'][k] else: alphas['Pre-test'][k] = alphas['Short'][k] # Select post-double specification based on regression of y on d and x if p_value_yx\u0026lt;0.05 or p_value_dx\u0026lt;0.05: alphas['Post-double'][k] = alphas['Long'][k] else: alphas['Post-double'][k] = alphas['Short'][k] return alphas alphas = pre_test() We can now plot the distributions (over simulations) of the estimated coefficients.\ndef plot_alphas(alphas, true_alpha): # Init plot K = len(alphas.columns) fig, axes = plt.subplots(1, K, figsize=(4*K, 5), sharey=True, sharex=True) # Make one plot for each set of coefficients for i, key in enumerate(alphas.columns): axes[i].hist(alphas[key].values, bins=30, lw=.1, color=f'C{int(i==3)*2}') axes[i].set_title(key) axes[i].axvline(true_alpha, c='r', ls='--') legend_text = [rf'$\\alpha=${true_alpha}', rf'$\\hat \\alpha=${np.mean(alphas[key]):.4f}'] axes[i].legend(legend_text, prop={'size': 10}, loc='upper right') plot_alphas(alphas, true_alpha=1) As we can see, the post-double selection estimator always correctly selects the long regression and therefore has the correct distribution.\nDouble-checks In the last post, we ran some simulations in order to investigate when pre-testing bias emerges. We saw that pre-testing is a problem for\nSmall sample sizes $n$ Intermediate values of $\\beta$ When the value of $\\beta$ depends on the sample size Let\u0026rsquo;s check that post-double selection removes regularization bias in all the previous cases.\nFirst, let\u0026rsquo;s simulate the distribution of the post-double selection estimator $\\hat \\alpha_{postdouble}$ for different sample sizes.\nNs = [100,300,1000,3000] alphas = {f'N = {n:.0f}': pre_test(N=n) for n in Ns} def compare_alphas(alphas, true_alpha): # Init plot fig, axes = plt.subplots(1, len(alphas), figsize=(4*len(alphas), 5), sharey=True, sharex=True) # Make one plot for each set of coefficients for i, key in enumerate(alphas.keys()): axes[i].hist(alphas[key]['Pre-test'], bins=30, lw=.1, alpha=0.5) axes[i].hist(alphas[key]['Post-double'], bins=30, lw=.1, alpha=0.5, color='C2') axes[i].set_title(key) axes[i].axvline(true_alpha, c='r', ls='--') axes[i].legend([rf'$\\alpha=${true_alpha}', 'Pre-test', 'Post-double'], prop={'size': 10}, loc='upper right') compare_alphas(alphas, true_alpha=1) For small samples, the distribution of the pre-testing estimator is not normal but rather bimodal. From the plots we can see that the post-double estimator is gaussian also in small sample sizes.\nNow we repeat the same exercise, but for different values of $\\beta$, the coefficient of past_sales on sales.\nbetas = 0.3 * np.array([0.1,0.3,1,3]) alphas = {f'beta = {b:.2f}': pre_test(b=b) for b in betas} compare_alphas(alphas, true_alpha=1) Again, the post-double selection estimator has a gaussian distribution irrespectively of the value of $\\beta$, while he pre-testing estimator suffers from regularization bias.\nFor the last simulation, we change both the coefficient and the sample size at the same time.\nbetas = 0.3 * 30 / np.sqrt(Ns) alphas = {f'N = {n:.0f}': pre_test(b=b, N=n) for n,b in zip(Ns,betas)} compare_alphas(alphas, true_alpha=1) Also in this last case, the post-double selection estimator performs well and inference is not distorted.\nDouble Debiased Machine Learning So far, we only have analyzed a linear, univariate example. What happens if the dimension of $X$ increases and we do not know the functional form through which $X$ affects $Y$ and $D$? In these cases, we can use machine learning algorithms to uncover these high-dimensional non-linear relationships.\nChernozhukov, Chetverikov, Demirer, Duflo, Hansen, Newey, and Robins (2018) investigate this setting. In particular, the authors consider the following partially linear model.\n$$ Y = \\alpha D + g(X) + u \\ D = m(X) + v $$\nwhere $Y$ is the outcome variable, $D$ is the treatment to interest and $X$ is a potentially high-dimensional set of control variables.\nNaive approach A naive approach to estimation of $\\alpha$ using machine learning methods would be, for example, to construct a sophisticated machine learning estimator for learning the regression function $\\alpha D$ + $g(X)$.\nSplit the sample in two: main sample and auxiliary sample [why? see note below] Use the auxiliary sample to estimate $\\hat g(X)$ Use the main sample to compute the orthogonalized component of $Y$ on $X$: $\\ \\hat u = Y - \\hat{g} (X)$ Use the main sample to estimate the residualized OLS estimator from regressing $\\hat u$ on $D$ $$ \\hat \\alpha = \\left( D\u0026rsquo; D \\right) ^{-1} D\u0026rsquo; \\hat u $$\nThis estimator is going to have two problems:\nSlow rate of convergence, i.e. slower than $\\sqrt(n)$ It will be biased because we are employing high dimensional regularized estimators (e.g. we are doing variable selection) Note (1): so far we have not talked about it, but variable selection procedure also introduce another type of bias: overfitting bias. This bias emerges because of the fact that the sample used to select the variables is the same that is used to estimate the coefficient of interest. This bias is easily accounted for with sample splitting: using different sub-samples for the selection and the estimation procedures.\nNote (2): why can we use the residuals from step 3 to estimate $\\alpha$ in step 4? Because of the Frisch-Waugh-Lovell theorem. If you are not familiar with it, I have written a blog post on the Frisch-Waugh-Lovell theorem here.\nOrthogonalization Now consider a second construction that employs an orthogonalized formulation obtained by directly partialling out the effect of $X$ from $D$ to obtain the orthogonalized regressor $v = D − m(X)$.\nSplit the sample in two: main sample and auxiliary sample\nUse the auxiliary sample to estimate $\\hat g(X)$ from\n$$ Y = \\alpha D + g(X) + u \\ $$\nUse the auxiliary sample to estimate $\\hat m(X)$ from\n$$ D = m(X) + v $$\nUse the main sample to compute the orthogonalized component of $D$ on $X$ as\n$$ \\hat v = D - \\hat m(X) $$\nUse the main sample to estimate the double-residualized OLS estimator as\n$$ \\hat \\alpha = \\left( \\hat{v}\u0026rsquo; D \\right) ^{-1} \\hat{v}\u0026rsquo; \\left( Y - \\hat g(X) \\right) $$\nThe estimator is root-N consistent! This means that not only the estimator converges to the true value as the sample sizes increases (i.e. it\u0026rsquo;s consistent), but also its standard deviation does (i.e. it\u0026rsquo;s root-N consistent).\nHowever, the estimator still has a lower rate of convergence because of sample splitting. The problem is solved by inverting the split sample, re-estimating the coefficient and averaging the two estimates. Note that this procedure is valid since the two estimates are independent by the sample splitting procedure.\nA Cautionary Tale Before we conclude, I have to mention a recent research paper by Hünermund, Louw, and Caspi (2022), in which the authors show that double-debiased machine learning can easily backfire, if we apply blindly.\nThe problem is related to bad control variables. If you have never heard this term, I have written an introductory blog post on good and bad control variables here. In short, conditioning the analysis on additional features is not always good for causal inference. Depending on the setting, there might exist variables that we want to leave out of our analysis since their inclusion can bias the coefficient of interest, preventing a causal interpretation. The simplest example is variables that are common outcomes, of both the treatment $D$ and outcome variable $Y$.\nThe double-debiased machine learning model implicitly assumes that the control variables $X$ are (weakly) common causes to both the outcome $Y$ and the treatment $D$. If this is the case, and no further mediated/indirect relationship exists between $X$ and $Y$, there is no problem. However, if, for example, some variable among the controls $X$ is a common effect instead of a common cause, its inclusion will bias the coefficient of interest. Moreover, this variable is likely to be highly correlated either with the outcome $Y$ or with the treatment $D$. In the latter case, this implies that post-double selection might include it in cases in which simple selection would have not. Therefore, in presence of bad control variables, doule-debiased machine learning might be even worse than simple pre-testing.\nIn short, as for any method, it is crucial to have a clear understanding of the method\u0026rsquo;s assumptions and to always check for potential violations.\nConclusion In this post, we have seen how to use post-double selection and, more generally, double debiased machine learning to get rid of an important source of bias: regularization bias.\nThis contribution by Victor Chernozhukov and co-authors has been undoubtedly one of the most relevant advances in causal inferences in the last decade. It is now widely employed in the industry and included in the most used causal inference packages, such as EconML (Microsoft) and causalml (Uber).\nIf you (understandably) feel the need for more material on double-debiased machine learning, but you do not feel like reading academic papers (also very understandable), here is a good compromise.\nIn this video lecture, Victor Chernozhukov himself presents the idea. The video lecture is relatively heavy on math and statistics, but you cannot get a more qualified and direct source than this!\nReferences [1] A. Belloni, D. Chen, V. Chernozhukov, C. Hansen, Sparse Models and Methods for Optimal Instruments With an Application to Eminent Domain (2012), Econometrica.\n[2] A. Belloni, V. Chernozhukov, C. Hansen, Inference on treatment effects after selection among high-dimensional controls (2014), The Review of Economic Studies.\n[3] V. Chernozhukov, D. Chetverikov, M. Demirer, E. Duflo, C. Hansen, W. Newey, J. Robins, Double/debiased machine learning for treatment and structural parameters (2018), The Econometrics Journal.\n[4] P. Hünermund, B. Louw, I. Caspi, Double Machine Learning and Automated Confounder Selection - A Cautionary Tale (2022), working paper.\nRelated Articles Double Debiased Machine Learning (part 1) Understanding Omitted Variable Bias Understanding The Frisch-Waugh-Lovell Theorem DAGs and Control Variables Code You can find the original Jupyter Notebook here:\nhttps://github.com/matteocourthoud/Blog-Posts/blob/main/pds.ipynb\n","date":1654387200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1654387200,"objectID":"bd599436ccb29a7c644f39b9a9005334","permalink":"https://matteocourthoud.github.io/post/pds/","publishdate":"2022-06-05T00:00:00Z","relpermalink":"/post/pds/","section":"post","summary":"In the previous part of this blog post, we have seen how pre-testing can distort inference, i.e., how selecting control variables depending on their statistical significance results in wrong confidence intervals for the variable of interest.","tags":null,"title":"Double Debiased Machine Learning (part 2)","type":"post"},{"authors":null,"categories":null,"content":"Causal inference, machine learning and regularization bias\nIn causal inference, we often estimate causal effects by conditioning the analysis on other variables. We usually refer to these variables as control variables or confounders. In randomized control trials or AB tests, conditioning can increase the power of the analysis, by reducing imbalances that have emerged despite randomization. However, conditioning is even more important in observational studies, where, absent randomization, it might be essential to recover causal effects.\nWhen we have many control variables, we might want to select the most relevant ones, ppossibly capturing nonlinearities and interactions. Machine learning algorithms are perfect for this task. However, in these cases, we are introducing a bias that is called regularization or pre-test, or feature selection bias. In this and the next blog post, I try to explain the source of the bias and a very poweful solution called double debiased machine learning, which has been probably one of the most relevant advancement at the intersection of machine learning and causal inference of the last decade.\nPre-Testing Since this is a complex topic, let\u0026rsquo;s start with a simple example.\nSuppose we were a firm and we are interested in the effect of advertisement spending on revenue: is advertisement worth the money? There are also a lot of other things that might influence sales, therefore, we are thinking of controlling for past sales in the analysis, in order to increase the power of our analysis.\nAssume the data generating process can be represented with the following Directed Acyclic Graph (DAG). If you are not familiar with DAGs, I have written a short introduction here.\nflowchart LR classDef included fill:#DCDCDC,stroke:#000000,stroke-width:2px; classDef excluded fill:#ffffff,stroke:#000000,stroke-width:2px; classDef unobserved fill:#ffffff,stroke:#000000,stroke-width:2px,stroke-dasharray: 5 5; D((ad spend)) Z((past sales)) Y((sales)) D --\u0026gt; Y Z -- ??? --\u0026gt; Y Z --\u0026gt; D class D,Y included; class Z excluded; linkStyle 0 stroke:#00ff00,stroke-width:4px; I import the data generating process dgp_tbd() from src.dgp and some plotting functions and libraries from src.utils.\n%matplotlib inline %config InlineBackend.figure_format = 'retina' from src.utils import * from src.dgp import dgp_pretest df = dgp_pretest().generate_data() df.head() ads sales past_sales 0 16.719800 19.196620 6.624345 1 7.732222 9.287491 4.388244 2 10.923469 11.816906 4.471828 3 8.457062 9.024376 3.927031 4 13.085146 12.814823 5.865408 We have data on $1000$ different markets, in which we observe current sales, the amount spent in advertisement and past sales.\nWe want to understand ads spending is effective in increasing sales. One possibility is to regress the latter on the former, using the following regression model, also called the short model.\n$$ \\text{sales} = \\alpha \\cdot \\text{ads} + \\varepsilon $$\nShould we also include past sales in the regression? Then the regression model would be the following, also called long model.\n$$ \\text{sales} = \\alpha \\cdot \\text{ads} + \\beta \\cdot \\text{past sales} + \\varepsilon $$\nSince we are not sure whether to condition the analysis on past sales, we could let the data decide: we could run the second regression and, if the effect of past sales, $\\beta$, is statistically significant, we are good with the long model, otherwise we run the short model.\nsmf.ols('sales ~ ads + past_sales', df).fit().summary().tables[1] coef std err t P\u003e|t| [0.025 0.975] Intercept 0.1405 0.185 0.758 0.448 -0.223 0.504 ads 0.9708 0.030 32.545 0.000 0.912 1.029 past_sales 0.3381 0.095 3.543 0.000 0.151 0.525 It seems that the effect of past sales on current sales is positive and significant. Therefore, we are happy with our specification and we conclude that the effect of ads on sales is positive and significant with a 95% confidence interval of $[0.912, 1.029]$.\nThe Bias There is an issue with this procedure: we are not taking into account the fact that we have run a test to decide whether to include past_sales in the regression. The fact that we have decided to include past_sales because its coefficient is significant does have an effect on the inference on the effect of ads on sales, $\\alpha$.\nThe best way to understand the problem is through simulations. Since we have access to the data generating process dgp_pretest() (unlike in real life), we can just test what would happen if we were repeating this procedure multiple times:\nWe draw a new sample from the data generating process. We regress sales on ads and past_sales. If the coefficient of past_sales is significant at the 95% level, we keep $\\hat \\alpha_{long}$ from (2). Otherwise, we regress sales on ads only, and we keep that coefficient $\\hat \\alpha_{short}$. I write a pre_test function to implement the procedure above. I also save the coefficients from both regressions, long and short, and the chosen one, called the pre-test coefficient.\nReminder: we are pre-testing the effect of past_sales on sales but the coefficient of interest is the one of ads on sales.\ndef pre_testing(d='ads', y='sales', x='past_sales', K=1000, **kwargs): # Init alpha = {'Long': np.zeros(K), 'Short': np.zeros(K), 'Pre-test': np.zeros(K)} # Loop over simulations for k in range(K): # Generate data df = dgp_pretest().generate_data(seed=k, **kwargs) # Compute coefficients alpha['Long'][k] = smf.ols(f'{y} ~ {d} + {x}', df).fit().params[1] alpha['Short'][k] = smf.ols(f'{y} ~ {d}', df).fit().params[1] # Compute significance of beta p_value = smf.ols(f'{y} ~ {d} + {x}', df).fit().pvalues[2] # Select specification based on p-value if p_value\u0026lt;0.05: alpha['Pre-test'][k] = alpha['Long'][k] else: alpha['Pre-test'][k] = alpha['Short'][k] return alpha alphas = pre_testing() We can now plot the distributions (over simulations) of the estimated coefficients.\ndef plot_alphas(alphas, true_alpha): # Init plot fig, axes = plt.subplots(1, len(alphas), figsize=(4*len(alphas), 5), sharey=True, sharex=True) # Make one plot for each set of coefficients for i, key in enumerate(alphas.keys()): axes[i].hist(alphas[key], bins=30, lw=.1) axes[i].set_title(key) axes[i].axvline(true_alpha, c='r', ls='--') legend_text = [r'$\\alpha=%.0f$' % true_alpha, r'$\\hat \\alpha=%.4f$' % np.mean(alphas[key])] axes[i].legend(legend_text, prop={'size': 10}, loc='upper right') plot_alphas(alphas, true_alpha=1) In the plot above, I have depicted the estimated coefficients, across simulations, for the different regression specifications.\nAs we can see from the first plot, if we were always running the long regression, our estimator $\\hat \\alpha_{long}$ would be unbiased and normally distributed. However, if we were always running the short regression (second plot), our estimator $\\hat \\alpha_{short}$ would be biased.\nThe pre-testing procedure generates an estimator $\\hat \\alpha_{pretest}$ that is a mix of the two: most of the times we select the correct specification, the long regression, but sometimes the pre-test fails to reject the null hypothesis of no effect of past sales on sales, $H_0 : \\beta = 0$, and we select the incorrect specification, running the short regression.\nImportantly, the pre-testing procedure does not generate a biased estimator. As we can see in the last plot, the estimated coefficient is very close to the true value, 1. The reason is that most of the time, the number of times we select the short regression is sufficiently small not to introduce bias, but not small enough to have valid inference.\nIndeed, pre-testing distorts inference: the distribution of the estimator $\\hat \\alpha_{pretest}$ is not normal anymore, but bimodal. The consequence is that our confidence intervals for $\\alpha$ are going to have the wrong coverage (contain the true effect with a different probability than the claimed one).\nWhen is pre-testing a problem? The problem of pre-testing arises because of the bias generated by running the short regression: omitted variable bias (OVB). In you are not familiar with OVB, I have written a short introduction here. In general however, we can express the omitted variable bias introduced by regressing $Y$ on $D$ ignoring $X$ as\n$$ \\text{OVB} = \\beta \\delta \\qquad \\text{ where } \\qquad \\beta := \\frac{Cov(X, Y)}{Var(X)}, \\quad \\delta := \\frac{Cov(D, X)}{Var(D)} $$\nWhere $\\beta$ is the effect of $X$ (past sales in our example) on $Y$ (sales) and $\\delta$ is the effect of $D$ (ads) on $X$.\nPre-testing is a problem if\nWe run the short regression instead of the long one and The effect of the bias is sensible What can help improving (1), i.e. the probability of correctly rejecting the null hypothesis of zero effect of past sales, $H_0 : \\beta = 0$? The answer is simple: a bigger sample size. If we have more observations, we can more precisely estimate $\\beta$ and it is going to be less likely that we commit a type 2 error and run the short regression instead of the long one.\nLet\u0026rsquo;s simulate the estimated coefficient $\\hat \\alpha$ under different sample sizes. Remember that the sample size used until now is $N=1000$.\nNs = [100,300,1000,3000] alphas = {f'N = {n:.0f}': pre_testing(N=n)['Pre-test'] for n in Ns} plot_alphas(alphas, true_alpha=1) As we can see from the plots, as the sample size increases (left to right), the bias decreases and the distribution of the estimator $\\hat \\alpha_{pretest}$ converges to a normal distribution.\nWhat happens instead if the value of $\\beta$ was different? It is probably going to affect point (2) in the previous paragraph, but how?\nIf $\\beta$ is very small, it is going to be hard to detect it, and we will often end up running the short regression, introducing a bias. However, if $\\beta$ is very small, it also implies that the magnitude of the bias is small and therefore it is not going to affect our estimate of $\\alpha$ much\nIf $\\beta$ is very big, it is going to be easy to detect and we will often end up running the long regression, avoiding the bias (which would have been very big though).\nLet\u0026rsquo;s simulate the estimated coefficient $\\hat \\alpha$ under different values of $\\beta$. The true value used until now was $\\beta = 0.3$.\nbetas = 0.3 * np.array([0.1,0.3,1,3]) alphas = {f'beta = {b:.2f}': pre_testing(b=b)['Pre-test'] for b in betas} plot_alphas(alphas, true_alpha=1) As we can see from the plots, as the value of $\\beta$ increases, the bias first appears and then disappears. When $\\beta$ is small (left plot), we often choose the short regression, but the bias is small and the average estimate is very close to the true value. For intermediate values of $\\beta$, the bias is sensible and it has a clear effect on inference. Lastly, for large values of $\\beta$ instead (right plot), we always run the long regression and the bias disappears.\nBut when is a coefficient big or small? And big or small with respect to what? The answer is simple: with respect to the sample size, or more accurately, with respect to the inverse of the square root of the sample size, $1 / \\sqrt{n}$. The reason is deeply rooted in the Central Limit Theorem, but I won\u0026rsquo;t cover it here.\nThe idea is easier to show than to explain, so let\u0026rsquo;s repeat the same simulation as above, but now we will increase both the coefficient and the sample size at the same time.\nbetas = 0.3 * 30 / np.sqrt(Ns) alphas = {f'N = {n:.0f}': pre_testing(b=b, N=n)['Pre-test'] for n,b in zip(Ns,betas)} plot_alphas(alphas, true_alpha=1) As we can see, now that $\\beta$ is proportional to $1 / \\sqrt{n}$, the distortion is not going away, not matter the sample size. Therefore, inference will always be wrong.\nWhile a coefficient that depends on the sample size might sound not intuitive, it captures well the idea of magnitude in a world where we do inference relying on asymptotic results, first among all the Central Limit Theorem. In fact, the Central Limit Theorem relieas on an infinitely large sample size. However, with an infinite amount of data, no coefficient is small and any non-zero effect is detected with certainty.\nPre-Testing and Machine Learning So far we talked about a linear regression with only 2 variables. Where is the machine learning we were promised?\nUsually we do not have just one control variable (or confounder), but many. Moreover, we might want to be flexible with respect to the functional form through which these control variables enter the model. In general, we will assume the following model:\n$$ Y = \\alpha D + g_0(X) + u \\newline D = m_0(X) + v $$\nWhere the effect of interest is still $\\alpha$, $X$ is potentially high dimensional and we do not take a stand on the functional form through which $X$ influences $D$ or $Y$.\nIn this setting, it is natural to use a machine learning algorithm to estimate $g_0$ and $m_0$. However, machine learning algorithms usually introduce a regularization bias that is comparable to pre-testing.\nPossibly, the \u0026ldquo;simplest\u0026rdquo; way to think about it is Lasso. Lasso is linear in $X$, with a penalization term that effectively just performs the variable selection we discussed above. Therefore, if we were to use Lasso of $X$ and $D$ on $Y$ we would be introducing regularization bias and inference would be distorted. The same goes for more complex algorithms.\nLastly, you might still wonder \u0026ldquo;why is the model linear in the treatment variable $D$?\u0026rdquo;. Doing inference is much easier in linear model, not only for computational reasons but also for interpretation. Moreover, if the treatment $D$ is binary, the linear functional form is without loss of generality. A stronger assumption is the additive separability of $D$ and $g(X)$.\nConclusion In this post, I have tried to explain how does regularization bias emerges and why it can the an issue in causal inference. This problem is inherently related to settings with many control variables or where we would like to have a model-free (i.e. non-parametric) when controlling for confounders. These are exactly the settings in which machine learning algorithms can be useful.\nIn the next post, I will cover a simple and yet incredibly powerful solution to this problem: double-debiased machine learning.\nReferences [1] A. Belloni, D. Chen, V. Chernozhukov, C. Hansen, Sparse Models and Methods for Optimal Instruments With an Application to Eminent Domain (2012), Econometrica.\n[2] A. Belloni, V. Chernozhukov, C. Hansen, Inference on treatment effects after selection among high-dimensional controls (2014), The Review of Economic Studies.\n[3] V. Chernozhukov, D. Chetverikov, M. Demirer, E. Duflo, C. Hansen, W. Newey, J. Robins, Double/debiased machine learning for treatment and structural parameters (2018), The Econometrics Journal.\nRelated Articles Understanding Omitted Variable Bias Understanding The Frisch-Waugh-Lovell Theorem DAGs and Control Variables Code You can find the original Jupyter Notebook here:\nhttps://github.com/matteocourthoud/Blog-Posts/blob/main/pretest.ipynb\n","date":1654300800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1654300800,"objectID":"1c03618e28411fd9e7c78ea524ff7f59","permalink":"https://matteocourthoud.github.io/post/pretest/","publishdate":"2022-06-04T00:00:00Z","relpermalink":"/post/pretest/","section":"post","summary":"Causal inference, machine learning and regularization bias\nIn causal inference, we often estimate causal effects by conditioning the analysis on other variables. We usually refer to these variables as control variables or confounders.","tags":null,"title":"Double Debiased Machine Learning (part 1)","type":"post"},{"authors":null,"categories":null,"content":"If you search the Wikipedia definition of Chi-Squared test, you get the following definition:\nPearson\u0026rsquo;s chi-squared test $\\chi^2$ is a statistical test applied to sets of categorical data to evaluate how likely it is that any observed difference between the sets arose by chance.\nWhat does it mean? Let\u0026rsquo;s see it together.\nTest 1: Discrete Distribution Suppose you want to test whether a dice is fair. You throw the dice 60 times and you count the number of times you get each outcome.\nLet\u0026rsquo;s simulate some data (from a fair dice).\n%matplotlib inline %config InlineBackend.figure_format = 'retina' from src.utils import * def generate_data_dice(N=60, seed=1): np.random.seed(seed) # Set seed for replicability dice_numbers = [1,2,3,4,5,6] # Dice numbers dice_throws = np.random.choice(dice_numbers, size=N) # Actual dice throws data = pd.DataFrame({\u0026quot;dice number\u0026quot;: dice_numbers, \u0026quot;observed\u0026quot;: [sum(dice_throws==n) for n in dice_numbers], \u0026quot;expected\u0026quot;: int(N / 6)}) return data data_dice = generate_data_dice() data_dice dice number observed expected 0 1 10 10 1 2 14 10 2 3 6 10 3 4 8 10 4 5 12 10 5 6 10 10 If we were throwing the dice a lot of times, we would expect the same number of observations for each outcome. However, there is inherent noise in the process. How can we tell whether the fact that we didn\u0026rsquo;t get exactly 10 observations for each outcome is just due to randomness or it\u0026rsquo;s because the dice is unfair?\nThe idea is to compute some statistic whose distribution is known under the assumption that the dice is fair, and then check if its value is \u0026ldquo;unusual\u0026rdquo; or not. If the value is particularly \u0026ldquo;unusual\u0026rdquo;, we reject the null hypothesis that the dice is fair.\nIn our case, the statistic we choose is the chi-squared $\\chi^{2}$ test-statistic.\nThe value of the Pearson\u0026rsquo;s chi-squared test-statistic is\n$$ T_{\\chi^2} = \\sum _{i=1}^{n} \\frac{(O_i - E_i)^{2}}{E_i} = N \\sum _{i=1}^{n} \\frac{\\left(O_i/N - p_i \\right)^2 }{p_i} $$\nwhere\n$O_{i}$ = the number of observations of type i. $N$ = total number of observations $E_{i}=N * p_{i}$ = the expected (theoretical) count of type $i$, asserted by the null hypothesis that the fraction of type $i$ in the population is $p_{i}$ $n$ = the number of cells in the table. def compute_chi2_stat(data): return sum( (data.observed - data.expected)**2 / data.expected ) chi2_stat = compute_chi2_stat(data_dice) chi2_stat 4.0 What do we make of this number? Is it unusual?\nIf the dice were fair, the test Pearson\u0026rsquo;s chi-squared test-statistic $T_{\\chi^2}$ would be distributed as a chi-squared distribution with $k-1$ degrees of freedom, $\\chi^2_{k-1}$. For the moment, take this claim at face value, we will verify it later, both empirically and theoretically. We will also discuss the degrees of freedom in detail later on.\nImportant! Do not confuse the chi-squared test statistic (a number) with the chi-squared distribution (a distribution).\nWhat does a chi-squared distribution with $n-1$ degrees of freedom, $\\chi^2_{k-1}$, look like?\nfrom scipy.stats import chi2 x = np.arange(0, 30, 0.001) # x-axis ranges from 0 to 30 with .001 steps chi2_5_pdf = chi2.pdf(x, df=5) # Chi-square distribution with 5 degrees of freedom plt.plot(x, chi2_5_pdf); How does the value of the statistic we have observed compares with its the distribution under the null hypothesis of a fair dice?\nplt.plot(x, chi2.pdf(x, df=5), label='chi2 distribution'); plt.axvline(chi2_stat, color='k', label='chi2 statistic') plt.legend(); The test statistic seems to fall well within the distribution, i.e. it does not seem to be an unusual event. Indeed, the question we want to answer is: \u0026ldquo;under the null hypothesis that the dice is fair, how unlikely is the statistic we have observed?\u0026rdquo;.\nThe last component we need in order to build a hypothesis test is a level of confidence, i.e. a threshold of \u0026ldquo;unlikeliness\u0026rdquo; of an event, below which we declare that the event is too unlikely under the model, for the model to be true. Let\u0026rsquo;s say we decide to set that threshold at 5%.\nIf the likelihood of observing an even that (or more) extreme than the one we have actually observed is less than 5%, we reject the null hypothesis that the dice is fair.\nWhat is this value for a chi-squared distribution with 5 degrees of freedom? We can compute the percent point function (ppf) of 95% for the chi-squared distribution, which is essentially the inverse of the cumulative distribution function. This value is often called the critical value.\nz95 = chi2.ppf(0.95, df=5) z95 11.070497693516351 Since our value is smaller than the critical value, we do not reject the null. The critical value is indeed critical because it splits the domain of the test statistic into two areas: the rejection area, where we reject the null hypothesis, and the non-rejection area, where we don\u0026rsquo;t.\nWe can plot the rejection and non-rejection areas in a plot.\ndef plot_test(x, stat, df): z95 = chi2.ppf(0.95, df=df) chi2_pdf = chi2.pdf(x, df=df) plt.plot(x, chi2_pdf); plt.fill_between(x[x\u0026gt;z95], chi2_pdf[x\u0026gt;z95], color='r', alpha=0.4, label='rejection area') plt.fill_between(x[x\u0026lt;z95], chi2_pdf[x\u0026lt;z95], color='g', alpha=0.4, label='non-rejection area') plt.axvline(chi2_stat, color='k', label='chi2 statistic') plt.ylim(0, plt.ylim()[1]) plt.legend(); plot_test(x, chi2_stat, df=5) From the plot, we can see that we do not reject the null hypothesis that the dice is fair for our value of the $\\chi^2$ test statistic.\nWhy the Chi-squared Distribution? How do we know that that particular statistic has that particular distribution?\nBefore digging into the math, we can check this claim via simulation. Since we have access to the data generating process, we can repeat the procedure above many times, i.e.\nroll a (fair) dice 60 times compute the chi-square statistic and then plot the distribution of chi square statistics.\ndef simulate_chi2stats(K, N, dgp): chi2_stats = [compute_chi2_stat(dgp(seed=k)) for k in range(K)] return np.array(chi2_stats) chi2_stats = simulate_chi2stats(K=100, N=60, dgp=generate_data_dice) plt.hist(chi2_stats, density=True, bins=30, alpha=0.3, color='C0'); plt.plot(x, chi2_5_pdf); Since we only did it 100 times, the distribution looks pretty coarse but vaguely close to its theoretical counterpart. Let\u0026rsquo;s try 1000 times.\nchi2_stats = simulate_chi2stats(K=1000, N=60, dgp=generate_data_dice) plt.hist(chi2_stats, density=True, bins=30, alpha=0.3, color='C0'); plt.plot(x, chi2_5_pdf); The empirical distribution of the test statistic is indeed very close to its theoretical counterpart.\nSome Statistics Why does the distribution of the test statistic look like that? Let\u0026rsquo;s now dig deeper into the math.\nThere are two things we need to know in order to understand the answer:\nthe Central Limit Theorem the relationship between a chi-squared and a normal distribution The Wikipedia definition of the Central Limit Theorem says that\n\u0026ldquo;In probability theory, the central limit theorem (CLT) establishes that, in many situations, when independent random variables are summed up, their properly normalized sum tends toward a normal distribution (informally a bell curve) even if the original variables themselves are not normally distributed.\u0026rdquo;\nWhere does a normal distribution come up in our case? If we look at a single row in our data, i.e. the occurrences of a specific dice throw, it can be interpreted as the sum of realization from a Bernoulli distribution with probability 1/6.\n\u0026ldquo;In probability theory and statistics, the Bernoulli distribution is the discrete probability distribution of a random variable which takes the value $1$ with probability $p$ and the value $0$ with probability $q=1-p$.\u0026rdquo;\nIn our case, the probability of getting a particular number is exactly 1/6. What is the distribution of the sum of its realizations? The Central Limit Theorem also tells us that:\n\u0026ldquo;If $X_1, X_2, \\dots , X_n, \\dots$ are random samples drawn from a population with overall mean $\\mu$ and finite variance $\\sigma^2$, and if $\\bar X_n$ is the sample mean of the first $n$ samples, then the limiting form of the distribution,\n$$ Z = \\lim_{n \\to \\infty} \\sqrt{n} \\left( \\frac{\\bar X_n - \\mu }{\\sigma} \\right) $$\nis a standard normal distribution.\u0026rdquo;\nTherefore, in our case, the distribution of the sum of Bernoulli distributions with mean $p$ is distributed as a normal distribution with\nmean $p$ variance $p * (1-p)$ Therefore, we can obtain a random variable that is asymptotically standard normal distributed as\n$$ \\lim_{n \\to \\infty} \\ \\sqrt{n} \\left( \\frac {\\bar X_n - p}{\\sqrt{p * (1-p)}} \\right) \\sim N(0,1) $$\nOur last piece: what is a chi-squared distribution? The Wikipedia definition says\n\u0026ldquo;If $Z_1, \u0026hellip;, Z_k$ are independent, standard normal random variables, then the sum of their squares,\n$$ Q = \\sum_{i=1}^k Z_i^2 $$ is distributed according to the chi-squared distribution with $k$ degrees of freedom.\u0026rdquo;\nI.e. the sum of standard normal distributions is a chi-squared distribution, where the degrees of freedom indicate the number of normal distributions we are summing over. Since the normalized sum of realizations of each dice number should converge to a standard normal distribution, their sum of squares should converge to a chi-squared distribution. I.e.\n$$ \\lim_{n \\to \\infty} \\ \\sum_k n \\frac{(\\bar X_n - p)^2}{p * (1-p)} \\sim \\chi^2_k $$\nThere is just one issue: the last distribution is not really independent from the others. In fact, as soon as we know that we have thrown 60 dices and how many 1s, 2s, 3s, 4s, and 5s we got, we can compute the number of 6s. Therefore, we should exclude one distribution since only 5 (or, in general, $k-1$) are truly independent.\nIn practice, however, we sum all distributions, but then we scale them down by multiplying them by $(1-p)$ so that we have\n$$ \\lim_{n \\to \\infty} \\ \\sum_k n \\frac{(\\bar X_n - p)^2}{p} \\sim \\chi^2_{k-1} $$\nwhich is exactly the formula we used to compute the test statistic:\n$$ T_{\\chi^2} = \\sum _{i=1}^{n} \\frac{(O_i - E_i)^{2}}{E_i} = N \\sum _{i=1}^{n} \\frac{\\left(O_i/N - p_i \\right)^2 }{p_i} $$\nTest 2: Independence Chi-squared tests can also be used to test independence between two variables. The idea is fundamentally the same as the test in the previous section: checking systematic differences between observed and expected values, across different variables.\nSuppose you have data on grades in a classroom, by gender. Grades go from $1$ to $4$. Assuming males and females are equally prepared for the test, you want to test whether there has been discrimination in grading.\nThe problem is again asserting whether the observed differences are random or systematic.\nLet\u0026rsquo;s generate some data (under the no discrimination assumption).\ndef generate_data_grades(N_male=60, N_female=40, seed=1): np.random.seed(seed) grade_scale = [1,2,3,4] p = [0.1, 0.2, 0.5, 0.2] grades_male = np.random.choice(grade_scale, size=N_male, p=p) grades_female = np.random.choice(grade_scale, size=N_female, p=p) data = pd.DataFrame({\u0026quot;grade\u0026quot;: grade_scale + grade_scale, \u0026quot;gender\u0026quot;: [\u0026quot;male\u0026quot; for i in grade_scale] + [\u0026quot;female\u0026quot; for i in grade_scale], \u0026quot;observed\u0026quot;: [sum(grades_male==n) for n in grade_scale] + [sum(grades_female==n) for n in grade_scale], }) data['expected gender'] = data.groupby(\u0026quot;gender\u0026quot;)[\u0026quot;observed\u0026quot;].transform(\u0026quot;mean\u0026quot;) data['expected grade'] = data.groupby(\u0026quot;grade\u0026quot;)[\u0026quot;observed\u0026quot;].transform(\u0026quot;mean\u0026quot;) data['expected'] = data['expected gender'] * data['expected grade'] / data['observed'].mean() return data data_grades = generate_data_grades() data_grades grade gender observed expected gender expected grade expected 0 1 male 9 15.0 5.5 6.6 1 2 male 14 15.0 10.5 12.6 2 3 male 27 15.0 24.5 29.4 3 4 male 10 15.0 9.5 11.4 4 1 female 2 10.0 5.5 4.4 5 2 female 7 10.0 10.5 8.4 6 3 female 22 10.0 24.5 19.6 7 4 female 9 10.0 9.5 7.6 Has there been discrimination? We again compare observed and expected grades, where expected grades are computed under the independence assumption: as the product of the marginal distributions of grade and gender.\nThe value of the test-statistic is\n$$ T_{\\chi^2} = \\sum_{i=1}^r \\sum_{j=1}^c \\frac{(O_{i,j} - E_{i,j})^2 }{ E_{i,j} } = N \\sum_{i,j} p_{i \\cdot} p_{\\cdot j} \\left( \\frac{O_{i,j}/N - p_{i \\cdot} p_{\\cdot j} }{ p_{i \\cdot} p_{\\cdot j}} \\right)^2 $$\nwhere\n$N$ is the total sample size (the sum of all cells in the table)\n$p_{i \\cdot} = \\frac{O_{i\\cdot }}{N} = \\sum_{j=1}^{c} \\frac{O_{i,j}}{N}$ is the fraction of observations of type i ignoring the column attribute (fraction of row totals), and\n$p_{\\cdot j} = \\frac{O_{\\cdot j}}{N} = \\sum_{i=1}^{r} \\frac{O_{i,j}}{N}$ is the fraction of observations of type j ignoring the row attribute (fraction of column totals).\nSo the formula for the test statistic is the same\nchi2_stat = compute_chi2_stat(data_grades) chi2_stat 3.490327550477927 As before, we can double-check whether the statistic is indeed distributed as a chi-squared with $k-1$ degrees of freedom by simulating the data generating process.\nchi2_stats = simulate_chi2stats(K=1000, N=60, dgp=generate_data_grades) plt.hist(chi2_stats, density=True, bins=30, alpha=0.3, color='C0'); plt.plot(x, chi2_5_pdf); The two distributions do not look similar anymore.\nWhat happened? We forgot to change the degrees of freedom! The general formula for the degrees of freedom when testing the independence of variables is $(N_i - 1) \\times (N_j - 1)$. So in our case, it\u0026rsquo;s $(4-1) \\times (2-1) = 3$.\nchi2_3_pdf = chi2.pdf(x, df=3) chi2_stats = simulate_chi2stats(K=1000, N=60, dgp=generate_data_grades) plt.hist(chi2_stats, density=True, bins=30, alpha=0.3, color='C0'); plt.plot(x, chi2_3_pdf); Now the empirical distribution is close to its theoretical counterpart.\nDo we reject the null hypothesis of independent distributions of gender and grades? We can visualize the value of the test statistic together with the rejection areas.\nplot_test(x, chi2_stat, df=3) We do not reject the null hypothesis of independend distributions of gender and grades.\nTest 3: Continuous Distributions As we have seen, the chi-square test can be used to compare observed means/frequencies against a null hypothesis. How can we use this statistic to test a distributional assumption?\nThe answer is simple: we can construct conditional means. The easiest way to do it is to bin the data into intervals and then check if the observed frequencies match the expected probabilities, within each bin.\nNote: a good practice is to have equally sized bins, in terms of expected probabilities, since it ensures that we have as many observations in each bin as possible.\nAs an example, let\u0026rsquo;s assume we have to analyze the customer service of a firm. We would like to understand if the number of complains follows an exponential distribution with paramenter $\\lambda=1$.\nfrom scipy.stats import expon def generate_complaints_data(N=100, cuts=4, seed=2): np.random.seed(seed) complaints = np.random.exponential(size=N) cat, bins = pd.qcut(complaints, cuts, retbins=True) p = [expon.cdf(bins[n+1]) - expon.cdf(bins[n]) for n in range(len(bins)-1)] data = pd.DataFrame({\u0026quot;bin\u0026quot;: cat.unique(), \u0026quot;observed\u0026quot;: [sum(cat==n) for n in cat.unique()], \u0026quot;expected\u0026quot;: np.dot(p, N)}) return data, complaints data_complaints, complaints = generate_complaints_data() data_complaints bin observed expected 0 (0.297, 0.573] 25 24.360534 1 (0.0121, 0.297] 25 17.974853 2 (0.573, 1.025] 25 20.489741 3 (1.025, 5.092] 25 35.258339 We have split the data into equally sized bins of size 25 and we have computed the expected number of observations within each bin, if the data was indeed exponentially distributed with parameter $\\lambda=1$.\nWe can plot the observed and realized distribution of the data.\n# Plot data plt.hist(complaints, density=True, bins=30, alpha=0.3, color='C0'); exp_pdf = expon.pdf(x) plt.plot(x, exp_pdf); The two distributions seem close but we need a test statistic in order to assess whether the differences are random or systematic.\nchi2_stat = compute_chi2_stat(data_complaints) chi2_stat 6.739890904809741 Do we reject the null hypothesis that the data is drawn from an exponential distribution? We decide by comparing the value of the test statistic with the rejection areas.\nplot_test(x, chi2_stat, df=3) Since the test statistic falls outside of the rejection area, we do not reject the null hypothesis that the data is drawn from an exponential distribution.\nConclusion In this tutorial, we have seen how to perform 3 hypoteses tests\ntesting if a set of means or sums is coming from the expected distribution testing if two distributions are independent or not testing a specific data generating process The underlying principle is the same: testing discrepancies between expected and observed count data.\nThe key statistic is Pearson\u0026rsquo;s chi-square statistic and the key distribution is the chi-squared distribution. We have seen how to compute the statistic, why it has a chi-squared distribution, and how to use this information to perform a statistical hypothesis test.\nCode You can find the original Jupyter Notebook here:\nhttps://github.com/matteocourthoud/Blog-Posts/blob/main/notebooks/chisquared.ipynb\n","date":1653955200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1653955200,"objectID":"6cc8886782d077be8d6a99ab075dcb4a","permalink":"https://matteocourthoud.github.io/post/chisquared/","publishdate":"2022-05-31T00:00:00Z","relpermalink":"/post/chisquared/","section":"post","summary":"If you search the Wikipedia definition of Chi-Squared test, you get the following definition:\nPearson\u0026rsquo;s chi-squared test $\\chi^2$ is a statistical test applied to sets of categorical data to evaluate how likely it is that any observed difference between the sets arose by chance.","tags":null,"title":"Understanding The Chi-Squared Test","type":"post"},{"authors":null,"categories":null,"content":"The problem of comparing distributions often arises in causal inference when we have to assess the quality of randomization.\nWhen we want to assess the causal effect of a policy (or, feature, campaign, drug, \u0026hellip;), the golden standard in causal inference are randomized control trials, also known in the industry as A/B tests. In practice, we select a sample for the study and we randomly split it into a control and a treatment group, to compare the outcomes between the two groups. The idea is that, under a set of assumption, randomization assures that only difference between the two groups is the treatment, on average. Therefore, we can attribute the differences in outcomes to the treatment effect alone.\nThe problem is that, despite randomization, the two groups are never identical. However, sometimes, they are not even \u0026ldquo;similar\u0026rdquo;. For example, we might have more females in one group, or older people, etc.. (we usually call these characteristics, covariates or control variables). When it happens, we cannot be certain anymore that the differences in the outcome is only due to the treatment and cannot be attributed to the inbalanced covariates instead. Therefore, it is always important, after randomization, to check whether all observed variables are balance across groups and whether there are no systematic differences. Another option, to be certain that certain covariates are balanced, is stratified sampling.\nIn this blog post, we are going to see different ways to compare two (or more) distributions and assess the magnitude and significance of their difference. We are going to consider two different approaches, graphical and numerical. The two approaches generally trade-off intuition with rigour: from plots we can assess subtle differences but it\u0026rsquo;s hard to assess whether these differences are systematic or due to noise.\nThe Data Let\u0026rsquo;s assume we need to perform an experiment on a group of individuals and we have randomized them into a treatment and control group. We would like them to be as comparable as possible, in order to attribute any difference between the two groups to the treatment effect alone. We also have divided the treatment group in different arms for testing different treatments.\nFor this example, I have simulated a dataset of 1000 individuals, for whom we observe a set of characteristics. I import the data generating process dgp_rnd_assignment() from src.dgp and some plotting functions and libraries from src.utils.\n%matplotlib inline %config InlineBackend.figure_format = 'retina' from src.utils import * from src.dgp import dgp_rnd_assignment df = dgp_rnd_assignment().generate_data() df.head() group arm gender age income 0 control arm 2 0 29.0 3967.16 1 control arm 4 1 32.0 2927.77 2 treatment NaN 0 29.0 1642.66 3 control arm 4 0 25.0 1867.64 4 treatment NaN 1 32.0 3202.35 We have information on $1000$ individuals, for which we observe gender, age and income.\nWe now want to understand whether the treatment and control groups are comparable or if there are systematic difference between them.\nPlots Let\u0026rsquo;s concentrate on one variable: income. Does the income distribution differ between the two groups?\nA first approach could be the boxplot. The boxplot is a good trade-off between summary statistics and data visualization. The center and the borders of the box represent the median and the first (Q1) and third quartile (Q3), respectively. The whiskers instead, extend to the first data points that are more than 1.5 times the interquartile range (Q3 - Q1) outside the box. The points that fall outside of the whiskers are plotted individually.\nTherefore, the boxplot provides both summary statistics (the box and the whiskers) and direct data visualization (the extreme data points).\nsns.boxplot(x='group', y='income', data=df); plt.title(\u0026quot;Boxplot\u0026quot;); It seems that the income distribution in the treatment group is slightly more dispersed: the orange box is larger and the extreme treatment points cover a wider range. However, the issue with the boxplot is that it hides the shape of the data, telling us some summary statistics but not showing us the actual data distribution.\nThe most intuitive way to plot a distribution is the histogram. The histogram groups the data into equally spaced bins and plots the number of observations within each bin.\nsns.histplot(x='income', data=df, hue='group', bins=50); plt.title(\u0026quot;Histogram\u0026quot;); There are multiple issues with this plot:\nThe two histograms are not comparable: we would like a density, not a count The number of bins is arbitrary We can solve the first issue using the stat option to plot the density instead of the count and setting the common_norm option to False to use the same normalization.\nsns.histplot(x='income', data=df, hue='group', bins=50, stat='density', common_norm=False); plt.title(\u0026quot;Density Histogram\u0026quot;); Now the two histograms are comparable!\nHowever, an important issue remains: the size of the bins is arbitrary. If we bunch the data less, we end up with bins with one observation at most, if we bunch the data more, we lose information. This is a classical bias-variance trade-off.\nOne possible solution is to use a kernel density function that tries to approximate the histogram with a continuous function, using kernel density estimation (KDE).\nsns.kdeplot(x='income', data=df, hue='group', common_norm=False); plt.title(\u0026quot;Kernel Density Function\u0026quot;); We can now visualize both distributions very intuitively.\nIf we had multiple categories, a very similar plot is the violinplot. The violinplot also performs kernel density estimation, but plots separate densities along the y axis so that they don\u0026rsquo;t overlap. By default, it also adds a miniature boxplot inside.\nsns.violinplot(x='arm', y='income', data=df); plt.title(\u0026quot;Violin Plot\u0026quot;); From the plot, it seems that the estimated kernel density of income is very similar across treatment arms.\nHowever, the issue with kernel density estimation is that it is somehow a black-box and might mask relevant features of the data.\nA much more transparent representation of the two distribution is their cumulative distribution function. At each point of the x axis (income) we plot the percentage of data points that have an equal or lower value. The main advantages of the cumulative distribution function are that\nwe do not need to make any arbitrary choice (e.g. number of bins) we do not need to perform any approximation (e.g. with KDE) sns.histplot(x='income', data=df, hue='group', bins=len(df), stat=\u0026quot;density\u0026quot;, element=\u0026quot;step\u0026quot;, fill=False, cumulative=True, common_norm=False); plt.title(\u0026quot;Cumulative distribution function\u0026quot;); We can now clearly see that there are relatively more observations with low income in the treatment group than in the control group. In fact, the blue line is above the orange line on the right and below the orange line on the left.\nA related alternative method is the qq-plot, where q stands for quantile. The qq-plot plots the quantiles of the two distributions against each other. If the distributions are the same, we should get the 45 degree line.\nThere is no native qq-plot function in Python and, while the statsmodels package provides a qqplot function, it is quite cumbersome. Therefore, we will do it by hand.\nFirst, we need to compute the quartiles of the two groups, using the percentile function.\nincome = df['income'].values income_t = df.loc[df.group=='treatment', 'income'].values income_c = df.loc[df.group=='control', 'income'].values df_pct = pd.DataFrame() df_pct['q_treatment'] = np.percentile(income_t, range(100)) df_pct['q_control'] = np.percentile(income_c, range(100)) Now we can plot the two quantile distributions against each other, plus the 45-degree line, representing the benchmark perfect fit.\nplt.scatter(x='q_control', y='q_treatment', data=df_pct, label='Actual fit'); sns.lineplot(x='q_control', y='q_control', data=df_pct, color='r', label='Line of perfect fit'); plt.xlabel('Quantile of income, control group') plt.ylabel('Quantile of income, treatment group') plt.legend() plt.title(\u0026quot;QQ plot\u0026quot;); The qq-plot delivers a very similar insight with respect to the cumulative distribution plot: income in the treatment group is generally lower.\nTests So far, we have seen different ways to visualize differences between distributions. The main advantage of visualization is intuition: we can eyeball the differences and intuitively assess them.\nHowever, we might want to be more rigorous and try to assess the statistical significance of the difference between the distributions, i.e. answer the question \u0026ldquo;is the observed difference systematic or due to sampling variation?\u0026rdquo;.\nWe are now going to analyze different tests to discern two distributions from each other.\nT-test The first and most common test is the student t-test. T-tests are generally used to compare means. In this case, we want to test whether the means of the income distribution is the same across the two groups. The test statistic for the two-means comparison test is given by:\n$$ stat = \\frac{|\\bar x_1 - \\bar x_2|}{\\sqrt{s_1 / n_1 + s_2 / n_2}} $$\nWhere $\\bar x$ is the sample mean and $s$ is the sample standard deviation. Under mild conditions, the test statistic is asymptotically distributed as a student t distribution.\nWe use the ttest_ind function from scipy to perform the t-test. The function returns both the test statistic and the implied p-value.\nfrom scipy.stats import ttest_ind stat, p_value = ttest_ind(income_c, income_t) print(f\u0026quot;t-test: statistic={stat:.4f}, p-value={p_value:.4f}\u0026quot;) t-test: statistic=-1.3192, p-value=0.1874 In general, it is common practice to always perform this test on all variables, when we are running a randomized control trial or A/B test. The results of these tests are usually collected into a table that is called balance table.\nWe can use the create_table_one function from the causalml library to generate the balance table. As the name of the function suggests, the balance table should always be the first table you present when performing an A/B test.\nfrom causalml.match import create_table_one df['treatment'] = df['group']=='treatment' create_table_one(df, 'treatment', ['gender', 'age', 'income']) Control Treatment SMD Variable n 704 296 age 31.94 (8.53) 35.88 (7.78) 0.4822 gender 0.51 (0.50) 0.58 (0.49) 0.1419 income 3166.07 (1321.89) 3296.32 (1645.58) 0.0873 In the first two columns, we can see the average of the different variables across the treatment and control groups, with standard errors in parenthesis. In the last column, we have the p-values of the t-test for the null hypothesis of zero difference in means.\nFrom the table, we observe that we cannot reject the null hypothesis of zero difference in mean for any variable, at the 95% confidence level.\nChi-Squared Test The chi-squared test is a very powerful test that can be used in many different settings. If you want to find out more about it, I have written a very comprehensive blog post here.\nOne of the least known applications of the chi-squared test, is testing the similarity between two distributions. The idea is to bin the observations of the two groups. If the two distributions were the same, we would expect the same frequency of observations in each bin. Importantly, we need enough observations in each bin, in order for the test to be valid. I generate bins corresponding to deciles of the distribution of income in the control group.\ndf_bins = pd.DataFrame() _, bins = pd.qcut(income_c, q=10, retbins=True) df_bins['bin'] = pd.cut(income_c, bins=bins).value_counts().index df_bins['income_c'] = pd.cut(income_c, bins=bins).value_counts().values df_bins['income_t'] = pd.cut(income_t, bins=bins).value_counts().values df_bins bin income_c income_t 0 (808.96, 1730.77] 70 42 1 (1730.77, 2127.948] 70 36 2 (2127.948, 2411.451] 70 25 3 (2411.451, 2670.312] 71 20 4 (2670.312, 2935.05] 70 26 5 (2935.05, 3208.662] 70 13 6 (3208.662, 3542.805] 71 25 7 (3542.805, 3972.996] 70 26 8 (3972.996, 4912.782] 70 41 9 (4912.782, 11634.39] 71 42 We can now perform the test by comparing the frequencies of the two distributions, across bins. The test statistic is given by:\n$$ stat = \\sum _{i=1}^{n} \\frac{(O_i - E_i)^{2}}{E_i} $$\nWhere the bins are indexed by $i$ and $O$ is the observed number of data points in bin $i$ and $E$ is the expected number of data points in bin $i$. Since we generated the bins using deciles of the distribution of income in the control group, we expect the number of observations per bin in the treatment group to be the same across bins. Under mild assumptions, the test statistic is asymptocally distributed as a chi-squared distribution.\nTo compute the test statistic and the p-value of the test, we use the chisquare function from scipy.\nfrom scipy.stats import chisquare df_bins['income_t_norm'] = df_bins['income_t'] / np.sum(df_bins['income_t']) * np.sum(df_bins['income_c']) stat, p_value = chisquare(df_bins['income_c'], df_bins['income_t_norm']) print(f\u0026quot;Chi-squared Test: statistic={stat:.4f}, p-value={p_value:.4f}\u0026quot;) Chi-squared Test: statistic=95.2526, p-value=0.0000 The p-value is practically zero, implying that we reject the null hypothesis of no difference between the two distributions.\nKolmogorov-Smirnov Test The idea of the Kolmogorov-Smirnov test, is to compare the cumulative distributions of the two groups. In particular, the Kolmogorov-Smirnov test statistic is the maximum absolute difference between the two cumulative distributions.\n$$ stat = \\sup _{x} \\ \\Big| \\ F_1(x) - F_2(x) \\ \\Big| $$\nWhere $F_1$ and $F_2$ are the two cumulative distribution functions and $x$ are the values of the underlying variable. Under mild conditions, the asymptotic distribution of the Kolmogorov-Smirnov test statistic is known.\nTo better understand the test, let\u0026rsquo;s plot the cumulative distribution functions and the test statistic. First, we compute the cumulative distribution functions.\ndf_ks = pd.DataFrame() df_ks['income'] = np.sort(df['income'].unique()) df_ks['F_control'] = df_ks['income'].apply(lambda x: np.mean(income_c\u0026lt;=x)) df_ks['F_treatment'] = df_ks['income'].apply(lambda x: np.mean(income_t\u0026lt;=x)) df_ks.head() income F_control F_treatment 0 808.96 0.001420 0.000000 1 831.93 0.002841 0.000000 2 893.28 0.002841 0.003378 3 925.08 0.004261 0.003378 4 951.21 0.004261 0.006757 We now need to find the point where the absolute distance between the cumulative distribution functions is largest.\nk = np.argmax( np.abs(df_ks['F_control'] - df_ks['F_treatment'])) tstat = np.abs(df_ks['F_treatment'][k] - df_ks['F_control'][k]) We can visualize the value of the test statistic, by plotting the two cumulative distribution functions and the value of the test statistic.\ny = (df_ks['F_treatment'][k] + df_ks['F_control'][k])/2 plt.plot('income', 'F_control', data=df_ks, label='Control') plt.plot('income', 'F_treatment', data=df_ks, label='Treatment') plt.errorbar(x=df_ks['income'][k], y=y, yerr=tstat/2, color='k', capsize=5, mew=3, label=f\u0026quot;Test statistic: {tstat:.4f}\u0026quot;) plt.legend(loc='center right'); plt.title(\u0026quot;Kolmogorov-Smirnov Test\u0026quot;); From the plot, we can see that the value of the test statistic corresponds to the distance between the two cumulative distributions at income=4000. For that value of income, we have the largest imbalance between the two groups.\nWe can now perform the actual test using the kstest function from scipy.\nfrom scipy.stats import kstest stat, p_value = kstest(income_t, income_c) print(f\u0026quot; Kolmogorov-Smirnov Test: statistic={stat:.4f}, p-value={p_value:.4f}\u0026quot;) Kolmogorov-Smirnov Test: statistic=0.0881, p-value=0.0730 The p-value is still above 5%: we do not reject the null hypothesis that the two distributions are the same, with 95% confidence.\nPermutation Testing A non-parametric alternative is permutation testing. The idea is that, under the null hypothesis, the two distributions should be the same, therefore shuffling the observations across groups, should not significantly alter any statistic.\nWe can then chose any statistic and compute how much more extreme it is for different permutations, with respect to its value in the original sample. For example, let\u0026rsquo;s use as a test statistic the sample mean of the treatment group.\nstats = [np.mean(np.random.choice(income, size=len(income_t), replace=False)) for _ in range(1000)] p_value = np.mean(stats \u0026gt; np.mean(income_t)) print(f\u0026quot;Permutation test: p-value={p_value:.4f}\u0026quot;) Permutation test: p-value=0.0820 The permutation test gives us a p-value very similar to the ones obtained with the other tests.\nHow do we interpret the p-value? It means that he sample mean of the treatment group in the data is larger than $1 - 0.082 = 91.8%$ of the sample means of the treatment group across the permuted samples.\nWe can visualize the test, by plotting the distribution of the test statistics against its sample value.\nplt.hist(stats, label='Permutation Statistics'); plt.axvline(x=np.mean(income_t), c='r', ls='--', label='Sample Statistic'); plt.legend(); plt.title('Permutation Test'); As we can see, the sample statistic is quite extreme with respect to the values in the permuted samples, but not excessively.\nConclusion TBD\n","date":1653782400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1653782400,"objectID":"81171df4e768a06dbf2989d511dc9c4f","permalink":"https://matteocourthoud.github.io/post/distr_compare/","publishdate":"2022-05-29T00:00:00Z","relpermalink":"/post/distr_compare/","section":"post","summary":"The problem of comparing distributions often arises in causal inference when we have to assess the quality of randomization.\nWhen we want to assess the causal effect of a policy (or, feature, campaign, drug, \u0026hellip;), the golden standard in causal inference are randomized control trials, also known in the industry as A/B tests.","tags":null,"title":"Comparing Distributions, From Zero to Hero","type":"post"},{"authors":null,"categories":null,"content":"In causal inference, bias is extremely problematic because it makes inference not valid. Bias generally means that an estimator will not deliver the estimate of the true effect, on average.\nThis is why, in general, we prefer estimators that are unbiased, at the cost of a higher variance, i.e. more noise. Does it mean that every biased estimator is useless? Actually no. Sometimes, with domain knowledge, we can still draw causal conclusions even with a biased estimator.\nIn this post, we are going to review a specific but frequent source of bias, omitted variable bias (OVB). We are going to explore the causes of the bias and leverage these insights to make causal statements, despite the bias.\nTheory Suppose we are interested in the effect of a variable $D$ on a variable $y$. However, there is a third variable $Z$ that we do not observe and that is correlated with both $D$ and $Y$. Assume the data generating process can be represented with the following Directed Acyclic Graph (DAG). If you are not familiar with DAGs, I have written a short introduction here.\nflowchart LR classDef included fill:#DCDCDC,stroke:#000000,stroke-width:2px; classDef excluded fill:#ffffff,stroke:#000000,stroke-width:2px; classDef unobserved fill:#ffffff,stroke:#000000,stroke-width:2px,stroke-dasharray: 5 5; D((D)) Z((Z)) Y((Y)) D --\u0026gt; Y Z --\u0026gt; D Z --\u0026gt; Y class D,Y excluded; class Z unobserved; Since there is a backdoor path from $D$ to $y$ passing through $Z$, we need to condition our analysis on $Z$ in order to recover the causal effect of $D$ on $y$. If we could observe $Z$, we would run a linear regression of $y$ on $D$ and $Z$ to estimate the following model:\n$$ y = \\alpha D + \\gamma Z + \\varepsilon $$\nwhere $\\alpha$ is the effect of interest. This regression is usually referred to as the long regression since it includes all variables of the model.\nHowever, since we do not observe $Z$, we have to estimate the following model:\n$$ y = \\alpha D + u $$\nThe corresponding regression is usually referred to as the short regression since it does not include all the variables of the model\nWhat is the consequence of estimating the short regression when the true model is the long one?\nIn that case, the OLS estimator of $\\alpha$ is\n$$ \\begin{align} \\hat \\alpha \u0026amp;= \\frac{Cov(D, y)}{Var(D)} = \\newline \u0026amp;= \\frac{Cov(D, \\alpha D + \\gamma Z + \\varepsilon)}{Var(D)} = \\newline \u0026amp;= \\frac{Cov(D, \\alpha D)}{Var(D)} + \\frac{Cov(D, \\gamma Z)}{Var(D)} + \\frac{Cov(D, \\varepsilon)}{Var(D)} = \\newline \u0026amp;= \\alpha + \\underbrace{ \\gamma \\frac{Cov(D, Z)}{Var(D)} }_{\\text{omitted variable bias}} \\end{align} $$\nTherefore, we can write the omitted variable bias as\n$$ \\text{OVB} = \\gamma \\delta \\qquad \\text{ where } \\qquad \\delta := \\frac{Cov(D, Z)}{Var(D)} $$\nThe beauty of this formula is its interpretability: the omitted variable bias consists in just two components, both extremely easy to interpret.\n$\\gamma$: the effect of $Z$ on $y$ $\\delta$: the effect of $D$ on $Z$ Additional Controls What happens if we had additional control variables in the regression? For example, assume that besides the variable of interest $D$, we also observe a vector of other variables $X$ so that the long regression is\n$$ y = \\alpha D + \\beta X + \\gamma Z + \\varepsilon $$\nThanks to the Frisch-Waugh-Lowell theorem, we can simply partial-out $X$ and express the omitted variable bias in terms of $D$ and $Z$.\n$$ \\text{OVB} = \\gamma \\times \\frac{Cov(D^{\\perp X}, Z^{\\perp X})}{Var(D^{\\perp X})} $$\nwhere $D^{\\perp X}$ are the residuals from regressing $D$ on $X$ and $Z^{\\perp X}$ are the residuals from regressing $Z$ on $X$. If you are not familiar with Frisch-Waugh-Lowell theorem, I have written a short note here.\nChernozhukov, Cinelli, Newey, Sharma, Syrgkanis (2022) further generalize to analysis the the setting in which the control variables $X$ and the unobserved variables $Z$ enter the long model with a general functional form $f$\n$$ y = \\alpha D + f(Z, X) + \\varepsilon $$\nYou can find more details in their paper, but the underlying idea remains the same.\nExample Suppose we were a researcher interested in the relationship between education and wages. Does investing in education pay off in terms of future wages? Suppose we had data on wages for people with different years of education. Why not looking at the correlation between years of education and wages?\nThe problem is that there might be many unobserved variables that are correlated with both education and wages. For simplicity, let\u0026rsquo;s concentrate on ability. People of higher ability might decide to invest more in education just because they are better in school and they get more opportunities. On the other hand, they might also get higher wages afterwards, purely because of their innate ability.\nWe can represent the data generating process with the following Directed Acyclic Graph (DAG).\nflowchart TD classDef included fill:#DCDCDC,stroke:#000000,stroke-width:2px; classDef excluded fill:#ffffff,stroke:#000000,stroke-width:2px; classDef unobserved fill:#ffffff,stroke:#000000,stroke-width:2px,stroke-dasharray: 5 5; D((education)) Z((ability)) Y((wage)) X1((age)) X2((gender)) D --\u0026gt; Y Z --\u0026gt; D Z --\u0026gt; Y X1 --\u0026gt; Y X2 --\u0026gt; Y class D,Y included; class X1,X2 excluded; class Z unobserved; Let\u0026rsquo;s load and inspect the data. I import the data generating process from src.dgp and some plotting functions and libraries from src.utils.\n%matplotlib inline %config InlineBackend.figure_format = 'retina' from src.utils import * from src.dgp import dgp_educ_wages df = dgp_educ_wages().generate_data(N=50) df.head() age gender education wage 0 62 male 6.0 3800.0 1 44 male 8.0 4500.0 2 63 male 8.0 4700.0 3 33 male 7.0 3500.0 4 57 female 6.0 4000.0 We have information on 300 individuals, for which we observe their age, their gender, the years of education, and the current monthly wage.\nSuppose we were directly regressing wage on education.\nshort_model = smf.ols('wage ~ education + gender + age', df).fit() short_model.summary().tables[1] coef std err t P\u003e|t| [0.025 0.975] Intercept 2657.8864 444.996 5.973 0.000 1762.155 3553.618 gender[T.male] 335.1075 132.685 2.526 0.015 68.027 602.188 education 95.9437 38.752 2.476 0.017 17.940 173.948 age 12.3120 6.110 2.015 0.050 0.013 24.611 The coefficient of education is positive and significant. However, we know there might be an omitted variable bias, because we do not observe ability. In terms of DAGs, there is a backdoor path from education to wage passing through ability that is not blocked and therefore biases our estimate.\nflowchart TD classDef included fill:#DCDCDC,stroke:#000000,stroke-width:2px; classDef excluded fill:#ffffff,stroke:#000000,stroke-width:2px; classDef unobserved fill:#ffffff,stroke:#000000,stroke-width:2px,stroke-dasharray: 5 5; D((education)) Z((ability)) Y((wage)) X1((age)) X2((gender)) D --\u0026gt; Y Z --\u0026gt; D Z --\u0026gt; Y X1 --\u0026gt; Y X2 --\u0026gt; Y class D,Y included; class X1,X2 excluded; class Z unobserved; linkStyle 0 stroke:#00ff00,stroke-width:4px; linkStyle 1,2 stroke:#ff0000,stroke-width:4px; Does it mean that all our analysis is garbage? Can we still draw some causal conclusion from the regression results?\nDirection of the Bias If we knew the signs of $\\gamma$ and $\\delta$, we could infer the sign of the bias, since it\u0026rsquo;s the product of the two signs.\n$$ \\text{OVB} = \\gamma \\delta \\qquad \\text{ where } \\qquad \\gamma := \\frac{Cov(Z, y)}{Var(Z)}, \\quad \\delta := \\frac{Cov(D, Z)}{Var(D)} $$\nwhich in our example is\n$$ \\text{OVB} = \\gamma \\delta \\qquad \\text{ where } \\qquad \\gamma := \\frac{Cov(\\text{ability}, \\text{wage})}{Var(\\text{ability})}, \\quad \\delta := \\frac{Cov(\\text{education}, \\text{ability})}{Var(\\text{education})} $$\nLet\u0026rsquo;s analyze the two correlations separately:\nThe correlation between ability and wage is most likely positive The correlation between ability and education is most likely positive Therefore, the bias is most likely positive. From this, we can conclude that our estimate from the regression on wage on education is most likely an overestimate of the true effect, which is most likely smaller.\nThis might seem like a small insight, but it\u0026rsquo;s actually huge. Now we can say with confidence that one year of education increases wages by at most 95 dollars per month, which is a much more informative statement than just saying that the estimate is biased.\nIn general, we can summarize the different possible effects of the bias in a 2-by-2 table.\nFurther Sensitivity Analysis Can we say more about the omitted variable bias without making strong assumptions?\nThe answer is yes! In particular, we can ask ourselves: how strong should the partial correlations $\\gamma$ and $\\delta$ be in order to overturn our conclusion?\nIn our example, we found a positive correlation between education and wages in the data. However, we know that we are omitting ability in the regression. The question is: how strong should the correlation between ability and wage, $\\gamma$, and between ability and education, $\\delta$, be in order to make the effect not significant or even negative?\nCinelli and Hazlett (2020) show that we can transform this question in terms of residual variation explained, i.e. the coefficient of determination, $R^2$. The advantage of this approach is interpretability. It is much easier to make a guess about the percentage of variance explained than to make a guess about the magnitude of a conditional correlation.\nThe authors wrote a companion package sensemakr to conduct the sensitivity analysis. You can find a detailed description of the package here.\nWe will now use the Sensemakr function. The main arguments of the Sensemakr function are:\nmodel: the regression model we want to analyze treatment: the feature/covariate of interest, in our case education The question we will try to answer is the following:\nHow much of the residual variation in education (x axis) and wage (y axis) does ability need to explain in order for the effect of education on wages to change sign?\nimport sensemakr sensitivity = sensemakr.Sensemakr(model = short_model, treatment = \u0026quot;education\u0026quot;) sensitivity.plot() plt.xlabel(\u0026quot;Partial $R^2$ of ability with education\u0026quot;); plt.ylabel(\u0026quot;Partial $R^2$ of ability with wage\u0026quot;); In the plot, we see how the partial (because conditional on age and gender) $R^2$ of ability with education and wage affects the estimated coefficient of education on wage. The $(0,0)$ coordinate, marked with a triangle, corresponds to the current estimate and reflects what would happen if ability had no explanatory power for both wage with education: nothing. As the explanatory power of ability grows (moving upwards and rightwards from the triangle), the estimated coefficient decreases, as marked by the level curves, until it becomes zero at the dotted red line.\nHow should we interpret the plot? We can see that we need ability to explain around 30% of the residual variation in both education and wage in order for the effect of education on wages to disappear, corresponding to the red line.\nOne question that you might (legitimately) have now is: what is 30%? Is it big or is it small? We can get a sense of the magnitude of the partial $R^2$ by benchmarking the results with the residual variance explained by another observed variable. Let\u0026rsquo;s use age for example.\nThe Sensemakr function accepts the following optional arguments:\nbenchmark_covariates: the covariate to use as a benchmark kd and ky: these arguments parameterize how many times stronger the unobserved variable (ability) is related to the treatment (kd) and to the outcome (ky) in comparison to the observed benchmark covariate (age). In our example, setting kd and ky equal to $[0.5, 1, 2]$ means we want to investigate the maximum strength of a variable half, same, or twice as strong as age (in explaining education and wage variation). sensitivity = sensemakr.Sensemakr(model = short_model, treatment = \u0026quot;education\u0026quot;, benchmark_covariates = \u0026quot;age\u0026quot;, kd = [0.5, 1, 2], ky = [0.5, 1, 2]) sensitivity.plot() plt.xlabel(\u0026quot;Partial $R^2$ of ability with education\u0026quot;); plt.ylabel(\u0026quot;Partial $R^2$ of ability with wage\u0026quot;); It looks like even if ability had twice as much explanatory power as age, the effect of education on wage would still be positive. But would it be statistically significant?\nWe can repeat the same exercise, looking at the t-statistic instead of the magnitude of the coefficient. We just need to set the sensitivity_of option in the plotting function equal to t-value.\nThe question that we are trying to answer in this case is:\nHow much of the residual variation in education (x axis) and wage (y axis) does ability need to explain in order for the effect of education on wages to become not significant?\nsensitivity.plot(sensitivity_of = 't-value') plt.xlabel(\u0026quot;Partial $R^2$ of ability with education\u0026quot;); plt.ylabel(\u0026quot;Partial $R^2$ of ability with wage\u0026quot;); From the plot, we can see, we need ability to explain around 5% to 10% of the residual variation in both education and wage in order for the effect of education on wage not to be significant. In particular, the red line plots the level curve for the t-statistic equal to 2.01, corresponding to a 5% significance level. From the comparison with age, we see that a slightly stronger explanatory power (bigger than 1.0x age) would be sufficient to make the coefficient of education on wage not statistically significant.\nConclusion In this post, I have introduced the concept of omitted variable bias. We have seen how it\u0026rsquo;s computed in a simple linear model and how we can exploit qualitative information about the variables to make inference in presence of omitted variable bias.\nThese tools are extremely useful since omitted variable bias is essentially everywhere. First of all, there are always factors that we do not observe, such as ability in our toy example. However, even if we could observe everything, omitted variable bias can also emerge in the form of model misspecification. Suppose that wages depended on age in a quadratic way. Then, omitting the quadratic term from the regression introduces bias, which can be analyzed with the same tools we have used for ability.\nReferences [1] C. Cinelli, C. Hazlett, Making Sense of Sensitivity: Extending Omitted Variable Bias (2019), Journal of the Royal Statistical Society.\n[2] V. Chernozhukov, C. Cinelli, W. Newey, A. Sharma, V. Syrgkanis, Long Story Short: Omitted Variable Bias in Causal Machine Learning (2022), working paper.\nRelated Articles The FWL Theorem, Or How To Make Regressions Intuitive DAGs and Control Variables Code You can find the original Jupyter Notebook here:\nhttps://github.com/matteocourthoud/Blog-Posts/blob/main/ovb.ipynb\n","date":1653436800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1653436800,"objectID":"19763d07d28effc38727045808305c10","permalink":"https://matteocourthoud.github.io/post/ovb/","publishdate":"2022-05-25T00:00:00Z","relpermalink":"/post/ovb/","section":"post","summary":"In causal inference, bias is extremely problematic because it makes inference not valid. Bias generally means that an estimator will not deliver the estimate of the true effect, on average.","tags":null,"title":"Omitted Variable Bias And What Can We Do About It","type":"post"},{"authors":null,"categories":null,"content":"When we want to visualize the relationship between two continuous variables, the go-to plot is the scatterplot. It\u0026rsquo;s a very intuitive visualization tool that allows us to directly look at the data. However, when we have a lot of data and/or when the data is skewed, scatterplots can be too noisy to be informative.\nIn this blog post, I am going to review a very powerful alternative to the scatterplot to visualize correlations between two variables: the binned scatterplot. Binned scatterplots are not only a great visualization tool, but they can also be used to do inference on the conditional distribution of the dependent variable.\nThe Scatterplot Let\u0026rsquo;s start with an example. Suppose we are an online marketplace where multiple firms offer goods that consumer can efficiently browse, compare and buy. Our dataset consists in a snapshot of the firms active on the marketplace.\nLet\u0026rsquo;s load the data and have a look at it. You can find the code for the data generating process here.\n%matplotlib inline %config InlineBackend.figure_format = 'retina' from src.utils import * from src.dgp import dgp_marketplace df = dgp_marketplace().generate_data(N=10_000) df.head() age sales online products 0 0.312777 450.858091 0 2 1 1.176221 1121.882449 1 3 2 1.764048 2698.714549 0 1 3 1.082742 1627.746386 0 3 4 3.156503 1464.593939 0 2 We have information on 10.000 firms. For each firm we know:\nage: the age of the firm sales: the monthly sales from last month online: whether the firm is only active online products: the number of products that the firm offers Suppose we are interested in understanding the relationship between age and sales. What is the life-cycle of sales?\nLet\u0026rsquo;s start with a simple scatterplot of sales over age.\nsns.scatterplot(x='age', y='sales', data=df); plt.title(\u0026quot;Sales by firm's age\u0026quot;); The plot is extremely noisy. We have a lot of observations, therefore, it is very difficult to visualize them all. If we had to guess, we could say that the relationship looks negative (sales decrease with age), but it would be a very uninformed guess.\nWe are now going to explore some plausible tweaks and alternatives.\nScatterplot Alternatives What can we do when we have an extremely dense scatterplot? One solution could be to plot the density of the observations, instead of the observations themselves.\nThere are multiple solutions in Python to visualize the density of a 2-dimensional distribution. A very useful one is seaborn jointplot. jointplot plots the joint distribution of two variables, together with the marginal distributions along the axis. The default option is the scatterplot, but one can also choose to add a regression line (reg), change the plot to a histogram (hist), a hexplot (hex), or a kernel density estimate (kde).\nLet\u0026rsquo;s try the hexplot, which is basically a histogram of the data, where the bins are hexagons, in the 2-dimensional space.\ns = sns.jointplot(x='age', y='sales', data=df, kind='hex', ); s.ax_joint.grid(False); s.ax_marg_y.grid(False); s.fig.suptitle(\u0026quot;Sales by firm's age\u0026quot;); Not much has changed. It looks like the distributions of age and sales are both very skewed and, therefore, most of the action is concentrated in a very small subspace.\nMaybe we could remove outliers and zoom-in on the area where most of the data is located. Let\u0026rsquo;s zoom-in on the bottom-left corner, on observations what have age \u0026lt; 3 and sales \u0026lt; 3000.\ns = sns.jointplot(x='age', y='sales', data=df.query(\u0026quot;age \u0026lt; 3 \u0026amp; sales \u0026lt; 3000\u0026quot;), kind=\u0026quot;hex\u0026quot;); s.ax_joint.grid(False); s.ax_marg_y.grid(False); s.fig.suptitle(\u0026quot;Sales by firm's age\u0026quot;); Now there is much less empty space, but it does not look like we are going far. The joint distribution is still too skewed. This is the case when the data follows some power distribution, as it\u0026rsquo;s often the case with business data.\nOne solution is to transform the variable, by taking the natural logarithm.\ndf['log_age'] = np.log(df['age']) df['log_sales'] = np.log(df['sales']) We can now plot the relationship between the logarithms of age and sales.\ns = sns.jointplot(x='log_age', y='log_sales', data=df, kind='hex'); s.ax_joint.grid(False); s.ax_marg_y.grid(False); s.fig.suptitle(\u0026quot;Sales by firm's age\u0026quot;, y=1.02); The logarithm definitely helped. Now the data is more spread across space, which means that the visualization is more informative. Moreover, it looks like there is no relationship between the two variables.\nHowever, there is still too much noise. Maybe data visualization alone is not sufficient do draw a conclusion.\nLet\u0026rsquo;s swap to a more structured approach: linear regression. Let\u0026rsquo;s linearly regress log_sales on log_age.\nsmf.ols('log_sales ~ log_age', df).fit().summary().tables[1] coef std err t P\u003e|t| [0.025 0.975] Intercept 7.3971 0.015 478.948 0.000 7.367 7.427 log_age 0.1690 0.010 16.888 0.000 0.149 0.189 The regression coefficient for log_age is positive and statistically significant (i.e. different from zero). It seems that all previous visualizations were very misleading. From none of the graphs above we could have guessed such a strong positive relationship.\nHowever, maybe this relationship is different for online-only firms and the rest of the sample. We need to control for this variable in order to avoid Simpson\u0026rsquo;s Paradox and, more generally, bias.\nWith linear regression, we can condition the analysis on covariates. Let\u0026rsquo;s add the binary indicator for online-only firms and the variable counting the number of products to the regression.\nsmf.ols('log_sales ~ log_age + online + products', df).fit().summary().tables[1] coef std err t P\u003e|t| [0.025 0.975] Intercept 6.5717 0.037 176.893 0.000 6.499 6.644 log_age 0.0807 0.010 7.782 0.000 0.060 0.101 online 0.1447 0.027 5.433 0.000 0.092 0.197 products 0.3456 0.014 24.110 0.000 0.317 0.374 The coefficient for log_age is still positive and statistically significant, but its magnitude has halved.\nWhat should we conclude? It seems that sales increase over age, on average. However, this pattern might be very non-linear.\nWithin the linear regression framework, one approach could be to add extra terms such as polynomials (age^2) or categorical features (e.g. age \u0026lt; 2). However, it would be really cool if there was a more flexible (i.e. non-parametric) approach that could inform us on the relationship between firm age and sales.\nIf only\u0026hellip;\nThe Binned Scatterplot The binned scatterplot is a very powerful tool that provides a flexible and parsimonious way of visualizing and summarizing conditional means (and not only) in large datasets.\nThe idea behind the binned scatterplot is to divide the conditioning variable, age in our example, into equally sized bins or quantiles, and then plot the conditional mean of the dependent variable, sales in our example, within each bin.\nDetails Cattaneo, Crump, Farrell, Feng (2021) have built an extremely good package for binned scatterplots in R, binsreg. Moreover, they have ported the package to Python. We can install binsreg directly from pip using pip install binsreg. You can find more information on the Python package here, while the original and detailed R package documentation can be found here.\nThe most important choice when building a binned scatterplot is the number of bins. The trade-off is the usual bias-variance trade-off. By picking a higher number of bins, we have more points in the graph. In the extreme, we end up having a standard scatterplot (assuming the conditioning variable is continuous). On the other hand, by decreasing the number bins, the plot will be more stable. However, in the extreme, we will have a single point representing the sample mean.\nCattaneo, Crump, Farrell, Feng (2021) prove that, in the basic binned scatterplot, the number of bins that minimizes the mean squared error is proportional to $n^{1/3}$, where $n$ is the number of observations. Therefore, in general, more observations lead to more bins.\nStarr and Goldfarb (2020) add the following consideration:\n\u0026ldquo;However other elements are also important. For example, holding the distribution of x constant, the more curvilinear the true relationship between x and y is, the more bins the algorithm will select (otherwise mean squared error will increase). This implies that even with large n, few bins will be chosen for relatively flat relationships. The calculation of the optimal number of bins in a basic binned scatterplot thus takes into account the amount and location of variation in the data available to identify the relationship between x and y.\u0026rdquo;\nIt is strongly recommended to use the default optimal number of bins. However, one can also set a customized number of bins in binsreg with the nbins option.\nBinned scatterplots however, do not just compute conditional means, for optimally chosen intervals, but they can also provide inference for these means. In particular, we can build confidence intervals around each data point. In the binsreg package, the option ci adds confidence intervals to the estimation results. The option takes as input a tuple of parameters (p, s) and uses a piecewise polynomial of degree p with s smoothness constraints to construct the confidence intervals. By default, the confidence intervals are not included in the plot. For what concerns the choice of p and s, the package documentation reports:\n\u0026ldquo;Recommended specification is ci=c(3,3), which adds confidence intervals based on cubic B-spline estimate of the regression function of interest to the binned scatter plot.\u0026rdquo;\nBinsreg One problem with the Python version of the package, is that is not very Python-ish. Therefore, I have wrapped the binsreg package into a function binscatter that takes care of cleaning and formatting the output in a nicely readable Pandas DataFrame.\nimport binsreg def binscatter(**kwargs): # Estimate binsreg est = binsreg.binsreg(**kwargs) # Retrieve estimates df_est = pd.concat([d.dots for d in est.data_plot]) df_est = df_est.rename(columns={'x': kwargs.get(\u0026quot;x\u0026quot;), 'fit': kwargs.get(\u0026quot;y\u0026quot;)}) # Add confidence intervals if \u0026quot;ci\u0026quot; in kwargs: df_est = pd.merge(df_est, pd.concat([d.ci for d in est.data_plot])) df_est = df_est.drop(columns=['x']) df_est['ci'] = df_est['ci_r'] - df_est['ci_l'] # Rename groups if \u0026quot;by\u0026quot; in kwargs: df_est['group'] = df_est['group'].astype(df[kwargs.get(\u0026quot;by\u0026quot;)].dtype) df_est = df_est.rename(columns={'group': kwargs.get(\u0026quot;by\u0026quot;)}) return df_est We can now proceed to estimate and visualize the binned scatterplot for age based on sales.\n# Estimate binsreg df_est = binscatter(x='age', y='sales', data=df, ci=(3,3)) df_est.head() group age bin isknot mid sales ci_l ci_r ci 0 Full Sample 0.012556 0 0 0 1624.779616 1312.439124 1905.535412 593.096288 1 Full Sample 0.037015 1 0 0 1664.078013 1435.438411 1893.888819 458.450408 2 Full Sample 0.065813 2 0 0 1779.657894 1555.909281 1968.681960 412.772679 3 Full Sample 0.094486 3 0 0 1976.464837 1740.530049 2216.800005 476.269956 4 Full Sample 0.125363 4 0 0 2015.833752 1796.489393 2280.237320 483.747927 The binscatter function outputs a dataset in which, for each bin of the conditioning variable, age, we have values and confidence intervals for the outcome variable, sales.\nWe can now plot the estimates.\n# Plot binned scatterplot sns.scatterplot(x='age', y='sales', data=df_est); plt.errorbar('age', 'sales', yerr='ci', data=df_est, ls='', lw=3, alpha=0.2); plt.title(\u0026quot;Sales by firm's age\u0026quot;); The plot is quite revealing. Now the relationship looks extremely non-linear with a sharp increase in sales at the beginning of the lifetime of a firm, followed by a plateau.\nMoreover, the plot is also telling us information regarding the distributions of age and sales. In fact, the plot is more dense on the left, where the distribution of age is concentrated. Also, confidence intervals are tighter on the left, where most of the conditional distribution of sales lies.\nAs we already discussed in the previous section, it might be important to control for other variables. For example, the number of products, since firms that sell more products probably survive longer in the markets and also make more sales.\nbinsreg allows to condition the analysis on any number of variables, with the w option.\n# Estimate binsreg df_est = binscatter(x='age', y='sales', w=['products'], data=df, ci=(3,3)) # Plot binned scatterplot sns.scatterplot(x='age', y='sales', data=df_est); plt.errorbar('age', 'sales', yerr='ci', data=df_est, ls='', lw=3, alpha=0.2); plt.title(\u0026quot;Sales by firm's age\u0026quot;); Conditional on number of products, the shape of the sales life-cycle changes further. Now, after an initial increase in sales, we observe a gradual decrease over time.\nDo online-only firms have different sales life-cycles with respect to mixed online-offline firms? We can produce different binned scatterplots by group using the option by.\n# Estimate binsreg df_est = binscatter(x='age', y='sales', by='online', w=['products'], data=df, ci=(3,3)) # Plot binned scatterplot sns.scatterplot(x='age', y='sales', data=df_est, hue='online'); plt.errorbar('age', 'sales', yerr='ci', data=df_est.query(\u0026quot;online==0\u0026quot;), ls='', lw=3, alpha=0.2); plt.errorbar('age', 'sales', yerr='ci', data=df_est.query(\u0026quot;online==1\u0026quot;), ls='', lw=3, alpha=0.2); plt.title(\u0026quot;Sales by firm's age\u0026quot;); From the binned scatterplot, we can see that online products have on average shorter lifetimes, with a higher initial peak in sales, followed by a sharper decline.\nConclusion In this blog post, we have analyzed a very powerful data visualization tool: the binned scatterplot. In particular, we have seen how to use the binsreg package to automatically pick the optimal number of bins and perform non-parametric inference on conditional means. However, the binsreg package offers much more than that and I strongly recommend checking its manual more in depth.\nReferences [1] E Starr, B Goldfarb, Binned Scatterplots: A Simple Tool to Make Research Easier and Better (2020), Strategic Management Journal.\n[2] M. D. Cattaneo, R. K. Crump, M. H. Farrell, Y. Feng, On Binscatter (2021), working paper.\n[3] P. Goldsmith-Pinkham, Lecture 6. Linear Regression II: Semiparametrics and Visualization, Applied Metrics PhD Course.\n","date":1653091200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1653091200,"objectID":"85b043e318961b36996123d1a50b9534","permalink":"https://matteocourthoud.github.io/post/binscatter/","publishdate":"2022-05-21T00:00:00Z","relpermalink":"/post/binscatter/","section":"post","summary":"When we want to visualize the relationship between two continuous variables, the go-to plot is the scatterplot. It\u0026rsquo;s a very intuitive visualization tool that allows us to directly look at the data.","tags":null,"title":"Goodbye Scatterplot, Welcome Binned Scatterplot","type":"post"},{"authors":null,"categories":null,"content":"An introduction to the Frisch-Waugh-Lowell theorem and how to use it to gain intuition in linear regressions\nThe Frisch-Waugh-Lowell theorem is a simple but yet powerful theorem that allows us to reduce multivariate regressions to univariate ones. This is extremely useful when we are interested in the relationship between two variables, but we still need to control for other factors, as it is often the case in causal inference.\nIn this blog post, I am going to introduce the Frisch-Waugh-Lowell theorem and illustrate some interesting applications.\nThe Theorem The theorem was first published by Ragnar Frisch and Frederick Waugh in 1933. However, since its proof was lengthy and cumbersome, Michael Lovell in 1963 provided a very simple and intuitive proof and his name was added to the theorem name.\nThe theorem states that, when estimating a model of the form\n$$ y_i = \\beta_1 x_{i,1} + \\beta_2 x_{i,2} + \\varepsilon_i $$\nthen, the following estimators of $\\beta_1$ are equivalent:\nthe OLS estimator obtained by regressing $y$ on $x_1$ and $x_2$ the OLS estimator obtained by regressing $y$ on $\\tilde x_1$ where $\\tilde x_1$ is the residual from the regression of $x_1$ on $x_2$ the OLS estimator obtained by regressing $\\tilde y$ on $\\tilde x_1$ where $\\tilde y$ is the residual from the regression of $y$ on $x_2$ Interpretation What did we actually learn?\nThe Frisch-Waugh-Lowell theorem is telling us that there are multiple ways to estimate a single regression coefficient. One possibility is to run the full regression of $y$ on $x$, as usual.\nHowever, we can also regress $x_1$ on $x_2$, take the residuals, and regress $y$ only those residuals. The first part of this process is sometimes referred to as partialling-out (or orthogonalization, or residualization) of $x_1$ with respect to $x_2$. The idea is that we are isolating the variation in $x_1$ that is orthogonal to $x_2$. Note that $x_2$ can be also be multi-dimensional (i.e. include multiple variables and not just one).\nWhy would one ever do that?\nThis seems like a way more complicated procedure. Instead of simply doing the regression in 1 step, now we need to do 2 or even 3 steps. It\u0026rsquo;s not intuitive at all. The main advantage comes from the fact that we have reduced a multivariate regression to a univariate one, making more tractable and more intuitive.\nWe will later explore more in detail three applications:\ndata visualization computational speed further applications for inference However, let\u0026rsquo;s first explore the theorem more in detail with an example.\nExample Suppose we were a retail chain, owning many different stores in different locations. We come up with a brilliant idea to increase sales: give away discounts in the form of coupons. We print a lot of coupons and we distribute them around.\nTo understand whether our marketing strategy worked, in each store, we check the average daily sales and which percentage of shoppers used a coupon. However, there is one problem: we are worried that higher income people are less likely to use the discount, but usually they spend more. To be safe, we also record the average income in the neighborhood of each store.\nWe can represent the data generating process with a Directed Acyclic Graph (DAG). If you are not familiar with DAGs, I have written a short introduction to Directed Acyclic Graphs here.\nflowchart LR classDef included fill:#DCDCDC,stroke:#000000,stroke-width:2px; classDef excluded fill:#ffffff,stroke:#000000,stroke-width:2px; classDef unobserved fill:#ffffff,stroke:#000000,stroke-width:2px,stroke-dasharray: 5 5; X1((coupons)) X2((income)) X3((weekday)) Y((sales)) X1 --\u0026gt; Y X2 --\u0026gt; X1 X2 --\u0026gt; Y X3 --\u0026gt; Y class X1,X2,X3,Y excluded; Let\u0026rsquo;s load and inspect the data. I import the data generating process from src.dgp and some plotting functions and libraries from src.utils.\n%matplotlib inline %config InlineBackend.figure_format = 'retina' from src.utils import * from src.dgp import dgp_store_coupons df = dgp_store_coupons().generate_data(N=50) df.head() sales coupons income dayofweek 0 821.7 0.199 66.243 2 1 602.3 0.245 43.882 3 2 655.1 0.162 44.718 5 3 625.8 0.269 39.270 4 4 696.6 0.186 58.654 1 We have information on 50 stores, for which we observe the percentage of customers that use coupons, daily sales (in thousand $), average income of the neighborhood (in thousand $), and day of the week.\nSuppose we were directly regressing sales on coupon usage. What would we get? I represent the result of the regression graphically, using seaborn regplot.\nsns.regplot(x=\u0026quot;coupons\u0026quot;, y=\u0026quot;sales\u0026quot;, data=df, ci=False, line_kws={'color':'r', 'label':'linear fit'}) plt.legend() plt.title(f\u0026quot;Sales and coupon usage\u0026quot;); It looks like coupons were a bad idea: in stores where coupons are used more, we observe lower sales.\nHowever, it might just be that people with higher income are using less coupons, while also spending more. If this was true, it could bias our results. In terms of the DAG, it means that we have a backdoor path passing through income, generating a non-causal relationship.\nflowchart LR classDef included fill:#DCDCDC,stroke:#000000,stroke-width:2px; classDef excluded fill:#ffffff,stroke:#000000,stroke-width:2px; classDef unobserved fill:#ffffff,stroke:#000000,stroke-width:2px,stroke-dasharray: 5 5; X1((coupons)) X2((income)) X3((weekday)) Y((sales)) X1 --\u0026gt; Y X2 --\u0026gt; X1 X2 --\u0026gt; Y X3 --\u0026gt; Y class X1,Y included; class X2,X3 excluded; linkStyle 1,2 stroke:#ff0000,stroke-width:4px; In order to recover the causal effect of coupons on sales we need to condition our analysis on income. This will block the non-causal path passing through income, leaving only the direct path from coupons to sales open, allowing us to estimate the causal effect.\nflowchart LR classDef included fill:#DCDCDC,stroke:#000000,stroke-width:2px; classDef excluded fill:#ffffff,stroke:#000000,stroke-width:2px; classDef unobserved fill:#ffffff,stroke:#000000,stroke-width:2px,stroke-dasharray: 5 5; X1((coupons)) X2((income)) X3((weekday)) Y((sales)) X1 --\u0026gt; Y X2 -.-\u0026gt; X1 X2 -.-\u0026gt; Y X3 --\u0026gt; Y class X1,X2,Y included; class X3 excluded; linkStyle 0 stroke:#00ff00,stroke-width:4px; Let\u0026rsquo;s implement this, by including income in the regression.\nsmf.ols('sales ~ coupons + income', df).fit().summary().tables[1] coef std err t P\u003e|t| [0.025 0.975] Intercept 161.4982 33.253 4.857 0.000 94.601 228.395 coupons 218.7548 50.058 4.370 0.000 118.052 319.458 income 9.5094 0.480 19.818 0.000 8.544 10.475 Now the estimated effect of coupons on sales is positive and significant. Coupons were actually a good idea after all.\nVerifying the Theorem Let\u0026rsquo;s now verify that the Frisch-Waugh-Lowell theorem actually holds. In particular, we want to check whether we get the same coefficient if, instead of regressing sales on coupons and income, we were\nregressing coupons on income computing the residuals coupons_tilde, i.e. the variation in coupons not explained by income regressing sales on coupons_tilde df['coupons_tilde'] = smf.ols('coupons ~ income', df).fit().resid smf.ols('sales ~ coupons_tilde - 1', df).fit().summary().tables[1] coef std err t P\u003e|t| [0.025 0.975] coupons_tilde 218.7548 1275.236 0.172 0.865 -2343.929 2781.438 Yes, the coefficient is the same! However, the standard errors now have increased a lot and the estimated coefficient is not significantly different from zero anymore.\nA better approach is to add a further step and repeat the same procedure also for sales:\nregressing sales on income computing the residuals sales_tilde, i.e. the variation in sales not explained by income and finally regress sales_tilde on coupons_tilde.\ndf['sales_tilde'] = smf.ols('sales ~ income', df).fit().resid smf.ols('sales_tilde ~ coupons_tilde - 1', df).fit().summary().tables[1] coef std err t P\u003e|t| [0.025 0.975] coupons_tilde 218.7548 49.025 4.462 0.000 120.235 317.275 The coefficient is still exactly the same, but now also the standard errors are almost identical.\nProjection What is partialling-out (or residualization, or orthogonalization) actually doing? What is happening when we take the residuals of coupons with respect to income?\nWe can visualize the procedure in a plot. First, let\u0026rsquo;s actually display the residuals of coupons with respect to income.\ndf[\u0026quot;coupons_hat\u0026quot;] = smf.ols('coupons ~ income', df).fit().predict() ax = sns.regplot(x=\u0026quot;income\u0026quot;, y=\u0026quot;coupons\u0026quot;, data=df, ci=False, line_kws={'color':'r', 'label':'linear fit'}) ax.vlines(df[\u0026quot;income\u0026quot;], np.minimum(df[\u0026quot;coupons\u0026quot;], df[\u0026quot;coupons_hat\u0026quot;]), np.maximum(df[\u0026quot;coupons\u0026quot;], df[\u0026quot;coupons_hat\u0026quot;]), linestyle='--', color='k', alpha=0.5, linewidth=1, label=\u0026quot;residuals\u0026quot;); plt.legend() plt.title(f\u0026quot;Coupons usage, income and residuals\u0026quot;); The residuals are the vertical dotted lines between the data and the linear fit, i.e. the part of the variation in coupons unexplained by income.\nBy partialling-out, we are removing the linear fit from the data and keeping only the residuals. We can visualize this procedure with a gif. I import the code from the src.figures file that you can find here.\nfrom src.figures import gif_projection gif_projection(x='income', y='coupons', df=df, gifname=\u0026quot;gifs/fwl.gif\u0026quot;) The original distribution of the data is on the left in blue, the partialled-out data in on the right in green. As we can see, partialling-out removes both the level and the trend in coupons that is explained by income.\nMultiple Controls We can use the Frisch-Waugh-Theorem also when we have multiple control variables. Suppose that we wanted to also include day of the week in the regression, to increase precision.\nsmf.ols('sales ~ coupons + income + dayofweek', df).fit().summary().tables[1] coef std err t P\u003e|t| [0.025 0.975] Intercept 124.2721 28.764 4.320 0.000 66.182 182.362 dayofweek[T.2] 7.7703 14.607 0.532 0.598 -21.729 37.270 dayofweek[T.3] 15.0895 11.678 1.292 0.204 -8.495 38.674 dayofweek[T.4] 28.2762 9.868 2.866 0.007 8.348 48.204 dayofweek[T.5] 44.0937 10.214 4.317 0.000 23.467 64.720 dayofweek[T.6] 50.7664 13.130 3.866 0.000 24.249 77.283 dayofweek[T.7] 57.3142 12.413 4.617 0.000 32.245 82.383 coupons 192.0262 39.140 4.906 0.000 112.981 271.071 income 9.8152 0.404 24.314 0.000 9.000 10.630 We can perform the same procedure as before, but instead of partialling-out only income, now we partial out both income and day of the week.\ndf['coupons_tilde'] = smf.ols('coupons ~ income + dayofweek', df).fit().resid df['sales_tilde'] = smf.ols('sales ~ income + dayofweek', df).fit().resid smf.ols('sales_tilde ~ coupons_tilde - 1', df).fit().summary().tables[1] coef std err t P\u003e|t| [0.025 0.975] coupons_tilde 192.0262 35.803 5.363 0.000 120.078 263.974 We still get exactly the same coefficient!\nApplications Let\u0026rsquo;s now inspect some useful applications of the FWL theorem.\nData Visualization One of the advantages of the Frisch-Waugh-Theorem is that it allows us to estimate the coefficient of interest from a univariate regression, i.e. with a single explanatory variable (or feature).\nTherefore, we can now represent the relationship of interest graphically. Let\u0026rsquo;s plot the residual sales against the residual coupons.\nsns.regplot(x=\u0026quot;coupons_tilde\u0026quot;, y=\u0026quot;sales_tilde\u0026quot;, data=df, ci=False, line_kws={'color':'r', 'label':'linear fit'}) plt.legend() plt.title(f\u0026quot;Residual sales and residual coupons\u0026quot;); Now it\u0026rsquo;s evident from the graph that the conditional relationship between sales and coupons is positive.\nOne problem with this approach is that the variables are hard to interpret: we now have negative values for both sales and coupons. Weird.\nWhy did it happen? It happened because when we partialled-out the variables, we included the intercept in the regression, effectively de-meaning the variables (i.e. normalizing their values so that their mean is zero).\nWe can solve this problem by scaling both variables, adding their mean.\ndf['coupons_tilde_scaled'] = df['coupons_tilde'] + np.mean(df['coupons']) df['sales_tilde_scaled'] = df['sales_tilde'] + np.mean(df['sales']) Now the magnitudes of the two variables are interpretable again.\nsns.regplot(x=\u0026quot;coupons_tilde_scaled\u0026quot;, y=\u0026quot;sales_tilde_scaled\u0026quot;, data=df, ci=False, line_kws={'color':'r', 'label':'linear fit'}) plt.legend() plt.title(f\u0026quot;Residual sales scaled and residual coupons scaled\u0026quot;); Is this a valid approach or did it alter our estimates? We can can check it by running the regression with the scaled partialled-out variables.\nsmf.ols('sales_tilde_scaled ~ coupons_tilde_scaled', df).fit().summary().tables[1] coef std err t P\u003e|t| [0.025 0.975] Intercept 641.6486 10.017 64.054 0.000 621.507 661.790 coupons_tilde_scaled 192.0262 36.174 5.308 0.000 119.294 264.758 The coefficient is exactly the same as before!\nComputational Speed Another application of the Frisch-Waugh-Lovell theorem is to increase the computational speed of linear estimators. For example it is used to compute efficient linear estimators in presence of high-dimensional fixed effects (day of the week in our example).\nSome packages that exploit the Frisch-Waugh-Lovell theorem include\nreghdfe in Stata pyhdfe in Python However, it\u0026rsquo;s important to also mention the fixest package in R, which is also exceptionally efficient in running regressions with high dimensional fixed effects.\nInference and Machine Learning Another important application of the FWL theorem sits at the intersection of machine learning and causal inference. I am referring to the work on post-double selection by Belloni, Chernozhukov, Hansen (2013) and the follow up work on \u0026ldquo;double machine learning\u0026rdquo; by Chernozhukov, Chetverikov, Demirer, Duflo, Hansen, Newey, Robins (2018).\nI plan to cover both applications in future posts, but I wanted to start with the basics. Stay tuned!\nReferences [1] R. Frisch and F. V. Waugh, Partial Time Regressions as Compared with Individual Trends (1933), Econometrica.\n[2] M. C. Lowell, Seasonal Adjustment of Economic Time Series and Multiple Regression Analysis (1963), Journal of the American Statistical Association.\n","date":1652659200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1652659200,"objectID":"b872a2396c23c56ce0402d4af67bebc7","permalink":"https://matteocourthoud.github.io/post/fwl/","publishdate":"2022-05-16T00:00:00Z","relpermalink":"/post/fwl/","section":"post","summary":"An introduction to the Frisch-Waugh-Lowell theorem and how to use it to gain intuition in linear regressions\nThe Frisch-Waugh-Lowell theorem is a simple but yet powerful theorem that allows us to reduce multivariate regressions to univariate ones.","tags":null,"title":"The FWL Theorem, Or How To Make All Regressions Intuitive","type":"post"},{"authors":null,"categories":null,"content":"When analyzing causal relationships, it is very hard to understand which variables to condition the analysis on, i.e. how to \u0026ldquo;split\u0026rdquo; the data so that we are comparing apples to apples. For example, if you want to understand the effect of having a tablet in class on studenta\u0026rsquo; performance, it makes sense to compare schools where students have similar socio-economic backgrounds. Otherwise, the risk is that only wealthier students can afford a tablet and, without controlling for it, we might attribute the effect to tablets instead of the socio-economic background.\nWhen the treatment of interest comes from a proper randomized experiment, we do not need to worry about conditioning on other variables. If tablets are distributed randomly across schools, and we have enough schools in the experiment, we do not have to worry about the socio-economic background of students. The only advantage of conditioning the analysis on some so-called \u0026ldquo;control variable\u0026rdquo; could be an increase in power. However, this is a different story.\nIn this post, we are going to have a brief introduction to Directed Acyclic Graphs and how they can be useful to select variables to condition a causal analysis on. Not only DAGs provide visual intuition on which variables we need to include in the analysis, but also on which variables we should not include, and why.\nDirected Acyclic Graphs Definitions Directed acyclic graphs (DAGs) provide a visual representation of the data generating process. Random variables are represented with letters (e.g. $X$) and causal relationships are represented with arrows (e.g. $\\to$). For example, we interpret\nflowchart LR classDef white fill:#FFFFFF,stroke:#000000,stroke-width:2px X((X)):::white --\u0026gt; Y((Y)):::white as $X$ (possibly) causes $Y$. We call a path between two variables $X$ and $Y$ any connection, independently of the direction of the arrows. If all arrows point forward, we call it a causal path, otherwise we call it a spurious path.\nflowchart LR classDef included fill:#DCDCDC,stroke:#000000,stroke-width:2px; classDef excluded fill:#ffffff,stroke:#000000,stroke-width:2px; X((X)) Y((Y)) Z1((Z1)) Z2((Z2)) Z3((Z3)) X --\u0026gt; Z1 Z1 --\u0026gt; Z2 Z3 --\u0026gt; Z2 Z3 --\u0026gt; Y class X,Y included; class Z1,Z2,Z3 excluded; In the example above, we have a path between $X$ and $Y$ passing through the variables $Z_1$, $Z_2$, and $Z_3$. Since not all arrows point forward, the path is spurious and there is no causal relationship of $X$ on $Y$. In fact, variable $Z_2$ is caused by both $Z_1$ and $Z_3$ and therefore blocks the path.\n$Z_2$ is called a collider.\nThe purpose of our analysis is to assess the causal relationship between two variables $X$ and $Y$. Directed acyclic graphs are useful because they provide us instructions on which other variables $Z$ we need to condition our analysis on. Conditioning the analysis on a variable means that we keep it fixed and we draw our conclusions ceteris paribus. For example, in a linear regression framework, inserting another regressor $Z$ means that we are computing the best linear approximation of the conditional expectation function of $Y$ given $X$, conditional on the observed values of $Z$.\nCausality In order to assess causality, we want to close all spurious paths between $X$ and $Y$. The questions now are:\nWhen is a path open? If it does not contain colliders. Otherwise, it is closed. How do you close an open path? You condition on at least one intermediate variable. How do you open a closed path? You condition on all colliders along the path. Suppose we are again interested in the causal relationship of $X$ on $Y$. Let\u0026rsquo;s consider the following graph\nflowchart LR classDef included fill:#DCDCDC,stroke:#000000,stroke-width:2px; classDef excluded fill:#ffffff,stroke:#000000,stroke-width:2px; X((X)) Y((Y)) Z1((Z1)) Z2((Z2)) Z3((Z3)) X --\u0026gt; Y X --\u0026gt; Z2 Z2 --\u0026gt; Y Z1 --\u0026gt; X Z1 --\u0026gt; Y X --\u0026gt; Z3 Y --\u0026gt; Z3 class X,Y included; class Z1,Z2,Z3 excluded; In this case, apart from the direct path, there are three non-direct paths between $X$ and $Y$ through the variables $Z_1$, $Z_2$, and $Z_3$.\nLet\u0026rsquo;s consider the case in which we analyze the relationship between $X$ and $Y$, ignoring all other variables.\nThe path through $Z_1$ is open but it is spurious The path through $Z_2$ is open and causal The path through $Z_3$ is closed since $Z_3$ is a collider and it is spurious Let\u0026rsquo;s draw the same graph indicating in grey variables that we are conditioning on, with dotted lines closed paths, with red lines spurious open paths, and with green lines causal open paths.\nflowchart LR classDef included fill:#DCDCDC,stroke:#000000,stroke-width:2px; classDef excluded fill:#ffffff,stroke:#000000,stroke-width:2px; X((X)) Y((Y)) Z1((Z1)) Z2((Z2)) Z3((Z3)) X --\u0026gt; Y X --\u0026gt; Z2 Z2 --\u0026gt; Y Z1 --\u0026gt; X Z1 --\u0026gt; Y X -.-\u0026gt; Z3 Y -.-\u0026gt; Z3 linkStyle 0,1,2 stroke:#00ff00,stroke-width:4px; linkStyle 3,4 stroke:#ff0000,stroke-width:4px; class X,Y included; class Z1,Z2,Z3 excluded; In this case, to assess the causal relationship between $X$ and $Y$ we need to close the path that passes through $Z_1$. We can do that by conditioning the analysis on $Z_1$.\nflowchart LR classDef included fill:#DCDCDC,stroke:#000000,stroke-width:2px; classDef excluded fill:#ffffff,stroke:#000000,stroke-width:2px; X((X)) Y((Y)) Z1((Z1)) Z2((Z2)) Z3((Z3)) X --\u0026gt; Y X --\u0026gt; Z2 Z2 --\u0026gt; Y Z1 -.-\u0026gt; X Z1 -.-\u0026gt; Y X -.-\u0026gt; Z3 Y -.-\u0026gt; Z3 linkStyle 0,1,2 stroke:#00ff00,stroke-width:4px; class X,Y,Z1 included; class Z2,Z3 excluded; Now we are able to recover the causal relationship between $X$ and $Y$ by conditioning on $Z_1$.\nWhat would happen if we were also conditioning on $Z_2$? In this case, we would close the path passing through $Z_2$ leaving only the direct path between $X$ and $Y$ open. We would then recover only the direct effect of $X$ on $Y$ and not the indirect one.\nflowchart LR classDef included fill:#DCDCDC,stroke:#000000,stroke-width:2px; classDef excluded fill:#ffffff,stroke:#000000,stroke-width:2px; X((X)) Y((Y)) Z1((Z1)) Z2((Z2)) Z3((Z3)) X --\u0026gt; Y X -.-\u0026gt; Z2 Z2 -.-\u0026gt; Y Z1 -.-\u0026gt; X Z1 -.-\u0026gt; Y X -.-\u0026gt; Z3 Y -.-\u0026gt; Z3 linkStyle 0 stroke:#00ff00,stroke-width:4px; class X,Y,Z1,Z2 included; class Z3 excluded; What would happen if we were also conditioning on $Z_3$? In this case, we would open the path passing through $Z_3$ which is a spurious path. We would then not be able to recover the causal effect of $X$ on $Y$.\nflowchart LR classDef included fill:#DCDCDC,stroke:#000000,stroke-width:2px; X((X)) Y((Y)) Z1((Z1)) Z2((Z2)) Z3((Z3)) X --\u0026gt; Y X -.-\u0026gt; Z2 Z2 -.-\u0026gt; Y Z1 -.-\u0026gt; X Z1 -.-\u0026gt; Y X --\u0026gt; Z3 Y --\u0026gt; Z3 linkStyle 0 stroke:#00ff00,stroke-width:4px; linkStyle 5,6 stroke:#ff0000,stroke-width:4px; class X,Y,Z1,Z2,Z3 included; Example: Class Size and Math Scores Suppose you are interested in the effect of class size on math scores. Are bigger classes better or worse for students\u0026rsquo; performance?\nAssume that the data generating process can be represented with the following DAG.\nflowchart TB classDef included fill:#DCDCDC,stroke:#000000,stroke-width:2px; classDef excluded fill:#ffffff,stroke:#000000,stroke-width:2px; classDef unobserved fill:#ffffff,stroke:#000000,stroke-width:2px,stroke-dasharray: 5 5; X((class size)) Y((math score)) Z1((class year)) Z2((good school)) Z3((math hours)) Z4((hist score)) U((ability)) X --\u0026gt; Y Z1 --\u0026gt; X X --\u0026gt; Z4 U --\u0026gt; Y U --\u0026gt; Z4 Z2 --\u0026gt; X Z2 --\u0026gt; Y Z2 --\u0026gt; Z4 Z3 --\u0026gt; Y class X,Y included; class Z1,Z2,Z3,Z4 excluded; class U unobserved; The variables of interest are highlighted. Moreover, the dotted line around ability indicates that this is a variable that we do not observe in the data.\nWe can now load the data and check what it looks like.\n%matplotlib inline %config InlineBackend.figure_format = 'retina' from src.utils import * from src.dgp import dgp_school df = dgp_school().generate_data() df.head() math_hours history_hours good_school class_year class_size math_score hist_score 0 3 3 1 3 15 13.009309 15.167024 1 2 3 1 3 19 13.047033 13.387456 2 2 4 0 1 25 8.330311 10.824070 3 3 4 1 3 22 11.322190 14.594394 4 3 3 1 4 15 12.338458 11.871626 What variables should we condition our regression on, in order to estimate the causal effect of class size on math scores?\nFirst of all, let\u0026rsquo;s look at what happens if we do not condition our analysis on any variable and we just regress math score on class size.\nsmf.ols('math_score ~ class_size', df).fit().summary().tables[1] coef std err t P\u003e|t| [0.025 0.975] Intercept 12.0421 0.259 46.569 0.000 11.535 12.550 class_size -0.0399 0.013 -3.025 0.003 -0.066 -0.014 The effect of class_size is negative and statistically different from zero.\nBut should we believe this estimated effect? Without controlling for anything, this is DAG representation of the effect we are capturing.\nflowchart TB classDef included fill:#DCDCDC,stroke:#000000,stroke-width:2px; classDef excluded fill:#ffffff,stroke:#000000,stroke-width:2px; classDef unobserved fill:#ffffff,stroke:#000000,stroke-width:2px,stroke-dasharray: 5 5; X((class size)) Y((math score)) Z1((class year)) Z2((good school)) Z3((math hours)) Z4((hist score)) U((ability)) X --\u0026gt; Y Z1 --\u0026gt; X X -.-\u0026gt; Z4 U --\u0026gt; Y U -.-\u0026gt; Z4 Z2 --\u0026gt; X Z2 --\u0026gt; Y Z2 --\u0026gt; Z4 Z3 --\u0026gt; Y linkStyle 0 stroke:#00ff00,stroke-width:4px; linkStyle 5,6 stroke:#ff0000,stroke-width:4px; class X,Y included; class Z1,Z2,Z3,Z4 excluded; class U unobserved; There is a spurious path passing through good school that biases our estimated coefficient. Intuitively, being enrolled in a better school improves the students\u0026rsquo; math scores and better schools might have smaller class sizes. We need to control for the quality of the school.\nsmf.ols('math_score ~ class_size + good_school', df).fit().summary().tables[1] coef std err t P\u003e|t| [0.025 0.975] Intercept 4.7449 0.247 19.176 0.000 4.259 5.230 class_size 0.2095 0.010 20.020 0.000 0.189 0.230 good_school 5.0807 0.130 39.111 0.000 4.826 5.336 Now the estimate of the effect of class size on math score is unbiased! Indeed, the true coefficient in the data generating process was $0.2$.\nflowchart TB classDef included fill:#DCDCDC,stroke:#000000,stroke-width:2px; classDef excluded fill:#ffffff,stroke:#000000,stroke-width:2px; classDef unobserved fill:#ffffff,stroke:#000000,stroke-width:2px,stroke-dasharray: 5 5; X((class size)) Y((math score)) Z1((class year)) Z2((good school)) Z3((math hours)) Z4((hist score)) U((ability)) X --\u0026gt; Y Z1 --\u0026gt; X X -.-\u0026gt; Z4 U --\u0026gt; Y U -.-\u0026gt; Z4 Z2 -.-\u0026gt; X Z2 -.-\u0026gt; Y Z2 --\u0026gt; Z4 Z3 --\u0026gt; Y linkStyle 0 stroke:#00ff00,stroke-width:4px; class X,Y,Z2 included; class Z1,Z3,Z4 excluded; class U unobserved; What would happen if we were to instead control for all variables?\nsmf.ols('math_score ~ class_size + good_school + math_hours + class_year + hist_score', df).fit().summary().tables[1] coef std err t P\u003e|t| [0.025 0.975] Intercept -0.7847 0.310 -2.529 0.012 -1.394 -0.176 class_size 0.1292 0.010 13.054 0.000 0.110 0.149 good_school 2.9815 0.170 17.533 0.000 2.648 3.315 math_hours 1.0516 0.048 21.744 0.000 0.957 1.147 class_year 0.0424 0.037 1.130 0.259 -0.031 0.116 hist_score 0.4116 0.027 15.419 0.000 0.359 0.464 The coefficient is again biased. Why?\nWe have opened a new spurious path by controlling for hist score. In fact, hist score is a collider and controlling for it has opened a path through hist score and ability that was otherwise closed.\nflowchart TB classDef included fill:#DCDCDC,stroke:#000000,stroke-width:2px; classDef excluded fill:#ffffff,stroke:#000000,stroke-width:2px; classDef unobserved fill:#ffffff,stroke:#000000,stroke-width:2px,stroke-dasharray: 5 5; X((class size)) Y((math score)) Z1((class year)) Z2((good school)) Z3((math hours)) Z4((hist score)) U((ability)) X --\u0026gt; Y Z1 --\u0026gt; X X --\u0026gt; Z4 U --\u0026gt; Y U --\u0026gt; Z4 Z2 -.-\u0026gt; X Z2 -.-\u0026gt; Y Z2 --\u0026gt; Z4 Z3 --\u0026gt; Y linkStyle 0 stroke:#00ff00,stroke-width:4px; linkStyle 2,3,4 stroke:#ff0000,stroke-width:4px; class X,Y,Z1,Z2,Z3,Z4 included; class U unobserved; The example was inspired by the following tweet.\nWe can illustrate this with Model 16 of the \u0026quot;Crash Course in Good and Bad Controls\u0026quot; (https://t.co/GcSNzhuVt2). Here X = class size, Y = math4, Z = read4, and U = student\u0026#39;s ability. Conditioning on Z opens the path X -\u0026gt; Z \u0026lt;- U -\u0026gt; Y and it is thus a \u0026quot;bad control.\u0026quot; https://t.co/KNfqtsMWwB pic.twitter.com/lUSigNYSJj\n\u0026mdash; Análise Real (@analisereal) March 12, 2022 Conclusion In this post, we have seen how to use Directed Acyclic Graphs to select control variables in a causal analysis. DAGs are very helpful tools since they provide an intuitive graphical representation of causal relationships between random variables. Contrary to common intuition that \u0026ldquo;the more information the better\u0026rdquo;, sometimes including extra variables might bias the analysis, preventing a causal interpretation of the results. In particular, we must pay attention not to include colliders that open spurious paths that would otherwise be closed.\nReferences [1] C. Cinelli, A. Forney, J. Pearl, A Crash Course in Good and Bad Controls (2018), working paper.\n[2] J. Pearl, Causality (2009), Cambridge University Press.\n[3] S. Cunningham, Chapter 3 of The Causal Inference Mixtape (2021), Yale University Press.\n","date":1650844800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1650844800,"objectID":"fd5f8adabb41c0c3eff505f3e4584cfc","permalink":"https://matteocourthoud.github.io/post/controls/","publishdate":"2022-04-25T00:00:00Z","relpermalink":"/post/controls/","section":"post","summary":"When analyzing causal relationships, it is very hard to understand which variables to condition the analysis on, i.e. how to \u0026ldquo;split\u0026rdquo; the data so that we are comparing apples to apples.","tags":null,"title":"DAGs and Control Variables","type":"post"},{"authors":null,"categories":null,"content":"Introduction During the second year of my PhD, I decided that I wanted to have a personal website. After (too many) hours of research, I decided to build it using Hugo, and I picked the Wowchemy theme, also known as Hugo Academic. In this tutorial, I am going to share my guide to building a website on Github Pages so that you don’t have to go through all the pain I went through 😁.\nBefore we start, I have to warn you. If you don’t care about personalization or if you have very little time to spend on building a website, I strongly recommend Google Sites, which is, in my opinion, the fastest and easiest way to build an academic website. However, if you enjoy customizing your website, or if you like my template, then this guide might be useful.\nAlso, note that Hugo offers many other website templates. I suggest checking them out. Some interesting alternatives are:\nResume Somrat UILite (paid) However, in this guide, I will concentrate on the Hugo Academic theme, since it’s the one I used for my website and I believe it’s the best one for building academic profile pages. But the first part of this guide is general and it works for any Hugo theme.\nCreate Website 0. Prerequisites Before we start, I will take for granted the following:\nthat you have an account on Github that you have R installed that you have RStudio installed 1. Create Github Repository First, go to your Github page and create a new repository (+ button in the top-right corner).\nName the repository username.github.io where username is your Github username.\nIn my case, my github username is matteocourthoud, therefore the repository is matteocourthoud.github.io and my personal website is https://matteocourthoud.github.io. Use the default settings when creating the repository.\n2. Install Blogdown and Hugo Now you need to install Blogdown, which is the program what will allow you to build and deploy your website, and Hugo, which is the template generator.\nSwitch to RStudio and type the following commands\n# Install blogdown install.packages(\u0026quot;blogdown\u0026quot;) # Install Hugo blogdown::install_hugo() Now everything should be ready!\n3. Setup folder Open RStudio and select New Project.\nSelect New Directory when asked where to create the project.\nThen select Website using blogdown as project type.\nNow you have to select a couple of options:\nDirectory name: here input the name of the folder which will contain all the website files. The name is irrelevant. I called mine website. Create project as a subdirectory of: select the directory in which you want to put the website folder. Theme: input wowchemy/starter-academic instead of the default theme. Note: if you want to install a different theme, just go on the corresponding Github page (for example https://github.com/caressofsteel/hugo-story) and instead of gcushen/hugo-academic, insert the corresponding Github repository (for example caressofsteel/hugo-story).\nIf you go into the website folder, it should look something like\n4. Build website To build the website, open the RProject file website.Rproj in RStudio and type\nblogdown::hugo_build(local=TRUE) This command will generate a public/ subfolder in which the actual code of the website is stored.\nDon’t ask me why, but the option local=TRUE seems to make a difference. Updating without it sometimes does not change the content in the public/ subfolder.\nTo preview the website, type in RStudio\nblogdown::serve_site() The following preview should automatically open in your browser.\nPreviewing the website is very useful as it allows you to see live changes locally inside RStudio, before publishing them. This is the main advantage of working in RStudio.\nIf the preview does not automatically open in your browser, and instead it previews inside RStudio Viewer panel, you can preview it in your browser using the upper left right-most button.\n5. Publish website Importantly, before pushing the code online, you need to open the file config.yaml and change the baseurl to your future website url, which will be https://username.github.io/, where username is your Github username.\nNow that you have set the correct url, you have to push the changes from the public/ folder to your username.github.io repository on Github.\nTo do that, you need to get to the website folder. Let’s assume that the path to your folder is Documents/website. Open the Terminal and type\ncd Documents/website/public The following code will link the public/ folder, containing the actual code of the website, to your username.github.io repository.\n# Init git in the /website/public/ folder git init # Add and commit the changes git add . git commit -m \u0026quot;first version of the website\u0026quot; # Set origin git remote add origin https://github.com/username/username.github.io.git # Rename local branch git branch -M main # And push your updates online git push -u origin main Wait a few seconds (or minutes for heavy changes) and your website should be online!\nIf the website is not working, you can check the following:\nIs there anything in your public/ folder? (does it even exist?) If not, something went wrong when compiling the website with blogdown::hugo_build(). Inside your public/ folder, there should be an index.html file. If you double-click on it, you should see a local preview of your website in your browser. If not, something in the website code is wrong. Is the content of your public/ folder exactly the same as the content of your Github repository? If not, something went wrong when pushing to Github. Did you name your Github repository username.github.io, where username is your Github username? Did you change the baseurl option in the file config.yaml to https://username.github.io/, where username is your Github username? You can check the list of websites deployments at https://github.com/username/username.github.io/deployments. Control that they correspond with your commits. If all the conditions are satisfied, but the website is still not online, maybe it’s just a matter of time. Have some patience.\nBasic Customization The basic files that you want to modify to customize your website are the following:\nconfig/_default/config.yaml: general website information config/_default/params.yaml: website customization config/_default/menus.yaml: top bar / menu customization content/authors/admin/_index.md: personal information For what concerns images, there are two main things you might want to modify:\nProfile picture: change the content/authors/admin/avatar.jpg picture Website icon: change the assets/media/icon.png picture In order to modify the widgets on your homepage, go to content/home/ and modify the files inside. If you want to remove a section, just open the corresponding file and select active: false. If there is no active option, just copy the line active: false in the corresponding file.\nOn my website, I have only the following sections set to true:\nabout projects posts contact To change the color palette of the website, go to data\\theme and generate a custom_theme.toml file with the following content:\n# Theme metadata name = \u0026quot;My custom theme\u0026quot; # Is theme light or dark? light = true # Primary primary = \u0026quot;#284f7a\u0026quot; # Menu menu_primary = \u0026quot;#fff\u0026quot; menu_text = \u0026quot;#34495e\u0026quot; menu_text_active = \u0026quot;#284f7a\u0026quot; menu_title = \u0026quot;#2b2b2b\u0026quot; # Home sections home_section_odd = \u0026quot;rgb(255, 255, 255)\u0026quot; home_section_even = \u0026quot;rgb(247, 247, 247)\u0026quot; [dark] link = \u0026quot;#bbdefb\u0026quot; link_hover = \u0026quot;#bbdefb\u0026quot; Then go to the config/_default/params.yaml file and set the theme to custom_theme.\nYou can get more information on how to personalize it here.\nTo change the font, go to data\\fonts and generate a custom_font.toml file with the following content:\n# Font style metadata name = \u0026quot;My custom font\u0026quot; # Optional Google font URL google_fonts = \u0026quot;family=Roboto+Mono\u0026amp;family=Source+Sans+Pro:wght@200;300;400;700\u0026quot; # Font families heading_font = \u0026quot;Source Sans Pro\u0026quot; body_font = \u0026quot;Source Sans Pro\u0026quot; nav_font = \u0026quot;Source Sans Pro\u0026quot; mono_font = \u0026quot;Roboto Mono\u0026quot; Then go to the config/_default/params.yaml file and set the font to custom_font.\nYou can get more information on how to personalize it here. Importantly, by default, the website supports only fonts of weight 400 and 700. If you want a lighter font, like the Source Sans Pro I use for my website, you have to dig into the advanced customization (which requires HTML and CSS skills).\nAdvanced Customization Advanced customization is possible but it’s a pain. You basically want to go inside themes\\github.com\\wowchemy\\wowchemy-hugo-modules\\wowchemy and start digging. Tip: you want to start digging in the following places:\nIn layouts\\partials to edit the HTML files In assets\\scss to edit the SCSS code If you want to copy my exact theme, I have published my custom theme here: https://github.com/matteocourthoud/custom-wowchemy-settings\nYou have to do the following:\ngo inside the theme folder copy the content of the custom-wowchemy-theme repository in a folder there go to the config.yaml file into the MODULES section change the second link to the folder with the custom settings Now your website should look quite similar to mine! :)\nExamples Here are some examples of advanced customizations you can do. For all the examples the baseline directory is you theme directory, themes/custom-wowchemy-theme if you renamed it as in the previous paragraph.\nExample 1\nWhat to have your section titles fixed on top of the screen?\nGo to assets/scss/wowchemy/widgets/_base.scss\nSearch for .section-heading h1\nIt should look like this\nAdd a couple of lines as follows\nNow the section titles should stay anchored at the top of the page\nExample 2\nDo you want to put a background image in your home page?\nPut the selected background image, for example image.png, into the static/img folder (the location itself does not matter)\nGo to assets/scss/wowchemy/widgets/_about.scss\nAdd the following lines anywhere in the code\nNow go to layouts/partials/widgets/about.html\nAdd the following lines after \u0026lt;!-- About widget --\u0026gt;\n`\nNow image.png should appear as background image in your homepage.\nGoogle Analytics In order for the website to be displayed in Google searches, you need to ask Google to track it.\nGo to the Google Search Console website Use the URL Inspection tool to inspect the URL of your personal website: https://username.github.io Use Request indexing to request Google to index your website so that it will apprear in Google searches. Under Sitemap provide the link to your website sitemap to Google. It should be https://username.github.io/sitemap.xml. In order to receive statistics on your website, you first need to get your associated tracking code.\nGo to the Google Analytics website Click Admin Select an account from the menu in the ACCOUNT column. Select a property from the menu in the PROPERTY column. Under PROPERTY, click Tracking Info \u0026gt; Tracking Code. Your tracking ID and property number are displayed at the top of the page. It should have the form UA-xxxxxxxxx-1 Now that we have the website tracking code, we need to insert it into the googleAnalytics section of the config/_default/params.yaml file.\nmarketing: google_analytics: 'UA-xxxxxxxxx-1' The mobile application of Google Analytics is particular intuitive and allows you to monitor your website traffic in detail. You just need to link the website from the Google Sesarch Console and then you can motitor you website from this platform. There is also a very nice mobile app for both Android and iOS to monitor your website from your smartphone.\nAnother good free tool to analyze the “quality” of your website is SEO Mechanic.\nReferences Here are the main resources I used to write this guide:\nWowchemy website: https://wowchemy.com/docs/getting-started/ Old Academic website: https://sourcethemes.com/academic/docs/install/ Guide for the Terminal: https://github.com/fliptanedo/FlipWebsite2017 ","date":1650240000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1650240000,"objectID":"50c5f0366dd809632c0e255ee6d27271","permalink":"https://matteocourthoud.github.io/post/website/","publishdate":"2022-04-18T00:00:00Z","relpermalink":"/post/website/","section":"post","summary":"Introduction During the second year of my PhD, I decided that I wanted to have a personal website. After (too many) hours of research, I decided to build it using Hugo, and I picked the Wowchemy theme, also known as Hugo Academic.","tags":null,"title":"How To Make A Personal Website with Hugo","type":"post"},{"authors":null,"categories":null,"content":"In this tutorial, we are going to see how to estimate the causal effect of a treatment on an outcome when treatment assignment is not random, but we observe both treated and untreated units before and after treatment. Under certain structural assumptions, especially parallel outcome trends in the absence of treatment, we can recover the average treatment effect.\nRequisites\nFor this tutorial, I assume you are familiar with the following concepts:\nRubin\u0026rsquo;s potential outcome framework Ordinary least squares regression Academic Application\nAs an academic application, we are going to replicate Minimum Wages and Employment: A Case Study of the Fast-Food Industry in New Jersey and Pennsylvania (1994) by Card and Krueger. The authors study the effect of a minimum wage policy in New Jersey on emplyment, by using Pennsylvania as a control state.\nBusiness Case\nAs a business case, we are going to study a firm that has run a TV ad campaign. The firm would like to understand the impact of the campaign on revenue and has randomized the campaign over municipalities.\nSetting We assume that for a set of i.i.d. subjects $i = 1, \u0026hellip;, n$ over $T$ time periods $t = 1 , \u0026hellip; , T$, we observed a tuple $(X_{it}, D_{it}, Y_{it})$ comprised of\na feature vector $X_{it} \\in \\mathbb R^{p}$ a treatment assignment $D_i \\in \\lbrace 0, 1 \\rbrace$ a response $Y_{it} \\in \\mathbb R$ We assume that treatment occurs between time $t=0$ and time $t=1$.\nAssumption 1: parallel trends\nIn the absence of treatment, the outcome $Y_{it}$ evolve in parallel across units, i.e. and their $\\gamma_{t}$ are the same.\n$$ Y_{it}^{(0)} - Y_{j,t}^{(0)} = \\alpha \\quad \\forall \\ t $$\nDiff-in-diffs In this setting, we cannot estimate any causal parameter with any other further assumption. What is the minimal number of assumptions that we could make in order to estimate a causal parameter?\nIf we were to assume that treatment was randomly assigned, we could retrieve the average treatment effect as a difference in means.\n$$ \\mathbb E[\\tau_t] = \\mathbb E \\big[ Y_{it} \\ \\big| \\ D_i = 1 \\big] - \\mathbb E \\big[ Y_{it} \\big| \\ D_i = 0 \\big] $$\nHowever, it would be a very strong assumption, and it would ignore some information that we possess: the time dimension (pre-post).\nIf we were to assume instead that no other shocks affected the treated units between period $t=0$ and $t=1$, we could retrieve the average treatment effect on the treated as a pre-post difference.\n$$ \\mathbb E[\\tau | D_i=1] = \\mathbb E \\big[ Y_{i1} \\ \\big| \\ D_i = 1 \\big] - \\mathbb E \\big[ Y_{i0} \\ \\big| \\ D_i = 1 \\big] $$\nHowever, it also this would be a very strong assumption, and it would ignore the fact that we have control units.\nCan we make less stringent assumption and still recover a causal parameter using both the availability of a (non-random) control group and the time dimension?\nDiD Model The model that is commonly assumed in diff-ind-diff settings, is the following\n$$ Y_{it} (D_{it}) = \\alpha_{i} + \\gamma_{t} + \\tau_{i} D_{it} $$\nFirst, let\u0026rsquo;s summarize the potential outcome values $Y^{(d)}_{it}$ in the simple $2 \\times 2$ setting.\n$t=0$ $t=1$ $D=0$ $\\gamma_0 + \\alpha_i$ $\\gamma_1 + \\alpha_i$ $D=1$ $\\gamma_0 + \\alpha_i + \\tau_i$ $\\gamma_1 + \\alpha_i + \\tau_i$ For a single unit, $i$, the pre-post outcome difference is given by\n$$ Y_{i1} - Y_{i0} = (\\gamma_1 - \\gamma_0) + \\tau_i (D_{i1} - D_{i0}) $$\nIf we take the difference of the expression above between treated and untreated units, we get\n$$ \\mathbb E \\Big[ Y_{i1} - Y_{i0} \\ \\Big| \\ D_{i1} - D_{i0} = 1 \\Big] - \\mathbb E \\Big[ Y_{i1} - Y_{i0} \\ \\Big| \\ D_{i1} - D_{i0} = 0 \\Big] = \\mathbb E \\Big[ \\tau_i \\ \\Big| \\ D_{i1} - D_{i0} = 1 \\Big] = ATT $$\nwhich is the average treatment effect on the treated (ATT).\nWe can get this double difference with the folowing regressio model\n$$ Y_{it} (D_{it}) = \\alpha_{i} + \\gamma_{t} + \\beta D_{it} + \\varepsilon_{it} $$\nwhere the OLS estimator $\\hat \\beta$ will be unbiased for the ATT.\nMultiple Time Periods What if we didn\u0026rsquo;t just have one pre-treatment period and one post-treatment period? Great! We can actually do more things.\nWe can partially test assumptions We can estimate dynamic effects We can run placebo tests How do we implement it? Run a regression with multiple interactions\n$$ Y_{it} (D_{it}) = \\alpha_{i} + \\gamma_{t} + \\sum_{t=1}^{T} \\beta_t D_{it} + \\varepsilon_{it} $$\nComments Parametric Assumption\nThe diff-in-diffs method makes a lot of parametric assumptions that are is easy to forget.\nInference\nBertrand, Duflo, and Mullainathan (2004) point out that conventional robust standard errors usually overestimate the actual standard deviation of the estimator. The authors recommend clustering the standard errors at the level of randomization (e.g. classes, counties, villages, \u0026hellip;).\nTesting pre-trends\nHaving multiple pre-treatment time periods is helpful for testing the parallel trends assumption. However, this practice can lead to pre-testing bias. In particular, if one selects results based on a pre-treatment parallel trend test, inference on the ATT gets distorderd.\nAcademic Application As an academic application, we are going to replicate Minimum Wages and Employment: A Case Study of the Fast-Food Industry in New Jersey and Pennsylvania (1994) by Card and Krueger. The authors study the effect of a minimum wage policy in New Jersey on emplyment, by using Pennsylvania as a control state.\nThe authors describe the setting as follows\nOn April 1, 1992, New Jersey\u0026rsquo;s minimum wage rose from $4.25 to $5.05 per hour. To evaluate the impact of the law, the authors surveyed 410 fast-food restaurants in New Jersey and eastern Pennsylvania before and after the rise. Comparisons of employment growth at stores in New Jersey and Pennsylvania (where the minimum wage was constant) provide simple estimates of the effect of the higher minimum wage.\nLet\u0026rsquo;s start by loading and inspecting the data.\n%matplotlib inline %config InlineBackend.figure_format = 'retina' from src.utils import * from src.data import import_ck94 df = pd.read_csv('data/ck94.csv') df.head() id after new_jersey chain employment hrsopen wage 0 3 0 0 wendys 34.0 12.0 5.00 1 4 0 0 wendys 24.0 12.0 5.50 2 6 0 0 burgerking 70.5 18.0 5.00 3 7 0 0 burgerking 23.5 24.0 5.00 4 8 0 0 kfc 11.0 10.0 5.25 We have information on fast food restaurants, indexed by i, at time t. We distinguish between before and faster treatment and between New Jersey nj and Pennsylvania restaurants. We also know the chain of the restaurant, the employment, the hours open hrsopen and the wage. We are interested on the effect on the policy on wages.\nLet\u0026rsquo;s start by producing the $2 \\times 2$ table of treatment-control before-after average outcomes.\ndf.pivot_table(index='new_jersey', columns='after', values='employment', aggfunc='mean') after 0 1 new_jersey 0 23.704545 21.825758 1 20.657746 21.048415 From the table we can see that a simple before-after comparison would give a small posive effect of $21.05 - 20.66 = 0.39$.\nOn the other hand, if one was doing an ex-post treated-control comparison, would get a negative effect of $21.05 - 21.83 = - 0.78$.\nThe difference-in-differences estimator takes into account the fact that\nThere is a pre-treatment level difference between New Jersey and Pennsylvania Employment was falling in Pennsylvania even without treatment The double difference in means gives a positive effect, significantly larger than any of the two previous estimates.\n$$ \\hat \\tau_{DiD} = \\Big( 21.05 - 20.66 \\Big) - \\Big( 21.83 - 23.70 \\Big) = 0.39 + 1.87 = 2.26 $$\nWe can replicate the result with a linear regression.\nsmf.ols('employment ~ new_jersey * after', df).fit().summary().tables[1] coef std err t P\u003e|t| [0.025 0.975] Intercept 23.7045 1.149 20.627 0.000 21.448 25.961 new_jersey -3.0468 1.276 -2.388 0.017 -5.552 -0.542 after -1.8788 1.625 -1.156 0.248 -5.070 1.312 new_jersey:after 2.2695 1.804 1.258 0.209 -1.273 5.812 The effect is $2.26$, but it is not significantly different from zero.\nBusiness Case We are given the following problem:\nA firm wants to test the impact of a TV advertisement campaign on revenue. The firm releases the ad on a random sample of municipalities and track the revenue over time, before and after the ad campaign.\nWe start by drawing a sample from the data generating process.\nfrom src.dgp import dgp_did dgp = dgp_did() df = dgp.generate_data() df.head() day id treated post revenue 0 1 1 0 False 3.599341 1 1 2 0 False -0.146912 2 1 3 0 False 0.696527 3 1 4 0 False 1.445169 4 1 5 0 False 1.659696 We have revenue data on a set of customers over time. We also know to which group they were assigned and whether the time is before or after the intervention.\nSince we do not have any control variable, we can directly visualize the revenue dynamics, distinguishing between treatment and control group.\nsns.lineplot(x=df['day'], y=df['revenue'], hue=df['treated']); plt.axvline(x=10, ls=\u0026quot;:\u0026quot;, color='C2'); plt.title('Revenue over time'); It seems like the treatment group was producing higher revenues before treatment and the gap has increased with treatment but it is closing over time.\nTo assess the magnitude of the effect and perform inference, we can regress revenue on a post-treatment dummy, a treatment dummy and their interaction.\nsmf.ols('revenue ~ post * treated', df).fit().summary().tables[1] coef std err t P\u003e|t| [0.025 0.975] Intercept 4.6357 0.078 59.428 0.000 4.483 4.789 post[T.True] 0.8928 0.110 8.093 0.000 0.676 1.109 treated 1.0558 0.110 9.571 0.000 0.839 1.272 post[T.True]:treated 0.1095 0.156 0.702 0.483 -0.196 0.415 While the coefficient for the interaction term is positive, it does not seem to be statistically significant. However, this might be due to the fact that the treatment effect is fading away over time.\nLet\u0026rsquo;s fit the same regression, with a linear time trend.\nsmf.ols('revenue ~ post * treated * day', df).fit().summary().tables[1] coef std err t P\u003e|t| [0.025 0.975] Intercept 4.1144 0.168 24.501 0.000 3.785 4.444 post[T.True] 0.2871 0.458 0.626 0.531 -0.612 1.186 treated 1.0788 0.237 4.543 0.000 0.613 1.544 post[T.True]:treated 1.5910 0.648 2.454 0.014 0.320 2.862 day 0.0948 0.027 3.502 0.000 0.042 0.148 post[T.True]:day -0.0221 0.038 -0.576 0.564 -0.097 0.053 treated:day -0.0042 0.038 -0.109 0.913 -0.079 0.071 post[T.True]:treated:day -0.0929 0.054 -1.716 0.086 -0.199 0.013 Now the treatment effect is positive and significant at the 5% level. And indeed, we estimate a decreasing trend, post treatment, for the treated. However, it is not statistically significant.\nReferences Video lecture on Difference-in-Differences by Paul Goldsmith-Pinkham (Yale) Chapter 13 of Causal Inference for The Brave and The True by Matheus Facure Chapter 9 of The Causal Inference Mixtape by Scott Cunningham Chapter 5 of Mostly Harmless Econometrics by Agrist and Pischke ","date":1649980800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1649980800,"objectID":"46a37fe1c4822f8dc30b06a61cf083c7","permalink":"https://matteocourthoud.github.io/post/diff_in_diffs/","publishdate":"2022-04-15T00:00:00Z","relpermalink":"/post/diff_in_diffs/","section":"post","summary":"In this tutorial, we are going to see how to estimate the causal effect of a treatment on an outcome when treatment assignment is not random, but we observe both treated and untreated units before and after treatment.","tags":null,"title":"Difference in Differences","type":"post"},{"authors":null,"categories":null,"content":"In this tutorial, we are going to see how to estimate causal effects when the treatment is not randomly assigned, but we have access to a third variable that is as good as randomly assigned and is correlated (only) with the treatment. These variables are called instrumental variables and are a powerful tool for causal inference, especially in observational studies.\nRequisites\nFor this tutorial, I assume you are familiar with the following concepts:\nRubin\u0026rsquo;s potential outcome framework Ordinary least squares regression Academic Application 1\nAs a first academic application, we are going to replicate Does Compulsory School Attendance Affect Schooling and Earnings? (1991) by Angrist and Krueger. The authors study the effect of education on wages.\nAcademic Application 2\nAs a further academic application, we are going to replicate The Colonial Origins of Comparative Development (2002) by Acemoglu, Johnson, Robinson. The authors study the effect of institutions on economic development.\nBusiness Case\nAs a business case, we are going to study a company that wants to find out whether subscribing to its newsletter has an effect on revenues. Since the travel agency cannot force customers to subscribing to the newsletter, it randomly sends reminder emails to infer the effect of the newsletter on revenues.\nSetting We assume that for a set of i.i.d. subjects $i = 1, \u0026hellip;, n$ we observed a tuple $(X_i, T_i, Y_i)$ comprised of\na feature vector $X_i \\in \\mathbb R^n$ a treatment variable $T_i \\in \\lbrace 0, 1 \\rbrace$ a response $Y_i \\in \\mathbb R$ Crucially, we do not assume unconfoundedness / strong ignorability hence\n$$ \\big \\lbrace Y_i^{(1)} , Y_i^{(0)} \\big \\rbrace \\ \\not \\perp \\ T_i \\ | \\ X_i $$\nInstrumental Variables The standard linear IV model is the following\n$$ Y_i = T_i \\alpha + X_i \\beta_1 + \\varepsilon_i \\newline T_i = Z_i \\gamma + X_i \\beta_2 + u_i $$\nWe assume there exists an instrumental variable $Z_i \\in \\mathbb R^k$ that satisfies the following assumptions.\nAssumption 1: Exclusion: $\\mathbb E [Z \\varepsilon] = 0$\nAssumption 2: Relevance: $\\mathbb E [Z T] \\neq 0$\nThe model can be represented by a DAG.\nfrom src.plots import dag_iv dag_iv() The IV estimator is instead unbiased\n$$ \\hat \\beta_{IV} = (Z\u0026rsquo;X)^{-1}(Z\u0026rsquo;Y) $$\nPotential Outcomes Perspective We need to extend the potential outcomes framework in order to allow for the instrumental variable $Z$. First we define the potential outcomes as $Y^{(D(Z_i))}(Z_i)$\nThe assumptions become\nExclusion: $Y^{(D(Z_i))}(Z_i) = Y^{(T(Z_i))}$ Relevance: $P(z) = \\mathbb E [T, Z=z]$ We assume that $Z$ is fully randomly assigned (while $T$ is not).\nWhat does IV estimate?\n$$ \\begin{aligned} \\mathbb E[Y_i | Z_i = 1] - \\mathbb E[Y_i | Z_i = 0] \u0026amp;= \\Pr (D_i^{(1)} - D_i^{(0)} = 1) \\times \\mathbb E \\Big[ Y_i^{(1)} - Y_i^{(0)} = 1 \\ \\Big | \\ D_i^{(1)} - D_i^{(0)} = 1 \\Big] - \\newline \u0026amp;- \\Pr (D_i^{(1)} - D_i^{(0)} = -1) \\times \\mathbb E \\Big[ Y_i^{(1)} - Y_i^{(0)} = 1 \\ \\Big | \\ D_i^{(1)} - D_i^{(0)} = -1 \\Big] \\end{aligned} $$\nIs this a quantity of interest? Almost. There are two issues.\nFirst, the first term is the treatment effect, but only for those individuals for whom $D_i^{(1)} - D_i^{(0)} = 1$, i.e. those that are induced into treatment by $Z_i$. These individuals are referred to as compliers.\nSecond, the second term is problematic since it removes from the first effect, the local effect of another subpopulation: $D_i^{(1)} - D_i^{(0)} = -1$, i.e. those that are induced out of treatment by $Z_i$. These individuals are referred to as defiers.\nWe can get rid of defiers with a simple assumption.\nAssumption 3: monotonocity: $D_i^{(1)} \\geq D_i^{(0)}$ (or viceversa)\nAll effects must be monotone in the same direction Fundamentally untestable Then, the IV estimator can be expressed as a ration between two differences in means\n$$ \\hat \\beta_{IV} = \\frac{\\mathbb E[Y_i | Z_i = 1] - \\mathbb E[Y_i | Z_i = 0]}{\\mathbb E[T_i | Z_i = 1] - \\mathbb E[T_i | Z_i = 0]} $$\nStructural Perspective One can interpret the IV estimator as a GMM estimator that uses the exclusion restriction as estimating equation.\n$$ \\hat \\beta_{GMM} = \\arg \\min_{\\beta} \\mathbb E \\Big[ Z (Y - \\alpha T - \\beta X) \\Big]^2 $$\nThe Algebra of IV TBD\nDemand and Supply TBD\nAcademic Application 1 As an research paper replication, we are going to replicate Does compulsory school attendance affect schooling and earnings? (1991) by Angrist and Krueger. The authors study the effect of education on wages.\nThe problem of studying the relationship of education on wages is that there might be factors that influence both education and wages but we do not observe, for example ability. Students that have higher ability might decide to stay longer in school and also get higher wages afterwards.\nThe idea of the authors is to use the quarter of birth as an instrument for education. In fact, quarter of birth is plausibly exogenous with respect to wages while, on the other hand, is correlated with education. Why? Students that are both in the last quarter of the year cannot drop out as early as other students and therefore are exposed to more eduction.\nWe can represent the DAG of their model as follows.\n%matplotlib inline %config InlineBackend.figure_format = 'retina' from src.utils import * dag_iv(Y=\u0026quot;wage\u0026quot;, T=\u0026quot;education\u0026quot;, Z=\u0026quot;quarter of birth\u0026quot;, U=\u0026quot;ability\u0026quot;) A shortcoming of this instrument comes out of the fact that the population of compliers is students that drop out of school as soon as possible, we will know the treatment effect only for this population. It\u0026rsquo;s important to keep this in mind when interpreting the results.\nLet\u0026rsquo;s load the data, freely available here.\ndf = pd.read_csv('data/ak91.csv') df.head() log_wage years_of_schooling date_of_birth year_of_birth quarter_of_birth state_of_birth 0 5.790019 12.0 1930.0 1930.0 1.0 45.0 1 5.952494 11.0 1930.0 1930.0 1.0 45.0 2 5.315949 12.0 1930.0 1930.0 1.0 45.0 3 5.595926 12.0 1930.0 1930.0 1.0 45.0 4 6.068915 12.0 1930.0 1930.0 1.0 37.0 We have the variables of interest, log_wage, years_of_schooling and quarter_of_birth, together with a set of controls.\nOLS If we were to ignore the endogeneity problem we would estimate a linear regression of log_wage on years_of_schooling, plus control dummy variables for the state_of_birth and year_of_birth.\nsmf.ols('log_wage ~ years_of_schooling', df).fit().summary().tables[1] coef std err t P\u003e|t| [0.025 0.975] Intercept 4.9952 0.004 1118.882 0.000 4.986 5.004 years_of_schooling 0.0709 0.000 209.243 0.000 0.070 0.072 IV We now use quarter_of_birth as an instrument for years_of_schooling. We cannot check the exclusion restriction condition, but we can check the relevance condition.\nLet\u0026rsquo;s start first by plotting average years_of_schooling by date of birth.\ngroup_df = df.groupby(\u0026quot;date_of_birth\u0026quot;).mean().reset_index() plt.figure(figsize=(15,6)) sns.lineplot(data=group_df, x=\u0026quot;date_of_birth\u0026quot;, y=\u0026quot;years_of_schooling\u0026quot;, zorder=1)\\ .set(title=\u0026quot;First Stage\u0026quot;, xlabel=\u0026quot;Year of Birth\u0026quot;, ylabel=\u0026quot;Years of Schooling\u0026quot;); for q in range(1, 5): x = group_df.loc[group_df['quarter_of_birth']==q, \u0026quot;date_of_birth\u0026quot;] y = group_df.loc[group_df['quarter_of_birth']==q, \u0026quot;years_of_schooling\u0026quot;] plt.scatter(x, y, marker=\u0026quot;s\u0026quot;, s=200, c=f\u0026quot;C{q}\u0026quot;) plt.scatter(x, y, marker=f\u0026quot;${q}$\u0026quot;, s=100, c=f\u0026quot;white\u0026quot;) As we can see, there is an upward trend but, within each year, people both in the last quarter usually have more years of schooling than people born in other quarters of the year.\nWe can check this correlation more formally by regressing years_of_schooling of a set of dummies for quarter_of_birth.\nsmf.ols('years_of_schooling ~ C(quarter_of_birth)', df).fit().summary().tables[1] coef std err t P\u003e|t| [0.025 0.975] Intercept 12.6881 0.011 1105.239 0.000 12.666 12.711 C(quarter_of_birth)[T.2.0] 0.0566 0.016 3.473 0.001 0.025 0.089 C(quarter_of_birth)[T.3.0] 0.1173 0.016 7.338 0.000 0.086 0.149 C(quarter_of_birth)[T.4.0] 0.1514 0.016 9.300 0.000 0.119 0.183 The relationship between years_of_schooling and quarter_of_birth is indeed statistically significant.\nDoes it translate it into higher wages? We can have a first glimpse of potential IV effects by plotting wages against the date of birth.\nplt.figure(figsize=(15,6)) sns.lineplot(data=group_df, x=\u0026quot;date_of_birth\u0026quot;, y=\u0026quot;log_wage\u0026quot;, zorder=1)\\ .set(title=\u0026quot;Reduced Form\u0026quot;, xlabel=\u0026quot;Year of Birth\u0026quot;, ylabel=\u0026quot;Log Wage\u0026quot;); for q in range(1, 5): x = group_df.loc[group_df['quarter_of_birth']==q, \u0026quot;date_of_birth\u0026quot;] y = group_df.loc[group_df['quarter_of_birth']==q, \u0026quot;log_wage\u0026quot;] plt.scatter(x, y, marker=\u0026quot;s\u0026quot;, s=200, c=f\u0026quot;C{q}\u0026quot;) plt.scatter(x, y, marker=f\u0026quot;${q}$\u0026quot;, s=100, c=f\u0026quot;white\u0026quot;) It seems that indeed people both in later quarters earn higher wages later in life.\nWe now turn into the estimation of the causal effect of education on wages.\ndf[['q1', 'q2', 'q3', 'q4']] = pd.get_dummies(df['quarter_of_birth']) from linearmodels.iv import IV2SLS IV2SLS.from_formula('log_wage ~ 1 + [years_of_schooling ~ q1 + q2 + q3]', data=df).fit().summary.tables[1] Parameter Estimates Parameter Std. Err. T-stat P-value Lower CI Upper CI Intercept 4.5898 0.2494 18.404 0.0000 4.1010 5.0786 years_of_schooling 0.1026 0.0195 5.2539 0.0000 0.0643 0.1409 The coefficient is slightly higher than the OLS coefficient. It\u0026rsquo;s important to remember that the estimated effect is specific to the subpopulation of people that drop out of school as soon as they can.\nResearch Paper Replication 2 In The Colonial Origins of Comparative Development (2002) by Acemoglu, Johnson, Robinson, the authors wish to determine whether or not differences in institutions can help to explain observed economic outcomes.\nHow do we measure institutional differences and economic outcomes?\nIn this paper,\neconomic outcomes are proxied by log GDP per capita in 1995, adjusted for exchange rates. institutional differences are proxied by an index of protection against expropriation on average over 1985-95, constructed by the Political Risk Services Group. The problem is that there might exist other factors that affects both the quality of institutions and GDP. The authors suggest the following problems as sources of endogeneity:\nricher countries may be able to afford or prefer better institutions variables that affect income may also be correlated with institutional differences the construction of the index may be biased; analysts may be biased towards seeing countries with higher income having better institutions The idea of the authors is to use settler\u0026rsquo;s mortality during the colonization period as an instrument for the quality of institutions. They hypothesize that higher mortality rates of colonizers led to the establishment of institutions that were more extractive in nature (less protection against expropriation), and these institutions still persist today.\nWe can represent their DAG as follows.\ndag_iv(Y=\u0026quot;GDP\u0026quot;, T=\u0026quot;institutions\u0026quot;, Z=\u0026quot;settlers' mortality\u0026quot;, U=\u0026quot;tons of stuff\u0026quot;) First, let\u0026rsquo;s load the data (available here) and have a look at it.\ndf = pd.read_csv('data/ajr02.csv',index_col=0) df.head() GDP Exprop Mort Latitude Neo Africa Asia Namer Samer logMort Latitude2 1 8.39 6.50 78.20 0.3111 0 1 0 0 0 4.359270 0.096783 2 7.77 5.36 280.00 0.1367 0 1 0 0 0 5.634790 0.018687 3 9.13 6.39 68.90 0.3778 0 0 0 0 1 4.232656 0.142733 4 9.90 9.32 8.55 0.3000 1 0 0 0 0 2.145931 0.090000 5 9.29 7.50 85.00 0.2683 0 0 0 1 0 4.442651 0.071985 The data contains the main variables, DGP, Exprop and Mort, plus some geographical information.\nOLS What would we get if we were to ignore the endogeneity problem? We estimate the following misspecified model by OLS\n$$ {GDP}_i = \\beta_0 + \\beta_1 {Exprop}_i + \\varepsilon_i $$\nreg1 = smf.ols('GDP ~ Exprop', df).fit() reg1.summary().tables[1] coef std err t P\u003e|t| [0.025 0.975] Intercept 4.6609 0.409 11.402 0.000 3.844 5.478 Exprop 0.5220 0.061 8.527 0.000 0.400 0.644 The coefficient of Exprop is positive and significant but we know it is a biased estimate of the causal effect.\nOne direction we could take in addressing the endogeneity problem could be to control for any factor that affects both GDP and Exprop. In particular, the authors consider the following sets of variables:\nclimat; proxied by latitude differences that affect both economic performance and institutions, eg. cultural, historical, etc.; controlled for with the use of continent dummies reg2 = smf.ols('GDP ~ Exprop + Latitude + Latitude2', df).fit() reg3 = smf.ols('GDP ~ Exprop + Latitude + Latitude2 + Asia + Africa + Namer + Samer', df).fit() from statsmodels.iolib.summary2 import summary_col summary_col(results=[reg1,reg2,reg3], float_format='%0.2f', stars = True, info_dict={'No. observations' : lambda x: f\u0026quot;{int(x.nobs):d}\u0026quot;}, regressor_order=['Intercept','Exprop','Latitude','Latitude2']) GDP I GDP II GDP III Intercept 4.66*** 4.55*** 5.95*** (0.41) (0.45) (0.68) Exprop 0.52*** 0.49*** 0.40*** (0.06) (0.07) (0.06) Latitude 2.16 0.42 (1.68) (1.47) Latitude2 -2.12 0.44 (2.86) (2.48) Africa -1.06** (0.41) Asia -0.74* (0.42) Namer -0.17 (0.40) Samer -0.12 (0.42) R-squared 0.54 0.56 0.71 R-squared Adj. 0.53 0.54 0.67 No. observations 64 64 64 The coefficient of Expropr decreases in magnitude but remains positive and significant after the addition of geographical control variables. This might suggest that the endogeneity problem is not very pronounced. However, it\u0026rsquo;s hard to say given the large number of factors that could affect both institutions and GDP.\nIV In order for Mort to be a valid instrument it needs to satisfy the two IV conditions:\nExclusion: Mort must be correlated to GDP only through Exprop Relevance: Mort must be correlated with Exprop The exclusion restriction condition is untestable, however, we may not be satisfied if settler mortality rates in the 17th to 19th centuries have a direct effect on current GDP (in addition to their indirect effect through institutions).\nFor example, settler mortality rates may be related to the current disease environment in a country, which could affect current economic performance.\nThe authors argue this is unlikely because:\nThe majority of settler deaths were due to malaria and yellow fever and had a limited effect on local people. The disease burden on local people in Africa or India, for example, did not appear to be higher than average, supported by relatively high population densities in these areas before colonization. The relevance condition is testable and we can check it by computing the partial correlation between Mort and Exprop. Let\u0026rsquo;s start by visual inspection first.\nsns.scatterplot(data=df, x='Mort', y='Exprop')\\ .set(title='First Stage', xlabel='Settler mortality', ylabel='Risk of expropriation'); Visually, the first stage seems weak, at best. However, a regression of Exprop on Mort can help us better assess whether the relationship is significant or not.\nsmf.ols('Exprop ~ Mort', df).fit().summary().tables[1] coef std err t P\u003e|t| [0.025 0.975] Intercept 6.7094 0.202 33.184 0.000 6.305 7.114 Mort -0.0008 0.000 -2.059 0.044 -0.002 -2.28e-05 The coefficient is negative, as expected, and statistically significant.\nThe second-stage regression results give us an unbiased and consistent estimate of the effect of institutions on economic outcomes.\n$$ {GDP}_i = \\beta_0 + \\beta_1 {Exprop}_i + \\varepsilon_i \\ {Exprop}_i = \\delta_0 + \\delta_1 {logMort}_i + v_i $$\nNote that while our parameter estimates are correct, our standard errors are not and for this reason, computing 2SLS ‘manually’ (in stages with OLS) is not recommended.\nWe can correctly estimate a 2SLS regression in one step using the linearmodels package, an extension of statsmodels\nNote that when using IV2SLS, the exogenous and instrument variables are split up in the function arguments (whereas before the instrument included exogenous variables)\nIV2SLS.from_formula('GDP ~ 1 + [Exprop ~ logMort]', data=df).fit().summary.tables[1] Parameter Estimates Parameter Std. Err. T-stat P-value Lower CI Upper CI Intercept 2.0448 1.1273 1.8139 0.0697 -0.1647 4.2542 Exprop 0.9235 0.1691 5.4599 0.0000 0.5920 1.2550 The result suggests a stronger positive relationship than what the OLS results indicated.\nBusiness Case We are given the following problem:\nA firm would like to understand whether its newsletter is working to increase revenue. However, it cannot force customers to subscribe to the newsletter. Instead, the firm sends a reminder email to a random sample of customers for the newsletter. Estimate the effect of the newsletter on revenue.\nWe start by drawing a sample from the data generating process.\nfrom src.dgp import dgp_newsletter dgp = dgp_newsletter() df = dgp.generate_data() df.head() reminder subscribe revenue 0 0 1 0.582809 1 1 0 3.427162 2 0 0 1.953731 3 0 0 2.902038 4 0 0 0.826724 From the data, we know the revenue per customer, whether it was sent a reminder for the newsletter and whether it actually decided to subscribe.\nIf we to estimate the effect of subscribe on revenue, we might get a biased estimate because the decision of subscribing is endogenous. For example, we can imagine that wealthier customers are generating more revenue but are also less likely to subscribe.\nWe can represent the model with a DAG.\ndag_iv(Y=\u0026quot;revenue\u0026quot;, T=\u0026quot;subscribe\u0026quot;, Z=\u0026quot;reminder\u0026quot;, U=\u0026quot;income\u0026quot;) OLS By directly inspecting the data, it seems that subscribed members actually generate less revenue than normal customers.\nsns.barplot(x='subscribe', y='revenue', data=df); A linear regression confirms the graphical intuition.\nsmf.ols('revenue ~ 1 + subscribe', df).fit().summary().tables[1] coef std err t P\u003e|t| [0.025 0.975] Intercept 1.7752 0.086 20.697 0.000 1.607 1.943 subscribe -0.7441 0.140 -5.334 0.000 -1.018 -0.470 However, if indeed wealthier customers generate more revenue and are less likely to subscribe, we have a negative omitted variable bias and we can expect the true effect of the newsletter to be bigger than the OLS estimate.\nIV Let\u0026rsquo;s now exploit the random variation induced by the discount. In order for our instrument to be valid, we need it to be exogenous (untestable) and relevant. We can test the relevance with the first stage regresssion of reminder on subscribe.\nsmf.ols('subscribe ~ 1 + reminder', df).fit().summary().tables[1] coef std err t P\u003e|t| [0.025 0.975] Intercept 0.2368 0.021 11.324 0.000 0.196 0.278 reminder 0.2790 0.029 9.488 0.000 0.221 0.337 It seems that the instrument is relevant. We can now estimate the IV regression.\nIV2SLS.from_formula('revenue ~ 1 + [subscribe ~ reminder]', data=df).fit().summary.tables[1] Parameter Estimates Parameter Std. Err. T-stat P-value Lower CI Upper CI Intercept 0.9485 0.2147 4.4184 0.0000 0.5278 1.3693 subscribe 1.4428 0.5406 2.6689 0.0076 0.3832 2.5023 The estimated coefficient has now flipped sign and turned positive! Ignoring the endogeneity problem would have lead us to the wrong conclusion.\nReferences Instrumental Variables video lecture by Paul Goldsmith-Pinkham (Yale) Instrumental Variables section from Matheus Facure\u0026rsquo;s Causal Inference for The Brave and The True Does compulsory school attendance affect schooling and earnings? (1991) by Angrist and Krueger The Colonial Origins of Comparative Development (2002) by Acemoglu, Johnson, Robinson ","date":1649980800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1649980800,"objectID":"c1ba7293fd6ef944fe34ab3ec2773d15","permalink":"https://matteocourthoud.github.io/post/iv/","publishdate":"2022-04-15T00:00:00Z","relpermalink":"/post/iv/","section":"post","summary":"In this tutorial, we are going to see how to estimate causal effects when the treatment is not randomly assigned, but we have access to a third variable that is as good as randomly assigned and is correlated (only) with the treatment.","tags":null,"title":"Instrumental Variables","type":"post"},{"authors":null,"categories":null,"content":"If you search \u0026ldquo;permutation test\u0026rdquo; on Wikipedia, you get the following definition:\nA permutation test (also called re-randomization test) is an exact statistical hypothesis test making use of the proof by contradiction in which the distribution of the test statistic under the null hypothesis is obtained by calculating all possible values of the test statistic under possible rearrangements of the observed data.\nWhat does it mean? In this tutorial we are going to see in detail what this definition means, how to implement permutation tests, and their pitfalls.\nExample 1: is a coin fair? Let\u0026rsquo;s start with an example: suppose you wanted to test whether a coin is fair. You throw the coin 10 times and you count the number of times you get heads. Let\u0026rsquo;s simulate the outcome.\nimport numpy as np np.random.seed(1) np.random.binomial(1, 0.5, 10) array([0, 1, 0, 0, 0, 0, 0, 0, 0, 1]) Out of 10 coin throws, we got only 2 heads. Does it mean that the coin is not fair?\nThe question that permutation testing is trying to answer is \u0026ldquo;how unlikely is the observed outcome under the null hypothesis that the coin is fair?\u0026rdquo;.\nIn this case we can directly compute this answer since we have a very little number of throws. The total number of outcomes is $2^{10}$. The number of as or more extreme outcomes, under the assumption that the coin is fair (50-50) is\n0 heads: ${10 \\choose 0} = 1$ 1 head: ${10 \\choose 1} = 10$ 2 heads: ${10 \\choose 2} = 45$ So that the probability of getting the same or a more extreme outcome is\nfrom scipy.special import comb (comb(10, 0) + comb(10, 1) + comb(10, 2)) / 2**10 0.0546875 This probability seems low but not too low.\nHowever, we have forgot one thing. We want to test whether the coin is fair in either direction. We would suspect that the coin is unfair if we were getting few heads (as we did), but also if we were getting many heads. Therefore, we should account for both extremes.\nsum([comb(10, i) for i in [0, 1, 2, 8, 9, 10]]) / 2**10 0.109375 This number should not be surprising since it\u0026rsquo;s exactly double the previous one.\nIt is common in statistics to say that an event is unusual if its probability is less than 1 in 20, i.e. $5%$. If we were adopting that threshold, we would not conclude that getting 2 heads in 10 trows is so unusual. However, getting just one, would be.\nsum([comb(10, i) for i in [0, 1, 9, 10]]) / 2**10 0.021484375 Hypothesis Testing The process we just went through is called hypothesis testing. The components of an hypothesis test are:\nA null hypothesis $H_0$\nin our case, that the coin war fair A test statistic $t$\nin our case, the number of zeros A level of significance $\\alpha$\nit is common to choose 5% The idea behind permutation testing is the following: in a setting in which we are checking whether one variable has an effect on another variable, the two variables should not be correlated, under the null hypothesis . Therefore, we could re-shuffle the treatment variable and re-compute the test statistic. Lastly, we can compute the p-value as the fraction of as or more extremes outcomes under re-shuffling of the data.\nExample 2: are women smarter? Suppose now we were interested in knowing whether females perform better in a test than men. Let\u0026rsquo;s start by writing the data generating process under the assumption of no difference in scores. However, only 30% of the sample will be female.\nimport pandas as pd # Data generating process def generate_data_gender(N=100, seed=1): np.random.seed(seed) # Set seed for replicability data = pd.DataFrame({\u0026quot;female\u0026quot;: np.random.binomial(1, 0.3, N), \u0026quot;test_score\u0026quot;: np.random.exponential(3, N)}) return data Let\u0026rsquo;s now generate a sample of size 100.\n# Generate data data_gender = generate_data_gender() data_gender.head() female test_score 0 0 1.186447 1 1 2.246348 2 0 6.513147 3 0 1.326091 4 0 7.175402 We can compute the treatment effect by computing the difference in mean outcomes between male and females.\ndef compute_score_diff(data): T = np.mean(data.loc[data['female']==1, 'test_score']) - np.mean(data.loc[data['female']==0, 'test_score']) return T T = compute_score_diff(data_gender) print(f\u0026quot;The estimated treatment effect is {T}\u0026quot;) The estimated treatment effect is -1.3612262580563321 It looks that females actually did worse than males. But is the difference statistically significant? We can perform a randomization test and compute the probability of observing a more extreme outcome.\nFirst, let\u0026rsquo;s write the permutation routine that takes a variable in the data and permutes it.\ndef permute(data, var, r): temp_data = data.copy() temp_data[var] = np.random.choice(data[var], size=len(data), replace=r) return temp_data We can now write the permutation test. It spits out a vector of statistics and prints the implied p-value.\ndef permutation_test(data, permute, var, compute_stat, K=1000, r=False): T = compute_stat(data) T_perm = [] for k in range(K): temp_data = permute(data, var, r) T_perm += [compute_stat(temp_data)] print(f\u0026quot;The p-value is {sum(np.abs(T_perm) \u0026gt;= np.abs(T))/K}\u0026quot;) return T_perm Ts = permutation_test(data_gender, permute, 'test_score', compute_score_diff) The p-value is 0.063 Apparently the result we have observed was quite unusual, but not at the 5% level. We can plot the distribution of statistics to visualize this result.\n%matplotlib inline %config InlineBackend.figure_format = 'retina' from src.utils import * def plot_test(T, Ts, title): plt.hist(Ts, density=True, bins=30, alpha=0.7, color='C0') plt.vlines([-T, T], ymin=plt.ylim()[0], ymax=plt.ylim()[1], color='C2') plt.title(title); plot_test(T, Ts, 'Distribution of score differences under permutation') As we can see, the observed difference in scores is quite extreme with respect the distribution generate by the permutation.\nOne issue with the permutation test we just ran is that it is computationally expensive to draw without replacement. The standard and much faster procedure is to draw without replacement.\nTs_repl = permutation_test(data_gender, permute, 'test_score', compute_score_diff, r=True) The p-value is 0.052 The p-value is virtually the same.\nHow accurate is the test? Since we have access to the data generating process, we can compute the true p-value via simulation. We draw many samples from the true data generating process and, for each, compute the difference in scores. The simulated p-value is going to be the frequency of more extreme statistics.\n# Function to simulate data and compute pvalue def simulate_stat(dgp, compute_stat, K=1000): T = compute_stat(dgp()) T_sim = [] for k in range(K): data = dgp(seed=k) T_sim += [compute_stat(data)] print(f\u0026quot;The p-value is {sum(np.abs(T_sim) \u0026gt;= np.abs(T))/K}\u0026quot;) return np.array(T_sim) T_sim = simulate_stat(generate_data_gender, compute_score_diff) The p-value is 0.038 Again, we can plot the distribution of simulated statistics to understand the computed p-value.\nplot_test(T, T_sim, 'Distribution of score differences under simulation') As expected, most of the mass lies within the interval, indicating a relatively extreme result. We have just been \u0026ldquo;unlucky\u0026rdquo; with the draw, but the permutation test was accurate.\nPermutation tests vs t-tests What is the difference between a t-test and a permutation test?\nPermutation test advantages:\ndoes not make distributional assumptions not sensible to outliers can be computed also for statistics whose distribution is not known Permutation test disadvantages:\ncomputationally intense very sample-dependent Example 3: is university worth? Let\u0026rsquo;s now switch to a new example to compare t-tests and permutation tests.\nAssume we want to check whether university is a worthy investment. We have information about whether individuals attended university and their future salary. The problem here is that income is a particularly skewed variable.\n# Data generating process def generate_data_income(N=1000, seed=1): np.random.seed(seed) # Set seed for replicability university = np.random.binomial(1, 0.5, N) # Treatment data = pd.DataFrame({\u0026quot;university\u0026quot;: university, \u0026quot;income\u0026quot;: np.random.lognormal(university, 2.3, N)}) return data data_income = generate_data_income() data_income.head() university income 0 0 5.305618 1 1 1.289598 2 0 6.507720 3 0 6.019961 4 0 0.034482 The distribution of income is very heavy tailed. Let\u0026rsquo;s plot its density across the two groups.\nsns.kdeplot(data=data_income, x=\u0026quot;income\u0026quot;, hue=\u0026quot;university\u0026quot;)\\ .set(title='Income density by group'); The distribution is so skewed that we cannot actually visually perceive differences between the two groups. Let\u0026rsquo;s compute the expected difference.\ndef compute_income_diff(data): T = np.mean(data.loc[data['university']==1, 'income']) - np.mean(data.loc[data['university']==0, 'income']) return T T = compute_income_diff(data_income) T 23.546974435985444 It looks like university graduates have higher income. Is this difference statistically different from zero? Let\u0026rsquo;s perform a permutation test.\nT_perm = permutation_test(data_income, permute, 'university', compute_income_diff) The p-value is 0.011 The permutation test is telling us that the difference is extremely unusual under the null hypothesis. In other words, it is very unlikely that university graduates earn the same income of non-university graduates.\nWhat would be the outcome of a standard t-test?\nfrom scipy.stats import ttest_ind ttest_ind(data_income.query('university==1')['income'], data_income.query('university==0')['income']) Ttest_indResult(statistic=1.5589492598056494, pvalue=0.1193254252009701) As we can see, the two tests provide extremely different results. The t-test is much more conservative, telling us that the unlikeliness of the data is just $12%$ compared to the $1.1%$ of the permutation test.\nThe reason is that we have extremely skewed data. The t-test is very sensible to extreme observation and will therefore compute a very high variance because of very few data points.\nThe permutation test can further address the problem of a skewed outcome distribution by using a test statistic that is more sensible to outliers. Let\u0026rsquo;s perform the permutation test using the trimmed mean instead of the mean.\nfrom scipy.stats import trim_mean def compute_income_mediandiff(data): T = np.median(data.loc[data['university']==1, 'income']) - np.median(data.loc[data['university']==0, 'income']) return T T_perm = permutation_test(data_income, permute, 'university', compute_income_mediandiff) The p-value is 0.0 In this case, the permutation test is extremely confident that the trimmed mean of the two groups is different.\nHowever, an advantage of the t-test is speed. Let\u0026rsquo;s compare the two tests by computing their execution time. Note that this is just a rough approximation since the permutation test could be sensible optimized.\nimport time # No replacement start = time.time() permutation_test(data_income, permute, 'university', compute_income_diff) print(f\u0026quot;Elapsed time without replacement: {time.time() - start}\u0026quot;) # Replacement start = time.time() ttest_ind(data_income.query('university==1')['income'], data_income.query('university==0')['income']) print(f\u0026quot;Elapsed time with replacement: {time.time() - start}\u0026quot;) The p-value is 0.016 Elapsed time without replacement: 0.28911614418029785 Elapsed time with replacement: 0.00125885009765625 The permutation test is 300 times slower. This can be a particularly relevant difference for larger sample sizes.\nConclusion In this tutorial, we have seen how to perform permutation tests across different data generating processes.\nThe underlying principle is the same: permute an variable that is assumed to be random under the null hypothesis and re-compute the test statistic. Then check how unusual was the test statistic in the original dataset.\n","date":1649980800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1649980800,"objectID":"a5a6a7d39cdee5c66211055b0347a2c5","permalink":"https://matteocourthoud.github.io/post/permutation_test/","publishdate":"2022-04-15T00:00:00Z","relpermalink":"/post/permutation_test/","section":"post","summary":"If you search \u0026ldquo;permutation test\u0026rdquo; on Wikipedia, you get the following definition:\nA permutation test (also called re-randomization test) is an exact statistical hypothesis test making use of the proof by contradiction in which the distribution of the test statistic under the null hypothesis is obtained by calculating all possible values of the test statistic under possible rearrangements of the observed data.","tags":null,"title":"Permutation Tests for Dummies","type":"post"},{"authors":null,"categories":null,"content":"In this tutorial, we are going to see how to design the most welfare-improving policy in presence of treatment effect heterogeneity and treatment costs or budget constraints.\nRequisites\nFor this tutorial, I assume you are familiar with the following concepts:\nRubin\u0026rsquo;s potential outcome framework Propensity weighting or uplifting AIPW or Double Robust Estimators Causal Trees Academic Replication\nWe are going to replicate the paper by Hanna and Olken (2018) in which the authors study the optimal allocation of a cash transfer to households in Peru. We slightly twist the original paper by actually assigning the transfer and assuming 100% consumption.\nBusiness Case\nWe are going to study a company that has to decide which consumers to target with ads.\nSetting We assume that for a set of i.i.d. subjects $i = 1, \u0026hellip;, n$ we observed a tuple $(X_i, T_i, Y_i)$ comprised of\na feature vector $X_i \\in \\mathbb R^n$ a treatment variable $T_i \\in \\lbrace 0, 1 \\rbrace$ a response $Y_i \\in \\mathbb R$ Assumption 1 : unconfoundedness (or ignorability, or selection on observables)\n$$ \\big \\lbrace Y_i^{(1)} , Y_i^{(0)} \\big \\rbrace \\ \\perp \\ T_i \\ | \\ X_i $$\ni.e. conditional on observable characteristics $X$, the treatment assignment $T$ is as good as random.\nAssumption 2: overlap (or bounded support)\n$$ \\exists \\eta \u0026gt; 0 \\ : \\ \\eta \\leq \\mathbb E \\left[ T_i = 1 \\ \\big | \\ X_i = x \\right] \\leq 1-\\eta $$\ni.e. no observation is deterministically assigned to the treatment or control group.\nPolicy Learning The objective of policy learning is to decide which people to treat. More explicitly, we want to learn a map from observable characteristics to a (usually binary) policy space.\n$$ \\pi : \\mathcal X \\to \\lbrace 0, 1 \\rbrace $$\nPolicy learning is closely related to the estimation of heterogeneous treatment effects. In fact, in both settings, we want to investigate how the treatment affects different individuals in different ways.\nThe main difference between policy learning and the estimation of heterogeneous treatment effects is the objective function. In policy learning, we are acting in a limited resources setting where providing treatment is costly and the cost could depend on individual characteristics. For example, it might be more costly to vaccinate individuals that live in remote areas. Therefore, one might not just want to treat individuals with the largest expected treatment effect, but the ones for whom treatment is most cost-effective.\nThe utilitarian value of a policy $\\pi$\n$$ V(\\pi) = \\mathbb E \\Big[ Y_i(\\pi(X_i)) \\Big] = \\mathbb E \\big[ Y^{(0)}_i \\big] + \\mathbb E \\big[ \\tau(X_i) \\pi(X_i) \\big] $$\nmeasures the expectation of the potential outcome $Y$ if we were to assign treatment $T$ according to policy $\\pi$. This expectation can be split into two parts:\nThe baseline expected potential outcome $\\mathbb E \\big[ Y^{(0)}_i \\big]$ The expected effect of the policy $\\mathbb E \\big[ \\tau(X_i) \\pi(X_i) \\big]$ The objective of policy learning is to learn a policy with high value $V(\\pi)$. As part (2) of the formula makes clear, you get a higher value if you treat the people with a high treatment effect $\\tau(x)$.\nA simple approach could be to assign treatment according to a thresholding rule $\\tau(x) \u0026gt; c$, where $c$ is some cost below which is not worth treating individuals (or there is not enough budget).\nHowever, estimating the conditional average treatment effect (CATE) function $\\tau(x)$ and learning a good policy $\\pi(x)$ are different problems.\nthe correct loss function for policy learning is not the mean squared error (MSE) on $\\tau(x)$ we want to maximize welfare! the CATE function $\\tau(x)$ might not use some features for targeting e.g. cannot discriminate based on race or gender you don\u0026rsquo;t want to have feature that people can influence e.g. use a self-reported measure that people can distort We would like to find a loss function $L(\\pi ; Y_i, X_i, T_i)$ such that\n$$ \\mathbb E \\big[ L(\\pi ; Y_i, X_i, T_i) \\big] = - V(\\pi) $$\nIPW Loss Kitagawa and Tenenov (2018) propose to learn an empirical estimate of the value function using inverse propensity weighting (IPW).\n$$ \\hat \\pi = \\arg \\max_{\\pi} \\Big\\lbrace \\hat V(\\pi) : \\pi \\in \\Pi \\Big\\rbrace $$\nwhere\n$$ \\hat V(\\pi) = \\frac{ \\mathbb I \\big(\\lbrace T_i = \\pi(X_i) \\rbrace \\big) }{ \\mathbb P \\big[ \\lbrace T_i = \\pi(X_i) \\rbrace \\ \\big| \\ X_i \\big] } Y_i $$\nThe authors show that under unconfoundedness, if the propensity score $e(x)$ is known and $\\Pi$ is not too complex, the value of the estimated policy converges to the optimal value.\nNote that this is a very different problem from the normal optimization problem with a MSE loss. In fact, we now have a binary argument in the loss function which makes the problem similar to a classification problem, in which we want to classify people into high gain and low gain categories.\nAIPW Loss If propensity score $e(x)$ is not known, we can use a doubly robust estimator, exactly as for the average treatment effect.\n$$ \\hat V = \\frac{1}{n} \\sum_{i=1}^{n} \\begin{cases} \\hat \\Gamma_i \\quad \u0026amp;\\text{if} \\quad \\pi(X_i) = 1 \\newline\n\\hat \\Gamma_i \\quad \u0026amp;\\text{if} \\quad \\pi(X_i) = 0 \\end{cases} $$ where\n$$ \\hat \\Gamma_i = \\hat \\mu^{(1)}(X_i) - \\hat \\mu^{(0)}(X_i) + \\frac{T_i }{\\hat e(X_i)} \\left( Y_i - \\hat \\mu^{(1)}(X_i) \\right) - \\frac{(1-T_i) }{1-\\hat e(X_i)} \\left( Y_i - \\hat \\mu^{(0)}(X_i) \\right) $$\nThe relationship with AIPW is that $\\hat \\tau_{AIPW} = \\frac{1}{n} \\sum_{i=1}^{n} \\hat \\Gamma_i$. Therefore, the objective function $V(\\pi)$ is build so that when we assign treatment to a unit we \u0026ldquo;gain\u0026rdquo; the double-robust score $\\hat \\tau_{AIPW}$, while, if we do not assign treatment, we \u0026ldquo;pay\u0026rdquo; the double-robust score $\\hat \\tau_{AIPW}$.\nAcademic Application For the academic applicaiton, we are going to replicate the paper by Hanna and Olken (2018) in which the authors study the optimal allocation of a cash transfer to households in Peru. We slightly twist the original paper by actually assigning the transfer and assuming 100% consumption.\nFirst, let\u0026rsquo;s load the modified dataset.\nfrom src.utils import * from src.dgp import dgp_ao18 %matplotlib inline %config InlineBackend.figure_format = 'retina' dgp = dgp_ao18() df = dgp.import_data() df.head() Unnamed: 0 d_fuel_other d_fuel_wood d_fuel_coal d_fuel_kerosene d_fuel_gas d_fuel_electric d_fuel_none d_water_other d_water_river ... d_lux_1 d_lux_2 d_lux_3 d_lux_4 d_lux_5 training h_hhsize cash_transfer consumption welfare 0 0 0 1 0 0 0 0 0 0 1 ... 0 0 0 0 0 0 1 0 211.0000 5.351858 1 1 0 0 0 0 1 0 0 0 0 ... 0 0 1 0 0 0 3 1 420.1389 6.040585 2 2 0 0 0 0 1 0 0 0 0 ... 0 0 1 0 0 0 4 0 390.8318 5.968277 3 3 0 0 0 0 1 0 0 0 0 ... 0 0 0 0 0 0 9 0 285.6018 5.654599 4 4 0 1 0 0 0 0 0 0 1 ... 0 0 0 0 0 0 8 0 118.0713 4.771289 5 rows × 78 columns\nAs we can see, we have a lot of information about individuals in Peru. Crucially for the research question, we observe\nwhether the household received a cash transfer, cash_transfer the household\u0026rsquo;s welfare afterwards, welfare_post assuming $$ \\text{welfare} = \\log (\\text{consumption}) $$ We would like to understand which individuals should be given a transfer, given that the transfer is costly. Let\u0026rsquo;s assume the transfer costs $0.3$ units of welfare.\nfrom econml.policy import DRPolicyForest cost = 0.3 policy = DRPolicyForest(random_state=1).fit(Y=df[dgp.Y] - cost*df[dgp.T], T=df[dgp.T], X=df[dgp.X]) We can partially visualize the policy by plotting a regression tree for the most important features.\n%matplotlib inline policy.plot(tree_id=1, max_depth=2, feature_names=dgp.X, fontsize=8) To understand if the estimated policy was effective, we can load the oracle dataset, with the potential outcomes.\ndf_oracle = dgp.import_data(oracle=True) From the oracle dataset, we can compute the actual value of the policy.\nT_hat = policy.predict(df[dgp.X]) V_policy = (df_oracle['welfare_1'].values - cost - df_oracle['welfare_0'].values) * T_hat print(f'Estimated policy value (N_T={sum(T_hat)}): {np.mean(V_policy) :.4}') Estimated policy value (N_T=21401): 0.05897 The value is positive, indicating that the treatment was effective. But how well did we do? We can compare the estimated policy with the oracle policy that assign treatment to each cost-effective unit.\nT_oracle = (df_oracle['welfare_1'] - df_oracle['welfare_0']) \u0026gt; cost V_oracle = (df_oracle['welfare_1'] - cost - df_oracle['welfare_0'] ) * T_oracle print(f'Oracle policy value (N_T={sum(T_oracle)}): {np.mean(V_oracle) :.4}') Oracle policy value (N_T=17630): 0.07494 We actually achieved 79% of the potential policy gains! Also note that our policy is too generous, treating more units than optimal. But how well would we have done if the same amount of cash transfers were given at random?\nT_rand = np.random.binomial(1, sum(T_hat)/len(df), len(df)) V_rand = (df_oracle['welfare_1'] - cost - df_oracle['welfare_0'] ) * T_rand print(f'Random policy value (N_T={sum(T_rand)}): {np.mean(V_rand) :.4}') Random policy value (N_T=21359): 0.0002698 A random assignment of the same amount of cash transfers would not achieve any effect. However, this assumes that we already know the optimal amount of funds to distribute. What if instead we had treated everyone?\nV_all = (df_oracle['welfare_1'] - cost - df_oracle['welfare_0'] ) print(f'All-treated policy value (N_T={len(df)}): {np.mean(V_all) :.4}') All-treated policy value (N_T=45378): 0.0004019 Indiscriminate treatment would again not achieve any effect. Lastly, what if we had just estimated the treatment effect using AIPW and used it as a threshold?\nfrom econml.dr import LinearDRLearner model = LinearDRLearner(random_state=1).fit(Y=df[dgp.Y], T=df[dgp.T], X=df[dgp.X]) T_ipw = model.effect(X=df[dgp.X], T0=0, T1=1) \u0026gt; cost V_ipw = (df_oracle['welfare_1'] - cost - df_oracle['welfare_0'] ) * T_ipw print(f'IPW policy value (N_T={sum(T_ipw)}): {np.mean(V_ipw) :.4}') IPW policy value (N_T=21003): 0.06293 We are actually doing better! Weird\u0026hellip;\nBusiness Case We are given the following problem:\nA firm would like to understand which customers to show an ad, in order to increase revenue. The firm ran a A/B test showing a random sample of customers an ad. First, try to understand if there is heterogeneity in treatment. Then, decide which customers to show the ad, given that ads are costly (1$ each). Further suppose that you cannot discriminate on gender. How do the results change?\nWe start by drawing a sample from the data generating process.\nfrom src.utils import * from src.dgp import dgp_ad dgp = dgp_ad() df = dgp.generate_data() df.head() male black age educ ad revenue 0 0 0 55.0 1 False -0.327221 1 1 1 47.0 2 False 0.659393 2 0 1 31.0 2 True 2.805178 3 0 1 51.0 2 False -0.508548 4 0 0 48.0 0 True 0.762280 We have information on the number of pages visited in the previous month, whether the user is located in the US, whether it connects by mobile and the revenue pre-intervention.\nWe are going to use the econml library to estimate the treatment effects. First, we use the DRLearner library to estimate heterogeneous treatment effects using a double robust estimator. We can specify both the model_propensity for $e(x)$ and the model_regression for $\\mu(x)$.\nfrom econml.dr import DRLearner model = DRLearner(random_state=1).fit(Y=df[dgp.Y], T=df[dgp.T], X=df[dgp.X]); We can plot a visual representation of the treatment effect heterogeneity using the SingleTreePolicyInterpreter function, which infers a tree representation of the treatment effects learned from another model.\nfrom econml.cate_interpreter import SingleTreeCateInterpreter SingleTreeCateInterpreter(max_depth=2, random_state=1).interpret(model, X=df[dgp.X]).plot(feature_names=dgp.X) It seems that the most relevant dimension of treatment heterogeneity is education.\nWe can now use policy learning to estimate a treatment policy. We use the DRPolicyTree from the econml package.\nfrom econml.policy import DRPolicyTree policy = DRPolicyTree(random_state=1, max_depth=2).fit(Y=df[dgp.Y], T=df[dgp.T], X=df[dgp.X]) policy.plot(feature_names=dgp.X) We will now assume that the treatment is costly.\ncost = 1 policy = DRPolicyTree(random_state=1, max_depth=2).fit(Y=df[dgp.Y]-cost*df[dgp.T], T=df[dgp.T], X=df[dgp.X]) policy.plot(feature_names=dgp.X) As we can see, the model decides to use race to discriminate treatment. However, let\u0026rsquo;s now suppose we cannot discriminate on race and gender.\nX_short = ['age', 'educ'] policy = DRPolicyTree(random_state=1, max_depth=2).fit(Y=df[dgp.Y]-cost*df[dgp.T], T=df[dgp.T], X=df[X_short]) policy.plot(feature_names=X_short) In this case, the model uses education instead of race in order to assign treatment.\nReferences Who Should Be Treated? Empirical Welfare Maximization Methods for Treatment Choice (2018) by Kitagawa and Tetenov Efficient Policy Learning (2017) by Athey and Wager Policy Learning video lecture by Stefan Wager (Stanford) Customer Segmentation case study by EconML ","date":1649980800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1649980800,"objectID":"7ba4e24eb8ae8d8c72aa72dc308152d7","permalink":"https://matteocourthoud.github.io/post/policy_learning/","publishdate":"2022-04-15T00:00:00Z","relpermalink":"/post/policy_learning/","section":"post","summary":"In this tutorial, we are going to see how to design the most welfare-improving policy in presence of treatment effect heterogeneity and treatment costs or budget constraints.\nRequisites\nFor this tutorial, I assume you are familiar with the following concepts:","tags":null,"title":"Policy Learning","type":"post"},{"authors":null,"categories":null,"content":"In this tutorial, we are going to see how to estimate causal effects when the treatment is not unconditionally randomly assigned, but we need to condition on observable features in order to assume treatment exogeneity. This might happen either when an experiment is stratified or in observational studies.\nRequisites\nFor this tutorial, I assume you are familiar with the following concepts:\nRubin\u0026rsquo;s potential outcome framework Ordinary least squares regression Academic Application\nAs an academic application, we are going to replicate Evaluating the Econometric Evaluations of Training Programs with Experimental Data (1986) by Lalonde and the followup paper Causal Effects in Non-Experimental Studies: Reevaluating the Evaluation of Training Programs (1999) by Dahejia and Wahba. These papers study a randomized intervention providing work experienced to improve labor market outcomes.\nBusiness Case\nTBD\nSetting We assume that for a set of i.i.d. subjects $i = 1, \u0026hellip;, n$ we observed a tuple $(X_i, D_i, Y_i)$ comprised of\na feature vector $X_i \\in \\mathbb R^n$ a treatment assignment $D_i \\in \\lbrace 0, 1 \\rbrace$ a response $Y_i \\in \\mathbb R$ Assumption 1 : unconfoundedness (or ignorability, or selection on observables, or conditional independence)\n$$ \\big \\lbrace Y_i^{(1)} , Y_i^{(0)} \\big \\rbrace \\ \\perp \\ D_i \\ | \\ X_i $$\ni.e. conditional on observable characteristics $X$, the treatment assignment $T$ is as good as random. What this assumption rules out is selection on unobservables. Moreover, it\u0026rsquo;s untestable.\nAssumption 2: overlap (or common support)\n$$ \\exists \\eta \u0026gt; 0 \\ : \\ \\eta \\leq \\mathbb E \\left[ D_i = 1 \\ \\big | \\ X_i = x \\right] \\leq 1-\\eta $$\ni.e. no observation is deterministically assigned to the treatment or control group. We need this assumption for counterfactual statements to make sense. If some observations had zero probability of (not) being treated, it would make no sense to try to estimate their counterfactual outcome in case they would have (not) being treated. Also this assumption is untestable.\nAssumption 3: stable unit treatment value (SUTVA)\n$$ Y_i^{(D_i)} \\perp D_j \\quad \\forall j \\neq i $$\ni.e. the potential outcome of one individual is independent from the treatment status of any other individual. Common violations of this assumption include\ngeneral equilibrium effects spillover effects This assumption is untestable.\nPropensity Scores Exogenous Treatment The fundamental problem of causal inference is that we do not observe counterfactual outcomes, i.e. we do not observe what would have happened to treated units if they had not received the treatment and viceversa.\nIf treatment is exogenous, we know that the difference in means identifies the average treatment effect $\\mathbb E[\\tau]$.\n$$ \\mathbb E[\\tau] = \\mathbb E \\big[ Y_i \\ \\big| \\ D_i = 1 \\big] - \\mathbb E \\big[ Y_i \\ \\big| \\ D_i = 0 \\big] = \\mathbb E \\big[ Y_{i}^{(1)} - Y_{i}^{(0)} \\big] $$\nTherefore, we can build an unbiased estimator of the average treatment effect as the empirical counterpart of the expression above\n$$ \\hat \\tau(Y, D) = \\frac{1}{n} \\sum_{i=1}^{n} \\big( D_i Y_i - (1-D_i) Y_i \\big) $$\nIn case treatment is not randomly assigned, we use the Thompson Horowitz (1952) estimator\n$$ \\hat \\tau(Y, D) = \\frac{1}{n} \\sum_{i=1}^{n} \\left( \\frac{D_i Y_i}{\\pi_{i}} - \\frac{(1-D_i) Y_i}{1 - \\pi_{i}} \\right) $$\nwhere $\\pi_{i} = \\Pr(D_i=1)$ is the probability of being treated, also known as propensity score. Sometimes the propensity score is known, for example when treatment is stratified. However, in general, it is not.\nConditionally Exogenous Treatment In many cases and especially in observational studies, treatment $D$ is not unconditionally exogenous, but it\u0026rsquo;s exogenous only after we condition on some characteristic $X$. If these characteristics are observables, we have the unconfoundedness assumption.\nUnder unconfoundedness, we can still identify the average treatment effect, as a conditional difference in means:\n$$ \\mathbb E[\\tau] = \\mathbb E \\big[ Y_{i}^{(1)} - Y_{i}^{(0)} \\ \\big| \\ X_i \\big] $$\nThe main problem is that we need to condition of the observables that actually make the unconfoundedness assumption hold. This might be tricky in two cases:\nwhen we have many observables when we do not know the functional form of the observables that we need to condition on The main contribution of Rosenbaum and Rubin (1983) is to show that if unconfoundedness holds, then\n$$ \\big \\lbrace Y_i^{(1)} , Y_i^{(0)} \\big \\rbrace \\ \\perp \\ D_i \\ | \\ \\pi(X_i) $$\ni.e. you only need to condition on $\\pi(X)$ in order to recover the average treatment effect.\n$$ \\mathbb E[\\tau] = \\mathbb E \\big[ Y_{i}^{(1)} - Y_{i}^{(0)} \\ \\big| \\ \\pi(X_i) \\big] $$\nThis implies the following inverse propensity-weighted estimator:\n$$ \\hat \\tau^{IPW} = \\frac{1}{n} \\sum_{i=1}^{n} \\left( \\frac{D_i Y_i}{\\hat \\pi(X_i)} - \\frac{(1-D_i) Y_i}{1 - \\hat \\pi(X_i)} \\right) $$\nwhich, under unconfoundedness is an unbiased estimator of the average treatment effect, $\\mathbb E \\left[\\hat \\tau^{IPW} \\right] = \\tau$.\nThis is a very practically relevant result since it tells us that we need to condition on a single variable instead of a potentially infinite dimensional array. The only thing we need to do is to estimate $\\pi(X_i)$.\nComments Actual vs Estimated Scores\nHirano and Ridder (2002) show that even when you know the true propensity score $\\pi(X)$, it\u0026rsquo;s better to plug in the estimated propensity score $\\hat \\pi(X)$. Why? The idea is that the deviation between the actual and the estimated propensity score is providing some additional information. Therefore, it is best to use the actual fraction of treated rather than the theoretical one.\nPropensity Scores and Regression\nWhat is the difference between running a regression with controls vs doing propensity score matching?\nAranow and Miller (2015) investigate this comparison in depth. First of all, whenever you are inserting control variables in a regression, you are implicitly thinking about propensity scores. Both approaches are implicitly estimating counterfactual outcomes. Usually OLS extrapolates further away from the actual support than propensity score does.\nIn the tweet (and its comments) below you can find further discussion and comments.\nThank you for tolerating such a vague poll question. Let me explain why I think this is a useful thing to bring front and certain, and highlight what I think is a flaw in how much of econometrics is taught, currently. 1/n pic.twitter.com/Wm2jFereYO\n\u0026mdash; Paul Goldsmith-Pinkham (@paulgp) December 14, 2021 Academic Application As an academic application, we are going to replicate Causal Effects in Non-Experimental Studies: Reevaluating the Evaluation of Training Programs (1999) by Dahejia and Wahba.\nThis study builds on a previous study: Evaluating the Econometric Evaluations of Training Programs with Experimental Data (1986) by Lalonde. In this study, the author compares observational and experimental methods. In particular, he studies an experimental intervention called the NSW (National Supported Work demonstration). The NSW is a temporary training program to give work experience to unemployed people.\nThe exogenous variation allows us to estimate the treatment effect as a difference in means. The author then asks: what if we didn\u0026rsquo;t have access to an experiment? In particular, what if we did not have information on the control group? He takes a sample of untreated people from the PSID panel and use them as a control group.\nExperimental Data Let\u0026rsquo;s start by loading the NSW data.\n%matplotlib inline %config InlineBackend.figure_format = 'retina' from src.utils import * df_nsw = pd.read_csv('data/l86_nsw.csv') df_nsw.head() treat age educ black hisp marr nodegree re74 re75 re78 0 1 37 11 1 0 1 1 0.0 0.0 9930.045898 1 1 22 9 0 1 0 1 0.0 0.0 3595.894043 2 1 30 12 1 0 0 0 0.0 0.0 24909.449219 3 1 27 11 1 0 0 1 0.0 0.0 7506.145996 4 1 33 8 1 0 0 1 0.0 0.0 289.789886 The treatment variable is treat and the outcome of interest is re78, the income in 1978. We also have access to a bunch of covariates.\ny = 're78' T = 'treat' X = df_nsw.columns[2:9] Was there selection on observables? Let\u0026rsquo;s summarize the data, according to treatment status.\ndf_nsw.groupby('treat').agg(['mean', 'std']).T.unstack(1) treat 0 1 mean std mean std age 25.053846 7.057745 25.816216 7.155019 educ 10.088462 1.614325 10.345946 2.010650 black 0.826923 0.379043 0.843243 0.364558 hisp 0.107692 0.310589 0.059459 0.237124 marr 0.153846 0.361497 0.189189 0.392722 nodegree 0.834615 0.372244 0.708108 0.455867 re74 2107.026651 5687.905639 2095.573693 4886.620354 re75 1266.909015 3102.982088 1532.055313 3219.250879 re78 4554.801120 5483.836001 6349.143502 7867.402183 It seems that covariates are balanced across treatment arms. Nothing seems to point towards selection on observables. Therefore, we can compute the average treatment effect as a simple difference in means\ndf_nsw.loc[df_nsw[T]==1, y].mean() - df_nsw.loc[df_nsw[T]==0, y].mean() 1794.3423818501024 Or equivalently in a regression\nest = smf.ols('re78 ~ treat', df_nsw).fit() est.summary().tables[1] coef std err t P\u003e|t| [0.025 0.975] Intercept 4554.8011 408.046 11.162 0.000 3752.855 5356.747 treat 1794.3424 632.853 2.835 0.005 550.574 3038.110 It looks like the effect is positive and significant.\nObservational Data Let\u0026rsquo;s now load a different dataset in which we have replaced the true control units with observations from the PSID sample.\ndf_psid = pd.read_csv('data/l86_psid.csv') Is this dataset balanced?\ndf_psid.groupby('treat').agg(['mean', 'std']).T.unstack(1) treat 0 1 mean std mean std age 36.094862 12.081030 25.816216 7.155019 educ 10.766798 3.176827 10.345946 2.010650 black 0.391304 0.489010 0.843243 0.364558 hisp 0.067194 0.250853 0.059459 0.237124 marr 0.735178 0.442113 0.189189 0.392722 nodegree 0.486166 0.500799 0.708108 0.455867 re74 11027.303390 10814.670751 2095.573693 4886.620354 re75 7569.222058 9041.944403 1532.055313 3219.250879 re78 9995.949977 11184.450050 6349.143502 7867.402183 People in the PSID control group are older, more educated, white, married and generally have considerably higher pre-intervention earnings (re74). This makes sense since the people selected for the NSW program are people that are younger, less experienced and unemployed.\nLalonde (1986) argues in favor of experimental approaches by showing that using a non-experimental setting, one would not be able to estimate the true treatment effect. Actually, one could even get statistically significant results of the opposite sign.\nLet\u0026rsquo;s repeat the regression exercise for the PSID data.\nsmf.ols('re78 ~ treat', df_psid).fit().summary().tables[1] coef std err t P\u003e|t| [0.025 0.975] Intercept 9995.9500 623.715 16.026 0.000 8770.089 1.12e+04 treat -3646.8065 959.704 -3.800 0.000 -5533.027 -1760.586 The estimated coefficient is negative and significant. The conclusion from Lalonde (1986) is\n\u0026ldquo;This comparison shows that many of the econometric procedures d not replicate the experimentally determined results\u0026rdquo;.\nDahejia and Wahba (1999) argue that with appropriate matching one would still be able to get a relatively precise estimate of the treatment effect. In particular, the argue in favor of controlling for pre-intervention income, re74 and re75.\nLet\u0026rsquo;s just linearly insert the control variables in the regression.\nsmf.ols('re78 ~ treat + ' + ' + '.join(X), df_psid).fit().summary().tables[1] coef std err t P\u003e|t| [0.025 0.975] Intercept -1089.9263 2913.224 -0.374 0.708 -6815.894 4636.042 treat 2642.1456 1039.655 2.541 0.011 598.694 4685.597 educ 521.5869 208.751 2.499 0.013 111.284 931.890 black -1026.6762 1006.433 -1.020 0.308 -3004.830 951.478 hisp -903.1023 1726.419 -0.523 0.601 -4296.394 2490.189 marr 1026.6143 943.788 1.088 0.277 -828.410 2881.639 nodegree -1469.3712 1166.114 -1.260 0.208 -3761.379 822.637 re74 0.1928 0.058 3.329 0.001 0.079 0.307 re75 0.4976 0.070 7.068 0.000 0.359 0.636 The treatment effect is now positive, borderline significant, and close to the experimental estimate of $1794$$. Moreover, it\u0026rsquo;s hard to tell whether this is the correct functional form for the control variables.\nInverse propensity score weighting Another option is to use inverse propensity score weighting. First, we need to estimate the treatment probability. Let\u0026rsquo;s start with a very simple standard model to predict binary outcomes.\nfrom sklearn.linear_model import LogisticRegressionCV pi = LogisticRegressionCV().fit(y=df_psid[T], X=df_psid[X]) df_psid['pscore'] = pi.predict_proba(df_psid[X])[:,1] How does the distribution of the propensity scores look like?\nsns.histplot(data=df_psid, x='pscore', hue=T, bins=20)\\ .set(title='Distribution of propensity scores, PSID data', xlabel=''); It seems that indeed we predict higher propensity scores for treated people, and viceversa, indicating a strong selection on observable. However, there is also a considerable amount of overlap.\nWe can now estimate the treatment effect by weighting by the inverse of the propensity score. First, let\u0026rsquo;s exclude observations with a very extreme predicted score.\ndf_psid1 = df_psid[(df_psid['pscore']\u0026lt;0.9) \u0026amp; (df_psid['pscore']\u0026gt;0.1)] Now we can need to construct the weights.\ndf_psid1['weight'] = df_psid1['treat'] / df_psid1['pscore'] + (1-df_psid1['treat']) / (1-df_psid1['pscore']) Finally, we run a weighted regression of income on the treatment program.\nest = smf.wls('re78 ~ treat', df_psid1, weights=df_psid1['weight']).fit() est.summary().tables[1] coef std err t P\u003e|t| [0.025 0.975] Intercept 4038.1507 512.268 7.883 0.000 3030.227 5046.074 treat 2166.8750 730.660 2.966 0.003 729.250 3604.500 The effect is positive, statistically significant and very close to the experimental estimate of $1794$$.\nWhat would have been the propensity scores if we had used the NSW experimental sample? If it\u0026rsquo;s a well done experiment with a sufficiently large sample, we would expect the propensity scores to concentrate around the percentage of people treated, $0.41$ in our data.\npi = LogisticRegressionCV().fit(y=df_nsw[T], X=df_nsw[X]) df_nsw['pscore'] = pi.predict_proba(df_nsw[X])[:,1] sns.histplot(data=df_nsw, x='pscore', hue=T, bins=20)\\ .set(title='Distribution of propensity scores, NSW data', xlabel=''); Indeed, now the distribution of the p-scores is concentrated around the treatment frequency in the data. Remarkably, the standard deviation is extremely tight.\nReferences The central role of the propensity score in observational studies for causal effects (1983) by Rosenbaum and Rubin Propensity Scores video lecture by Paul Goldsmith-Pinkham (Yale) Propensity Scores video lecture by Stefan Wager (Stanford) ","date":1649980800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1649980800,"objectID":"f430af05f2329e440db7e1f50ed7187c","permalink":"https://matteocourthoud.github.io/post/propensity_score/","publishdate":"2022-04-15T00:00:00Z","relpermalink":"/post/propensity_score/","section":"post","summary":"In this tutorial, we are going to see how to estimate causal effects when the treatment is not unconditionally randomly assigned, but we need to condition on observable features in order to assume treatment exogeneity.","tags":null,"title":"Propensity Score Matching","type":"post"},{"authors":null,"categories":null,"content":"In this tutorial, we are going to see how to estimate causal effects when treatment assignment is not random, but determined by a forcing variable such as a test or a requirement. In this case, we can get a local estimate of the treatment effect by comparing units just above and just below the threshold by assuming that there is no sorting/gaming around it.\nRequisites\nFor this tutorial, I assume you are familiar with the following concepts:\nRubin\u0026rsquo;s potential outcome framework Ordinary least squares regression Non-parametric regression Instrumental variables Academic Application\nAs an academic application, we are going to replicate Do voters affect or elect policies? Evidence from the US House (2004) by Lee, Moretti, Butler. The authors study whether electoral strength has an effect on policies. To identify the effect, they the advantage that is given by incumbency status, and the quasi-exogeneity given by close elections.\nBusiness Case\nTBD\nSetting We assume that for a set of i.i.d. subjects $i = 1, \u0026hellip;, n$ we observed a tuple $(X_i, D_i, Y_i, Z_i)$ comprised of\na feature vector $X_i \\in \\mathbb R^n$ a treatment assignment $D_i \\in \\lbrace 0, 1 \\rbrace$ a response $Y_i \\in \\mathbb R$ outcome of interest that depends on both $X_i$ and $D_i$ a forcing variable $Z_i \\in \\mathbb R$ variable that determines treatment assignment $D_i$ We normalize the forcing variable $Z_i$ such that $Z_i=0$ corresponds to the cutoff for treatment assignment. We will distinguish two cases for the effect of $Z_i$ on $D_i$:\nSharp RD: $D_i = (Z_i \\geq 0)$\ntreatment is exactly determined by the cutoff Fuzzy RD: $\\lim_{z \\to 0_{-}} \\mathbb E[D_i | Z_i=z] \\neq \\lim_{z \\to 0_{+}} \\mathbb E[D_i | Z_i=z]$\ntreatment probability changes at the cutoff Assumption 1 : CE smoothness\nAssumption 2: no sorting\nRegression Discontinuity The key behind regression discontinuity is what is called a forcing variable that determines treatment assignment. Common examples include test scores for university enrollment (you need a certain test score to get access university) or income for some policy eligibility (you need to be below a certain income threshold to be eligible for a subsidy).\nClearly, in this setting, treatment is not exogenous. However, the idea behind regression discontinuity is that units sufficiently close to the discontinuity $Z_i=0$ are sufficiently similar so that we can attribute differences in the outcome $Y_i$ to the treatment $T_i$.\nWhat does sufficiently exactly mean?\nIn practice, we are assuming a certain degree of smoothness of the conditional expectation function $\\mathbb E[D_i | Z_i=z]$. If this assumption holds, we can estimate the local average treatment effect\n$$ \\tau^{LATE} = \\lim_{z \\to 0_{+}} \\mathbb E[Y_i | Z_i=z] - \\lim_{z \\to 0_{-}} \\mathbb E[Y_i | Z_i=z] = \\mathbb E \\big[ Y_{i}^{(1)} - Y_{i}^{(0)} | Z_i=0 \\big] $$\nNote that this is the average treatment effect for a very narrow set of individuals: those that are extremely close to the cutoff.\nData Challenge Regression discontinuity design is a particularly data hungry procedure. In fact, we need to\nhave a very good flexible approximation of the conditional expectation of the outcome $Y_i$ at the cutoff $Z_i=0$ while also accounting for the effect of the forcing variable $Z$ on the outcome $Y$ If we knew the functional form of $\\mathbb E[Y_i | Z_i]$, it would be easy.\nMcCrary Test Regression Kink Design Academic Application As an academic application, we are going to replicate Do voters affect or elect policies? Evidence from the US House (2004) by Lee, Moretti, Butler. The authors study whether electoral strength has an effect on policies. To identify the effect, they the advantage that is given by incumbency status, and the quasi-exogeneity given by close elections.\n%matplotlib inline %config InlineBackend.figure_format = 'retina' from src.utils import * df = sm.datasets.get_rdataset('close_elections_lmb', package='causaldata').data df = pd.read_csv('data/l08.csv') df.head() state district id score year demvoteshare democrat lagdemocrat lagdemvoteshare 0 1 1 3 64.339996 1948 0.553026 1 0 0.469256 1 1 1 4 60.279999 1948 0.553026 1 0 0.469256 2 1 1 5 57.060001 1950 0.582441 1 1 0.553026 3 1 1 6 73.830002 1950 0.582441 1 1 0.553026 4 1 1 7 42.959999 1954 0.569626 1 1 0.539680 The first thing we would like to inspect, is the distribution of democratic vote shares demvoteshare, against their lagged values lagdemvoteshare.\nsns.scatterplot(df['lagdemvoteshare'], df['demvoteshare'])\\ .set(title='Vote share and incumbency status', xlabel='Dem Vote Share (t-1)', ylabel='Dem Vote Share (t)'); The plot is extremely messy. However we can already see some discontinuity at the threshold: it seems that incumbents do not get vote shares below 0.35.\nTo have a more transparent representation of the data, we can use a binscatterplot. Binscatterplots are very similar to histograms with a main difference: instead of having a fixed width, they have a fixed number of observations per bin.\nfrom scipy.stats import binned_statistic def binscatter(x, y, bins=30, area=True, **kwargs): y_bins, x_edges, _ = binned_statistic(x, y, statistic='mean', bins=bins) x_bins = (x_edges[:-1] + x_edges[1:]) / 2 p = sns.scatterplot(x_bins, y_bins, **kwargs) if area: y_std, _, _ = binned_statistic(x, y, statistic='std', bins=bins) plt.fill_between(x_bins, y_bins-y_std, y_bins+y_std, alpha=0.2, color='C0') return p We can now plot the average vote share by previous vote share. The shades represent one standard deviation, at the bin level.\nbinscatter(df['lagdemvoteshare'], df['demvoteshare'], bins=100)\\ .set(title='Vote share and incumbency status', xlabel='Dem Vote Share (t-1)', ylabel='Dem Vote Share (t)'); plt.axvline(x=0.5, ls=\u0026quot;:\u0026quot;, color='C2'); plt.title('Vote share and incumbency status'); Now it seems quite clear that there exist a discontinuity at $0.5$. We can get a first estimate of the local average treatment effect by assuming a linear model and running a linear regression.\nsmf.ols('demvoteshare ~ lagdemvoteshare + (lagdemvoteshare\u0026gt;0.5)', df).fit().summary().tables[1] coef std err t P\u003e|t| [0.025 0.975] Intercept 0.2173 0.005 46.829 0.000 0.208 0.226 lagdemvoteshare \u003e 0.5[T.True] 0.0956 0.003 33.131 0.000 0.090 0.101 lagdemvoteshare 0.4865 0.011 42.539 0.000 0.464 0.509 The effect is positive and statistically significant. We can also allow the slope of the line to differ on the two sides of the discontinuity.\ndf = df.sort_values('lagdemvoteshare') model = smf.ols('demvoteshare ~ lagdemvoteshare * (lagdemvoteshare\u0026gt;0.5)', df).fit() model.summary().tables[1] coef std err t P\u003e|t| [0.025 0.975] Intercept 0.2256 0.007 34.588 0.000 0.213 0.238 lagdemvoteshare \u003e 0.5[T.True] 0.0747 0.012 6.334 0.000 0.052 0.098 lagdemvoteshare 0.4653 0.016 28.547 0.000 0.433 0.497 lagdemvoteshare:lagdemvoteshare \u003e 0.5[T.True] 0.0418 0.023 1.827 0.068 -0.003 0.087 Let\u0026rsquo;s plot the predicted vote share over the previous graph.\nbinscatter(df['lagdemvoteshare'], df['demvoteshare'], bins=100, alpha=0.5)\\ .set(title='Vote share and incumbency status', xlabel='Dem Vote Share (t-1)', ylabel='Dem Vote Share (t)'); plt.plot(df['lagdemvoteshare'], model.fittedvalues, color='C1') plt.axvline(x=0.5, ls=\u0026quot;:\u0026quot;, color='C2'); Now that we have established a discontinuity at the cutoff, we need to check the RD assumptions.\nFirst, is there sorting across the cutoff? In this case, are democratic politicians more or less likely to lose close elections than republicans? We can plot the distribution of (lagged) vote shares and inspect its shape at the cutoff.\nsns.histplot(df['lagdemvoteshare'], bins=100)\\ .set(title='Distribution of lagged dem vote share', xlabel='') plt.axvline(x=0.5, ls=\u0026quot;:\u0026quot;, color='C2'); If looks pretty smooth. If anything, there is a loss of density at the cutoff, plausibly indicating stronger competition when the competition is close. However, if does not seem particularly asymmetric.\nA placebo test that we can run is to check if the forcing variable has an effect on variables on which we do not expect to have an effect. In this setting, the most intuitive placebo outcome is previous elections: we do not expect that being on either side of the cutoff today is related to any past outcome.\nIn our case, we can simply swap the two variables to run the test.\nbinscatter(df['demvoteshare'], df['lagdemvoteshare'], bins=100)\\ .set(title='Vote share and incumbency status', xlabel='Dem Vote Share (t)', ylabel='Dem Vote Share (t-1)'); plt.axvline(x=0.5, ls=\u0026quot;:\u0026quot;, color='C2'); The distribution of vote shares in the past period does not seem to be discontinuous in the incumbency status today, as expected.\nReferences Regression discontinuity video lecture by Paul Goldsmith-Pinkham (Yale) ","date":1649980800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1649980800,"objectID":"f64bf2b2655c6c47ee0de6f4b449652a","permalink":"https://matteocourthoud.github.io/post/regression_discontinuity/","publishdate":"2022-04-15T00:00:00Z","relpermalink":"/post/regression_discontinuity/","section":"post","summary":"In this tutorial, we are going to see how to estimate causal effects when treatment assignment is not random, but determined by a forcing variable such as a test or a requirement.","tags":null,"title":"Regression Discontinuity","type":"post"},{"authors":null,"categories":null,"content":"In this page, I collect lectures and materials for graduate courses in Economics and Social Sciences.\nI will only link to lectures and materials that are freely available. I will not link to courses hosted on MOOC websites or that require university credentials to access.\nA special mention goes to the following:\nThe NBER that during each Summer Institute has a lecture series The Chamberlain Seminar that since 2021 started hosting and recording tutorial sessions Video Lectures Course Title Author University Year Material Machine Learning and Causal Inference Susan Athey et al. Stanford 2022 Yes Causal Inference with Panel Data Yiqing Xu Washington U 2021 No Industrial Organization Chris Conlon NYU 2021 Yes Panel Data Econometrics Chris Conlon NYU 2021 Yes Machine Learning with Graphs Yure Leskovec Stanford 2021 Yes Applied Methods Paul Goldsmith-Pinkham Yale 2021 Yes DiD Reading Group misc misc 2021 Yes Computational Economics Kenneth Judd Stanford 2020 Yes Reinforcement Learning Emma Brunskill Stanford 2020 Yes Causal Inference Brady Neal Quebec AI Institute 2020 Yes Natural Language Understanding Christopher Potts Stanford 2019 No Material Course Title Author University Year Computational Economics Florial Oswald Bocconi 2021 Data Science for Economists Grant McDermott Oregon 2020 Industrial Organization John Asker UCLA 2020 Topics in Empirical Industrial Organization Kohei Kawaguchi Hong Kong 2020 Industrial Organization Victor Aguirregabiria Toronto 2019 Econometrics Tyler Ransom Oklahoma 2020 Machine Learning in Econometrics Martin Spindler Munich 2020 Structural Econometrics Robert Miller Carnegie Mellon 2019 ","date":1644451200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1644451200,"objectID":"736f48d7831cb89e669f79a89666f064","permalink":"https://matteocourthoud.github.io/post/courses/","publishdate":"2022-02-10T00:00:00Z","relpermalink":"/post/courses/","section":"post","summary":"In this page, I collect lectures and materials for graduate courses in Economics and Social Sciences.\nI will only link to lectures and materials that are freely available. I will not link to courses hosted on MOOC websites or that require university credentials to access.","tags":null,"title":"Free Courses in Economics","type":"post"},{"authors":null,"categories":null,"content":"In this page, I collect information about conferences in Economics and Finance.\nIf you know about public conferences or meetings that are missing from this list, please either contact me or edit the table on Github!\nNote that conferences are ordered by deadline and not by conference date.\nJanuary Conference Organizer Field Deadline Target Date Annual Congress of the Swiss Society of Economics and Statistics SSES All January 31 All 23/06/21 February Conference Organizer Field Deadline Target Date Bergen Competition Policy Conference NHH Comp policy February 03 All 23/04/20 CEPR/JIE Conference on Applied IO CEPR IO February 10 Junior 08/06/21 Economics and Computation ACM SIGecom Theory February 10 All 11/07/21 ES North American Summer Meeting Econometric Society All February 12 Senior 16/06/21 EEA Summer Meeting European Economic Association All February 15 All 23/08/21 ES European Summer Meeting Econometric Society All February 15 Senior 23/08/21 Annual Meeting of the Society for Economic Dynamics SED Macro February 15 All 01/07/21 GAMES 2020 Game Theory Society Game Theory February 20 All 19/07/21 Annual GEP/CEPR Postgraduate Conference University of Nottingham Policy February 26 Junior 06/05/21 Doctoral Workshop on the Economics of Digitization TSE Digitalization February 28 Junior 12/05/22 March Conference Organizer Field Deadline Target Date Annual IIOC Northeastern University IO March 01 All 30/04/21 Young Economists\u0026rsquo; Meeting University fo Munich All March 08 Junior 01/10/20 GSE Summer Forum Barcelona GSE All March 14 All 07/06/21 EARIE NHH IO March 15 All 27/08/21 Economics of Media Workshop Queen’s University IO March 15 Junior 12/06/20 QMUL Economics and Finance Workshop Queen Mary University All March 20 Junior 26/05/20 Virtual Finance and Economics Conference Yale University Finecon March 25 All 17/04/20 DC IO Day 2020 Georgetown University IO March 31 All 15/05/20 April Conference Organizer Field Deadline Target Date SITE Stanford University Theory April 01 All 12/06/21 NBER Summer Meeting Workshop NBER All April 05 All 12/07/21 AEA Annual Meeting AEA All April 15 All 07/01/22 Swiss IO Day University of Bern IO April 16 All 11/06/21 Econometric Society - North American Winter Meetings ES All April 21 All 06/01/22 CRESSE CRESSE IO April 30 All 26/07/21 May Conference Organizer Field Deadline Target Date European Research Workshop in International Trade CEPR Trade May 02 All 22/10/21 Warsaw International Economic Meeting Warsaw University All May 03 All 01/07/20 Warwick Economics PhD Conference University of Warwick All May 09 PhD 24/06/21 Annual Conference on Antitrust Economics and Competition Policy Northwestern University Comp policy May 17 All 17/09/21 NBER Economics of AI Conference NBER AI May 31 All 23/09/21 June Conference Organizer Field Deadline Target Date Equity and Access in Algorithms, Mechanisms, and Optimization (EAAMO) ACM AI June 14 All 05/10/21 FTC Micro Conference FTC IO June 23 Senior 04/11/21 July Conference Organizer Field Deadline Target Date Empirics and Methods in Economics Conference Northwestern \u0026amp; Chicago Empirical July 30 Junior 22/10/20 August Conference Organizer Field Deadline Target Date AI Policy Conference ETH Zurich AI August 1 All 14/09/21 Finance, Organizations and Markets (FOM) Conference Dartmouth College Finance, IO August 14 All 28/10/21 September Conference Organizer Field Deadline Target Date Causal Inference \u0026amp; Machine Learning: Why now? NeurIPS Econometrics September 18 All 13/12/21 ES European Winter Meeting Econometricc Society All September 19 Senior 13/12/21 Causal Data Science Conference causalscience All September 30 All 15/11/21 October Conference Organizer Field Deadline Target Date Digital Economics Conference TSE Digital October 3 All 13/01/22 Asia-Pacific IO Conference Asia-Pacific IO Society IO October 22 All 13/12/21 Young Swiss Economists Meeting SSES All October 25 Junior 11/02/21 November Conference Organizer Field Deadline Target Date Next Generation of Antitrust, Data Privacy and Data Protection Scholars Conference NYU IO November 1 All 28/01/22 Spring Meeting of Young Economists University of Bologna All November 12 Junior 17/06/21 NBER IO Winter Meeting NBER IO November 20 Senior 12/02/21 MaCCI Annual Conference University of Mannheim IO November 30 All 12/03/21 Postal Economics Conference TSE Digital November 30 All 07/04/22 December Conference Organizer Field Deadline Target Date Early-Career Behavioral Economics Conference Princeton University Behavioral December 15 junior 03/06/21 Undefined Conference Organizer Field Deadline Target Date Annual Conference on Innovation Economics Northwestern University Innovation forthcoming All 27/08/20 Conference on Mechanism and Institution Design Universität Klagenfurt Market Design closed All 11/06/20 D-TEA Conference HEC Paris Theory closed All 16/06/20 Economics Graduate Student Conference Washington University All closed Junior 07/11/20 NBER Summer Institute NBER All invitation Senior 06/07/20 CCP Annual Conference Centre for Competition Policy Comp policy closed All 24/06/21 RES Annual Conference Royal Economics Society All closed Senior 12/04/21 JEI Student Conference Harvard University All canceled Junior 20/06/20 Annual Conference on Network Science and Economics Becker Friedman Institute Networks canceled All 27/03/20 Annual SAET Conference Society for the Advancement of Economic Theory Theory closed All 13/06/21 ","date":1641772800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1641772800,"objectID":"023c54056b60665075513e8450a12257","permalink":"https://matteocourthoud.github.io/post/conferences/","publishdate":"2022-01-10T00:00:00Z","relpermalink":"/post/conferences/","section":"post","summary":"In this page, I collect information about conferences in Economics and Finance.\nIf you know about public conferences or meetings that are missing from this list, please either contact me or edit the table on Github!","tags":null,"title":"Economics Conferences","type":"post"},{"authors":null,"categories":null,"content":"Welcome to my tutorial on how to set up a remote machine and deploy your code there. I will first analyze SSH and then look at two specific applications: coding in Python and Julia.\nSetup In order to start working on a remote server you need\nthe server local shell SSH installed SSH, or Secure Shell, is a protocol designed to transfer data between a client and a server (two computers basically) over an untrusted network.\nThe way SSH works is it encrypts the connection using a pair of keys and the server, which is the computer you would connect to, is usually waiting for an SSH connection on Port 22.\nSSH is normally installed by default. To check if you have SSH installed, open the terminal and write ssh. You should receive a message that looks like this\nusage: ssh [-1246AaCfGgKkMNnqsTtVvXxYy] [-b bind_address] [-c cipher_spec] [-D [bind_address:]port] [-E log_file] [-e escape_char] [-F configfile] [-I pkcs11] [-i identity_file] [-J [user@]host[:port]] [-L address] [-l login_name] [-m mac_spec] [-O ctl_cmd] [-o option] [-p port] [-Q query_option] [-R address] [-S ctl_path] [-W host:port] [-w local_tun[:remote_tun]] [user@]hostname [command] If SSH is not installed, you can install it using the following commands.\nsudo apt-get install openssh-server sudo systemctl enable ssh sudo systemctl start ssh Now that you have installed SSH, we are ready to setup a remote connection.\nFrom the computer you want to access remotey, generate the public key.\nssh-keygen -t rsa You will be asked for a location. If you decide to enter one manually then that will be the pair’s location, if you leave the default one it will be inside the .ssh hidden folder in your home directory.\nNow you will be prompted for a password. If you enter one you will be asked for it every time you use the key, this works for added security. If you don’t want a password just press enter and continue without one.\nTwo files were created. One file ends with the ‘.pub’ extension and the other one doesn’t. The file that ends with ‘.pub’ is your public key. This key needs to be in the computer you want to connect to (the server) inside a file called authorized_keys . You can accomplish this with the following command:\nssh-copy-id username@ip For example in my case to send the key to my computer it would be:\nssh-copy-id sergiop@132.132.132.132 If you have MacOS there’s a chance you don’t have ssh-copy-id installed, in that case you can install it using\nbrew install ssh-copy-id If you haven’t installed brew, you can install it by following this guide.\nConnect To permanently add the SSH key, you can use the follwing command\nssh-add directory\\key.pem Lastly, to connect, just type the following command.\nssh username@ip Where username is the server name and ip is the public IP adress, e.g. 132.132.132.132.\nIf your server is not public, you will not be able to access it.\nIf your server is password protected, you will be prompted to insert a password when you connect. If not, you should protect it with a password.\nManaging screens While you are connected to the remote terminal, any disturbance to your connection will interrupt the code. In order to avoid that, you want to create separate screens. This will allow your code to run remotely undisturbed, irrespectively of your connection.\nFirst, you need to install screen.\nbrew install screen To create a new screen, just type\nscreen Now you can lunch your code.\nAfter that, you want to detach from that screen so that the code can run remotely undisturbed.\nscreen -d Another option is to use ctrl+a followed by ctrl+d. This will detach the screen without the need to type anythin in the terminal, in case the terminal is busy (most likely).\nTo list the current active screens type\nscreen -ls If you want to check at any time that your code is running, without re-attaching to the screen, you can just type\ntop which is the general command to check active processes. To exit, use ctrl+z, which generally terminates processes in the terminal.\nTo reattach to your screen, type\nscreen -r In case you have multiple screens (you can check with screen -ls), you can reattach to a specific one by typing\nscreen -r 12345 where 12345 is the id of the screen.\nTo kill a screen, type\nscreen -XS 12345 quit where again 12345 is the id of the screen.\nPython and Pycharm If you are coding in Python, PyCharm is one of the best IDEs. Among many features, it offers the possibility to set a remote compiler for your pthon console and to sync input and output files automatically.\nFirst, you need to have setup a remote SSH connection following the steps above. Importantly, you need to have added the public key to your machine using the ssh-add command, as explained above.\nThen open Pytharm, go to the lower-right corner, where the current interpreter is listed (e.g. Pytohn 3.8), click it and select interpreter settings.\nClick on the gear icon ⚙️ on the top-right corner and select add.\nInsert the server host (IP address, e.g. 132.132.132.132) and username (e.g. sergiop).\nNext, you have to insert your credentials. If you have a password, insert it, otherwise you have to insert the path to your SSH key file.\nLastly, select the remote interpreter. If you are using a python version that is not default, browse to the preferred python installation folder. Also, check the box for execute code giving this interpreter with root privileges via sudo.\nYou can also select which remote folder to sync with your local project. By default, you are given a tmp/pycharm_project_XX folder. You can change it if you want. I recommend also to have the last option checked: automatically sync project files to the server. This will automatically synch all remote changes with your local machine, in your local project folder.\nJulia and Juno If you are coding in Julia, Juno is the best IDE around. It’s an integration with Atom with a dedicated compiler, local variables, syntax highlight, autocompletion.\nOn Atom, you first need to install the ftp-remote-edit package.\nThen go to the menu item Packages \u0026gt; Ftp-Remote-Edit \u0026gt; Toggle.\nA new Remote panel will open with the default button to Edit a new server.\nClick it and you will be able to set up your remote connection.\nPress New Insert your username in The name of the server, for example sergiop Insert your ip adress in The hostname or IP adress of the server, for example 123.123.123.123 Select SFTP - SSH File Transfer Protocol under Protocol Select your Logon option. You can either insert your password every time, just once, or use a keyfile. Insert again your username in Username for autentication, again for example sergiop If you don’t want to start from the root folder, you can change the Initial Directory Now you will be able to see your remote directory (named for example sergiop) in the Remote panel.\nTo start using Julia remotely, just start a new remote Julia process from the menu on the left.\nNow you are ready to deploy your Julia code on your remote server!\nJupyter Notebooks If you want to have a Jupyter Notebook running remotely, the steps are the following. The main advantage of a Jupyter Notebook is that it allows you to mix text and code in a single file, similarly to RMarkdown, with the advantage of not being contrained to use a R (or Python) kernel. For example, I often use Jupyter Notebook with Julia or Matlab Kernels. Moreover, you can also make nice slides out of it!\nFirst, connect to the remote machine. Look at section 1 to set up your SSH connection.\nssh username@ip Start a Jupyter Notebook in the remote machine.\njupyter notebook --no-browser The command will open a jupyter notebook in the remote machine. To connect to it, we need to know which port it used. The default port is 8888. If that port is busy, it will look for another available one. We can see the port from the output in terminal.\nJupyter Notebook is running at: http://localhost:XXXX/…\nWhere XXXX is the repote port used.\nNow we need to forward the remote port XXXX to our local YYYY port.\nOpen a new local shell. Type\nssh -L localhost:YYYY:localhost:XXXX username@ip Where YYYY can be anything. I’d use the default port 8888.\nssh -L localhost:8889:localhost:8888 username@ip Now go to your browser and type\nlocalhost:YYYY Which in my case is\nlocalhost:8889 This will open the remote Jupyter Notebook.\nDone!\nIn case you want to check which Jupiter notebooks are running, type\njupyter notebook list To kill a notebook use\njupyter notebook stop XXXX Sources How To Setup And Use SSH For Remote Connections Connecting to a Julia session on a remote machine Running a Jupyter notebook from a remote server ","date":1638748800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1638748800,"objectID":"2a9a4ca9e5b7b3d6e6bbc3e3e8d7c4b3","permalink":"https://matteocourthoud.github.io/post/ssh/","publishdate":"2021-12-06T00:00:00Z","relpermalink":"/post/ssh/","section":"post","summary":"Welcome to my tutorial on how to set up a remote machine and deploy your code there. I will first analyze SSH and then look at two specific applications: coding in Python and Julia.","tags":null,"title":"How to Work on a Remote Machine via SSH","type":"post"},{"authors":["Matteo Courthoud"],"categories":null,"content":"The use of algorithms to set prices is particularly popular in online marketplaces, where sellers need to take quick decisions in complex dynamic environments. In this article, I investigate the role of online marketplaces in facilitating or preventing collusion among sellers that use pricing algorithms. In particular, I investigate a platform that has the ability to give prominence to certain products and automates this decision through a reinforcement learning algorithm, that maximizes the platform\u0026rsquo;s profits. Depending on whether the business model of the platform is more aligned with consumer welfare or with sellers\u0026rsquo; profits (e.g., if it collects quantity or profit fees), the platform either prevents or facilitates collusion among algorithmic sellers. If the platform is also active as a seller, the so-called dual role, it is able to both induce sellers to set high prices and appropriate most of the profits. Importantly, self-preferencing only happens during the learning phase and not in equilibrium. I investigate a potential solution: separating the sales and marketplace divisions. The policy is effective but does not fully restore the competitive outcome when the fee is distortive, as in the case of a revenue fee.\n","date":1635120000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1635120000,"objectID":"349267903040a0c9c407d09a2d417e7c","permalink":"https://matteocourthoud.github.io/project/alg_platform/","publishdate":"2021-10-25T00:00:00Z","relpermalink":"/project/alg_platform/","section":"project","summary":"I show that online marketplaces, by algorithmically controlling consumers' attention, have the incentives and ability to either facilitate or prevent algorithmic collusion, depending on their business model. I also explore platforms' dual role.","tags":["Industrial Organization","Antitrust","Artificial Intelligence"],"title":"Algorithmic Collusion on Online Marketplaces","type":"project"},{"authors":null,"categories":null,"content":"Ok, this is a fun post. I am choosing… my color palette!\nI have decided to unify all the color palettes I have on my website, slides, graphs, etc… into a unique universal color palette.\nMain Color First of all, I have to choose my main color.\nHere are some shades of it.\nRelated Palettes Now I will build a couple of colors palettes based on it.\nThe first one, is red oriented.\nSecond one, is green oriented.\nColor Sequence Now I need a high contrast scheme for graphs. I add one color at the time to check that contrast is always maximized.\nhttps://coolors.co/003f5c-ff6e54-f9f871-2db88b-955196) A milder version of the same palette is:\nhttps://coolors.co/00798c-d1495b-edae49-52a369-756ab2)","date":1635033600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1635033600,"objectID":"4b0693de6f93a308b40b3cf73631212d","permalink":"https://matteocourthoud.github.io/post/palette/","publishdate":"2021-10-24T00:00:00Z","relpermalink":"/post/palette/","section":"post","summary":"Ok, this is a fun post. I am choosing… my color palette!\nI have decided to unify all the color palettes I have on my website, slides, graphs, etc… into a unique universal color palette.","tags":null,"title":"My Color Palette","type":"post"},{"authors":["Matteo Courthoud","Gregory Crawford"],"categories":null,"content":"In recent merger cases across complementary markets, antitrust authorities have expressed foreclosure concerns. In particular, the presence of scale economies in one market might propagate to the complementary market, ultimately leading to the monopolization of both. In this paper, we investigate the interplay between two foreclosure practices: exclusionary bundling and predatory pricing in the setting of complementary markets with economies of scale. We show that the two practices are complementary when markets display economies of scale, exclusionary bundling is more likely and, when bundling is allowed, predatory pricing is more likely. We show that this outcome is due to exit-inducing behavior of dominant firms: shutting down predatory incentives restores competition in both markets. We investigate different policies: banning mergers between market leaders, allowing product bundling only when more than one firm is integrated and able to offer the bundle, and lastly knowledge sharing across firms in order to limit the economies of scale. All policies are effective, each for a different reason.\n","date":1633046400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1633046400,"objectID":"129fc19e6524f0b4b6a49d907df9a3de","permalink":"https://matteocourthoud.github.io/project/foreclosure/","publishdate":"2021-10-01T00:00:00Z","relpermalink":"/project/foreclosure/","section":"project","summary":"We use a computational model of market dynamics to investigate the interplay between exclusionary bundling and predatory pricing. We show that the two foreclosure practices are complementary and we investigate potential policy interventions.","tags":["Industrial Organization","Antitrust","Computation","Dynamics"],"title":"Foreclosure Complementarities","type":"project"},{"authors":null,"categories":null,"content":"In this page, I collect anquestions that I frequently asked myself during my PhD, possibly with answers.\nPersonally, the article for PhD students that helped me the most is “Doing research” by Paul Niehaus. But beware, it might not work for everyone.\nStarting the PhD Information “PhDs: the tortuous truth”, Chris Woolston, 2019. “Why doing a PhD is often a waste of time”, The Economist, 2016 “Should you do a PhD?\u0026quot;, Daniel K. Sokol, 2012. “So, you want to go to a grad school in economics?\u0026quot;, Ceyhun Elgin and Mario Solis-Garcia, 2007. Applying “How to Ask Your Professor for a Letter of Recommendation”, James Tierney, 2020. “Pre-Doc Guide”, Alvin Christian, 2019. “Advice for Applying to Grad School in Economics”, Susan Athey, 2016. “The complete guide to getting into an economics PhD program”, Miles Kimball, 2013. “The 12 Step Program for Grad School”, Erik Zwick. Starting “Reflections on Grad School in Economics”, Nick Hagerty, 2020. “How to survive your first year of graduate school in economics”, Matthew Pearson, 2005. During the PhD Mental Health “Graduate Student Mental Health: Lessons from American Economics Departments”, Bolotnyy, Valentin, Matthew Basilico, and Paul Barreira, 2021. “Mental Health, Bullying, Career Uncertainty”, Colleen Flahert, 2019. “How mindfulness can help Ph.D. students deal with mental health challenges”, Katie Langin, 2019. “Managing Your Mental Health as a PhD Student”, Joanna Hughes, 2019. “What Makes It So Hard to Ask for Help?\u0026quot;, Joan Rosenberg, 2019. “Grad school depression almost took me to the end of the road—but I found a new start”, Francis Aguisanda, 2018. “Faking it”, Chris Woolston, 2016. “Panic and a PhD”, Jack Leeming, 2016. “There’s an awful cost to getting a PhD that no one talks about”, Jennifer Walker, 2015. Research and Ideas “Advice for Academic Research”, Ricardo Dahis, 2021. “Sins of Omission and the Practice of Economics”, George A. Akerlof, 2020. “Doing research”, Paul Niehaus, 2019. “An unofficial guidebook for PhD students in economics and education”, Alex Eble, 2018. “The Research Productivity of New PhDs in Economics: The Surprisingly High Non-Success of the Successful”, John P. Conley and Ali Sina Önder, 2014. [“How to get started on research in economics?\u0026quot;](http://econ.lse.ac.uk/staff/spischke/phds/How to start.pdf), Steve Pischke, 2009. “The Importance of Stupidity in Scientific Research”, Martin A. Schwartz, 2008. “7 Rules for Maximizing Your Creative Output”, Steve Pavlina, 2007. “How To Build An Economic Model in Your Spare Time”, Hal. R. Varian, 1998. [“Ph.D. Thesis Research: Where do I Start?\u0026quot;](http://www.columbia.edu/~drd28/Thesis Research.pdf), Don Davis. Presenting “Unfair Questions”, David Schindler, 2021. “Beamer Tips for Presentations”, Paul Goldsmith-Pinkham, 2020. “Public Speaking for Academic Economists”, Rachel Meager, 2017. “How to present your job market paper”, Eliana La Ferrara, 2018. “How To Give a Lunch Talk”, Adam Guren, 2018. “The Discussant’s Art”, Chris Blattman, 2010. “How to be a Great Conference Participants”, Art Carden, 2009. [“The “Big 5” and Other Ideas For Presentations”](http://econ.lse.ac.uk/staff/spischke/phds/The Big 5.pdf), Cox, Donald, 2000. “How to Give an Applied Micro Talk”, Jesse M. Shapiro. “Tips on How to Avoid Disaster in Presentations”, Monika Piazzesi. [“Seminar Slides “](https://www.ssc.wisc.edu/~bhansen/placement/Seminar Slides.pdf), Bruce Hansen. Writing [“5 Steps Toward a Paper”](https://www.google.com/url?q=https%3A%2F%2Fwww.dropbox.com%2Fs%2Fq7wjaidl5w91srt%2FGuest lecture FS.pdf%3Fdl%3D0\u0026amp;sa=D\u0026amp;sntz=1\u0026amp;usg=AFQjCNG_nRs6QlkZzWBHAy0PjF4jfEYBAw), Frank Schilbach, 2019. “Novelist Cormac McCarthy’s tips on how to write a great science paper”, Van Savage and Pamela Yeh, 2019. “The “Middle Bits” Formula for Applied Papers”, Marc Bellamare, 2018. “The Conclusion Formula”, Marc Bellamare, 2018. [“The Introduction Formula”](https://www.albany.edu/spatial/training/5-The Introduction Formula.pdf), Keith Head, 2015. “Writing Tips For Economics Research Papers”, Plamen Nikolov, 2013. “The Ten Most Important Rules of Writing Your Job Market Paper”, Goldin, Claudia and Lawrence Katz, 2008. “Writing Tips for Ph.D. Students”, John Cochrane, 2005. “Writing Papers: A Checklist”, Michael Kremer. Referiing “How To Write A Good Referee Report”, Tatyana Deryugina, 2019. “How to Review Manuscripts”, Elsevier, 2015. “Contributing to Public Goods: My 20 Rules for Refereeing”, Marc F. Bellemare, 2012 Finishing the PhD The Job Market “A Guide and Advice for Economists on the U.S. Junior Academic Job Market 2018-2019 Edition”, John Cawley, 2018. “Academic job market advice for economics, political science, public policy, and other professional schools”, Blattman, Christopher, 2015. “How I Learned to Stop Worrying and Love the Job Market”, Erik Zwick, 2014. The Private Sector “My Journey from Economics PhD to Data Scientist in Tech”, Rose Tan, 2021. “Tech Industry Jobs for Econ PhDs”, Scarlet Chen, 2020. “My Journey from Econ PhD to Tech”, Scarlet Chen, 2020. “Why it is not a ‘failure’ to leave academia”, Philipp Kruger, 2018. The Tenure Track “The Awesomest 7-Year Postdoc or: How I Learned to Stop Worrying and Love the Tenure-Track Faculty Life”, Radhika Nagpal, 2013. More You can find more resources here:\nAEA Mentoring Reading Materials Johannes Pfeifer Job Market Resources Kristoph Kronenberg Resources Patrick Button Resources Ryan Edwards Resources Jennifer Doleac Resources Amanda Agan Writing and Presentation Advice Random forum Resource Collection ","date":1633046400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1633046400,"objectID":"3640fe4bf948f5a3cef62c6fa0ffdd0f","permalink":"https://matteocourthoud.github.io/post/phd_faq/","publishdate":"2021-10-01T00:00:00Z","relpermalink":"/post/phd_faq/","section":"post","summary":"In this page, I collect anquestions that I frequently asked myself during my PhD, possibly with answers.\nPersonally, the article for PhD students that helped me the most is “Doing research” by Paul Niehaus.","tags":null,"title":"PhD Frequently Asked Questions","type":"post"},{"authors":["Matteo Courthoud"],"categories":null,"content":"Reinforcement learning algorithms are gradually replacing humans in many decision-making processes, such as pricing in high-frequency markets. Recent studies on algorithmic pricing have shown that algorithms can learn sophisticated grim-trigger strategies with the intent of keeping supra-competitive prices. This paper focuses on algorithmic collusion detection. One frequent suggestion is to look at the inputs of the strategies, for example at whether the algorithms condition their prices on previous competitors\u0026rsquo; prices. The first part of the paper shows that this approach might not be sufficient to detect collusion since the algorithms can learn reward-punishment schemes that are fully independent of the rival’s actions. The mechanism that ensures the stability of supra-competitive prices is self-punishment.\nThe second part of the paper explores a novel test for algorithmic collusion detection. The test builds on the intuition that as algorithms are able to learn to collude, they might be able to learn to exploit collusive strategies. In fact, since they are not designed to learn sub-game perfect equilibrium strategies, there is the possibility that their strategies could be exploited. When one algorithm is unilaterally retrained, keeping the collusive strategies of its competitor fixed, it learns more profitable strategies. Usually, these strategies are more competitive, but not always. Since this change in strategies happens only when algorithms are colluding, retraining can be used as a test to detect algorithmic collusion.\nTo make the test implementable, the last part of the paper studies whether one could get the same insights on collusive behavior using only observational data, from a single algorithm. The result is a unilateral empirical test for algorithmic collusion that does not require any assumptions neither on the algorithms themselves nor on the underlying environment. The key insight is that algorithms, during their learning phase, produce natural experiments that allow an observer to estimate their behavior in counterfactual scenarios. The simulations show that, at least in a controlled experimental setting, the test is extremely successful in detecting algorithmic collusion.\n","date":1631404800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1631404800,"objectID":"a444b4a0948fc8347013080cc97eb487","permalink":"https://matteocourthoud.github.io/project/alg_detection/","publishdate":"2021-09-12T00:00:00Z","relpermalink":"/project/alg_detection/","section":"project","summary":"I show that algorithms can learn reward-punishment schemes that are fully independent from the rival’s actions and I propose a model-free test for algorithmic collusion based on historical data.","tags":["Industrial Organization","Antitrust","Artificial Intelligence"],"title":"Algorithmic Collusion Detection","type":"project"},{"authors":null,"categories":null,"content":"In this page, I collect useful resources for coding for researchers in social sciences. A mention goes to Maximilian Kasy that inspired me to build this page.\nA quick legend:\n📗 book 🌐 webpage 📈 charts 🎥 videos Econometrics and Statistics 📗Bruce Hansen’s Econometrics: By far the best freely available and regularly updated resource for Econometrics Machine Learning 📗The Elements of Statistical Learning: General introduction to machine learning 📗Gaussian Processes for Machine Learning: Extremely useful tools for nonparametric Bayesian modeling 📗Deep Learning: The theory and implementation of neural nets 📗Understanding Machine Learning: From Theory to Algorithms: An introduction to statistical learning theory in the tradition of Vapnik 📗Reinforcement Learning - An Introduction: Adaptive learning for Markov decision problems 📗Algorithms: Introduction to the theory of algorithms 🌐Tensorflow Playground: Visualisation tool for neural networks 🌐Artificial Intelligence: Online lectures on AI 🌐The Ethical Algorithm: How to impose normative constraints on ML and other algorithms Python 🌐RealPython: Collection of Python tutorials, from introductory to advanced. Also contains learning paths for specific topics 🌐QuantEcon Python Tutorials and economic applications in Python, especially for macroeconomics 🌐Cheat Sheets: Collection of cheat sheets for python 🌐Structuring a Python project: Advanced tutorial on how to structure a Python program 🌐IDE Guide: Comparison of IDEs for Python. Suggested: PyCharm 🌐Configuring remote interpreters via SSH: How to use Python remotely via SSH via PyCharm 📈Visualization in Python: How to make nice graphs in Python, with a dedicated jupyter notebook 📈Python Graph Gallery: Graph examples in Python Matlab 🌐User defined classes in Matlab: How to work with classes in Matlab 🌐Julyter Notebooks: How to run a jupyter notebook with Matlab kernel 📈Graph Tips in Matlab and link2: Suggestions on how to make pretty graphs in Matlab Julia 🌐Julia Manual: Julia unfortunately lacks a big community and tutorials, but it has a very good manual 🌐QuantEcon Julia Tutorials and economic applications in Julia, especially for macroeconomics 🌐IDE Guide: Guide for IDEs for Julia. Suggested: Juno for Atom. R 📗An Introduction to R: Complete introduction to base R\n📗R for Data Science Introduction to data analysis using R, focused on the tidyverse packages\n📗Advanced R: In depth discussion of programming in R\n📗Hands-On Machine Learning with R: Fitting ML models in R\n🌐Bayesian statistics using Stan and link\n🌐RStudio Cheat Sheets for various extensions, including data processing, visualization, writing web apps, …\n📈R Graph Gallery: Graph examples in R\n📈Nice Graphs with code\nA collection of elaborate graphs with code in R\nOthers 📗Github Advanced: Advanced guide for version control with Github 🎥The Missing Semester of Your CS Education Video lectures and notes on tools for computer scientists (version control, debugging, …) 📈PGF plots in Latex: Gallery and examples to make plots directly in Latex 🌐Work remotely from server: How to setup SSH for remote computing ","date":1629763200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1629763200,"objectID":"47e14b4466dc75da698fe3b2f882bdb1","permalink":"https://matteocourthoud.github.io/post/coding/","publishdate":"2021-08-24T00:00:00Z","relpermalink":"/post/coding/","section":"post","summary":"In this page, I collect useful resources for coding for researchers in social sciences. A mention goes to Maximilian Kasy that inspired me to build this page.\nA quick legend:","tags":null,"title":"Coding Resources for Social Sciences","type":"post"},{"authors":null,"categories":null,"content":"In this page, I explain how to work with the WRDS database using Python.\nSetup The first thing we need to do, is to set up a connection to the WRDS database. I am assuming you have credentials to log in. Check the log in page to make sure.\nThe second requirement is the wrds Python package.\npip3 install wrds Now, in order to connect to the WRDS database, you just need to run the following commang in Python.\nimport wrds db = wrds.Connection() Then, you will be propted to input your WRDS username and password.\nHowever, if you are using a Python IDE such as PyCharm, you cannot run the command from the Python Console. Moreover, you might want to save your credentials once and for all, so that you don’t have to log in every time.\nFirst, walk to your home directory from the Terminal (/Users/username).\ncd Now create an empty .pgpass file.\ntouch .pgpass Now you write your_username and your_password into the .pgpass file.\necho \u0026quot;wrds-pgdata.wharton.upenn.edu:9737:wrds:your_username:your_password\u0026quot; \u0026gt;\u0026gt; .pgpass You also need to restrict permissions to the file.\nchmod 600 ~/.pgpass Now you can go back to your Python IDE and access the database by just inputing your username.\nimport wrds db = wrds.Connection(wrds_username='your_username') If everything works, you should see the following output.\nLoading library list... Done Query The available functions are:\ndb.connection() db.list_libraries() db.list_tables() db.get_table() db.describe_table() db.raw_sql() db.close() I make a simple example of how they work. Suppose first you want to list all the libraries in the WRDS database.\ndb.list_libraries() Then you can list all the datasets within a given library.\ndb.list_tables(library='comp') Before downloading a table, you can describe it.\ndf = db.describe_table(library='comp', table='funda') To download the dataset you can use the get_table() function.\ndf = db.get_table(library='comp', table='funda') You can restrict both the rows and the columns you want to query.\ndf_short = db.get_table(library='comp', table='funda', columns = ['conm', 'gvkey', 'cik'], obs=5) You can also query the database directly using SQL.\ndf_sql = db.raw_sql('''select conm, gvkey, cik FROM comp.funda WHERE fyear\u0026gt;2010 AND (indfmt='INDL')''') Sources Querying WRDS Data using Python Using Python on WRDS Platform Introduction to the WRDS Python Package WRDS Data Access Via Python API ","date":1615248000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1615248000,"objectID":"95c5fd9f29ea012e404664238d9d0a7d","permalink":"https://matteocourthoud.github.io/post/wrds/","publishdate":"2021-03-09T00:00:00Z","relpermalink":"/post/wrds/","section":"post","summary":"In this page, I explain how to work with the WRDS database using Python.\nSetup The first thing we need to do, is to set up a connection to the WRDS database.","tags":null,"title":"How to access WRDS in Python","type":"post"},{"authors":["Matteo Courthoud"],"categories":null,"content":"Dynamic stochastic games notoriously suffer from a curse of dimensionality that makes computing the Markov Perfect Equilibrium of large games infeasible. This article compares the existing approximation methods and alternative equilibrium concepts that have been proposed in the literature to overcome this problem. No method clearly dominates the others but some are dominated in all dimensions. In general, alternative equilibrium concepts outperform sampling-based approximation methods. I propose a new game structure, games with random order, in which players move sequentially and the order of play is unknown. The Markov Perfect equilibrium of this game consistently outperforms all existing approximation methods in terms of approximation accuracy while still being extremely efficient in terms of computational time.\n","date":1609459200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1609459200,"objectID":"942fdcc5da1de7f590684b3218f68964","permalink":"https://matteocourthoud.github.io/project/approximations/","publishdate":"2021-01-01T00:00:00Z","relpermalink":"/project/approximations/","section":"project","summary":"I compare existing approximation methods to compute Markow Perfect Equilibrium in dynamic stochastic games with large state spaces. I also propose a new approximation method called \"Games with Random Order\".","tags":["Industrial Organization","Computation","Dynamics"],"title":"Approximation Methods for Large Dynamic Stochastic Games","type":"project"},{"authors":null,"categories":null,"content":"In this page, I collect information about summer schools in Economics.\nIf you know about summer schools that are missing from this list, please either contact me or edit the table on Github!\n2020 Name and Link Field Organizer Location Instructor(s) Dates Deadline Fee Aid Dynamic Structural Econometrics Econometrics, IO Econometrics Society Zurich John Rust et al. June 15-21 March 15 500$ no CRESSE Comp. Policy, IO Crete various June 20 - July 02 FCFS 3200€ -30% Digital Economy Comp. Policy, IO Barcelona GSE Barcelona Martin Peitz July 13-17 March 10 550€ maybe Social Networks, Platforms… Comp. Policy, IO PSE Paris from Paris June 15-19 March 31 1200€ no 2019 Name and Link Field Organizer Location Instructor(s) Dates Deadline Fee Aid Dynamic Structural Econometrics Econometrics, IO Econometrics Society Chicago John Rust et al. July 08-14 March 15 500$ no CRESSE Competition Policy, IO various Crete various June 20 - July 02 FCFS 3200€ -30% Empirical Analysis of Firm Performance IO, Trade CEMFI Madrid Jan de Loecker August 19-23 Panel Data Econometrics Econometrics CEMFI Madrid Steve Bond September 02-06 2018 Name and Link Field Organizer Location Instructor(s) Dates Deadline Fee Aid Dynamic Structural Models Econometrics, IO University of Copenhagen Copenhagen John Rust et al. May 28 - Jun 03 March 15 600€ no Empirical Analysis of Innovation in Oligopoly Industries ","date":1583798400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1583798400,"objectID":"0f0d27452021e254ab9744c41138bb19","permalink":"https://matteocourthoud.github.io/post/summer_schools/","publishdate":"2020-03-10T00:00:00Z","relpermalink":"/post/summer_schools/","section":"post","summary":"In this page, I collect information about summer schools in Economics.\nIf you know about summer schools that are missing from this list, please either contact me or edit the table on Github!","tags":null,"title":"Summer Schools in Economics","type":"post"},{"authors":["Matteo Courthoud"],"categories":null,"content":"I generate a time-varying measure of S\u0026amp;P500 firm similarity using a zero-shot clustering model. The model takes as input BERT embeddings of product descriptions and is trained on market definitions from the EU commission. The objective is to estimate the causal effect of common ownership on product similarity.\n","date":1580515200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1580515200,"objectID":"8945db35dd8049bb20237e33023618f5","permalink":"https://matteocourthoud.github.io/project/ownership/","publishdate":"2020-02-01T00:00:00Z","relpermalink":"/project/ownership/","section":"project","summary":"I generate a time-varying measure of S\\\u0026P500 firm similarity using a zero-shot clustering model. The model takes as input BERT embeddings of product descriptions and is trained on market definitions from the EU commission. The objective is to estimate the causal effect of common ownership on product similarity.","tags":["Industrial Organization","Antitrust","Computation","Dynamics"],"title":"Ownership and Product Similarity","type":"project"},{"authors":[],"categories":[],"content":"Create slides in Markdown with Wowchemy Wowchemy | Documentation\nFeatures Efficiently write slides in Markdown 3-in-1: Create, Present, and Publish your slides Supports speaker notes Mobile friendly slides Controls Next: Right Arrow or Space Previous: Left Arrow Start: Home Finish: End Overview: Esc Speaker notes: S Fullscreen: F Zoom: Alt + Click PDF Export: E Code Highlighting Inline code: variable\nCode block:\nporridge = \u0026quot;blueberry\u0026quot; if porridge == \u0026quot;blueberry\u0026quot;: print(\u0026quot;Eating...\u0026quot;) Math In-line math: $x + y = z$\nBlock math:\n$$ f\\left( x \\right) = ;\\frac{{2\\left( {x + 4} \\right)\\left( {x - 4} \\right)}}{{\\left( {x + 4} \\right)\\left( {x + 1} \\right)}} $$\nFragments Make content appear incrementally\n{{% fragment %}} One {{% /fragment %}} {{% fragment %}} **Two** {{% /fragment %}} {{% fragment %}} Three {{% /fragment %}} Press Space to play!\nOne **Two** Three A fragment can accept two optional parameters:\nclass: use a custom style (requires definition in custom CSS) weight: sets the order in which a fragment appears Speaker Notes Add speaker notes to your presentation\n{{% speaker_note %}} - Only the speaker can read these notes - Press `S` key to view {{% /speaker_note %}} Press the S key to view the speaker notes!\nOnly the speaker can read these notes Press S key to view Themes black: Black background, white text, blue links (default) white: White background, black text, blue links league: Gray background, white text, blue links beige: Beige background, dark text, brown links sky: Blue background, thin dark text, blue links night: Black background, thick white text, orange links serif: Cappuccino background, gray text, brown links simple: White background, black text, blue links solarized: Cream-colored background, dark green text, blue links Custom Slide Customize the slide style and background\n{{\u0026lt; slide background-image=\u0026quot;/media/boards.jpg\u0026quot; \u0026gt;}} {{\u0026lt; slide background-color=\u0026quot;#0000FF\u0026quot; \u0026gt;}} {{\u0026lt; slide class=\u0026quot;my-style\u0026quot; \u0026gt;}} Custom CSS Example Let\u0026rsquo;s make headers navy colored.\nCreate assets/css/reveal_custom.css with:\n.reveal section h1, .reveal section h2, .reveal section h3 { color: navy; } Questions? Ask\nDocumentation\n","date":1549324800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1549324800,"objectID":"0e6de1a61aa83269ff13324f3167c1a9","permalink":"https://matteocourthoud.github.io/slides/example/","publishdate":"2019-02-05T00:00:00Z","relpermalink":"/slides/example/","section":"slides","summary":"An introduction to using Wowchemy's Slides feature.","tags":[],"title":"Slides","type":"slides"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"f26b5133c34eec1aa0a09390a36c2ade","permalink":"https://matteocourthoud.github.io/admin/config.yml","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/admin/config.yml","section":"","summary":"","tags":null,"title":"","type":"wowchemycms"}]