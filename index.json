[{"authors":null,"categories":null,"content":"I am an Economics PhD student at the University of Zürich. My research focuses on algorithms, platform markets, and competition policy combining tools from Industrial Organization, causal inference, and machine learning. Besides my research, I taught several courses in econometrics and machine learning and worked as an external consultant for a large online platform.\nI believe in knowledge sharing, and I try to contribute by sharing my code, tutorials and lecture notes. I write on Medium for Towards Data Science.\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"https://matteocourthoud.github.io/author/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/","section":"authors","summary":"I am an Economics PhD student at the University of Zürich. My research focuses on algorithms, platform markets, and competition policy combining tools from Industrial Organization, causal inference, and machine learning.","tags":null,"title":"","type":"authors"},{"authors":null,"categories":null,"content":"⚠️ WORK IN PROGRESS! ⚠️\nWelcome to my Data Science with Python course!\nYou can find all the Jupyter notebook on my Github page here.\nPlease, if you find any typos or mistakes, open a new issue. Or even better, fork the repo and submit a pull request. I am happy to share my work and I am even happier if it can be useful.\nContent  Data Structures  Lists Tuples Sets Dictionaries Numpy arrays Pandas DataFrames Pyspark DataFrames   Data Exploration  Import, export data Descriprives and summary statistics Pivot tables and aggregation   Data Types  Numerical data String data Time data Missing data   Data Wrangling  Rows: sorting, indexing, \u0026hellip;. Columns: renaming, ordering, \u0026hellip;. Collapse and aggregate Reshape Concatenate and merge   Plotting  Distributions Time Series Correlations Regression Geographical data   Machine Learning Pipeline  Data exploration Encoding and normalization Missing values Weighting Prediction Cross-validation   Web Scraping  Pandas APIs Static Webscraping Dynamic Webscraping   TBD  What is missing? Let me know!    Contacts All feedback is greatly appreciated!\n","date":1633046400,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":1633046400,"objectID":"e74e2a6ba6bd604ca04f081def878330","permalink":"https://matteocourthoud.github.io/course/data-science/","publishdate":"2021-10-01T00:00:00Z","relpermalink":"/course/data-science/","section":"course","summary":"⚠️ WORK IN PROGRESS! ⚠️\nWelcome to my Data Science with Python course!\nYou can find all the Jupyter notebook on my Github page here.\nPlease, if you find any typos or mistakes, open a new issue.","tags":["tutorial","data science","programming","python"],"title":"Data Science with Python","type":"book"},{"authors":null,"categories":null,"content":"Welcome to my notes for the Machine Learning for Economic Analysis course by Damian Kozbur @UZH!\nThe exercise sessions are entirely coded in Python on Jupyter Notebooks. The examples heavily borrow from An Introduction to Statistical Learning by James, Witten, Tibshirani, Friedman and its advanced version Elements of Statistical Learning by Hastie, Tibshirani, Friedman. Other recommended free resources are the documentation of the Python library scikit-learn and Bruce Hansen\u0026rsquo;s Econometrics book.\nYou can find all the Jupyter Notebooks on my Github page HERE.\nPlease, if you find any typos or mistakes, open a new issue. Or even better, fork the repo and submit a pull request. I am happy to share my work and I am even happier if it can be useful.\nContent   OLS Regression\n ISLR, chapter 3 ESL, chapter 3 Econometrics, chapters 3 and 4    Instrumental Variables\n Econometrics, chapter 12.1-12.12    Nonparametric Regression\n ISLR, chapter 7 ESL, chapter 5 Econometrics, chapters 19 and 20    Cross-validation\n ISLR, chapter 5 ESL, chapter 7    Lasso and Forward Regression\n ISLR, chapter 6 ESL, chapters 3 and 18 Econometrics, chapter 29.2-29.5    Convexity and Optimization\n  Trees and Forests\n ISLR, chapter 8 ESL, chapters 9, 10, 15, 16 Econometrics, chapter 29.6-29.9    Neural Networks\n ESL, chapter 11    Post-Double Selection\n Econometrics, chapter 3.18 Belloni, Chen, Chernozhukov, Hansen (2012) Belloni, Chernozhukov, Hansen (2014) Chernozhukov, Chetverikov, Demirer, Duflo, Hansen, Newey, Robins (2018)    Unsupervised Learning\n ISLR, chapter 10 ESL, chapter 14    Pre-requisites Students should be familiar with the following concepts:\n Matrix Algebra  Econometrics, appendix A.1-A.10   Conditional Expectation and Projection  Econometrics, chapter 2.1-2.25   Large Sample Asymptotics  Econometrics, chapter 6.1-6.5   Python basics  Quant-Econ Tutorial    Readings  Athey, S., \u0026amp; Imbens, G. W. (n.d.). Machine Learning Methods Economists Should Know About. 62. Belloni, A., Chen, H., Chernozhukov, V., \u0026amp; Hansen, C. B. (2012). Sparse Models and Methods for Optimal Instruments With an Application to Eminent Domain. Econometrica, 80(6), 2369–2429. https://doi.org/10.3982/ECTA9626 Belloni, A., Chernozhukov, V., \u0026amp; Hansen, C. (2014). Inference on Treatment Effects after Selection among High-Dimensional Controls. The Review of Economic Studies, 81(2), 608–650. https://doi.org/10.1093/restud/rdt044 Chernozhukov, V., Chetverikov, D., Demirer, M., Duflo, E., Hansen, C., Newey, W., \u0026amp; Robins, J. (2018). Double/debiased machine learning for treatment and structural parameters. The Econometrics Journal, 21(1), C1–C68. https://doi.org/10.1111/ectj.12097 Franks, A., Miller, A., Bornn, L., \u0026amp; Goldsberry, K. (2015). Characterizing the spatial structure of defensive skill in professional basketball. The Annals of Applied Statistics, 9(1), 94–121. https://doi.org/10.1214/14-AOAS799 Gentzkow, M., Shapiro, J. M., \u0026amp; Taddy, M. (2019). Measuring Group Differences in High‐Dimensional Choices: Method and Application to Congressional Speech. Econometrica, 87(4), 1307–1340. https://doi.org/10.3982/ECTA16566 Kleinberg, J., Lakkaraju, H., Leskovec, J., Ludwig, J., \u0026amp; Mullainathan, S. (2017). Human Decisions and Machine Predictions. The Quarterly Journal of Economics. https://doi.org/10.1093/qje/qjx032 Kleinberg, J., Ludwig, J., Mullainathan, S., \u0026amp; Obermeyer, Z. (2015). Prediction Policy Problems. American Economic Review, 105(5), 491–495. https://doi.org/10.1257/aer.p20151023 Mullainathan, S., \u0026amp; Spiess, J. (2017). Machine Learning: An Applied Econometric Approach. Journal of Economic Perspectives, 31(2), 87–106. https://doi.org/10.1257/jep.31.2.87 Wager, S., \u0026amp; Athey, S. (2018). Estimation and Inference of Heterogeneous Treatment Effects using Random Forests. Journal of the American Statistical Association, 113(523), 1228–1242. https://doi.org/10.1080/01621459.2017.1319839  Sources These exercise sessions heavily borrow from\n Jordi Warmenhoven\u0026rsquo;s git repo ISLR-python Quant-Econ website Prof. Damian Kozbur past UZH PhD Econometrics Class Clark Science Center Machine Learning couse UC Berkeley Convex Optimization and Approximation class by Moritz Hardt Morvan Zhou and Yunjey Choi pytorch tutorials Daniel Godoy excellent article on Pytorch in Medium\u0026rsquo;s towardsdatascience  ","date":1633046400,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":1633046400,"objectID":"ed1edeaa10df72e3bccd0c8259f6ef0b","permalink":"https://matteocourthoud.github.io/course/ml-econ/","publishdate":"2021-10-01T00:00:00Z","relpermalink":"/course/ml-econ/","section":"course","summary":"Welcome to my notes for the Machine Learning for Economic Analysis course by Damian Kozbur @UZH!\nThe exercise sessions are entirely coded in Python on Jupyter Notebooks. The examples heavily borrow from An Introduction to Statistical Learning by James, Witten, Tibshirani, Friedman and its advanced version Elements of Statistical Learning by Hastie, Tibshirani, Friedman.","tags":["tutorial","economics","programming","julia"],"title":"Machine Learning for Economics","type":"book"},{"authors":null,"categories":null,"content":"⚠️ WORK IN PROGRESS! ⚠️\nWelcome to my notes and code for a graduate course in Empirical Industrial Organization!\nAll code is written in Julia. I borrow heavily from other sources (below), but all mistakes are mine. The RMarkdown notebooks can be found here.\nPlease, if you find any typos or mistakes, open a new issue. Or even better, fork the repo and submit a pull request. I am happy to share my work and I am even happier if it can be useful.\nContent The course will cover the following content\n Introduction Production Function Estimation Demand Estimation [slides] Demand Identification Merger Analysis Entry and Exit Single-Agent Dynamics [slides] Dynamic Games [slides] Auctions Media  The course also covers the baseline replication of the following papers\n Logit Demand [slides] Berry, Levinsohn, Pakes (1995) [slides] Rust (1987) [slides]  Sources The course heavily borrows from the following sources\n Kohei Kawaguchi’s Empirical IO course in R Gregory Crawford Graduate Empirical IO course at UZH Phil Haile and Mitrsuru Igami Graduate Empirical IO course at Yale  More references can be found within each single session.\n","date":1636502400,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":1636502400,"objectID":"3afc2fc1930ae28d1e7933e2e3c47670","permalink":"https://matteocourthoud.github.io/course/empirical-io/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/course/empirical-io/","section":"course","summary":"These are my notes and code for an advanced course in Empirical Industrial Organization. All code is written in Julia. The original notebooks can be found on my Github page.","tags":null,"title":"PhD Industrial Organization","type":"book"},{"authors":null,"categories":null,"content":"Welcome to my lecture notes for graduate Econometrics!\nThese notes were initially born as my personal summary for the PhD Econometrics course of professor Damian Kozbur in Zurich. The first draft was the result of an intense collaborative effort together with Chiara Aina and Paolo Mengano. During the years I have expanded the first draft in order to make it more comprehensive and include Julia code examples. All errors are mine.\nPlease, if you find any typos or mistakes, open a new issue. Or even better, fork the repo and submit a pull request. I am happy to share my work and I am even happier if it can be useful.\nContent My notes cover the following content\n Matrix Algebra [slides]  Hansen (2021). \u0026ldquo;Econometrics\u0026rdquo;. Appendix A. Greene (2006). \u0026ldquo;Econometric Analysis\u0026rdquo;. Appendix A: Matrix Algebra.   Probability Theory [slides]  Greene (2006). \u0026ldquo;Econometric Analysis\u0026rdquo;. Appendix B: Probability and Distribution Theory. Greene (2006). \u0026ldquo;Econometric Analysis\u0026rdquo;. Appendix C: Estimation and Inference.   Asymptotic Theory [slides]  Hansen (2021). \u0026ldquo;Econometrics\u0026rdquo;. Chapters 6. Wooldridge (2010). \u0026ldquo;Econometric Analysis of Cross Section and Panel Data\u0026rdquo;. Chapter 3: Basic Asymptotic Theory. Halmos (2006). \u0026ldquo;Lectures on Ergodic Theory\u0026rdquo;. Greene (2006). \u0026ldquo;Econometric Analysis\u0026rdquo;. Appendix D: Large Sample Distribution Theory. Hayashi (2000). \u0026ldquo;Econometrics\u0026rdquo;. Chapter 2: Large-Sample Theory.   Inference [slides]  Hansen (2021). \u0026ldquo;Econometrics\u0026rdquo;. Chapters 9: Hypothesis Testing.   OLS Algebra [slides]  Hansen (2021). \u0026ldquo;Econometrics\u0026rdquo;. Chapters 3 and 4. Hayashi (2000). \u0026ldquo;Econometrics\u0026rdquo;. Chapter 1: Finite-Sample Properties of OLS.   OLS Inference [slides]  Hansen (2021). \u0026ldquo;Econometrics\u0026rdquo;. Chapters 7. Hayashi (2000). \u0026ldquo;Econometrics\u0026rdquo;. Chapter 2: Large-Sample Theory.   Endogeneity [slides]  Hansen (2021). \u0026ldquo;Econometrics\u0026rdquo;. Chapters 12 and 13. Hayashi (2000). \u0026ldquo;Econometrics\u0026rdquo;. Chapter 3: Single-Equation GMM. Belloni, A., Chen, H., Chernozhukov, V., \u0026amp; Hansen, C. B. (2012). Sparse Models and Methods for Optimal Instruments With an Application to Eminent Domain. Econometrica, 80(6), 2369–2429.   Non-parametrics [slides]  Hansen (2021). \u0026ldquo;Econometrics\u0026rdquo;. Chapters 19, 20 and 21. Newey, W. K. (1997). Convergence rates and asymptotic normality for series estimators. Journal of Econometrics, 79(1), 147–168.   Selection [slides]  Hansen (2021). \u0026ldquo;Econometrics\u0026rdquo;. Chapter 24. Hastie, Tibshirani, Friedman (2001). \u0026ldquo;The Elements of Statistical Learning\u0026rdquo;. Belloni, A., Chernozhukov, V., \u0026amp; Hansen, C. (2014). Inference on Treatment Effects after Selection among High-Dimensional Controls. The Review of Economic Studies, 81(2), 608–650.    Sources  Kozbur (2019), PhD Econometrics - Lecture Notes. Hansen (2021), \u0026ldquo;Econometrics\u0026rdquo;. Wooldridge (2010), \u0026ldquo;Econometric Analysis of Cross Section and Panel Data\u0026rdquo;. Greene (2006), \u0026ldquo;Econometric Analysis\u0026rdquo;. Hayashi (2000), \u0026ldquo;Econometrics\u0026rdquo;.  ","date":1635465600,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":1635465600,"objectID":"fbda65a0e2851a55117043a99bfb2df3","permalink":"https://matteocourthoud.github.io/course/metrics/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/course/metrics/","section":"course","summary":"These are my notes and code for an advanced course in Econometrics. All code is written in Julia. The original notebooks can be found on my Github page.","tags":null,"title":"PhD Econometrics","type":"book"},{"authors":null,"categories":null,"content":"import numpy as np import pandas as pd  For most of this course we are going to work with pandas DataFrames. However, it\u0026rsquo;s important to start with an introduction to the different types of data structures available in Python, their characteristics, their differences and their comparative advantages.\nThe most important data structures in Python are:\n lists tuples sets dictionaries numpy arrays pandas DataFrames pyspark DataFrames  Lists A list is a mutable array data structure in Python with the following characteristics:\n can hold any type of data can hold different types of data at the same time can be modified  We can generate lists using square brackets.\nl = [12, \u0026quot;world\u0026quot;, [3,4,5]] print(l)  [12, 'world', [3, 4, 5]]  Since lists are ordered, we can access their element by calling the position of the element in the list.\nl[0]  12  Since lists are mutable, we can modify their elements.\nl[0] = 'hello' print(l)  ['hello', 'world', [3, 4, 5]]  We can add elements to a list using .append().\nl.append(23) l  ['hello', 'world', [3, 4, 5], 23]  We can remove elements by calling del on it.\ndel l[0] print(l)  ['world', [3, 4, 5], 23]  We can combine two lists using +. Note that this operation does not modify the list but generates a new one.\nl + [23]  ['world', [3, 4, 5], 23, 23]  We can also generate lists using comprehensions.\nl = [n for n in range(3)] print(l)  [0, 1, 2]  Comprehensions are a powerful tool!\nl = [n+10 for n in range(10) if (n%2==0) and (n\u0026gt;4)] print(l)  [16, 18]  Tuples A tuple is an immutable array data structure in Python with the following characteristics:\n can hold any type of data can hold different types of data at the same time can not be modified  We can generate tuples using curve brackets.\n# A list of different data types t = (12, \u0026quot;world\u0026quot;, [3,4,5]) print(t)  (12, 'world', [3, 4, 5])  Since tuples are ordered, we can access their element by calling the position of the element in the list.\nt[0]  12  Since tuples are unmutable, we cannot modify their elements.\n# Try to modify element try: t[0] = 'hello' except Exception as e: print(e)  'tuple' object does not support item assignment  # Try to add element try: t.append('hello') except Exception as e: print(e)  'tuple' object has no attribute 'append'  # Try to remove element try: del t[0] except Exception as e: print(e)  'tuple' object doesn't support item deletion  We can combine two tuples using +. Note that this operation does not modify the tuple but generates a new one. Also note that to generate a 1-element tuple we need to insert a comma.\nt + (23,)  (12, 'world', [3, 4, 5], 23)  We can generate tuples using comprehensions, but we need to specify it\u0026rsquo;s a tuple.\nt = tuple(n for n in range(3)) print(t)  (0, 1, 2)  Sets A set is a mutable array data structure in Python with the following characteristics:\n can only hold hashable types can hold different types of data at the same time cannot be modified cannot contain duplicates  We can generate using curly brackets.\ns = {12, \u0026quot;world\u0026quot;, (3,4,5)} print(s)  {(3, 4, 5), 12, 'world'}  Since sets are unordered and unindexed, we cannot access single elements by calling their position.\n# Try to access element by position try: s[0] except Exception as e: print(e)  'set' object is not subscriptable  Since sets are unordered, we cannot modify their elements by specifying the position.\n# Try to modify element try: s[0] = 'hello' except Exception as e: print(e)  'set' object does not support item assignment  However, since sets are mutable, we can add elements using .add().\ns.add('hello') print(s)  {'hello', (3, 4, 5), 12, 'world'}  However, we cannot add duplicates.\ns.add('hello') print(s)  {'hello', (3, 4, 5), 12, 'world'}  We can delete elements of a set using .remove().\ns.remove('hello') print(s)  {(3, 4, 5), 12, 'world'}  We can also generate sets using comprehensions.\ns = {n for n in range(3)} print(s)  {0, 1, 2}  Dictionaries A dictionary is a mutable array data structure in Python with the following characteristics:\n can hold any type can hold different types at the same time can be modified items are named  We can generate dictionaries using curly brackets. Since elements are indexed by keys, we have to provide one for each element.\nDictionary keys can be of any hashable type. A hashable object has a hash value that never changes during its lifetime, and it can be compared to other objects. Hashable objects that compare as equal must have the same hash value.\nImmutable types like strings and numbers are hashable and work well as dictionary keys. You can also use tuple objects as dictionary keys as long as they contain only hashable types themselves.\nd = {\u0026quot;first\u0026quot;: 12, 2: \u0026quot;world\u0026quot;, (3,): [3,4,5]} print(d)  {'first': 12, 2: 'world', (3,): [3, 4, 5]}  Since dictionaries are indexed but not ordered, we can only access elements by the corresponding key. If the corresponding key does not exist, we do not access any element.\ntry: d[0] except Exception as e: print(e)  0  We can access all values of the dictionary using .values().\nd.values()  dict_values([12, 'world', [3, 4, 5]])  We can access all keys of the dictionary using .keys().\nd.keys()  dict_keys(['first', 2, (3,)])  We can access both values and keys using .items().\nd.items()  dict_items([('first', 12), (2, 'world'), ((3,), [3, 4, 5])])  This gives us a list of tuples which we can iterate on.\n[f\u0026quot;{key}: {value}\u0026quot; for key, value in d.items()]  ['first: 12', '2: world', '(3,): [3, 4, 5]']  Since dictionaries are mutable, we can modify their elements.\nd[\u0026quot;first\u0026quot;] = 'hello' print(d)  {'first': 'hello', 2: 'world', (3,): [3, 4, 5]}  If we try to modify an element that does not exist, the element is added to the dictionary.\nd[0] = 'hello' print(d)  {'first': 'hello', 2: 'world', (3,): [3, 4, 5], 0: 'hello'}  We can remove elements using del.\ndel d[0] print(d)  {'first': 'hello', 2: 'world', (3,): [3, 4, 5]}  We can cannot combine two dictionaries using +. We can only add one element at the time.\ntry: d + {\u0026quot;fourth\u0026quot;: (1,2)} except Exception as e: print(e)  unsupported operand type(s) for +: 'dict' and 'dict'  We can also generate dictionaries using comprehensions.\nd = {f\u0026quot;k{n}\u0026quot;: n+1 for n in range(3)} print(d)  {'k0': 1, 'k1': 2, 'k2': 3}  Numpy Arrays A numpy array is a mutable array data structure in Python with the following characteristics:\n can hold any type of data can only hold one type at the same time can be modified can be multi-dimensional support matrix operations  We can generate numpy arrays are generated the np.array() function on a list.\na = np.array([1,2,3]) print(a)  [1 2 3]  We can make 2-dimensional arrays (a matrix) as lists of lists.\nm = np.array([[1,2,3] , [4,5,6]]) print(m)  [[1 2 3] [4 5 6]]  Since numpy arrays are mutable, we can modify elements by calling the index of the numpy array.\nm[0,0] = 89 print(m)  [[89 2 3] [ 4 5 6]]  We can check the shape of a numpy array using .shape\nm.shape  (2, 3)  We can expand dimensions of a numpy array using .expand_dims().\na = np.expand_dims(a,1) print(a)  [[1] [2] [3]]  We can transpose matrices using .T.\na = a.T print(a)  [[1 2 3]]  We add elements to a numpy array using np.concatenate(). All elements must have the same number of dimensions and must have the same number of elements along the concatenation axis.\nm = np.concatenate((m, a), axis=0) print(m)  [[89 2 3] [ 4 5 6] [ 1 2 3]]  We cannot remove elements of numpy arrays.\ntry: del a[0] except Exception as e: print(e)  cannot delete array elements  We can do matrix operations between numpy arrays. For example, we can do multiplication with @.\na @ m  array([[100, 18, 24]])  If we use * we get dot (or element-wise) multiplication instead.\na * m  array([[89, 4, 9], [ 4, 10, 18], [ 1, 4, 9]])  There is a wide array of functions available in numpy. For example np.invert() inverts a squared matrix.\nnp.invert(m)  array([[-90, -3, -4], [ -5, -6, -7], [ -2, -3, -4]])  Comparison Which data structure should you use and why? Let\u0026rsquo;s compare different data types\nK = 100_000 l = [n for n in range(K)] t = tuple(n for n in range(K)) s = {n for n in range(K)} a = np.array([n for n in range(K)])  Size\nWhich data type is more efficient?\nimport sys def compare_size(list_objects): for o in list_objects: print(f\u0026quot;Size of {type(o)}: {sys.getsizeof(o)}\u0026quot;)  compare_size([l, t, s, a])  Size of \u0026lt;class 'list'\u0026gt;: 800984 Size of \u0026lt;class 'tuple'\u0026gt;: 800040 Size of \u0026lt;class 'set'\u0026gt;: 4194520 Size of \u0026lt;class 'numpy.ndarray'\u0026gt;: 800104  Size is very similar for lists, tuples and numpy arrays.\nSpeed\nWhich data type is faster?\nimport time def compare_time(list_objects): for o in list_objects: start = time.time() [x**2 for x in o] end = time.time() print(f\u0026quot;Time of {type(o)}: {end - start}\u0026quot;)  compare_time([l, t, s, a])  Time of \u0026lt;class 'list'\u0026gt;: 0.021067142486572266 Time of \u0026lt;class 'tuple'\u0026gt;: 0.02071404457092285 Time of \u0026lt;class 'set'\u0026gt;: 0.021012067794799805 Time of \u0026lt;class 'numpy.ndarray'\u0026gt;: 0.010493993759155273  Numpy arrays are faster at math operations.\n","date":1651363200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1651363200,"objectID":"33f7e685bb800e80f98e57773c27fa92","permalink":"https://matteocourthoud.github.io/course/data-science/01_data_structures/","publishdate":"2022-05-01T00:00:00Z","relpermalink":"/course/data-science/01_data_structures/","section":"course","summary":"import numpy as np import pandas as pd  For most of this course we are going to work with pandas DataFrames. However, it\u0026rsquo;s important to start with an introduction to the different types of data structures available in Python, their characteristics, their differences and their comparative advantages.","tags":null,"title":"Data Structures","type":"book"},{"authors":null,"categories":null,"content":"This chapter follows closely Chapter 3 of An Introduction to Statistical Learning by James, Witten, Tibshirani, Friedman.\n# Remove warnings import warnings warnings.filterwarnings('ignore')  # Import everything import pandas as pd import numpy as np import seaborn as sns import statsmodels.api as sm from sklearn.linear_model import LinearRegression from numpy.linalg import inv from numpy.random import normal as rnorm from statsmodels.stats.outliers_influence import OLSInfluence  # Setup matplotlib for graphs import matplotlib.pyplot as plt from mpl_toolkits.mplot3d import axes3d # Set global parameters %matplotlib inline plt.style.use('seaborn-white') plt.rcParams['lines.linewidth'] = 3 plt.rcParams['figure.figsize'] = (10,6) plt.rcParams['figure.titlesize'] = 20 plt.rcParams['axes.titlesize'] = 18 plt.rcParams['axes.labelsize'] = 14 plt.rcParams['legend.fontsize'] = 14  You can inspect all the available global parameter options here.\n1.1 Simple Linear Regression First, let\u0026rsquo;s load the Advertising dataset. It contains information on displays sales (in thousands of units) for a particular product and a list of advertising budgets (in thousands of dollars) for TV, radio, and newspaper media.\nWe open the dataset using the pandas library which is the library for handling datasets and data analysis in Python.\n# Advertisement spending data advertising = pd.read_csv('data/Advertising.csv', usecols=[1,2,3,4])  Let\u0026rsquo;s have a look at the content. We can have a glance at the first rows by using the function head.\n# Preview of the data advertising.head()   .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }  \n  TV Radio Newspaper Sales     0 230.1 37.8 69.2 22.1   1 44.5 39.3 45.1 10.4   2 17.2 45.9 69.3 9.3   3 151.5 41.3 58.5 18.5   4 180.8 10.8 58.4 12.9     We can have a general overview of the dataset using the function info.\n# Overview of all variables advertising.info()  \u0026lt;class 'pandas.core.frame.DataFrame'\u0026gt; RangeIndex: 200 entries, 0 to 199 Data columns (total 4 columns): # Column Non-Null Count Dtype --- ------ -------------- ----- 0 TV 200 non-null float64 1 Radio 200 non-null float64 2 Newspaper 200 non-null float64 3 Sales 200 non-null float64 dtypes: float64(4) memory usage: 6.4 KB  We can have more information on the single variables using the function describe.\n# Summary of all variables advertising.describe()   .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }  \n  TV Radio Newspaper Sales     count 200.000000 200.000000 200.000000 200.000000   mean 147.042500 23.264000 30.554000 14.022500   std 85.854236 14.846809 21.778621 5.217457   min 0.700000 0.000000 0.300000 1.600000   25% 74.375000 9.975000 12.750000 10.375000   50% 149.750000 22.900000 25.750000 12.900000   75% 218.825000 36.525000 45.100000 17.400000   max 296.400000 49.600000 114.000000 27.000000     If you just want to call a variable in pandas, you have 3 options:\n use squared brackets as if the varaible was a component of a dictionary use or dot subscripts as if the variable was a function of the data use the loc function (best practice)  # 1. Brackets advertising['TV']  0 230.1 1 44.5 2 17.2 3 151.5 4 180.8 ... 195 38.2 196 94.2 197 177.0 198 283.6 199 232.1 Name: TV, Length: 200, dtype: float64  # 2. Brackets advertising.TV  0 230.1 1 44.5 2 17.2 3 151.5 4 180.8 ... 195 38.2 196 94.2 197 177.0 198 283.6 199 232.1 Name: TV, Length: 200, dtype: float64  # The loc function advertising.loc[:,'TV']  0 230.1 1 44.5 2 17.2 3 151.5 4 180.8 ... 195 38.2 196 94.2 197 177.0 198 283.6 199 232.1 Name: TV, Length: 200, dtype: float64  Note that the loc function is more powerful and is generally used to subset lines and columns.\n# Select multiple columns and subset of rows advertising.loc[0:5,['Sales','TV']]   .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }  \n  Sales TV     0 22.1 230.1   1 10.4 44.5   2 9.3 17.2   3 18.5 151.5   4 12.9 180.8   5 7.2 8.7     Suppose we are interested in the (linear) relationship between sales and tv advertisement.\n$$ sales ≈ \\beta_0 + \\beta_1 TV. $$\nHow are the two two variables related? Visual inspection: scatterplot.\n# Figure 3.1 def make_fig_3_1a(): # Init figure fig, ax = plt.subplots(1,1) ax.set_title('Figure 3.1'); # Plot scatter and best fit line sns.regplot(x=advertising.TV, y=advertising.Sales, ax=ax, order=1, ci=None, scatter_kws={'color':'r', 's':20}) ax.set_xlim(-10,310); ax.set_ylim(ymin=0) ax.legend(['Least Squares Fit','Data']);  make_fig_3_1a()  Estimating the Coefficients How do we estimate the best fit line? Minimize the Residual Sum of Squares (RSS).\nFirst, suppose we have a dataset $\\mathcal D = {x_i, y_i}_{i=1}^N$. We define the prediction of $y$ based on $X$ as\n$$ \\hat y_i = \\hat \\beta X_i $$\nThe residuals are the unexplained component of $y$\n$$ e_i = y_i - \\hat y_i $$\nOur objective function (to be minimized) is the Resdual Sum of Squares (RSS):\n$$ RSS := \\sum_{n=1}^N e_i^2 $$\nAnd the OLS coefficient is defined as its minimizer:\n$$ \\hat \\beta_{OLS} := \\arg\\min_{\\beta} \\sum_{n=1}^N e_i^2 = \\arg\\min_{\\beta} \\sum_{n=1}^N (y_i - X_i \\beta)^2 $$\nLet\u0026rsquo;s use the sklearn library to fit a linear regression model of Sales on TV advertisement.\n# Define X and y X = advertising.TV.values.reshape(-1,1) y = advertising.Sales.values # Fit linear regressions reg = LinearRegression().fit(X,y) print(reg.intercept_) print(reg.coef_)  7.0325935491276885 [0.04753664]  We can visualize the residuals as the vertical distances between the data and the prediction line. The objective function RSS is the sum of the squares of the lengths of vertical lines.\n# Compute predicted values y_hat = reg.predict(X) # Figure 3.1 def make_figure_3_1b(): # Init figure fig, ax = plt.subplots(1,1) ax.set_title('Figure 3.1'); # Add residuals sns.regplot(x=advertising.TV, y=advertising.Sales, ax=ax, order=1, ci=None, scatter_kws={'color':'r', 's':20}) ax.vlines(X, np.minimum(y,y_hat), np.maximum(y,y_hat), linestyle='--', color='k', alpha=0.5, linewidth=1) plt.legend(['Least Squares Fit','Data','Residuals']);  make_figure_3_1b()  The closed form solution in matrix algebra is $$ \\hat \\beta_{OLS} = (X\u0026rsquo;X)^{-1}(X\u0026rsquo;y) $$\nPython has a series of shortcuts to make the syntax less verbose. However, we still need to import the inv function from numpy. In Matlab it would be (X'*X)^{-1}*(X'*y), almost literal.\n# Compute OLS coefficient with matrix algebra beta = inv(X.T @ X) @ X.T @ y print(beta)  [0.08324961]  Why is the result different?\nWe are missing one coefficient: the intercept. Our regression now looks like this\n# New figure 1 def make_new_figure_1(): # Init figure fig, ax = plt.subplots(1,1) fig.suptitle('Role of the Intercept') # Add new line on the previous plot sns.regplot(x=advertising.TV, y=advertising.Sales, ax=ax, order=1, ci=None, scatter_kws={'color':'r', 's':10}) ax.plot(X, beta*X, color='g') plt.xlim(-10,310); plt.ylim(ymin=0); ax.legend(['With Intercept', 'Without intercept']);  make_new_figure_1()  How do we insert an intercept using matrix algebra? We add a column of ones.\n$$ X_1 = [\\boldsymbol{1}, X] $$\n# How to insert intercept? Add constant: column of ones one = np.ones(np.shape(X)) X1 = np.concatenate([one,X],axis=1) print(np.shape(X1))  (200, 2)  Now we compute again the coefficients as before.\n$$ \\hat \\beta_{OLS} = (X_1\u0026rsquo;X_1)^{-1}(X_1\u0026rsquo;y) $$\n# Compute beta OLS with intercept beta_OLS = inv(X1.T @ X1) @ X1.T @ y print(beta_OLS)  [7.03259355 0.04753664]  Now we have indeed obtained the same exact coefficients.\nWhat does minimizing the Residual Sum of Squares means in practice? How does the objective function looks like?\nfrom sklearn.preprocessing import scale # First, scale the data X = scale(advertising.TV, with_mean=True, with_std=False).reshape(-1,1) y = advertising.Sales regr = LinearRegression().fit(X,y) # Create grid coordinates for plotting B0 = np.linspace(regr.intercept_-2, regr.intercept_+2, 50) B1 = np.linspace(regr.coef_-0.02, regr.coef_+0.02, 50) xx, yy = np.meshgrid(B0, B1, indexing='xy') Z = np.zeros((B0.size,B1.size)) # Calculate Z-values (RSS) based on grid of coefficients for (i,j),v in np.ndenumerate(Z): Z[i,j] =((y - (xx[i,j]+X.ravel()*yy[i,j]))**2).sum()/1000 # Minimized RSS min_RSS = r'$\\beta_0$, $\\beta_1$ for minimized RSS' min_rss = np.sum((regr.intercept_+regr.coef_*X - y.values.reshape(-1,1))**2)/1000 min_rss  2.1025305831313514  # Figure 3.2 - Regression coefficients - RSS def make_fig_3_2(): fig = plt.figure(figsize=(15,6)) fig.suptitle('RSS - Regression coefficients') ax1 = fig.add_subplot(121) ax2 = fig.add_subplot(122, projection='3d') # Left plot CS = ax1.contour(xx, yy, Z, cmap=plt.cm.Set1, levels=[2.15, 2.2, 2.3, 2.5, 3]) ax1.scatter(regr.intercept_, regr.coef_[0], c='r', label=min_RSS) ax1.clabel(CS, inline=True, fontsize=10, fmt='%1.1f') # Right plot ax2.plot_surface(xx, yy, Z, rstride=3, cstride=3, alpha=0.3) ax2.contour(xx, yy, Z, zdir='z', offset=Z.min(), cmap=plt.cm.Set1, alpha=0.4, levels=[2.15, 2.2, 2.3, 2.5, 3]) ax2.scatter3D(regr.intercept_, regr.coef_[0], min_rss, c='r', label=min_RSS) ax2.set_zlabel('RSS') ax2.set_zlim(Z.min(),Z.max()) ax2.set_ylim(0.02,0.07) # settings common to both plots for ax in fig.axes: ax.set_xlabel(r'$\\beta_0$') ax.set_ylabel(r'$\\beta_1$') ax.set_yticks([0.03,0.04,0.05,0.06]) ax.legend()  make_fig_3_2()  Assessing the Accuracy of the Coefficient Estimates How accurate is our regression fit? Suppose we were drawing different (small) samples from the same data generating process, for example\n$$ y_i = 2 + 3x_i + \\varepsilon_i $$\nwhere $x_i \\sim N(0,1)$ and $\\varepsilon \\sim N(0,3)$.\n# Init N = 30; # Sample size K = 100; # Number of simulations beta_hat = np.zeros((2,K)) x = np.linspace(-4,4,N) # Set seed np.random.seed(1) # K simulations for i in range(K): # Simulate data x1 = np.random.normal(0,1,N).reshape([-1,1]) X = np.concatenate([np.ones(np.shape(x1)), x1], axis=1) epsilon = np.random.normal(0,5,N) beta0 = [2,3] y = X @ beta0 + epsilon # Estimate coefficients beta_hat[:,i] = inv(X.T @ X) @ X.T @ y  # new figure 2 def make_new_fig_2(): # Init figure fig, ax = plt.subplots(1,1) for i in range(K): # Plot line ax.plot(x, beta_hat[0,i] + x*beta_hat[1,i], color='blue', alpha=0.2, linewidth=1) if i==K-1: ax.plot(x, beta_hat[0,i] + x*beta_hat[1,i], color='blue', alpha=0.2, linewidth=1, label='Estimated Lines') # Plot true line ax.plot(x, 2 + 3*x, color='red', linewidth=3, label='True Line'); ax.set_xlabel('X'); ax.set_ylabel('Y'); ax.legend(); ax.set_xlim(-4,4);  make_new_fig_2()  The regplot command lets us automatically draw confidence intervals. Let\u0026rsquo;s draw the last simulated dataset with conficence intervals.\nfig, ax = plt.subplots(1,1) # Plot last simulation scatterplot with confidence interval sns.regplot(x=x1, y=y, ax=ax, order=1, scatter_kws={'color':'r', 's':20}); ax.set_xlabel('X'); ax.set_ylabel('Y'); ax.legend(['Best fit','Data', 'Confidence Intervals']);  As we can see, depending on the sample, we get a different estimate of the linear relationship between $x$ and $y$. However, there estimates are on average correct. Indeed, we can visualize their distribution.\n# Plot distribution of coefficients plot = sns.jointplot(x=beta_hat[0,:], y=beta_hat[1,:], color='red', edgecolor=\u0026quot;white\u0026quot;); plot.ax_joint.axvline(x=2); plot.ax_joint.axhline(y=3); plot.set_axis_labels('beta_0', 'beta_1');  How do we compute confidence intervals by hand?\n$$ Var(\\hat \\beta_{OLS}) = \\sigma^2 (X\u0026rsquo;X)^{-1} $$\nwhere $\\sigma^2 = Var(\\varepsilon)$. Since we do not know $Var(\\varepsilon)$, we estimate it as $Var(e)$.\n$$ \\hat Var(\\hat \\beta_{OLS}) = \\hat \\sigma^2 (X\u0026rsquo;X)^{-1} $$\nIf we assume the standard errors are normally distributed (or we apply the Central Limit Theorem, assuming $n \\to \\infty$), a 95% confidence interval for the OLS coefficient takes the form\n$$ CI(\\hat \\beta_{OLS}) = \\Big[ \\hat \\beta_{OLS} - 1.96 \\times \\hat SE(\\hat \\beta_{OLS}) \\ , \\ \\hat \\beta_{OLS} + 1.96 \\times \\hat SE(\\hat \\beta_{OLS}) \\Big] $$\nwhere $\\hat SE(\\hat \\beta_{OLS}) = \\sqrt{\\hat Var(\\hat \\beta_{OLS})}$.\n# Import again X and y from example above X = advertising.TV.values.reshape(-1,1) X1 = np.concatenate([np.ones(np.shape(X)), X], axis=1) y = advertising.Sales.values # Compute residual variance X_hat = X1 @ beta_OLS e = y - X_hat sigma_hat = np.var(e) var_beta_OLS = sigma_hat * inv(X1.T @ X1) # Take elements on the diagonal and square them std_beta_OLS = [var_beta_OLS[0,0]**.5, var_beta_OLS[1,1]**.5] print(std_beta_OLS)  [0.4555479737400674, 0.0026771203500466564]  The statsmodels library allows us to produce nice tables with parameter estimates and standard errors.\n# Table 3.1 \u0026amp; 3.2 est = sm.OLS.from_formula('Sales ~ TV', advertising).fit() est.summary().tables[1]    coef std err t P|t| [0.025 0.975]   Intercept  7.0326  0.458  15.360  0.000  6.130  7.935   TV  0.0475  0.003  17.668  0.000  0.042  0.053   Assessing the Accuracy of the Model What metrics can we use to assess whether the model is a good model, in terms of capturing the relationship between the variables?\nFirst, we can compute our objective function: the Residual Sum of Squares (RSS). Lower values of our objective function imply that we got a better fit.\n# RSS with regression coefficients RSS = sum(e**2) print(RSS)  2102.530583131351  The problem with RSS as a metric is that it\u0026rsquo;s hard to compare different regressions since its scale depends on the magnitude of the variables.\nOne measure of fit that does not depend on the magnitude of the variables is $R^2$: the percentage of our explanatory variable explained by the model\n$$ R^2 = 1 - \\frac{\\text{RSS}}{\\text{TSS}} $$\nwhere\n$$ TSS = \\sum_{i=1}^N (y_i - \\bar y)^2 $$\n# TSS TSS = sum( (y-np.mean(y))**2 ) # R2 R2 = 1 - RSS/TSS print(R2)  0.6118750508500709  Can the $R^2$ metric be negative? When?\n2.2 Multiple Linear Regression What if we have more than one explanatory variable? Spoiler: we already did, but one was a constant.\nLet\u0026rsquo;s have a look at the regression of Sales on Radio and TV advertisement expenditure separately.\n# Table 3.3 (1) est = sm.OLS.from_formula('Sales ~ Radio', advertising).fit() est.summary().tables[1]    coef std err t P|t| [0.025 0.975]   Intercept  9.3116  0.563  16.542  0.000  8.202  10.422   Radio  0.2025  0.020  9.921  0.000  0.162  0.243   # Table 3.3 (2) est = sm.OLS.from_formula('Sales ~ Newspaper', advertising).fit() est.summary().tables[1]    coef std err t P|t| [0.025 0.975]   Intercept  12.3514  0.621  19.876  0.000  11.126  13.577   Newspaper  0.0547  0.017  3.300  0.001  0.022  0.087   It seems that both Radio and Newspapers are positively correlated with Sales. Why don\u0026rsquo;t we estimate a unique regression with both dependent variables?\nEstimating the Regression Coefficients Suppose now we enrich our previous model adding all different forms of advertisement:\n$$ \\text{Sales} = \\beta_0 + \\beta_1 \\text{TV} + \\beta_2 \\text{Radio} + \\beta_3 \\text{Newspaper} + \\varepsilon $$\nWe estimate it using the statsmodels ols library.\n# Table 3.4 est = sm.OLS.from_formula('Sales ~ TV + Radio + Newspaper', advertising).fit() est.summary().tables[1]    coef std err t P|t| [0.025 0.975]   Intercept  2.9389  0.312  9.422  0.000  2.324  3.554   TV  0.0458  0.001  32.809  0.000  0.043  0.049   Radio  0.1885  0.009  21.893  0.000  0.172  0.206   Newspaper  -0.0010  0.006  -0.177  0.860  -0.013  0.011   Why now it seems that there is no relationship between Sales and Newspaper while the univariate regression told us the opposite?\nLet\u0026rsquo;s explore the correlation between those variables.\n# Table 3.5 - Correlation Matrix advertising.corr()   .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }  \n  TV Radio Newspaper Sales     TV 1.000000 0.054809 0.056648 0.782224   Radio 0.054809 1.000000 0.354104 0.576223   Newspaper 0.056648 0.354104 1.000000 0.228299   Sales 0.782224 0.576223 0.228299 1.000000     Let\u0026rsquo;s try to inspect the relationship visually. Note that now the linear best fit is going to be 3-dimensional. In order to make it visually accessible, we consider only on TV and Radio advertisement expediture as dependent variables. The best fit will be a plane instead of a line.\n# Fit regression est = sm.OLS.from_formula('Sales ~ Radio + TV', advertising).fit() print(est.params)  Intercept 2.921100 Radio 0.187994 TV 0.045755 dtype: float64  # Create a coordinate grid Radio = np.arange(0,50) TV = np.arange(0,300) B1, B2 = np.meshgrid(Radio, TV, indexing='xy') # Compute predicted plane Z = np.zeros((TV.size, Radio.size)) for (i,j),v in np.ndenumerate(Z): Z[i,j] =(est.params[0] + B1[i,j]*est.params[1] + B2[i,j]*est.params[2]) # Compute residuals e = est.predict() - advertising.Sales  # Figure 3.5 - Multiple Linear Regression def make_fig_3_5(): # Init figure fig = plt.figure() ax = axes3d.Axes3D(fig, auto_add_to_figure=False) fig.add_axes(ax) fig.suptitle('Figure 3.5'); # Plot best fit plane ax.plot_surface(B1, B2, Z, color='k', alpha=0.3) points = ax.scatter3D(advertising.Radio, advertising.TV, advertising.Sales, c=e, cmap=\u0026quot;seismic\u0026quot;, vmin=-5, vmax=5) plt.colorbar(points, cax=fig.add_axes([0.9, 0.1, 0.03, 0.8])) ax.set_xlabel('Radio'); ax.set_xlim(0,50) ax.set_ylabel('TV'); ax.set_ylim(bottom=0) ax.set_zlabel('Sales'); ax.view_init(20, 20)  make_fig_3_5()  Some Important Questions How do you check whether the model fit well the data with multiple regressors? statmodels and most regression packages automatically outputs more information about the least squares model.\n# Measires of fit est.summary().tables[0]  OLS Regression Results  Dep. Variable: Sales  R-squared:   0.897   Model: OLS  Adj. R-squared:   0.896   Method: Least Squares  F-statistic:   859.6   Date: Mon, 03 Jan 2022  Prob (F-statistic): 4.83e-98   Time: 18:28:21  Log-Likelihood:   -386.20   No. Observations:  200  AIC:   778.4   Df Residuals:  197  BIC:   788.3   Df Model:  2       Covariance Type: nonrobust       First measure: the F-test. The F-test tries to answe the question \u0026ldquo;Is There a Relationship Between the Response and Predictors?\u0026rdquo;\nIn particular, it tests the following hypothesis\n$$ H_1: \\text{is at least one coefficient different from zero?} $$\nagainst the null hypothesis\n$$ H_0: \\beta_0 = \\beta_1 = \u0026hellip; = 0 $$\nThis hypothesis test is performed by computing the F-statistic,\n$$ F=\\frac{(\\mathrm{TSS}-\\mathrm{RSS}) / p}{\\operatorname{RSS} /(n-p-1)} $$\nLet\u0026rsquo;s try to compute it by hand.\n# Init X = advertising[['Radio', 'TV']] y = advertising.Sales e = y - est.predict(X) RSS = np.sum(e**2) TSS = np.sum((y - np.mean(y))**2) (n,p) = np.shape(X) # Compute F F = ((TSS - RSS)/p) / (RSS/(n-p-1)) print('F = %.4f' % F)  F = 859.6177  A rule of thumb is to reject $H_0$ if $F \u0026gt; 10$.\nWe can also test that a particular subset of coefficients are equal to zero. In that case, we just substitute the Total Sum of Squares (TSS) with the Residual Sum of Squares under the null.\n$$ F=\\frac{(\\mathrm{RSS_0}-\\mathrm{RSS}) / p}{\\operatorname{RSS} /(n-p-1)} $$\ni.e. we perfome the regression under the null hypothesis and we compute\n$$ RSS_0 = \\sum_{n=1}^N (y_i - X_i \\beta)^2 \\quad s.t. \\quad H_0 $$\n2.3 Other Considerations in the Regression Model Qualitative Predictors What if some variables are qualitative instead of quantitative? Let\u0026rsquo;s change dataset and use the credit dataset.\n# Credit ratings dataset credit = pd.read_csv('data/Credit.csv', usecols=list(range(1,12)))  This dataset contains information on credit ratings, i.e. each person is assigned a Rating score based on his/her own individual characteristics.\nLet\u0026rsquo;s have a look at data types.\n# Summary credit.info()  \u0026lt;class 'pandas.core.frame.DataFrame'\u0026gt; RangeIndex: 400 entries, 0 to 399 Data columns (total 11 columns): # Column Non-Null Count Dtype --- ------ -------------- ----- 0 Income 400 non-null float64 1 Limit 400 non-null int64 2 Rating 400 non-null int64 3 Cards 400 non-null int64 4 Age 400 non-null int64 5 Education 400 non-null int64 6 Gender 400 non-null object 7 Student 400 non-null object 8 Married 400 non-null object 9 Ethnicity 400 non-null object 10 Balance 400 non-null int64 dtypes: float64(1), int64(6), object(4) memory usage: 34.5+ KB  As we can see, some variables like Gender, Student or Married are not numeric.\nWe can have a closer look at what these variables look like.\n# Look at data credit.head()   .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }  \n  Income Limit Rating Cards Age Education Gender Student Married Ethnicity Balance     0 14.891 3606 283 2 34 11 Male No Yes Caucasian 333   1 106.025 6645 483 3 82 15 Female Yes Yes Asian 903   2 104.593 7075 514 4 71 11 Male No No Asian 580   3 148.924 9504 681 3 36 11 Female No No Asian 964   4 55.882 4897 357 2 68 16 Male No Yes Caucasian 331     Let\u0026rsquo;s consider the variable Student. From a quick inspection it looks like it\u0026rsquo;s a binary Yes/No variable. Let\u0026rsquo;s check by listing all its values.\n# What values does the Student variable take? credit['Student'].unique()  array(['No', 'Yes'], dtype=object)  What happens if you pass a binary varaible to statsmodel? It automatically generates a dummy out of it.\n# Table 3.7 est = sm.OLS.from_formula('Balance ~ Student', credit).fit() est.summary().tables[1]    coef std err t P|t| [0.025 0.975]   Intercept  480.3694  23.434  20.499  0.000  434.300  526.439   Student[T.Yes]  396.4556  74.104  5.350  0.000  250.771  542.140   If a variable takes more than one value, statsmodel automatically generates a uniqe dummy for each level (-1).\n# Table 3.8 est = sm.OLS.from_formula('Balance ~ Ethnicity', credit).fit() est.summary().tables[1]    coef std err t P|t| [0.025 0.975]   Intercept  531.0000  46.319  11.464  0.000  439.939  622.061   Ethnicity[T.Asian]  -18.6863  65.021  -0.287  0.774  -146.515  109.142   Ethnicity[T.Caucasian]  -12.5025  56.681  -0.221  0.826  -123.935  98.930   Relaxing the Additive Assumption We have seen that both TV and Radio advertisement are positively associated with Sales. What if there is a synergy? For example it might be that if someone sees an ad both on TV and on the radio, s/he is much more likely to buy the product.\nConsider the following model\n$$ \\text{Sales} ≈ \\beta_0 + \\beta_1 \\text{TV} + \\beta_2 \\text{Radio} + \\beta_3 \\text{TV} \\times \\text{Radio} $$\nwhich can be rewritten as\n$$ \\text{Sales} ≈ \\beta_0 + (\\beta_1 + \\beta_3 \\text{Radio}) \\times \\text{TV} + \\beta_2 \\text{Radio} $$\nLet\u0026rsquo;s estimate the linear regression model, with the intercept.\n# Table 3.9 - Interaction Variables est = sm.OLS.from_formula('Sales ~ TV + Radio + TV*Radio', advertising).fit() est.summary().tables[1]    coef std err t P|t| [0.025 0.975]   Intercept  6.7502  0.248  27.233  0.000  6.261  7.239   TV  0.0191  0.002  12.699  0.000  0.016  0.022   Radio  0.0289  0.009  3.241  0.001  0.011  0.046   TV:Radio  0.0011  5.24e-05  20.727  0.000  0.001  0.001   A positive and significant interaction term indicates a hint of a sinergy effect.\nHeterogeneous Effects We can do interactions with qualitative variables as well. Conside the credit rating dataset.\nWhat if Balance depends by Income differently, depending on whether one is a Student or not?\nConsider the following model:\n$$ \\text{Balance} ≈ \\beta_0 + \\beta_1 \\text{Income} + \\beta_2 \\text{Student} + \\beta_3 \\text{Income} \\times \\text{Student} $$\nThe last coefficient $\\beta_3$ should tell us how much Balance increases in Income for Students with respect to non-Students.\nIndeed, we can decompose the regression in the following equivalent way:\n$$ \\text{Balance} ≈ \\beta_0 + \\beta_1 \\text{Income} + \\beta_2 \\text{Student} + \\beta_3 \\text{Income} \\times \\text{Student} $$\nwhich can be interpreted in the following way since Student is a binary variable\n  If the person is not a student $$ \\text{Balance} ≈ \\beta_0 + \\beta_1 \\text{Income} $$\n  If the person is a student $$ \\text{Balance} ≈ (\\beta_0 + \\beta_2) + (\\beta_1 + \\beta_3 ) \\text{Income} $$\n  We are allowing not only for a different intercept for Students, $\\beta_0 \\to \\beta_0 + \\beta_2$, but also for a different impact of Income, $\\beta_1 \\to \\beta_1 + \\beta_3$.\nWe can visually inspect the distribution of Income across the two groups.\n# Divide data into students and non-students x_student = credit.loc[credit.Student=='Yes','Income'] y_student = credit.loc[credit.Student=='Yes','Balance'] x_nonstudent = credit.loc[credit.Student=='No','Income'] y_nonstudent = credit.loc[credit.Student=='No','Balance']  # Make figure 3.8 def make_fig_3_8(): # Init figure fig, ax = plt.subplots(1,1) fig.suptitle('Figure 3.8') # Relationship betweeen income and balance for students and non-students ax.scatter(x=x_nonstudent, y=y_nonstudent, facecolors='None', edgecolors='k', alpha=0.5); ax.scatter(x=x_student, y=y_student, facecolors='r', edgecolors='r', alpha=0.7); ax.legend(['non-student', 'student']); ax.set_xlabel('Income'); ax.set_ylabel('Balance');  make_fig_3_8()  It is hard from the scatterplot to see whether there is a different relationship between income and balance for students and non-students.\nLet\u0026rsquo;s fit two separate regressions.\n# Interaction between qualitative and quantative variables est1 = sm.OLS.from_formula('Balance ~ Income + Student', credit).fit() reg1 = est1.params est2 = sm.OLS.from_formula('Balance ~ Income + Student + Income*Student', credit).fit() reg2 = est2.params print('Regression 1 - without interaction term') print(reg1) print('\\nRegression 2 - with interaction term') print(reg2)  Regression 1 - without interaction term Intercept 211.142964 Student[T.Yes] 382.670539 Income 5.984336 dtype: float64 Regression 2 - with interaction term Intercept 200.623153 Student[T.Yes] 476.675843 Income 6.218169 Income:Student[T.Yes] -1.999151 dtype: float64  Without the interaction term, the two lines have different levels but the same slope. Introducing an interaction term allows the two groups to have different responses to Income.\nWe can visualize the relationship in a graph.\n# Income (x-axis) income = np.linspace(0,150) # Balance without interaction term (y-axis) student1 = np.linspace(reg1['Intercept']+reg1['Student[T.Yes]'], reg1['Intercept']+reg1['Student[T.Yes]']+150*reg1['Income']) non_student1 = np.linspace(reg1['Intercept'], reg1['Intercept']+150*reg1['Income']) # Balance with iteraction term (y-axis) student2 = np.linspace(reg2['Intercept']+reg2['Student[T.Yes]'], reg2['Intercept']+reg2['Student[T.Yes]']+ 150*(reg2['Income']+reg2['Income:Student[T.Yes]'])) non_student2 = np.linspace(reg2['Intercept'], reg2['Intercept']+150*reg2['Income'])  # Figure 3.7 def make_fig_3_7(): fig, (ax1,ax2) = plt.subplots(1,2, figsize=(12,5)) fig.suptitle('Figure 3.7') # Plot best fit with and without interaction ax1.plot(income, student1, 'r', income, non_student1, 'k') ax2.plot(income, student2, 'r', income, non_student2, 'k') titles = ['Dummy', 'Dummy + Interaction'] for ax, t in zip(fig.axes, titles): ax.legend(['student', 'non-student'], loc=2) ax.set_xlabel('Income') ax.set_ylabel('Balance') ax.set_ylim(ymax=1550) ax.set_title(t)  make_fig_3_7()  Non-Linear Relationships What if we allow for further non-linearities? Let\u0026rsquo;s change dataset again and use the car dataset.\n# Automobile dataset (dropping missing values) auto = pd.read_csv('data/Auto.csv', na_values='?').dropna()  This dataset contains information of a wide variety of car models.\nauto.head()   .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }  \n  mpg cylinders displacement horsepower weight acceleration year origin name     0 18.0 8 307.0 130 3504 12.0 70 1 chevrolet chevelle malibu   1 15.0 8 350.0 165 3693 11.5 70 1 buick skylark 320   2 18.0 8 318.0 150 3436 11.0 70 1 plymouth satellite   3 16.0 8 304.0 150 3433 12.0 70 1 amc rebel sst   4 17.0 8 302.0 140 3449 10.5 70 1 ford torino     Suppose we wanted to understand which car caracteristics are correlated with higher efficiency, i.e. mpg (miles per gallon).\nConsider in particular the relationship between mpg and horsepower. It might be a highly non-linear relationship.\n$$ \\text{mpg} ≈ \\beta_0 + \\beta_1 \\text{horsepower} + \\beta_2 \\text{horsepower}^2 + \u0026hellip; ??? $$\nHow many terms should we include?\nLet\u0026rsquo;s look at the data to understand if it naturally suggests non-linearities.\nfig, ax = plt.subplots(1,1) # Plot polinomials of different degree plt.scatter(x=auto.horsepower, y=auto.mpg, facecolors='None', edgecolors='k', alpha=.3) plt.ylim(5,55); plt.xlim(40,240); plt.xlabel('horsepower'); plt.ylabel('mpg');  The relationship looks non-linear but in which way exactly? Let\u0026rsquo;s try to fit polinomials of different degrees.\ndef make_fig_38(): # Figure 3.8 fig, ax = plt.subplots(1,1) ax.set_title('Figure 3.8') # Plot polinomials of different degree plt.scatter(x=auto.horsepower, y=auto.mpg, facecolors='None', edgecolors='k', alpha=.3) sns.regplot(x=auto.horsepower, y=auto.mpg, ci=None, label='Linear', scatter=False, color='orange') sns.regplot(x=auto.horsepower, y=auto.mpg, ci=None, label='Degree 2', order=2, scatter=False, color='lightblue') sns.regplot(x=auto.horsepower, y=auto.mpg, ci=None, label='Degree 5', order=5, scatter=False, color='g') plt.legend() plt.ylim(5,55) plt.xlim(40,240);  make_fig_38()  As we can see, the tails are highly unstable depending on the specification.\nLet\u0026rsquo;s add a quadratic term\n# Table 3.10 auto['horsepower2'] = auto.horsepower**2 auto.head(3)   .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }  \n  mpg cylinders displacement horsepower weight acceleration year origin name horsepower2     0 18.0 8 307.0 130 3504 12.0 70 1 chevrolet chevelle malibu 16900   1 15.0 8 350.0 165 3693 11.5 70 1 buick skylark 320 27225   2 18.0 8 318.0 150 3436 11.0 70 1 plymouth satellite 22500     How does the regression change?\nest = sm.OLS.from_formula('mpg ~ horsepower + horsepower2', auto).fit() est.summary().tables[1]    coef std err t P|t| [0.025 0.975]   Intercept  56.9001  1.800  31.604  0.000  53.360  60.440   horsepower  -0.4662  0.031  -14.978  0.000  -0.527  -0.405   horsepower2  0.0012  0.000  10.080  0.000  0.001  0.001   Non-Linearities How can we assess if there are non-linearities and of which kind? We can look at the residuals.\nIf the residuals show some kind of pattern, probably we could have fit the line better. Moreover, we can use the pattern itself to understand how.\n# Linear fit X = auto.horsepower.values.reshape(-1,1) y = auto.mpg regr = LinearRegression().fit(X, y) auto['pred1'] = regr.predict(X) auto['resid1'] = auto.mpg - auto.pred1 # Quadratic fit X2 = auto[['horsepower', 'horsepower2']] regr.fit(X2, y) auto['pred2'] = regr.predict(X2) auto['resid2'] = auto.mpg - auto.pred2  # Figure 3.9 def make_fig_39(): fig, (ax1,ax2) = plt.subplots(1,2, figsize=(12,5)) fig.suptitle('Figure 3.9') # Left plot sns.regplot(x=auto.pred1, y=auto.resid1, lowess=True, ax=ax1, line_kws={'color':'r', 'lw':1}, scatter_kws={'facecolors':'None', 'edgecolors':'k', 'alpha':0.5}) ax1.hlines(0,xmin=ax1.xaxis.get_data_interval()[0], xmax=ax1.xaxis.get_data_interval()[1], linestyles='dotted') ax1.set_title('Residual Plot for Linear Fit') # Right plot sns.regplot(x=auto.pred2, y=auto.resid2, lowess=True, line_kws={'color':'r', 'lw':1}, ax=ax2, scatter_kws={'facecolors':'None', 'edgecolors':'k', 'alpha':0.5}) ax2.hlines(0,xmin=ax2.xaxis.get_data_interval()[0], xmax=ax2.xaxis.get_data_interval()[1], linestyles='dotted') ax2.set_title('Residual Plot for Quadratic Fit') for ax in fig.axes: ax.set_xlabel('Fitted values') ax.set_ylabel('Residuals')  make_fig_39()  It looks like the residuals from the linear fit (on the left) exibit a pattern:\n positive values at the tails negative values in the center  This suggests a quadratic fit. Indeed, the residuals when we include horsepower^2 (on the right) seem more uniformly centered around zero.\nOutliers Observations with high residuals have a good chance of being highly influentials. However, they do not have to be.\nLet\u0026rsquo;s use the following data generating process:\n $X \\sim N(0,1)$ $\\varepsilon \\sim N(0,0.5)$ $\\beta_0 = 3$ $y = \\beta_0 X + \\varepsilon$  np.random.seed(1) # Generate random y n = 50 X = rnorm(1,1,(n,1)) e = rnorm(0,0.5,(n,1)) b0 = 3 y = X*b0 + e  Now let\u0026rsquo;s change observation 20 so that it becomes an outlier, i.e. it has a high residual.\n# Generate outlier X[20] = 1 y[20] = 7 # Short regression without observation number 41 X_small = np.delete(X, 20) y_small = np.delete(y, 20)  Let\u0026rsquo;s now plot the data and the residuals\n# Figure 3.12 def make_fig_3_12(): # Init figure fig, (ax1,ax2) = plt.subplots(1,2, figsize=(12,5)) fig.suptitle('Figure 3.12') # Plot 1 ax1.scatter(x=X, y=y, facecolors='None', edgecolors='k', alpha=.5) sns.regplot(x=X, y=y, ax=ax1, order=1, ci=None, scatter=False, line_kws={'color':'r', 'lw':1}) sns.regplot(x=X_small, y=y_small, ax=ax1, order=1, ci=None, scatter=False, line_kws={'color':'b', 'lw':1}) ax1.set_xlabel('X'); ax1.set_ylabel('Y'); ax1.legend(['With obs. 20', 'Without obs. 20'], fontsize=12); # Hihglight outliers ax1.scatter(x=X[20], y=y[20], facecolors='None', edgecolors='r', alpha=1) ax1.annotate(\u0026quot;20\u0026quot;, (1.1, 7), color='r') # Compute fitted values and residuals r = regr.fit(X, y) y_hat = r.predict(X) e = np.abs(y - y_hat) # Plot 2 ax2.scatter(x=y_hat, y=e, facecolors='None', edgecolors='k', alpha=.5) ax2.set_xlabel('Fitted Values'); ax2.set_ylabel('Residuals'); ax2.hlines(0,xmin=ax2.xaxis.get_data_interval()[0], xmax=ax2.xaxis.get_data_interval()[1], linestyles='dotted',color='k') # Highlight outlier ax2.scatter(x=y_hat[20], y=e[20], facecolors='None', edgecolors='r', alpha=1) ax2.annotate(\u0026quot;20\u0026quot;, (2.2, 3.6), color='r');  make_fig_3_12()  High Leverage Points A better concept of \u0026ldquo;influential observation\u0026rdquo; is the Leverage, which represents how much an observation is distant from the others in terms of observables.\nThe leverage formula of observation $i$ is\n$$ h_i = x_i (X' X)^{-1} x_i' $$\nHowever, leverage alone is not necessarily enough for an observation to being highly influential.\nLet\u0026rsquo;s modify observation 41 so that it has a high leverage.\n# Generate observation with high leverage X[41] = 4 y[41] = 12 # Short regression without observation number 41 X_small = np.delete(X_small, 41) y_small = np.delete(y_small, 41) # Compute leverage H = X @ inv(X.T @ X) @ X.T h = np.diagonal(H) # Compute fitted values and residuals y_hat = X @ inv(X.T @ X) @ X.T @ y e = np.abs(y - y_hat)  What happens now that we have added an observation with high leverage? How does the levarage look like?\n# Figure 3.13 def make_fig_3_13(): # Init figure fig, (ax1,ax2) = plt.subplots(1,2, figsize=(12,5)) fig.suptitle('Figure 3.12') # Plot 1 ax1.scatter(x=X, y=y, facecolors='None', edgecolors='k', alpha=.5) ax1.scatter(x=X[[20,41]], y=y[[20,41]], facecolors='None', edgecolors='r', alpha=1) sns.regplot(x=X, y=y, ax=ax1, order=1, ci=None, scatter=False, line_kws={'color':'r', 'lw':1}) sns.regplot(x=X_small, y=y_small, ax=ax1, order=1, ci=None, scatter=False, line_kws={'color':'b', 'lw':1}) ax1.set_xlabel('X'); ax1.set_ylabel('Y'); ax1.axis(xmax=4.5); ax1.legend(['With obs. 20,41', 'Without obs. 20,41']); # Highlight points ax1.annotate(\u0026quot;20\u0026quot;, (1.1, 7), color='r') ax1.annotate(\u0026quot;41\u0026quot;, (3.6, 12), color='r'); # Plot 2 ax2.scatter(x=h, y=e, facecolors='None', edgecolors='k', alpha=.5) ax2.set_xlabel('Leverage'); ax2.set_ylabel('Residuals'); ax2.hlines(0,xmin=ax2.xaxis.get_data_interval()[0], xmax=ax2.xaxis.get_data_interval()[1], linestyles='dotted',color='k') # Highlight outlier ax2.scatter(x=h[[20,41]], y=e[[20,41]], facecolors='None', edgecolors='r', alpha=1); # Highlight points ax2.annotate(\u0026quot;20\u0026quot;, (0, 3.7), color='r') ax2.annotate(\u0026quot;41\u0026quot;, (0.14, 0.4), color='r');  make_fig_3_13()  Influential Observations As we have seen, being an outliers or having high leverage alone might be not enough to conclude that an observation is influential.\nWhat really matters is a combination of both: observations with high leverage and high residuals, i.e. observations that are not only different in terms of observables (high leverage) but are also different in terms of their relationship between observables and dependent variable (high residual).\nLet\u0026rsquo;s now modify observation 7 so that it is an outlier and has high leverage.\n# Generate outlier with high leverage X[7] = 4 y[7] = 7  # Short regression without observation number 41 X_small = np.delete(X, 7) y_small = np.delete(y, 7) # Compute leverage H = X @ inv(X.T @ X) @ X.T h = np.diagonal(H) # Compute fitted values and residuals r = regr.fit(X, y) y_hat = r.predict(X) e = np.abs(y - y_hat)  Now the best linear fit line has noticeably moved.\ndef make_fig_extra_3(): fig, (ax1,ax2) = plt.subplots(1,2, figsize=(12,5)) # Plot 1 ax1.scatter(x=X, y=y, facecolors='None', edgecolors='k', alpha=.5) ax1.scatter(x=X[[7,20,41]], y=y[[7,20,41]], facecolors='None', edgecolors='r', alpha=1) sns.regplot(x=X, y=y, ax=ax1, order=1, ci=None, scatter=False, line_kws={'color':'r', 'lw':1}) sns.regplot(x=X_small, y=y_small, ax=ax1, order=1, ci=None, scatter=False, line_kws={'color':'b', 'lw':1}) ax1.set_xlabel('X'); ax1.set_ylabel('Y'); ax1.axis(xmax=4.5); ax1.legend(['With obs. 7,20,41', 'Without obs. 7,20,41']); # Highlight points ax1.annotate(\u0026quot;7\u0026quot;, (3.7, 7), color='r') ax1.annotate(\u0026quot;20\u0026quot;, (1.15, 7.05), color='r') ax1.annotate(\u0026quot;41\u0026quot;, (3.6, 12), color='r'); # Plot 2 ax2.scatter(x=h, y=e, facecolors='None', edgecolors='k', alpha=.5) ax2.set_xlabel('Leverage'); ax2.set_ylabel('Residuals'); ax2.hlines(0,xmin=ax2.xaxis.get_data_interval()[0], xmax=ax2.xaxis.get_data_interval()[1], linestyles='dotted',color='k') # Highlight outlier ax2.scatter(x=h[[7,20,41]], y=e[[7,20,41]], facecolors='None', edgecolors='r', alpha=1); # Highlight points ax2.annotate(\u0026quot;7\u0026quot;, (0.12, 4.0), color='r'); ax2.annotate(\u0026quot;20\u0026quot;, (0, 3.8), color='r') ax2.annotate(\u0026quot;41\u0026quot;, (0.12, 0.9), color='r');  make_fig_extra_3()  Collinearity Collinearity is the situation in which two dependent varaibles are higly correlated with each other. Algebraically, this is a problem because the $X\u0026rsquo;X$ matrix becomes almost-non-invertible.\nLet\u0026rsquo;s have a look at the ratings dataset.\n# Inspect dataset sns.pairplot(credit[['Age', 'Balance', 'Limit', 'Rating']], height=1.8);  If we zoom into the variable Limit, we see that for example it is not very correlated with Age but is very correlated with Rating.\n# Figure 3.14 def make_fig_3_14(): # Init figure fig, (ax1,ax2) = plt.subplots(1,2, figsize=(12,5)) fig.suptitle('Figure 3.14') # Left plot ax1.scatter(credit.Limit, credit.Age, facecolor='None', edgecolor='brown') ax1.set_ylabel('Age') # Right plot ax2.scatter(credit.Limit, credit.Rating, facecolor='None', edgecolor='brown') ax2.set_ylabel('Rating') for ax in fig.axes: ax.set_xlabel('Limit') ax.set_xticks([2000,4000,6000,8000,12000])  make_fig_3_14()  If we regress Balance on Limit and Age, the coefficient of Limit is positive and highly significant.\n# Regress balance on limit and age reg1 = sm.OLS.from_formula('Balance ~ Limit + Age', credit).fit() reg1.summary().tables[1]    coef std err t P|t| [0.025 0.975]   Intercept  -173.4109  43.828  -3.957  0.000  -259.576  -87.246   Limit  0.1734  0.005  34.496  0.000  0.163  0.183   Age  -2.2915  0.672  -3.407  0.001  -3.614  -0.969   However, if we regress Balance on Limit and Rating, the coefficient of Limit is now not significant anymore.\n# Regress balance on limit and rating reg2 = sm.OLS.from_formula('Balance ~ Limit + Rating', credit).fit() reg2.summary().tables[1]    coef std err t P|t| [0.025 0.975]   Intercept  -377.5368  45.254  -8.343  0.000  -466.505  -288.569   Limit  0.0245  0.064  0.384  0.701  -0.101  0.150   Rating  2.2017  0.952  2.312  0.021  0.330  4.074   Looking at the objective function, the Residual Sum of Squares, helps understanding what is the problem.\n# First scale variables y = credit.Balance regr1 = LinearRegression().fit(scale(credit[['Age', 'Limit']].astype('float'), with_std=False), y) regr2 = LinearRegression().fit(scale(credit[['Rating', 'Limit']], with_std=False), y) # Create grid coordinates for plotting B_Age = np.linspace(regr1.coef_[0]-3, regr1.coef_[0]+3, 100) B_Limit = np.linspace(regr1.coef_[1]-0.02, regr1.coef_[1]+0.02, 100) B_Rating = np.linspace(regr2.coef_[0]-3, regr2.coef_[0]+3, 100) B_Limit2 = np.linspace(regr2.coef_[1]-0.2, regr2.coef_[1]+0.2, 100) X1, Y1 = np.meshgrid(B_Limit, B_Age, indexing='xy') X2, Y2 = np.meshgrid(B_Limit2, B_Rating, indexing='xy') Z1 = np.zeros((B_Age.size,B_Limit.size)) Z2 = np.zeros((B_Rating.size,B_Limit2.size)) Limit_scaled = scale(credit.Limit.astype('float'), with_std=False) Age_scaled = scale(credit.Age.astype('float'), with_std=False) Rating_scaled = scale(credit.Rating.astype('float'), with_std=False) # Calculate Z-values (RSS) based on grid of coefficients for (i,j),v in np.ndenumerate(Z1): Z1[i,j] =((y - (regr1.intercept_ + X1[i,j]*Limit_scaled + Y1[i,j]*Age_scaled))**2).sum()/1000000 for (i,j),v in np.ndenumerate(Z2): Z2[i,j] =((y - (regr2.intercept_ + X2[i,j]*Limit_scaled + Y2[i,j]*Rating_scaled))**2).sum()/1000000  # Figure 3.15 def make_fig_3_15(): # Init figure fig, (ax1,ax2) = plt.subplots(1,2, figsize=(12,5)) fig.suptitle('Figure 3.15') # Minimum min_RSS = r'$\\beta_0$, $\\beta_1$ for minimized RSS' # Left plot CS = ax1.contour(X1, Y1, Z1, cmap=plt.cm.Set1, levels=[21.25, 21.5, 21.8]) ax1.scatter(reg1.params[1], reg1.params[2], c='r', label=min_RSS) ax1.clabel(CS, inline=True, fontsize=10, fmt='%1.1f') ax1.set_ylabel(r'$\\beta_{Age}$') # Right plot CS = ax2.contour(X2, Y2, Z2, cmap=plt.cm.Set1, levels=[21.5, 21.8]) ax2.scatter(reg2.params[1], reg2.params[2], c='r', label=min_RSS) ax2.clabel(CS, inline=True, fontsize=10, fmt='%1.1f') ax2.set_ylabel(r'$\\beta_{Rating}$') #ax2.set_xticks([-0.1, 0, 0.1, 0.2]) for ax in fig.axes: ax.set_xlabel(r'$\\beta_{Limit}$') ax.legend()  make_fig_3_15()  As we can see, in the left plot the minimum is much better defined than in the right plot.\n","date":1646784000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1646784000,"objectID":"39506bbd73c328171beb64584765d130","permalink":"https://matteocourthoud.github.io/course/ml-econ/01_regression/","publishdate":"2022-03-09T00:00:00Z","relpermalink":"/course/ml-econ/01_regression/","section":"course","summary":"This chapter follows closely Chapter 3 of An Introduction to Statistical Learning by James, Witten, Tibshirani, Friedman.\n# Remove warnings import warnings warnings.filterwarnings('ignore')  # Import everything import pandas as pd import numpy as np import seaborn as sns import statsmodels.","tags":null,"title":"Linear Regression","type":"book"},{"authors":null,"categories":null,"content":"Basics Matrix Definition A real $n \\times m$ matrix $A$ is an array\n$$ A= \\begin{bmatrix} a_{11} \u0026amp; a_{12} \u0026amp; \\dots \u0026amp; a_{1m} \\newline a_{21} \u0026amp; a_{22} \u0026amp; \\dots \u0026amp; a_{2m} \\newline \\vdots \u0026amp; \\vdots \u0026amp; \\ddots \u0026amp; \\vdots \\newline a_{n1} \u0026amp; a_{n2} \u0026amp; \\dots \u0026amp; a_{nm} \\end{bmatrix} $$\nWe write $[A]_ {ij} = a_ {ij}$ to indicate the $(i,j)$-element of $A$.\n We will usually take the convention that a real vector $x \\in \\mathbb R^n$ is identified with an $n \\times 1$ matrix.\n The $n \\times n$ identity matrix $I_n$ is given by\n$$ [I_n] _ {ij} = \\begin{cases} 1 \\ \\ \\ \\text{if} \\ i=j \\newline 0 \\ \\ \\ \\text{if} \\ i \\neq j \\end{cases} $$\nFundamental Operations  Two $n \\times m$ matrices, $A,B$, are added element-wise so that $[A+B]_{ij} = [A] _{ij} + [B] _{ij}$. A matrix $A$ can be multiplied by a scalar $c\\in \\mathbb{R}$ in which case we set $[cA]_{ij} = c[A] _{ij}$. An $n \\times m$ matrix $A$ can be multiplied with an $m \\times p$ matrix $B$. The product $AB$ is defined according to the rule $[AB] _ {ij} = \\sum_{k=1}^m [A] _{ik} [B] _{kj}$. An $n \\times n$ matrix is invertible if there exists a matrix $B$ such that $AB=I$. In this case, we use the notational convention of writing $B = A^{-1}$. Matrix transposition is defined by $[A'] _{ij} = [A] _{ji}$.  Trace and Determinant The trace of a square matrix $A$ with dimension $n \\times n$ is $\\text{tr}(A) = \\sum_{i=1}^n a_{ii}$.\nThe determinant of a square $n \\times n$ matrix A is defined according to one of the following three (equivalent) definitions.\n Recursively as $det(A) = \\sum_{i=1}^n a_{ij} (-1)^{i+j} det([A]{-i,-j})$ where $[A]{-i,-j}$ is the matrix obtained by deleting the $i$th row and the $j$th column. $A \\mapsto det(A)$ under the unique alternating multilinear map on $n \\times n$ matrices such that $I \\mapsto 1$.  Linear Independence Vectors $x_1,\u0026hellip;,x_k$ are linearly independent if the only solution to the equation $b_1x_1 + \u0026hellip; + b_k x_k=0, \\ b_j \\in \\mathbb R$, is $b_1=b_2=\u0026hellip;=b_k=0$.\nUseful Identities  $(A+B)' =A'+B'$ $(AB)C = A(BC)$ $A(B+C) = AB+AC$ $(AB') = B\u0026rsquo;A'$ $(A^{-1})' = (A')^{-1}$ $(AB)^{-1} = B^{-1}A^{-1}$ $\\text{tr}(cA) = c\\text{tr}(A)$ $\\text{tr}(A+B) = \\text{tr}(A) + \\text{tr}(B)$ $\\text{tr}(AB) =\\text{tr}(BA)$ $det(I)=1$ $det(cA) = c^ndet(A)$ if $A$ is $n \\times n$ and $c \\in \\mathbb R$ $det(A) = det(A')$ $det(AB) = det(A)det(B)$ $det(A^{-1}) = (det(A))^{-1}$ $A^{-1}$ exists iff $det(A) \\neq 0$ $rank(A) = rank(A') = rank(A\u0026rsquo;A) = rank(AA')$ $A^{-1}$ exists iff $rank(A)=n$ for $A$ $n \\times n$ $rank(AB) \\leq \\min \\lbrace rank(A), rank(B) \\rbrace$  Matrix Rank The rank of a matrix, $rank(A)$ is equal to the maximal number of linearly independent rows for $A$.\nLet $A$ be an $n \\times n$ matrix. The $n \\times 1$ vector $x \\neq 0$ is an eigenvector of $A$ with corresponding eigenvalue $\\lambda$ is $Ax = \\lambda x$.\nDefinitions  A matrix $A$ is diagonal if $[A]_ {ij} \\neq 0$ only if $i=j$. An $n \\times n$ matrix $A$ is orthogonal if $A\u0026rsquo;A = I$ A matrix $A$ is symmetric if $[A]_ {ij} = [A]_ {ji}$. An $n \\times n$ matrix $A$ is idempotent if $A^2=A$. The matrix of zeros ($[A]_ {ij} =0$ for each $i,j$) is simply denoted 0. An $n \\times n$ matrix $A$ is nilpotent if $A^k=0$ for some integer $k\u0026gt;0$.  Spectral Decomposition Spectral Theorem Theorem: Let $A$ be an $n \\times n$ symmetric matrix. Then $A$ can be factored as $A = C \\Lambda C'$ where $C$ is orthogonal and $\\Lambda$ is diagonal.\nIf we postmultiply $A$ by $C$, we get\n $AC = C \\Lambda C\u0026rsquo;C$ and $AC = C \\Lambda$.   This is a matrix equation which can be split into columns. The $i$th column of the equation reads $A c_i = \\lambda_i c_i$ which corresponds to the definition of eigenvalues and eigenvectors. So if the decomposition exists, then $C$ is the eigenvector matrix and $\\Lambda$ contains the eigenvalues.\n Rank and Trace Theorem: The rank of a symmetric matrix equals the number of non zero eigenvalues.\nProof: $rank(A) = rank(C\\Lambda C') = rank(\\Lambda) = | \\lbrace i: \\lambda_i \\neq 0 \\rbrace |$. $$\\tag*{$\\blacksquare$}$$\nTheorem: The nonzero eigenvalues of $AA'$ and $A\u0026rsquo;A$ are identical.\nTheorem: The trace of a symmetric matrix equals the sum of its eignevalues.\nProof: $tr(A) = tr(C \\Lambda C') = tr((C \\Lambda)C') = tr(C\u0026rsquo;C \\Lambda) = tr(\\Lambda) = \\sum_ {i=1}^n \\lambda_i.$ $$\\tag*{$\\blacksquare$}$$\nTheorem: The determinant of a symmetric matrix equals the product of its eignevalues.\nProof: $det(A) = det(C \\Lambda C') = det(C)det(\\Lambda)det(C') = det(C)det(C')det(\\Lambda) = det(CC') det(\\Lambda) = det(I)det(\\Lambda) = det(\\Lambda) = \\prod_ {i=1}^n \\lambda_i.$ $$\\tag*{$\\blacksquare$}$$\nEigenvalues Theorem: For any symmetric matrix $A$, the eigenvalues of $A^2$ are the square of the eignevalues of $A$, and the eigenvectors are the same.\nProof: $A = C \\Lambda C' \\implies A^2 = C \\Lambda C' C \\Lambda C' = C \\Lambda I \\Lambda C' = C \\Lambda^2 C'$ $$\\tag*{$\\blacksquare$}$$\nTheorem: For any symmetric matrix $A$, and any integer $k\u0026gt;0$, the eigenvalues of $A^k$ are the $k$th power of the eigenvalues of $A$, and the eigenvectors are the same.\nTheorem: Any square symmetric matrix $A$ with positive eigenvalues can be written as the product of a lower triangular matrix $L$ and its (upper triangular) transpose $L' = U$. That is $A = LU = LL'$\n Note that $$ A = LL' = LU = U\u0026rsquo;U = (L')^{-1}L^{-1} = U^{-1}(U')^{-1} $$ where $L^{-1}$ is lower triangular and $U^{ -1}$ is upper trianguar. You can check this for the $2 \\times 2$ case. Also note that the validity of the theorem can be extended to symmetric matrices with non- negative eigenvalues by a limiting argument. However, then the proof is not constructive anymore.\n Quadratic Forms and Definite Matrices Definition A quadratic form in the $n \\times n$ matrix $A$ and $n \\times 1$ vector $x$ is defined by the scalar $x\u0026rsquo;Ax$.\n $A$ is negative definite (ND) if for each $x \\neq 0$, $x\u0026rsquo;Ax \u0026lt; 0$ $A$ is negative semidefinite (NSD) if for each $x \\neq 0$, $x\u0026rsquo;Ax \\leq 0$ $A$ is positive definite (PD) if for each $x \\neq 0$, $x\u0026rsquo;Ax \u0026gt; 0$ $A$ is positive semidefinite (PSD) if for each $x \\neq 0$, $x\u0026rsquo;Ax \\geq 0$  Equivalence Theorem: Let $A$ be a symmetric matrix. Then $A$ is PD(ND) $\\iff$ all of its eigenvalues are positive (negative).\nSome more results:\n If a symmetric matrix $A$ is PD (PSD, ND, NSD), then $\\text{det}(A) \u0026gt;(\\geq,\u0026lt;,\\leq) 0$. If symmetric matrix $A$ is PD (ND) then $A^{-1}$ is symmetric PD (ND). The identity matrix is PD (since all eigenvalues are equal to 1). Every symmetric idempotent matrix is PSD (since the eigenvalues are only 0 or 1).  Theorem: If $A$ is $n\\times k$ with $n\u0026gt;k$ and $rank(A)=k$, then $A\u0026rsquo;A$ is PD and $AA'$ is PSD.\nThe semidefinite partial order is defined by $A \\geq B$ iff $A-B$ is PSD.\nTheorem: Let $A$, $B$ be symmetric,square , PD, conformable. Then $A-B$ is PD iff $A^{-1}-B^{-1}$ is PD.\nMatrix Calculus Comformable Matrices We first define matrices blockwise when they are conformable. In particular, we assume that if $A_1, A_2, A_3, A_4$ are matrices with appropriate dimensions then the matrix $$ A = \\begin{bmatrix} A_1 \u0026amp; A_1 \\newline A_3 \u0026amp; A_4 \\end{bmatrix} $$ is defined in the obvious way.\nMatrix Functions Let $F: \\mathbb R^m \\times \\mathbb R^n \\rightarrow \\mathbb R^p \\times \\mathbb R^q$ be a matrix valued function. More precisely, given a real $m \\times n$ matrix $X$, $F(X)$ returns the $p \\times q$ matrix\n$$ \\begin{bmatrix} f_ {11}(X) \u0026amp; \u0026hellip; \u0026amp; f_ {1q}(X) \\newline \\vdots \u0026amp; \\ddots \u0026amp; \\vdots \\newline f_ {p1}(X)\u0026amp; \u0026hellip; \u0026amp; f_ {pq}(X) \\end{bmatrix} $$\nMatrix Derivatives The derivative of $F$ with respect to the matrix $X$ is the $mp \\times nq$ matrix $$ \\frac{\\partial F(X)}{\\partial X} = \\begin{bmatrix} \\frac{\\partial F(X)}{\\partial x_ {11}} \u0026amp; \u0026hellip; \u0026amp; \\frac{\\partial F(X)}{\\partial x_ {1n}} \\newline \\vdots \u0026amp; \\ddots \u0026amp; \\vdots \\newline \\frac{\\partial F(X)}{\\partial x_ {m1}} \u0026amp; \u0026hellip; \u0026amp; \\frac{\\partial F(X)}{\\partial x_ {mn}} \\end{bmatrix} $$ where each $\\frac{\\partial F(X)}{\\partial x_ {ij}}$ is a $p\\times q$ matrix given by\n$$ \\frac{\\partial F(X)}{\\partial x_ {ij}} = \\begin{bmatrix} \\frac{\\partial f_ {11}(X)}{\\partial x_ {ij}} \u0026amp; \u0026hellip; \u0026amp; \\frac{\\partial f_ {1q}(X)}{\\partial x_ {ij}} \\newline \\vdots \u0026amp; \\ddots \u0026amp; \\vdots \\newline \\frac{\\partial f_ {p1}(X)}{\\partial x_ {ij}} \u0026amp; \u0026hellip; \u0026amp; \\frac{\\partial f_ {pq}(X)}{\\partial x_ {ij}} \\end{bmatrix} $$ The most important case is when $F: \\mathbb R^n \\rightarrow \\mathbb R$ since this simplifies the derivation of the least squares estimator. Also, the trickiest thing is to make sure that dimensions are correct.\nUseful Results in Matrix Calculus  $\\frac{\\partial b\u0026rsquo;x}{\\partial x}= b$ for $dim(b) = dim(x)$ $\\frac{\\partial B\u0026rsquo;x}{\\partial x}= B$ for arbitrary, conformable $B$ $\\frac{\\partial B\u0026rsquo;x}{\\partial x'}= B'$ for arbitrary, conformable $B$ $\\frac{\\partial x\u0026rsquo;Ax}{\\partial x} = (A + A')x$ $\\frac{\\partial x\u0026rsquo;Ax}{\\partial A} = xx'$ $\\frac{\\partial x\u0026rsquo;Ax}{\\partial x} = det(A) (A^{-1})'$ $\\frac{\\partial \\ln det(A)}{\\partial A} = (A^{-1})'$  ","date":1635465600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1635465600,"objectID":"48eeb5e589225f9ba0ed86de5ea1d965","permalink":"https://matteocourthoud.github.io/course/metrics/01_matrices/","publishdate":"2021-10-29T00:00:00Z","relpermalink":"/course/metrics/01_matrices/","section":"course","summary":"Basics Matrix Definition A real $n \\times m$ matrix $A$ is an array\n$$ A= \\begin{bmatrix} a_{11} \u0026amp; a_{12} \u0026amp; \\dots \u0026amp; a_{1m} \\newline a_{21} \u0026amp; a_{22} \u0026amp; \\dots \u0026amp; a_{2m} \\newline \\vdots \u0026amp; \\vdots \u0026amp; \\ddots \u0026amp; \\vdots \\newline a_{n1} \u0026amp; a_{n2} \u0026amp; \\dots \u0026amp; a_{nm} \\end{bmatrix} $$","tags":null,"title":"Matrix Algebra","type":"book"},{"authors":null,"categories":null,"content":"import numpy as np import pandas as pd  Setup For the scope of this tutorial we are going to use AirBnb Scraped data for the city of Bologna. The data is freely available at Inside AirBnb: http://insideairbnb.com/get-the-data.html.\nA description of all variables in all datasets is avaliable here.\nWe are going to use 2 datasets:\n listing dataset: contains listing-level information pricing dataset: contains pricing data, over time  Importing Data Pandas has a variety of function to import data\n pd.read_csv() pd.read_html() pd.read_parquet()  Importatly for our purpose, pd.read_csv() can directly import data from the web.\nThe first dataset that we are going to import is the dataset of Airbnb listings in Bologna. It contains listing-level information.\nurl_listings = \u0026quot;http://data.insideairbnb.com/italy/emilia-romagna/bologna/2021-12-17/visualisations/listings.csv\u0026quot; df_listings = pd.read_csv(url_listings)  The second dataset that we are going to use is the dataset of calendar prices. This time the dataset is compressed but we can use the compression option to import it directly.\nurl_prices = \u0026quot;http://data.insideairbnb.com/italy/emilia-romagna/bologna/2021-12-17/data/calendar.csv.gz\u0026quot; df_prices = pd.read_csv(url_prices, compression=\u0026quot;gzip\u0026quot;)  Inspecting Data  Methods\n info() head() describe()   The first way yo have a quick look at the data is the info() method. If called with the option verbose=False, it gives a quick overview of the dimensions of the data.\ndf_listings.info(verbose=False)  \u0026lt;class 'pandas.core.frame.DataFrame'\u0026gt; RangeIndex: 3453 entries, 0 to 3452 Columns: 18 entries, id to license dtypes: float64(4), int64(8), object(6) memory usage: 485.7+ KB  If we want to know how the data looks like, we can use the head() method. It prints the first 5 lines of the data by default.\ndf_listings.head()   .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }  \n  id name host_id host_name neighbourhood_group neighbourhood latitude longitude room_type price minimum_nights number_of_reviews last_review reviews_per_month calculated_host_listings_count availability_365 number_of_reviews_ltm license     0 42196 50 sm Studio in the historic centre 184487 Carlo NaN Santo Stefano 44.48507 11.34786 Entire home/apt 68 3 180 2021-11-12 1.32 1 161 6 NaN   1 46352 A room in Pasolini's house 467810 Eleonora NaN Porto - Saragozza 44.49168 11.33514 Private room 29 1 300 2021-11-30 2.20 2 248 37 NaN   2 59697 COZY LARGE BEDROOM in the city center 286688 Paolo NaN Santo Stefano 44.48817 11.34124 Private room 50 1 240 2020-10-04 2.18 2 327 0 NaN   3 85368 Garden House Bologna 467675 Anna Maria NaN Santo Stefano 44.47834 11.35672 Entire home/apt 126 2 40 2019-11-03 0.34 1 332 0 NaN   4 145779 SINGLE ROOM 705535 Valerio NaN Porto - Saragozza 44.49306 11.33786 Private room 50 10 69 2021-12-05 0.55 9 365 5 NaN     We can print a description of the data using describe(). If we have many variables, it\u0026rsquo;s best to print it transposed using the .T attribute.\ndf_listings.describe().T[:5]   .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }  \n  count mean std min 25% 50% 75% max     id 3453.0 2.950218e+07 1.523988e+07 42196.0000 1.748597e+07 3.078707e+07 4.220094e+07 5.385496e+07   host_id 3453.0 1.236424e+08 1.160756e+08 38468.0000 2.550007e+07 8.845438e+07 2.005926e+08 4.354316e+08   neighbourhood_group 0.0 NaN NaN NaN NaN NaN NaN NaN   latitude 3453.0 4.449756e+01 1.173569e-02 44.4236 4.449186e+01 4.449699e+01 4.450271e+01 4.455093e+01   longitude 3453.0 1.134509e+01 1.986071e-02 11.2320 1.133732e+01 1.134519e+01 1.135406e+01 1.142027e+01     You can select which variables to display using the include option. include='all' includes also categorical variables.\ndf_listings.describe(include='all').T[:5]   .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }  \n  count unique top freq mean std min 25% 50% 75% max     id 3453.0 NaN NaN NaN 29502177.118158 15239877.346777 42196.0 17485973.0 30787074.0 42200938.0 53854962.0   name 3453 3410 Luxury Industrial Design LOFT, HEPA UV airpuri... 5 NaN NaN NaN NaN NaN NaN NaN   host_id 3453.0 NaN NaN NaN 123642405.854619 116075571.230048 38468.0 25500072.0 88454378.0 200592620.0 435431590.0   host_name 3444 747 Andrea 101 NaN NaN NaN NaN NaN NaN NaN   neighbourhood_group 0.0 NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN     We can get the list of columns using the .columns attribute.\ndf_listings.columns  Index(['id', 'name', 'host_id', 'host_name', 'neighbourhood_group', 'neighbourhood', 'latitude', 'longitude', 'room_type', 'price', 'minimum_nights', 'number_of_reviews', 'last_review', 'reviews_per_month', 'calculated_host_listings_count', 'availability_365', 'number_of_reviews_ltm', 'license'], dtype='object')  We can get the index using the .index attribute,\ndf_listings.index  RangeIndex(start=0, stop=3453, step=1)  Data Selection We can access single columns as if the DataFrame was a dictionary.\ndf_listings['price']  0 68 1 29 2 50 3 126 4 50 ... 3448 32 3449 45 3450 50 3451 134 3452 115 Name: price, Length: 3453, dtype: int64  We can select rows and columns by index, using the .iloc attribute.\ndf_listings.iloc[:7, 5:9]   .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }  \n  neighbourhood latitude longitude room_type     0 Santo Stefano 44.48507 11.34786 Entire home/apt   1 Porto - Saragozza 44.49168 11.33514 Private room   2 Santo Stefano 44.48817 11.34124 Private room   3 Santo Stefano 44.47834 11.35672 Entire home/apt   4 Porto - Saragozza 44.49306 11.33786 Private room   5 Navile 44.51628 11.33074 Private room   6 Santo Stefano 44.48787 11.35392 Entire home/apt     If we want to condition only on rows or columns, we have use : for the unrestricted dimesion, otherwise we get an error.\ndf_listings.iloc[:, 5:9].head()   .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }  \n  neighbourhood latitude longitude room_type     0 Santo Stefano 44.48507 11.34786 Entire home/apt   1 Porto - Saragozza 44.49168 11.33514 Private room   2 Santo Stefano 44.48817 11.34124 Private room   3 Santo Stefano 44.47834 11.35672 Entire home/apt   4 Porto - Saragozza 44.49306 11.33786 Private room     Instead, the .loc attribute allows us to use row and column names.\ndf_listings.loc[:, ['neighbourhood', 'latitude', 'longitude']].head()   .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }  \n  neighbourhood latitude longitude     0 Santo Stefano 44.48507 11.34786   1 Porto - Saragozza 44.49168 11.33514   2 Santo Stefano 44.48817 11.34124   3 Santo Stefano 44.47834 11.35672   4 Porto - Saragozza 44.49306 11.33786     We can also select ranges.\ndf_listings.loc[:, 'neighbourhood':'room_type'].head()   .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }  \n  neighbourhood latitude longitude room_type     0 Santo Stefano 44.48507 11.34786 Entire home/apt   1 Porto - Saragozza 44.49168 11.33514 Private room   2 Santo Stefano 44.48817 11.34124 Private room   3 Santo Stefano 44.47834 11.35672 Entire home/apt   4 Porto - Saragozza 44.49306 11.33786 Private room     There is an easy way to select numerical columns, the .select_dtypes() function.\ndf_listings.select_dtypes(include=['number']).head()   .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }  \n  id host_id neighbourhood_group latitude longitude price minimum_nights number_of_reviews reviews_per_month calculated_host_listings_count availability_365 number_of_reviews_ltm     0 42196 184487 NaN 44.48507 11.34786 68 3 180 1.32 1 161 6   1 46352 467810 NaN 44.49168 11.33514 29 1 300 2.20 2 248 37   2 59697 286688 NaN 44.48817 11.34124 50 1 240 2.18 2 327 0   3 85368 467675 NaN 44.47834 11.35672 126 2 40 0.34 1 332 0   4 145779 705535 NaN 44.49306 11.33786 50 10 69 0.55 9 365 5     Other types include\n object for strings bool for booleans int for integers float for floats (numbers that are not integers)  We can also use logical operators to selet rows.\ndf_listings.loc[df_listings['number_of_reviews']\u0026gt;500, :].head()   .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }  \n  id name host_id host_name neighbourhood_group neighbourhood latitude longitude room_type price minimum_nights number_of_reviews last_review reviews_per_month calculated_host_listings_count availability_365 number_of_reviews_ltm license     52 884148 APOSA FLAT / CITY CENTER - BO 4664996 Vie D'Acqua Di Sandra Maria NaN Santo Stefano 44.49945 11.34566 Entire home/apt 46 1 668 2021-12-11 6.24 5 252 20 NaN   92 1435627 heart of Bologna Piazza Maggiore 7714013 Carlotta NaN Porto - Saragozza 44.49321 11.33569 Entire home/apt 56 2 508 2021-12-12 5.08 1 131 69 NaN   98 1566003 \"i portici di via Piella \" 8325248 Massimo NaN Santo Stefano 44.49855 11.34411 Entire home/apt 51 2 764 2021-12-14 7.62 3 119 120 NaN   131 2282623 S.Orsola zone,parking for free and self check-in 11658074 Cecilia NaN San Donato - San Vitale 44.49328 11.36650 Entire home/apt 38 1 689 2021-10-24 7.20 1 5 72 NaN   175 3216486 Stanza Privata 16289536 Fabio NaN Navile 44.50903 11.34200 Private room 82 1 569 2021-12-05 6.93 1 7 5 NaN     We can use logical operations as well. But remember to use paranthesis.\nNote: the and and or expressions do not work in this setting. We have to use \u0026amp; and | instead.\ndf_listings.loc[(df_listings['number_of_reviews']\u0026gt;300) \u0026amp; (df_listings['reviews_per_month']\u0026gt;7), :].head()   .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }  \n  id name host_id host_name neighbourhood_group neighbourhood latitude longitude room_type price minimum_nights number_of_reviews last_review reviews_per_month calculated_host_listings_count availability_365 number_of_reviews_ltm license     98 1566003 \"i portici di via Piella \" 8325248 Massimo NaN Santo Stefano 44.498550 11.344110 Entire home/apt 51 2 764 2021-12-14 7.62 3 119 120 NaN   131 2282623 S.Orsola zone,parking for free and self check-in 11658074 Cecilia NaN San Donato - San Vitale 44.493280 11.366500 Entire home/apt 38 1 689 2021-10-24 7.20 1 5 72 NaN   204 4166793 Centralissimo a Bologna 8325248 Massimo NaN Santo Stefano 44.500920 11.344560 Entire home/apt 71 2 750 2021-12-10 9.21 3 233 84 NaN   751 15508481 Monolocale in zona fiera /centro 99632788 Walid NaN Navile 44.514462 11.353731 Entire home/apt 64 1 475 2021-12-01 7.56 1 4 48 NaN   773 15886516 Monolocale nel cuore del ghetto ebraico di Bol... 103024123 Catia NaN Santo Stefano 44.495080 11.347220 Entire home/apt 58 1 428 2021-12-15 7.88 1 285 17 NaN     For a single column (i.e. a Series), we can get the unique values using the unique() function.\ndf_listings['neighbourhood'].unique()  array(['Santo Stefano', 'Porto - Saragozza', 'Navile', 'San Donato - San Vitale', 'Savena', 'Borgo Panigale - Reno'], dtype=object)  For multiple columns, we can use the drop_duplicates function.\ndf_listings[['neighbourhood', 'room_type']].drop_duplicates()   .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }  \n  neighbourhood room_type     0 Santo Stefano Entire home/apt   1 Porto - Saragozza Private room   2 Santo Stefano Private room   5 Navile Private room   7 Navile Entire home/apt   8 Porto - Saragozza Entire home/apt   19 San Donato - San Vitale Private room   24 Savena Private room   36 Borgo Panigale - Reno Entire home/apt   41 San Donato - San Vitale Entire home/apt   70 Porto - Saragozza Hotel room   75 Borgo Panigale - Reno Private room   110 Santo Stefano Hotel room   111 Savena Entire home/apt   388 Porto - Saragozza Shared room   678 Navile Shared room   1393 Savena Shared room   1416 San Donato - San Vitale Shared room   1572 San Donato - San Vitale Hotel room   1637 Santo Stefano Shared room   1751 Navile Hotel room     Aggregation and Pivot Tables We can compute statistics by group using .groupby().\ndf_listings.groupby('neighbourhood')[['price', 'reviews_per_month']].mean()   .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }  \n  price reviews_per_month   neighbourhood       Borgo Panigale - Reno 83.020548 0.983488   Navile 142.200993 1.156745   Porto - Saragozza 129.908312 1.340325   San Donato - San Vitale 91.618138 0.933011   Santo Stefano 119.441841 1.344810   Savena 69.626016 0.805888     If you want to perform more than one function, maybe on different columns, you can use .aggregate() which can be shortened to .agg(). It takes as argument a dictionary with variables as keys and lists of functions as values.\ndf_listings.groupby('neighbourhood').agg({\u0026quot;reviews_per_month\u0026quot;: [\u0026quot;mean\u0026quot;], \u0026quot;price\u0026quot;: [\u0026quot;min\u0026quot;, np.max]}).reset_index()   .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead tr th { text-align: left; }  \n   neighbourhood reviews_per_month price     mean min amax     0 Borgo Panigale - Reno 0.983488 9 1429   1 Navile 1.156745 14 5000   2 Porto - Saragozza 1.340325 7 9999   3 San Donato - San Vitale 0.933011 10 1600   4 Santo Stefano 1.344810 11 9999   5 Savena 0.805888 9 680     The problem with this syntax is that it generates a hierarchical structure for variable names, which might not be so easy to work with. In the example above, to access the mean price, you have to use df.price[\u0026quot;min\u0026quot;].\nTo perform variable naming and aggregation and the same time, you can ise the following syntax: agg(output_var = (\u0026quot;input_var\u0026quot;, function)).\ndf_listings.groupby('neighbourhood').agg(mean_reviews=(\u0026quot;reviews_per_month\u0026quot;, \u0026quot;mean\u0026quot;), min_price=(\u0026quot;price\u0026quot;, \u0026quot;min\u0026quot;), max_price=(\u0026quot;price\u0026quot;, np.max)).reset_index()   .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }  \n  neighbourhood mean_reviews min_price max_price     0 Borgo Panigale - Reno 0.983488 9 1429   1 Navile 1.156745 14 5000   2 Porto - Saragozza 1.340325 7 9999   3 San Donato - San Vitale 0.933011 10 1600   4 Santo Stefano 1.344810 11 9999   5 Savena 0.805888 9 680     We can make pivot tables with the .pivot_table() function. It takes the folling arguments:\n index: rows columns: columns values: values aggfunc: aggregation function  df_listings.pivot_table(index='neighbourhood', columns='room_type', values='price', aggfunc='mean')   .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }  \n room_type Entire home/apt Hotel room Private room Shared room   neighbourhood         Borgo Panigale - Reno 96.700935 NaN 45.487179 NaN   Navile 172.140000 1350.000000 68.416107 28.0   Porto - Saragozza 148.410926 102.375000 83.070234 16.5   San Donato - San Vitale 106.775000 55.000000 61.194030 59.0   Santo Stefano 129.990260 103.827586 80.734177 95.4   Savena 86.301370 NaN 46.229167 22.5     ","date":1651363200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1651363200,"objectID":"3a99f2f89f6c5ffad6aa8f0bb403e587","permalink":"https://matteocourthoud.github.io/course/data-science/02_data_exploration/","publishdate":"2022-05-01T00:00:00Z","relpermalink":"/course/data-science/02_data_exploration/","section":"course","summary":"import numpy as np import pandas as pd  Setup For the scope of this tutorial we are going to use AirBnb Scraped data for the city of Bologna. The data is freely available at Inside AirBnb: http://insideairbnb.","tags":null,"title":"Data Exploration","type":"book"},{"authors":null,"categories":null,"content":"# Remove warnings import warnings warnings.filterwarnings('ignore')  # Import everything import pandas as pd import numpy as np import seaborn as sns import statsmodels.api as sm from numpy.linalg import inv from statsmodels.iolib.summary2 import summary_col from linearmodels.iv import IV2SLS  # Import matplotlib for graphs import matplotlib.pyplot as plt from mpl_toolkits.mplot3d import axes3d # Set global parameters %matplotlib inline plt.style.use('seaborn-white') plt.rcParams['lines.linewidth'] = 3 plt.rcParams['figure.figsize'] = (10,6) plt.rcParams['figure.titlesize'] = 20 plt.rcParams['axes.titlesize'] = 18 plt.rcParams['axes.labelsize'] = 14 plt.rcParams['legend.fontsize'] = 14  2.1 Simple Linear Regression In Acemoglu, Johnson, Robinson (2002), \u0026ldquo;The Colonial Origins of Comparative Development\u0026quot; the authors wish to determine whether or not differences in institutions can help to explain observed economic outcomes.\nHow do we measure institutional differences and economic outcomes?\nIn this paper,\n economic outcomes are proxied by log GDP per capita in 1995, adjusted for exchange rates. institutional differences are proxied by an index of protection against expropriation on average over 1985-95, constructed by the Political Risk Services Group.  These variables and other data used in the paper are available for download on Daron Acemoglu’s webpage.\nThe original dataset in in Stata .dta format but has been converted to .csv.\nFirst, let\u0026rsquo;s load the data and have a look at it.\n# Load Acemoglu Johnson Robinson Dataset df = pd.read_csv('data/AJR02.csv',index_col=0) df.head()   .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }  \n  GDP Exprop Mort Latitude Neo Africa Asia Namer Samer logMort Latitude2     1 8.39 6.50 78.20 0.3111 0 1 0 0 0 4.359270 0.096783   2 7.77 5.36 280.00 0.1367 0 1 0 0 0 5.634790 0.018687   3 9.13 6.39 68.90 0.3778 0 0 0 0 1 4.232656 0.142733   4 9.90 9.32 8.55 0.3000 1 0 0 0 0 2.145931 0.090000   5 9.29 7.50 85.00 0.2683 0 0 0 1 0 4.442651 0.071985     Let’s use a scatterplot to see whether any obvious relationship exists between GDP per capita and the protection against expropriation.\n# Plot relationship between GDP and expropriation rate fig, ax = plt.subplots(1,1) ax.set_title('Figure 1: joint distribution of GDP and expropriation') df.plot(x='Exprop', y='GDP', kind='scatter', s=50, ax=ax);  The plot shows a fairly strong positive relationship between protection against expropriation and log GDP per capita.\nSpecifically, if higher protection against expropriation is a measure of institutional quality, then better institutions appear to be positively correlated with better economic outcomes (higher GDP per capita).\nGiven the plot, choosing a linear model to describe this relationship seems like a reasonable assumption.\nWe can write our model as\n$$ {GDP}_i = \\beta_0 + \\beta_1 {Exprop}_i + \\varepsilon_i $$\nwhere:\n $ \\beta_0 $ is the intercept of the linear trend line on the y-axis $ \\beta_1 $ is the slope of the linear trend line, representing the marginal effect of protection against risk on log GDP per capita $ \\varepsilon_i $ is a random error term (deviations of observations from the linear trend due to factors not included in the model)  The most common technique to estimate the parameters ($ \\beta $’s) of the linear model is Ordinary Least Squares (OLS).\nAs the name implies, an OLS model is solved by finding the parameters that minimize the sum of squared residuals, i.e.\n$$ \\underset{\\hat{\\beta}}{\\min} \\sum^N_{i=1}{\\hat{u}^2_i} $$\nwhere $ \\hat{u}_i $ is the difference between the observation and the predicted value of the dependent variable.\nTo estimate the constant term $ \\beta_0 $, we need to add a column of 1’s to our dataset (consider the equation if $ \\beta_0 $ was replaced with $ \\beta_0 x_i $ and $ x_i = 1 $)\nNow we can construct our model in statsmodels using the OLS function.\nWe will use pandas dataframes with statsmodels, however standard arrays can also be used as arguments\n# Regress GDP on Expropriation Rate reg1 = sm.OLS.from_formula('GDP ~ Exprop', df) type(reg1)  statsmodels.regression.linear_model.OLS  So far we have simply constructed our model.\nWe need to use .fit() to obtain parameter estimates $ \\hat{\\beta}_0 $ and $ \\hat{\\beta}_1 $\n# Fit regression results = reg1.fit() type(results)  statsmodels.regression.linear_model.RegressionResultsWrapper  We now have the fitted regression model stored in results.\nTo view the OLS regression results, we can call the .summary() method.\nNote that an observation was mistakenly dropped from the results in the original paper (see the note located in maketable2.do from Acemoglu’s webpage), and thus the coefficients differ slightly.\nresults.summary()  OLS Regression Results  Dep. Variable: GDP  R-squared:   0.540   Model: OLS  Adj. R-squared:   0.532   Method: Least Squares  F-statistic:   72.71   Date: Mon, 03 Jan 2022  Prob (F-statistic): 4.84e-12   Time: 18:31:09  Log-Likelihood:   -68.214   No. Observations:  64  AIC:   140.4   Df Residuals:  62  BIC:   144.7   Df Model:  1       Covariance Type: nonrobust         coef std err t P|t| [0.025 0.975]   Intercept  4.6609  0.409  11.402  0.000  3.844  5.478   Exprop  0.5220  0.061  8.527  0.000  0.400  0.644    Omnibus:  7.134  Durbin-Watson:   2.081   Prob(Omnibus):  0.028  Jarque-Bera (JB):   6.698   Skew: -0.784  Prob(JB):   0.0351   Kurtosis:  3.234  Cond. No.   31.2  Warnings:[1] Standard Errors assume that the covariance matrix of the errors is correctly specified. From our results, we see that\n The intercept $ \\hat{\\beta}_0 = 4.63 $. The slope $ \\hat{\\beta}_1 = 0.53 $. The positive $ \\hat{\\beta}_1 $ parameter estimate implies that. institutional quality has a positive effect on economic outcomes, as we saw in the figure. The p-value of 0.000 for $ \\hat{\\beta}_1 $ implies that the effect of institutions on GDP is statistically significant (using p \u0026lt; 0.05 as a rejection rule). The R-squared value of 0.611 indicates that around 61% of variation in log GDP per capita is explained by protection against expropriation.  Using our parameter estimates, we can now write our estimated relationship as\n$$ \\widehat{GDP}_i = 4.63 + 0.53 \\ {Exprop}_i $$\nThis equation describes the line that best fits our data, as shown in Figure 2.\nWe can use this equation to predict the level of log GDP per capita for a value of the index of expropriation protection.\nFor example, for a country with an index value of 6.51 (the average for the dataset), we find that their predicted level of log GDP per capita in 1995 is 8.09.\nmean_expr = np.mean(df['Exprop']) mean_expr  6.5160937500000005  predicted_logpdp95 = results.params[0] + results.params[1] * mean_expr predicted_logpdp95  8.062499999999995  An easier (and more accurate) way to obtain this result is to use .predict() and set $ constant = 1 $ and $ {Exprop}_i = mean_expr $\nresults.predict(exog=[1, mean_expr])  We can obtain an array of predicted $ {GDP}_i $ for every value of $ {Exprop}_i $ in our dataset by calling .predict() on our results.\nPlotting the predicted values against $ {Exprop}_i $ shows that the predicted values lie along the linear line that we fitted above.\nThe observed values of $ {GDP}_i $ are also plotted for comparison purposes\n# Make first new figure def make_new_fig_2(): # Init figure fig, ax = plt.subplots(1,1) ax.set_title('Figure 2: OLS predicted values') # Drop missing observations from whole sample df_plot = df.dropna(subset=['GDP', 'Exprop']) sns.regplot(x=df_plot['Exprop'], y=df_plot['GDP'], ax=ax, order=1, ci=None, line_kws={'color':'r'}) ax.legend(['predicted', 'observed']) ax.set_xlabel('Exprop') ax.set_ylabel('GDP') plt.show()  make_new_fig_2()  ERROR! Session/line number was not unique in database. History logging moved to new session 305  2.2 Extending the Linear Regression Model So far we have only accounted for institutions affecting economic performance - almost certainly there are numerous other factors affecting GDP that are not included in our model.\nLeaving out variables that affect $ GDP_i $ will result in omitted variable bias, yielding biased and inconsistent parameter estimates.\nWe can extend our bivariate regression model to a multivariate regression model by adding in other factors that may affect $ GDP_i $.\n[AJR01] consider other factors such as:\n the effect of climate on economic outcomes; latitude is used to proxy this differences that affect both economic performance and institutions, eg. cultural, historical, etc.; controlled for with the use of continent dummies  Let’s estimate some of the extended models considered in the paper (Table 2) using data from maketable2.dta\n# Add constant term to dataset df['const'] = 1 # Create lists of variables to be used in each regression X1 = df[['const', 'Exprop']] X2 = df[['const', 'Exprop', 'Latitude', 'Latitude2']] X3 = df[['const', 'Exprop', 'Latitude', 'Latitude2', 'Asia', 'Africa', 'Namer', 'Samer']] # Estimate an OLS regression for each set of variables reg1 = sm.OLS(df['GDP'], X1, missing='drop').fit() reg2 = sm.OLS(df['GDP'], X2, missing='drop').fit() reg3 = sm.OLS(df['GDP'], X3, missing='drop').fit()  Now that we have fitted our model, we will use summary_col to display the results in a single table (model numbers correspond to those in the paper)\ninfo_dict={'No. observations' : lambda x: f\u0026quot;{int(x.nobs):d}\u0026quot;} results_table = summary_col(results=[reg1,reg2,reg3], float_format='%0.2f', stars = True, model_names=['Model 1','Model 2','Model 3'], info_dict=info_dict, regressor_order=['const','Exprop','Latitude','Latitude2']) results_table    Model 1 Model 2 Model 3   const 4.66*** 4.55*** 5.95***    (0.41) (0.45) (0.68)   Exprop 0.52*** 0.49*** 0.40***    (0.06) (0.07) (0.06)   Latitude  2.16 0.42     (1.68) (1.47)   Latitude2  -2.12 0.44     (2.86) (2.48)   Africa   -1.06**      (0.41)   Asia   -0.74*      (0.42)   Namer   -0.17      (0.40)   Samer   -0.12      (0.42)   No. observations 64 64 64   2.3 Endogeneity As [AJR01] discuss, the OLS models likely suffer from endogeneity issues, resulting in biased and inconsistent model estimates.\nNamely, there is likely a two-way relationship between institutions an economic outcomes:\n richer countries may be able to afford or prefer better institutions variables that affect income may also be correlated with institutional differences the construction of the index may be biased; analysts may be biased towards seeing countries with higher income having better institutions  To deal with endogeneity, we can use two-stage least squares (2SLS) regression, which is an extension of OLS regression.\nThis method requires replacing the endogenous variable $ {Exprop}_i $ with a variable that is:\n correlated with $ {Exprop}_i $ not correlated with the error term (ie. it should not directly affect the dependent variable, otherwise it would be correlated with $ u_i $ due to omitted variable bias)  We can write our model as\n$$ {GDP}_i = \\beta_0 + \\beta_1 {Exprop}_i + \\varepsilon_i \\ {Exprop}_i = \\delta_0 + \\delta_1 {logMort}_i + v_i $$\nThe new set of regressors logMort is called an instrument, which aims to remove endogeneity in our proxy of institutional differences.\nThe main contribution of [AJR01] is the use of settler mortality rates to instrument for institutional differences.\nThey hypothesize that higher mortality rates of colonizers led to the establishment of institutions that were more extractive in nature (less protection against expropriation), and these institutions still persist today.\nUsing a scatterplot (Figure 3 in [AJR01]), we can see protection against expropriation is negatively correlated with settler mortality rates, coinciding with the authors’ hypothesis and satisfying the first condition of a valid instrument.\n# Dropping NA's is required to use numpy's polyfit df2 = df.dropna(subset=['logMort', 'Exprop']) X = df2['logMort'] y = df2['Exprop']  # Make new figure 2 def make_new_figure_2(): # Init figure fig, ax = plt.subplots(1,1) ax.set_title('Figure 3: First-stage') # Fit a linear trend line sns.regplot(x=X, y=y, ax=ax, order=1, scatter=True, ci=None, line_kws={\u0026quot;color\u0026quot;: \u0026quot;r\u0026quot;}) ax.set_xlim([1.8,8.4]) ax.set_ylim([3.3,10.4]) ax.set_xlabel('Log of Settler Mortality') ax.set_ylabel('Average Expropriation Risk 1985-95');  make_new_figure_2()  The second condition may not be satisfied if settler mortality rates in the 17th to 19th centuries have a direct effect on current GDP (in addition to their indirect effect through institutions).\nFor example, settler mortality rates may be related to the current disease environment in a country, which could affect current economic performance.\n[AJR01] argue this is unlikely because:\n The majority of settler deaths were due to malaria and yellow fever and had a limited effect on local people. The disease burden on local people in Africa or India, for example, did not appear to be higher than average, supported by relatively high population densities in these areas before colonization.  As we appear to have a valid instrument, we can use 2SLS regression to obtain consistent and unbiased parameter estimates.\nFirst stage The first stage involves regressing the endogenous variable ($ {Exprop}_i $) on the instrument.\nThe instrument is the set of all exogenous variables in our model (and not just the variable we have replaced).\nUsing model 1 as an example, our instrument is simply a constant and settler mortality rates $ {logMort}_i $.\nTherefore, we will estimate the first-stage regression as\n$$ {Exprop}_i = \\delta_0 + \\delta_1 {logMort}_i + v_i $$\n# Add a constant variable df['const'] = 1 # Fit the first stage regression and print summary results_fs = sm.OLS(df['Exprop'], df.loc[:,['const', 'logMort']], missing='drop').fit() results_fs.summary()  OLS Regression Results  Dep. Variable: Exprop  R-squared:   0.274   Model: OLS  Adj. R-squared:   0.262   Method: Least Squares  F-statistic:   23.34   Date: Mon, 03 Jan 2022  Prob (F-statistic): 9.27e-06   Time: 18:31:10  Log-Likelihood:   -104.69   No. Observations:  64  AIC:   213.4   Df Residuals:  62  BIC:   217.7   Df Model:  1       Covariance Type: nonrobust         coef std err t P|t| [0.025 0.975]   const  9.3659  0.611  15.339  0.000  8.145  10.586   logMort  -0.6133  0.127  -4.831  0.000  -0.867  -0.360    Omnibus:  0.047  Durbin-Watson:   1.592   Prob(Omnibus):  0.977  Jarque-Bera (JB):   0.154   Skew:  0.060  Prob(JB):   0.926   Kurtosis:  2.792  Cond. No.   19.4  Warnings:[1] Standard Errors assume that the covariance matrix of the errors is correctly specified. We need to retrieve the predicted values of $ {Exprop}_i $ using .predict().\nWe then replace the endogenous variable $ {Exprop}_i $ with the predicted values $ \\widehat{Exprop}_i $ in the original linear model.\nOur second stage regression is thus\n$$ {GDP}_i = \\beta_0 + \\beta_1 \\widehat{Exprop}_i + u_i $$\nSecond stage # Second stage df['predicted_Exprop'] = results_fs.predict() results_ss = sm.OLS.from_formula('GDP ~ predicted_Exprop', df).fit() # Print results_ss.summary()  OLS Regression Results  Dep. Variable: GDP  R-squared:   0.462   Model: OLS  Adj. R-squared:   0.453   Method: Least Squares  F-statistic:   53.24   Date: Mon, 03 Jan 2022  Prob (F-statistic): 6.58e-10   Time: 18:31:10  Log-Likelihood:   -73.208   No. Observations:  64  AIC:   150.4   Df Residuals:  62  BIC:   154.7   Df Model:  1       Covariance Type: nonrobust         coef std err t P|t| [0.025 0.975]   Intercept  2.0448  0.830  2.463  0.017  0.385  3.705   predicted_Exprop  0.9235  0.127  7.297  0.000  0.671  1.177    Omnibus: 10.463  Durbin-Watson:   2.052   Prob(Omnibus):  0.005  Jarque-Bera (JB):   10.693   Skew: -0.806  Prob(JB):   0.00476   Kurtosis:  4.188  Cond. No.   57.8  Warnings:[1] Standard Errors assume that the covariance matrix of the errors is correctly specified. The second-stage regression results give us an unbiased and consistent estimate of the effect of institutions on economic outcomes.\nThe result suggests a stronger positive relationship than what the OLS results indicated.\nNote that while our parameter estimates are correct, our standard errors are not and for this reason, computing 2SLS ‘manually’ (in stages with OLS) is not recommended.\nWe can correctly estimate a 2SLS regression in one step using the linearmodels package, an extension of statsmodels\nNote that when using IV2SLS, the exogenous and instrument variables are split up in the function arguments (whereas before the instrument included exogenous variables)\n# IV regression iv = IV2SLS(dependent=df['GDP'], exog=df['const'], endog=df['Exprop'], instruments=df['logMort']).fit() # Print iv.summary  IV-2SLS Estimation Summary  Dep. Variable: GDP  R-squared:  0.2205   Estimator: IV-2SLS  Adj. R-squared:  0.2079   No. Observations: 64  F-statistic:  29.811   Date: Mon, Jan 03 2022  P-value (F-stat)  0.0000   Time: 18:31:10  Distribution:  chi2(1)   Cov. Estimator: robust             Parameter Estimates   Parameter Std. Err. T-stat P-value Lower CI Upper CI   const 2.0448 1.1273 1.8139 0.0697 -0.1647 4.2542   Exprop 0.9235 0.1691 5.4599 0.0000 0.5920 1.2550  Endogenous: ExpropInstruments: logMortRobust Covariance (Heteroskedastic)Debiased: False Given that we now have consistent and unbiased estimates, we can infer from the model we have estimated that institutional differences (stemming from institutions set up during colonization) can help to explain differences in income levels across countries today.\n[AJR01] use a marginal effect of 0.94 to calculate that the difference in the index between Chile and Nigeria (ie. institutional quality) implies up to a 7-fold difference in income, emphasizing the significance of institutions in economic development.\n2.4 Matrix Algebra The OLS parameter $ \\beta $ can also be estimated using matrix algebra and numpy.\nThe linear equation we want to estimate is (written in matrix form)\n$$ y = X\\beta + \\varepsilon $$\n# Init X = df[['const', 'Exprop']].values Z = df[['const', 'logMort']].values y = df['GDP'].values  To solve for the unknown parameter $ \\beta $, we want to minimize the sum of squared residuals\n$$ \\underset{\\hat{\\beta}}{\\min} \\ \\hat{\\varepsilon}'\\hat{\\varepsilon} $$\nRearranging the first equation and substituting into the second equation, we can write\n$$ \\underset{\\hat{\\beta}}{\\min} \\ (Y - X\\hat{\\beta})' (Y - X\\hat{\\beta}) $$\nSolving this optimization problem gives the solution for the $ \\hat{\\beta} $ coefficients\n$$ \\hat{\\beta} = (X\u0026rsquo;X)^{-1}X\u0026rsquo;y $$\n# Compute beta OLS beta_OLS = inv(X.T @ X) @ X.T @ y print(beta_OLS)  [4.66087966 0.52203367]  As we as see above, the OLS coefficient might suffer from endogeneity bias. We can solve the issue by instrumenting the predicted average expropriation rate with the average settler mortality.\nIf we define settler mortality as $Z$, our full model is\n$$ y = X\\beta + \\varepsilon \\ X = Z\\gamma + \\mu $$\nWhere we refer to the second equation as second stage and to the first equation as the reduced form equation. In our case, since the number of endogenous varaibles is equal to the number of insturments, there are two equivalent estimators that do not suffer from endogeneity bias: 2SLS and IV.\nIV, the one stage estimator\n$$ \\hat \\beta_{IV} = (Z\u0026rsquo;X)^{-1} Z' y $$\n# Compute beta IV beta_IV = inv(Z.T @ X) @ Z.T @ y print(beta_IV)  [2.0447613 0.92351936]  One of the hypothesis behind the IV estimator is the relevance of the instrument, i.e. we have a strong predictor in the first stage. This is the only hypothesis that we can empirically assess by checking the significance of the first stage coefficient.\n$$ \\hat \\gamma = (Z' Z)^{-1} Z\u0026rsquo;X \\ \\hat Var (\\hat \\gamma) = \\sigma_u^2 (Z' Z)^{-1} $$\nwhere\n$$ u = X - Z \\hat \\gamma $$\n# Estimate first stage coefficient gamma_hat = (inv(Z.T @ Z) @ Z.T @ X) print(gamma_hat[1,1])  -0.613289272386864  # Compute variance of the estimator u = X - Z @ gamma_hat var_gamma_hat = np.var(u) * inv(Z.T @ Z) # Compute standard errors std_gamma_hat = var_gamma_hat[1,1]**.5 print(std_gamma_hat)  0.08834733362858548  # Compute 95% confidence interval CI = [gamma_hat[1,1] - 1.96*std_gamma_hat, gamma_hat[1,1] + 1.96*std_gamma_hat] print(CI)  [-0.7864500462988916, -0.4401284984748365]  The first stage coefficient is negative and significant, i.e. settler mortality is negatively correlated with the expropriation rate.\nHow does it work when we have more instruments than endogenous variables? Two-State Least Squares.\n Regress $X$ on $Z$ and obtain $\\hat X$: $$ \\hat X = Z (Z' Z)^{-1} Z\u0026rsquo;X $$ Regress $Y$ on $\\hat X$ and obtain $\\hat \\beta_{2SLS}$ $$ \\hat \\beta_{2SLS} = (\\hat X' \\hat X)^{-1} \\hat X' y $$  In our case, just for the sake of exposition, let\u0026rsquo;s generate a second instrument: the settler mortality squared, logMort_2 = logMort^2.\ndf['logMort_2'] = df['logMort']**2  # Define Z Z1 = df[['const', 'logMort', 'logMort_2']].values # Compute beta 2SLS in two steps X_hat = Z1 @ inv(Z1.T @ Z1) @ Z1.T @ X beta_2SLS = inv(X_hat.T @ X_hat) @ X_hat.T @ y print(beta_2SLS)  [3.08817432 0.76339075]  The 2SLS estimator does not have to be actually estimated in two stages. Combining the two formulas above, we get\n$$ \\hat{\\beta} _ {2SLS} = \\Big( X\u0026rsquo;Z (Z\u0026rsquo;Z)^{-1} Z\u0026rsquo;X \\Big)^{-1} \\Big( X\u0026rsquo;Z (Z\u0026rsquo;Z)^{-1} Z\u0026rsquo;y \\Big) $$\nwhich can be computed in one step.\n# Compute beta 2SLS in one step beta_2SLS = inv(X_hat.T @ Z1 @ inv(Z1.T @ Z1) @ Z1.T @ X_hat) @ X_hat.T @ Z1 @ inv(Z1.T @ Z1) @ Z1.T @ y print(beta_2SLS)  [3.08817432 0.76339075]  ","date":1646784000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1646784000,"objectID":"1d1a4399e0bf93a4fbd939183d1d975f","permalink":"https://matteocourthoud.github.io/course/ml-econ/02_iv/","publishdate":"2022-03-09T00:00:00Z","relpermalink":"/course/ml-econ/02_iv/","section":"course","summary":"# Remove warnings import warnings warnings.filterwarnings('ignore')  # Import everything import pandas as pd import numpy as np import seaborn as sns import statsmodels.api as sm from numpy.linalg import inv from statsmodels.","tags":null,"title":"Instrumental Variables","type":"book"},{"authors":null,"categories":null,"content":"Introduction Setting Oligopoly Supply\n  firms produce differentiated goods/products\n  selling to consumers with heterogeneous preferences\n  static model, complete information\n  products are given\n  equilibrium: NE for each product/market\n    Cost Function Variable cost of product $j$: $C_j (Q_j , w_{jt} , \\mathbb \\omega_{jt}, \\gamma)$\n  $Q_j$: total quantity of good $j$ sold\n  $w_{jt}$ observable cost shifters; may include product characteristics $x_{jt}$ that will affect demand (later)\n  $\\omega_{jt}$ unobserved cost shifters (“cost shocks”); may be correlated with latent demand shocks (later)\n  $\\gamma$: parameters\n  Notes\n for multi-product firms, we’ll assume variable cost additive across products for simplicity we ignore fixed costs: these affect entry/exit/innovation but not pricing, conditional on these things  Notation Some other variables\n $J_t$: products/goods/choices in market $t$ (for now $J_t = J$) $P_t = (p_{1t},\u0026hellip;,p_{Jt})$: prices of all goods $\\boldsymbol X_t = ( \\boldsymbol x_{1t} , … , \\boldsymbol x_{Jt})$ : other characteristics of goods affecting demand (observed and unobserved to us)  In general\n I use bold for arrays in dimensions that are not $i$ (consumers), $j$ (firms) or $t$ (markets)  For example product characteristics $\\boldsymbol x_{jt} = \\lbrace x_{jt}^1,, \u0026hellip;, x_{jt}^K \\rbrace$   I use CAPS for variables aggregated over $j$ (firms)  For example vector of prices in market $t$: $P_t = (p_{1t},\u0026hellip;,p_{Jt})$    Equilibrium Pricing   Demand system:\n$$ q_{jt} = Q_j ( P_t, \\boldsymbol X_t) \\quad \\text{for} \\quad j = 1,\u0026hellip;,J. $$\n  Profit function\n$$ \\pi_{jt} = Q_j (P_t, \\boldsymbol X_t) \\Big[p_{jt} − mc_j (w_{jt}, \\omega_{jt}, \\gamma) \\Big] $$\n  FOC wrt to $p_{jt}$:\n$$ p_{jt} = mc_{jt} - Q_j (P_t, \\boldsymbol X_t) \\left(\\frac{\\partial Q_j}{\\partial p_{jt}}\\right)^{-1} $$\n  Inverse elasticity pricing (i.e., monopoly pricing) against the “residual demand curve” $Q_j (P_t, \\boldsymbol X_t)$:\n$$ \\frac{p_{jt} - mc_{jt}}{p_{jt}} = - \\frac{Q_j (P_t, \\boldsymbol X_t)}{p_{jt}} \\left(\\frac{\\partial Q_j}{\\partial p_{jt}}\\right)^{-1} $$\n  What do we get?   Holding all else fixed, markups/prices depend on the own-price elasticities of residual demand. Equilibrium depends, further, on how a change in price of one good affects the quantities sold of others, i.e., on cross-price demand elasticities\n  If we known demand, we can also perform a small miracle:\n  Re-arrange FOC\n$$ mc_{jt} = p_{jt} + Q_j (P_t, \\boldsymbol X_t)\\left(\\frac{\\partial Q_j}{\\partial p_{jt}}\\right)^{-1} $$\n  Supply model + estimated demand $\\to$ estimates of marginal costs!\n    If we know demand and marginal costs, we can”predict” a lot of stuff - i.e., give the quantitative implications of the model for counterfactual worlds\n  Issues   Typically we need to know levels/elasticities of demand at particular points; i.e., effects of one price change holding all else fixed\n  The main challenge: unobserved demand shifters (“demand shocks”) at the level of the good×market (e.g., unobserved product char or market-specific variation in mean tastes for products)\n  demand shocks are among the things that must be held fixed to measure the relevant demand elasticities etc.\n  explicit modeling of these demand shocks central in the applied IO literature following S. Berry, Levinsohn, and Pakes (1995) (often ignored outside this literature).\n  Key Challenge The demand of product $j$\n$$ q_{jt} (\\boldsymbol X_{t}, P_t, \\Xi_t) $$\ndepends on:\n  $P_t$: $J$-vector of all goods’ prices in market $t$\n  $\\boldsymbol X_t$: $J \\times k$ matrix of all non-price observables in market $t$\n  $\\Xi_t$: J-vector of demand shocks for all goods in market $t$\n  Key insight: we have an endogeneity problem even if prices were exogenous!\nPrice Endogeneity Adds to the Challenge   all $J$ endogenous prices are on RHS of demand for each good\n  equilibrium pricing implies that each price depends on all demand shocks and all cost shocks\n  prices endogenous\n  control function generally is not a valid solution\n    clear that we need sources of exogenous price variation, but\n  what exactly is required?\n  how do we proceed?\n    BLP: Model Goals of BLP Model of S. Berry, Levinsohn, and Pakes (1995)\n parsimonious specification to generate the distribution $F_U (\\cdot| P, \\Xi)$ of random utilities sufficiently rich heterogeneity in preferences to permit reasonable/flexible substitution patterns be explicit about unobservables, including the nature of endogeneity “problem(s)” use the model to reveal solutions to the identification problem, including appropriate instruments computationally feasible (in early 1990s!) algorithm for consistent estimation of the model and standard errors.  Utility Specification Utility of consumer $i$ for product $j$\n$$ u_{ijt} = \\boldsymbol x_{jt} \\boldsymbol \\beta_{it} - \\alpha p_{jt} + \\xi_{jt} + \\epsilon_{ijt} $$\nWhere\n  $\\boldsymbol x_{jt}$: $K$-vector of characteristics of product $j$ in market $t$\n  $\\boldsymbol \\beta_{it} = (\\beta_{it}^{1}, \u0026hellip;, \\beta_{it}^K)$: vector of tastes for characteristics $1,…,K$ in market $t$\n  $\\beta_{it}^k = \\beta_0^k + \\sigma_k \\zeta_{it}^k$\n  $\\beta_0^k$: fixed taste for characteristic $k$ (the usual $\\beta$)\n  $\\zeta_{it}^k$: random taste, i.i.d. across consumers and markets $t$\n      $\\alpha$: price elasticity\n  $p_{jt}$ price of product $j$ in market $t$\n  $\\xi_{jt}$: unobservable product shock at the level of products $j$ $\\times$ market $t$\n  $\\epsilon_{ijt}$: idiosyncratic (and latent) taste\n  Exogenous and Endogenous Product Characteristics Utility of consumer $i$ for product $j$\n$$ u_{ijt} = \\boldsymbol x_{jt} \\beta_{it} - \\alpha p_{jt} + \\xi_{jt} + \\epsilon_{ijt} $$\n  exogenous characteristics: $\\boldsymbol x_{jt} \\perp \\xi_{jt}$\n  endogenous characteristics: $p_{jt}$ (usually a scalar, price)\n typically each $p_{jt}$ will depend on whole vector $\\Xi_t = (\\xi_{1t} , . . . , \\xi_{Jt} )$  and on own costs $mc_{jt}$ and others’ costs $mc_{-jt}$   we need to distinguish true effects of prices on demand from the effects of $\\Xi_t$ ; this will require instruments of course the equation above is not an estimating equation ($u_{ijt}$ not observed) because prices and quantities are all endogenous - indeed determined - simultaneously, you may suspect (correctly) that instruments for prices alone may not suffice.    Utility Specification, Rewritten Rewrite\n$$ \\begin{align} u_{ijt} \u0026amp;= \\boldsymbol x_{jt} \\boldsymbol \\beta_{it} - \\alpha p_{jt} + \\xi_{jt} + \\epsilon_{ijt} = \\newline \u0026amp;= \\delta_{jt} + \\nu_{ijt} \\end{align} $$\nwhere\n $\\delta_{jt} = \\boldsymbol x_{jt} \\boldsymbol \\beta_0 - \\alpha p_{jt} + \\xi_{jt}$  mean utility of good $j$ in market $t$   $\\nu_{ijt} = \\sum_{k} x_{jt}^{k} \\sigma^{k} \\zeta_{i t}^{k} + \\epsilon_{ijt} \\equiv \\boldsymbol x_{jt} \\tilde{\\boldsymbol \\beta}{it} + \\epsilon{ijt}$  We split $\\beta_{it}$ into its random ($\\tilde{\\beta}_{it}$) and non-random ($\\beta_0$) part    From Consumer Utility to Demand With a continuum of consumers in each market: market shares = choice probabilities\n P.S. continuum not needed, enough that sampling error on choice probs negligible compared to that of moments based on variation across products/markets  $$ s_{jt} (\\Delta_t, \\boldsymbol X_t, \\boldsymbol \\sigma) = \\Pr (y_{it} = j) = \\int_{\\mathcal A_j (\\Delta_t)} \\text d F_{\\nu} \\Big(\\nu_{i0t}, \\nu_{i1t}, \u0026hellip; , \\nu_{iJt} \\ \\Big| \\ \\boldsymbol X_t, \\boldsymbol \\sigma \\Big) $$\n where $$ \\mathcal A_j(\\Delta_t) = \\Big\\lbrace (\\nu_{i0t}, \\nu_{i1t}, \u0026hellip; , \\nu_{iJt} ) \\in \\mathbb{R}^{J+1}: \\delta_{jt} + \\nu_{ijt} \\geq \\delta_{kt} + \\nu_{ikt} \\ , \\ \\forall k \\Big\\rbrace $$ In words: market share of firm $j$ is the frequency of consumers buying good $j$  Demand is just shares $s_{jt}$ per market size $M_t$ $$ q_{jt} = M_t \\times s_j (\\Delta_t, \\boldsymbol X_t, \\boldsymbol \\sigma) $$\nWhy Random Coefficients? Without random coefficients $$ \\begin{aligned} u_{ijt} \u0026amp;= \\underbrace{\\boldsymbol x_{jt} \\boldsymbol \\beta_0 - \\alpha p_{jt} + \\xi_{jt}} + \\epsilon_{ijt} \\newline \u0026amp;= \\hspace{3.4em} \\delta_{jt} \\hspace{3.4em} + \\epsilon_{ijt} \\end{aligned} $$ If $\\epsilon_{ijt}$ are iid and independent of $(\\boldsymbol X_t, P_t)$, e.g. as in the multinomial logit or probit models,\n products differ only in mean utilities $\\delta_{jt}$ $\\to$ market shares depend only on the mean utilities $\\to$ price elasticities (own and cross) depend only on mean utilities too  Implication: two products with the same market shares have the same cross elasticities w.r.t. all other products\nDoes this matter? Yes!\n Mercedes class-A and Fiat Panda might both have low market shares But realistically should have very different cross-price elasticities w.r.t. BMW series-2  What is the issue?\n  Models (like MNL) that have only iid additive taste shocks impose very restrictive relationships between the levels of market shares and the matrix of own and cross-price derivatives\n Impact on counterfactuals!    Restrictions only coming from model assumptions (analytical convenience)\n  Models always imporse restrictions\n necessary for estimation but must allow flexibility in the relevant dimensions    How do random coefficients help? In reality:\n goods differ in multiple dimensions consumers have (heterogeneous) preferences over these differences  How do random coefficients capture it?\n large $\\beta_i^k$ $\\leftrightarrow$ strong taste for characteristic $k$  e.g., maximum speed for sport car   Consumer $i$’s first choice likely to have high value of $x^k$ $i$’s second choice too!  Mark: cross elasticities are always about 1st vs. 2nd choices    Incorporating this allows more sensible substitution patterns\n competition is mostly “local” i.e., between firms offering products appealing to the same consumers.  Which random coefficients? Which characteristics have random coefficients?\n dummies for subsets of products?  S. T. Berry (1994): covers the nested logit as a special case   certain horizontal or vertical characteristics?  parts of $(\\boldsymbol X_t, P_t)$?    In practice\n Choice depends on the application and data set, including instruments Too many RC’s (w.r.t quantity of data available) $\\to$ imprecise estimates of $\\boldsymbol \\sigma$  BLP: Estimation Setting Observables\n $\\boldsymbol X_t$: product characteristics $P_t$: prices $S_t$: observed market shares $\\boldsymbol W_t$: observable cost shifters $\\boldsymbol Z_t$: excluded instruments  Sketch of procedure\n start with demand model alone suppose $ F_{\\nu{=tex}} (\\cdot {=tex} | \\boldsymbol {=tex}X, \\boldsymbol {=tex}\\sigma {=tex})$ is known (i.e., $\\sigma$ known) for each market $t$, find mean utilities $\\Delta_t \\in \\mathbb R$ such that $s_{jt} (\\Delta_t, \\boldsymbol X_t, \\boldsymbol \\sigma) = s^{obs}_{jt} \\ \\forall j$  i.e.,“invert” model at observed market shares to find mean utilities $\\boldsymbol \\delta$ where $s^{obs}_{jt}$ are the observed market shares   using IV ,e.g. $\\mathbb E [\\boldsymbol z_{jt} | \\xi_{jt} ] = 0$, estimate the equation  Issues  What instruments? Will the “inversion” step actually work? What about $\\boldsymbol \\sigma$?? Formal estimator? Computational algorithm(s)? Supply side  additional restrictions (moment conditions)  help estimation of demand   additional parameters: marginal cost function  why? may care directly and needed for counterfactuals that change equilibrium quantities unless $mc$ is constant      Instruments We need intruments for all endogenous variables—prices and quantities—independently.\n  Excluded cost shifters $\\boldsymbol W_t$ (classic)\n Usually: wages, material costs, shipping cost to market $t$, taxes/tariffs, demand shifters from other markets    Or proxies for them\n Usually: price of same good in another mkt (“Hausman instruments”)    Markup shifters:\n  Usually: characteristics of “nearby” markets (“Waldfogel instruments”)\n  Logic: income/age/education in San Francisco might affect prices in Oakland but might be independent fo Oakland preferences\n    Product characteristics of other firms in the same market $\\boldsymbol X_{-jt}$\n “BLP instruments” affect quantities directly; affect prices (markups) via equilibrium only    Inversion How do we get from market shares to prices??\nGiven x,σ and any positive shares sh, define the following mapping $\\Phi : \\mathbb R^j \\to \\mathbb R^j$ $$ \\Phi (\\Delta_t) = \\Delta_t + \\log\\Big( \\hat S^{obs}_t \\Big) - \\log \\Big( S_t (\\Delta_t, \\boldsymbol X_t, \\boldsymbol \\sigma) \\Big) $$ S. T. Berry (1994): for any nonzero shares sh, Φ is a contraction\n under mild conditions on the linear random coefficients random utility model extreme value and normal random coeff not necessary  What does it imply?\n It has a unique fixed point: we can compute $\\delta_{jt} = \\delta (S_t, \\boldsymbol X_t, \\boldsymbol \\sigma)$ We can compute the fixed point iterating the contraction from any initial guess $\\Delta_{0t}$  What about $\\sigma$? What we we got?\n inversion result: for any market shares and any $\\boldsymbol \\sigma$, we can find a vector of mean utilities $\\Delta_t$ that rationalizes the data with the BLP model a non-identification result? there is no information about $\\boldsymbol \\sigma$ from market shares?  What are we forgetting?\n Cross-market variation! We can get the mean utilities $\\delta_{jt} = \\boldsymbol x_{jt} \\boldsymbol \\beta_0 - \\alpha p_{jt} + \\xi_{jt}$ As in OLS, use $\\boldsymbol z_{jt} \\perp \\xi_{jt}$ to get identification of $(\\alpha, \\boldsymbol \\beta_0, \\boldsymbol \\sigma)$  Identification of $\\sigma$ We are trying to estimate $(\\alpha, \\boldsymbol \\beta_0, \\boldsymbol \\sigma)$ from $$ \\mathbb E \\Big[ \\xi_{jt} (\\alpha, \\boldsymbol \\beta_0, \\boldsymbol \\sigma) \\cdot \\boldsymbol z_{jt} \\Big] = \\mathbb E \\Big[ \\big( \\delta_{jt}(\\boldsymbol \\sigma) - \\boldsymbol x_{jt} \\boldsymbol \\beta_0 + \\alpha p_{jt} \\big) \\cdot \\boldsymbol z_{jt} \\Big] $$ What kind of intruments $\\boldsymbol z_{jt}$ do we need?\n $\\boldsymbol x_{jt}$ (for $\\boldsymbol \\beta_0$) intruments for $p_{jt}$ (for $\\alpha$) but also something for $\\boldsymbol \\sigma$!  BLP Estimation Steps\n Take guess of parameters $(\\alpha, \\boldsymbol \\beta_0, \\boldsymbol \\sigma)$ From observed market shared $S^{obs}{t}$ and $\\boldsymbol \\sigma$ get mean utilities $\\delta{jt} (\\boldsymbol \\sigma)$ Use also $(\\alpha, \\boldsymbol \\beta_0)$ to get $\\xi_{jt} (\\alpha, \\boldsymbol \\beta_0, \\boldsymbol \\sigma)$ Compute empirical moments $\\frac{1}{JT} \\xi_{jt} (\\alpha, \\boldsymbol \\beta_0, \\boldsymbol \\sigma) \\cdot \\boldsymbol z_{jt}$  The GMM estimator is $(\\hat \\alpha, \\boldsymbol{\\hat{\\beta}_0}, \\boldsymbol{\\hat{\\sigma}})$ that get the empirical moments as close to $0$ as possible.\nIssues\n Computing $S_t (\\Delta_t, \\boldsymbol X_t, \\boldsymbol \\sigma)$ involves a high dimensional integral  Use simulation to approximate distribution of random tastes $\\zeta_{it}^k$ P.S. recall that we have decomposed random coefficients $\\beta_{it}^k$ as $\\beta_{it}^k = \\beta_0^k + \\sigma_k \\zeta_{it}^k$   $\\xi_{jt} (\\alpha, \\boldsymbol \\beta_0, \\boldsymbol \\sigma)$ has no closed form solution  Compute it via contraction MPEC?    Computation Nested fixed point algorithm Sketch of the algorithm\n Draw a vector of consumer tastes Until you have found a minimum for $\\mathbb E \\Big[ \\xi_{jt} (\\alpha, \\boldsymbol \\beta_0, \\boldsymbol \\sigma) \\cdot \\boldsymbol z_{jt} \\Big]$ do  Pick a vector of parameter values $(\\alpha, \\boldsymbol \\beta_0, \\boldsymbol \\sigma)$ Initialize mean utilities $\\delta_{jt}^0$ Until $\\big|\\big| \\Delta_{t}^{n+1} - \\Delta_{t}^{n} \\big|\\big| \u0026lt; tolerance$ do  Compute implied shares: $s_{jt} (\\Delta_{t}^{n}, \\boldsymbol X_t, \\boldsymbol \\sigma) = \\int \\frac{\\exp \\left[ \\boldsymbol x_{j t} \\boldsymbol{\\tilde{\\beta}}{it}+\\delta{j t}\\right]}{1+\\sum_{j^{\\prime}} \\exp \\left[\\boldsymbol x_{j^{\\prime} t} \\boldsymbol{\\tilde{\\beta}}{it}+\\delta{j' t}\\right]} f\\left( \\boldsymbol{\\tilde{\\beta}}{it} \\mid \\theta\\right) d \\tilde{\\beta}{i t}$ Update mean utilities: $\\Delta_{t}^{n+1} = \\Delta_{t}^{n} + \\log\\Big( \\hat S^{obs}t \\Big) - \\log \\Big( S_t (\\Delta{t}^{n}, \\boldsymbol X_t, \\boldsymbol \\sigma) \\Big)$   Compute $\\xi_{jt} = \\delta_{jt} - \\boldsymbol x_{jt} \\boldsymbol \\beta_0 + \\alpha p_{jt}$ Compute $\\mathbb E \\Big[ \\xi_{jt} (\\alpha, \\boldsymbol \\beta_0, \\boldsymbol \\sigma) \\cdot \\boldsymbol z_{jt} \\Big]$    Notes  Important to draw shocks outside the optimization routine!  Appendix References [references] Berry, Steven T. 1994. “Estimating Discrete-Choice Models of Product Differentiation.” The RAND Journal of Economics, 242–62.\n Berry, Steven, James Levinsohn, and Ariel Pakes. 1995. “Automobile Prices in Market Equilibrium.” Econometrica: Journal of the Econometric Society, 841–90.\n  ","date":1635465600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1635465600,"objectID":"ce928b138560543460b6303f1927ab29","permalink":"https://matteocourthoud.github.io/course/empirical-io/02_demand_estimation/","publishdate":"2021-10-29T00:00:00Z","relpermalink":"/course/empirical-io/02_demand_estimation/","section":"course","summary":"Introduction Setting Oligopoly Supply\n  firms produce differentiated goods/products\n  selling to consumers with heterogeneous preferences\n  static model, complete information\n  products are given\n  equilibrium: NE for each product/market","tags":null,"title":"Demand Estimation","type":"book"},{"authors":null,"categories":null,"content":"Probability Probability Space A probability space is a triple $(\\Omega, \\mathcal A, P)$ where\n $\\Omega$ is the sample space. $\\mathcal A$ is the $\\sigma$-algebra on $\\Omega$. $P$ is a probability measure.  The sample space $\\Omega$ is the space of all possible events.\nWhat is a $\\sigma$-algebra and a probability measure?\nSigma Algebra A nonempty set (of subsets of $\\Omega$) $\\mathcal A \\in 2^\\Omega$ is a sigma algebra ($\\sigma$-algebra) of $\\Omega$ if the following conditions hold:\n $\\Omega \\in \\mathcal A$ If $A \\in \\mathcal A$, then $(\\Omega - A) \\in \\mathcal A$ If $A_1, A_2, \u0026hellip; \\in \\mathcal A$, then $\\bigcup _ {i=1}^{\\infty} A_i \\in \\mathcal A$   The smallest $\\sigma$-algebra is $\\lbrace \\emptyset, \\Omega \\rbrace$ and the largest one is $2^\\Omega$ (in cardinality terms).\n Suppose $\\Omega = \\mathbb R$. Let $\\mathcal{C} = \\lbrace (a, b],-\\infty \\leq a\u0026lt;b\u0026lt;\\infty \\rbrace$. Then the Borel $\\sigma$- algebra on $\\mathbb R$ is defined by $$ \\mathcal B (\\mathbb R) = \\sigma (\\mathcal C) $$\nProbability Measure A probability measure $P: \\mathcal A \\to [0,1]$ is a set function with domain $\\mathcal A$ and codomain $[0,1]$ such that\n $P(A) \\geq 0 \\ \\forall A \\in \\mathcal A$ $P$ is $\\sigma$-additive: is $A_n \\in \\mathcal A$ are pairwise disjoint events ($A_j \\cap A_k = \\emptyset$ for $j \\neq k$), then $$ P\\left(\\bigcup _ {n=1}^{\\infty} A_{n} \\right)=\\sum _ {n=1}^{\\infty} P\\left(A_{n}\\right) $$ $P(\\Omega) = 1$  Properties Some properties of probability measures\n $P\\left(A^{c}\\right)=1-P(A)$ $P(\\emptyset)=0$ For $A, B \\in \\mathcal{A}$, $P(A \\cup B)=P(A)+P(B)-P(A \\cap B)$ For $A, B \\in \\mathcal{A}$, if $A \\subset B$ then $P(A) \\leq P(B)$ For $A_n \\in \\mathcal{A}$, $P \\left(\\cup _ {n=1}^\\infty A_{n} \\right) \\leq \\sum _ {n=1}^\\infty P(A_n)$ For $A_n \\in \\mathcal{A}$, if $A_n \\uparrow A$ then $\\lim _ {n \\to \\infty} P(A_n) = P(A)$  Conditional Probability Let $A, B \\in \\mathcal A$ and $P(B) \u0026gt; 0$, the conditional probability of $A$ given $B$ is $$ P(A | B)=\\frac{P(A \\cap B)}{P(B)} $$\nTwo events $A$ and $B$ are independent if $P(A \\cap B)=P(A) P(B)$.\nLaw of Total Probability Theorem (Law of Total Probability)\nLet $(E_n) _ {n \\geq 1}$ be a finite or countable partition of $\\Omega$. Then, if $A \\in \\mathcal A$, $$ P(A) = \\sum_n P(A | E_n ) P(E_n) $$\nBayes Theorem Theorem (Bayes Theorem)\nLet $(E_n) _ {n \\geq 1}$ be a finite or countable partition of $\\Omega$, and suppose $P(A) \u0026gt; 0$. Then, $$ P(E_n | A) = \\frac{P(A | E_n) P(E_n)}{\\sum_m P(A | E_m) P(E_m)} $$\nFor a single event $E \\in \\Omega$, $$ P(E|A) = \\frac{P(A|E) P(E)}{P(A)} $$\nRandom Variables Definition A random variable $X$ on a probability space $(\\Omega,\\mathcal A, P)$ is a (measurable) mapping $X : \\Omega \\to \\mathbb{R}$ such that $$ \\forall B \\in \\mathcal{B}(\\mathbb{R}), \\quad X^{-1}(B) \\in \\mathcal{A} $$\n The measurability condition states that the inverse image is a measurable set of $\\Omega$ i.e. $X^{-1}(B) \\in \\mathcal A$. This is essential since probabilities are defined only on $\\mathcal A$.\n In words, a random variable it’s a mapping from events to real numbers such that each interval on the real line can be mapped back into an element of the sigma algebra (it can be the empty set).\nDistribution Function Let $X$ be a real valued random variable. The distribution function (also called cumulative distribution function) of $X$, commonly denoted $F_X(x)$ is defined by $$ F_X(x) = \\Pr(X \\leq x) $$\n Properties\n $F$ is monotone non-decreasing $F$ is right continuous $\\lim _ {x \\to - \\infty} F(x)=0$ and $\\lim _ {x \\to + \\infty} F(x)=1$   The random variables $(X_1, .. , X_n)$ are independent if and only if $$ F _ {(X_1, \u0026hellip; , X_n)} (x) = \\prod _ {i=1}^n F_{X_i} (x_i) \\quad \\forall x \\in \\mathbb R^n $$\nDensity Function Let $X$ be a real valued random variable. $X$ has a probability density function if there exists $f_X(x)$ such that for all measurable $A \\subset \\mathbb{R}$, $$ P(X \\in A) = \\int_A f_X(x) \\mathrm{d} x $$\nMoments Expected Value The expected value of a random variable, when it exists, is given by $$ \\mathbb{E}[ X ] = \\int_ \\Omega X(\\omega) \\mathrm{d} P $$ When $X$ has a density, then $$ \\mathbb{E} [ X ] = \\int_ \\mathbb{R} x f_X (x) \\mathrm{d} x = \\int _ \\mathbb{R} x \\mathrm{d} F_X (x) $$\nThe empirical expectation (or sample average) is given by $$ \\mathbb{E}_n [x_i] = \\frac{1}{n} \\sum _ {i=1}^N x_i $$\nVariance and Covariance The covariance of two random variables $X$, $Y$ defined on $\\Omega$ is $$ Cov(X, Y ) = \\mathbb{E}[ (X - \\mathbb{E}[ X ]) (Y - \\mathbb{E}[ Y ]) ] = \\mathbb{E}[XY ] - \\mathbb{E}[ X ]E[ Y ] $$ In vector notation, $Cov(X, Y) = \\mathbb{E}[XY'] - \\mathbb{E}[ X ]\\mathbb{E}[Y']$.\nThe variance of a random variable $X$, when it exists, is given by $$ Var(X) = \\mathbb{E}[ (X - \\mathbb{E}[ X ])^2 ] = \\mathbb{E}[X^2] - \\mathbb{E}[ X ]^2 $$ In vector notation, $Var(X) = \\mathbb{E}[XX'] - \\mathbb{E}[ X ]\\mathbb{E}[X']$.\nProperties Let $X, Y, Z, T \\in \\mathcal{L}^{2}$ and $a, b, c, d \\in \\mathbb{R}$\n $Cov(X, X) = Var(X)$ $Cov(X, Y) = Cov(Y, X)$ $Cov(aX + b, Y) = a \\ Cov(X,Y)$ $Cov(X+Z, Y) = Cov(X,Y) + Cov(Z,Y)$ $Cov(aX + bZ, cY + dT) = ac * Cov(X,Y) + ad * Cov(X,T) + bc * Cov(Z,Y) + bd * Cov(Z,T)$  Let $X, Y \\in \\mathcal L^1$ be independent. Then, $\\mathbb E[XY] = \\mathbb E[ X ] \\mathbb E[ Y ]$.\nIf $X$ and $Y$ are independent, then $Cov(X,Y) = 0$.\n Note that the converse does not hold: $Cov(X,Y) = 0 \\not \\to X \\perp Y$.\n Sample Variance The sample variance is given by $$ Var_n (x_i) = \\frac{1}{n} \\sum _ {i=1}^N (x_i - \\bar{x})^2 $$ where $\\bar{x_i} = \\mathbb{E}_n [x_i] = \\frac{1}{n} \\sum _ {i=1}^N x_i$.\nFinite Sample Bias Theorem Theorem: The expected sample variance $\\mathbb{E} [\\sigma^2_n] = \\mathbb{E} \\left[ \\frac{1}{n} \\sum _ {i=1}^N \\left(y_i - \\mathbb{E}_n[ Y ] \\right)^2 \\right]$ gives an estimate of the population variance that is biased by a factor of $\\frac{1}{n}$ and is therefore referred to as biased sample variance.\nProof: $$ \\begin{aligned} \u0026amp;\\mathbb{E}[\\sigma^2_n] = \\mathbb{E} \\left[ \\frac{1}{n} \\sum _ {i=1}^n \\left( y_i - \\mathbb{E}_n [ Y ] \\right)^2 \\right] = \\newline \u0026amp;= \\mathbb{E} \\left[ \\frac{1}{n} \\sum _ {i=1}^n \\left( y_i - \\frac{1}{n} \\sum _ {i=1}^n y_i \\right )^2 \\right] = \\newline \u0026amp;= \\frac{1}{n} \\sum _ {i=1}^n \\mathbb{E} \\left[ y_i^2 - \\frac{2}{n} y_i \\sum _ {j=1}^n y_j + \\frac{1}{n^2} \\sum _ {j=1}^n y_j \\sum _ {k=1}^{n}y_k \\right] = \\newline \u0026amp;= \\frac{1}{n} \\sum _ {i=1}^n \\left[ \\frac{n-2}{n} \\mathbb{E}[y_i^2] - \\frac{2}{n} \\sum _ {j\\neq i} \\mathbb{E}[y_i y_j] + \\frac{1}{n^2} \\sum _ {j=1}^n \\sum _ {k\\neq j} \\mathbb{E}[y_j y_k] + \\frac{1}{n^2} \\sum _ {j=1}^n \\mathbb{E}[y_j^2] \\right] = \\newline \u0026amp;= \\frac{1}{n} \\sum _ {i=1}^n \\left[ \\frac{n-2}{n}(\\mu^2 + \\sigma^2) - \\frac{2}{n} (n-1) \\mu^2 + \\frac{1}{n^2} n(n-1)\\mu^2 + \\frac{1}{n^2} n (\\mu^2 + \\sigma^2)]\\right] = \\newline \u0026amp;= \\frac{n-1}{n} \\sigma^2 \\end{aligned} $$ $$\\tag*{$\\blacksquare$}$$\nInequalities   Triangle Inequality: if $\\mathbb{E} [ X ] \u0026lt; \\infty$, then $$ |\\mathbb{E} [ X ] | \\leq \\mathbb{E} [|X|] $$\n  Markov’s Inequality: if $\\mathbb{E}[ X ] \u0026lt; \\infty$, then $$ \\Pr(|X| \u0026gt; t) \\leq \\frac{1}{t} \\mathbb{E}[|X|] $$\n  Chebyshev’s Inequality: if $\\mathbb{E}[X^2] \u0026lt; \\infty$, then $$ \\Pr(|X- \\mu|\u0026gt; t \\sigma) \\leq \\frac{1}{t^2}\\Leftrightarrow \\Pr(|X- \\mu|\u0026gt; t ) \\leq \\frac{\\sigma^2}{t^2} $$\n  Cauchy-Schwarz’s Inequality: $$ \\mathbb{E} [|XY|] \\leq \\sqrt{\\mathbb{E}[X^2] \\mathbb{E}[Y^2]} $$\n  Minkowski Inequality: $$ \\left( \\sum _ {k=1}^n | x_k + y_k |^p \\right) ^ {\\frac{1}{p}} \\leq \\left( \\sum _ {k=1}^n | x_k |^p \\right) ^ {\\frac{1}{p}} + \\left( \\sum _ {k=1}^n | y_k | ^p \\right) ^ { \\frac{1}{p} } $$\n  Jensen’s Inequality: if $g( \\cdot)$ is concave (e.g. logarithmic function), then $$ \\mathbb{E}[g(x)] \\leq g(\\mathbb{E}[ X ]) $$ Similarly, if $g(\\cdot)$ is convex (e.g. exponential function), then $$ \\mathbb{E}[g(x)] \\geq g(\\mathbb{E}[ X ]) $$\n  Law of Iterated Expectations Theorem (Law of Iterated Expectations) $$ \\mathbb{E}(Y) = \\mathbb{E}_X [\\mathbb{E}(Y|X)] $$ \u0026gt; This states that the expectation of the conditional expectation is the unconditional expectation. \u0026gt; \u0026gt; In other words the average of the conditional averages is the unconditional average.\nLaw of Total Variance Theorem (Law of Total Variance) $$ Var(Y) = Var_X (\\mathbb{E}[Y |X]) + \\mathbb{E}_X [Var(Y|X)] $$\nSince variances are always non-negative, the law of total variance implies $$ Var(Y) \\geq Var_X (\\mathbb{E}[Y |X]) $$\nDistributions Normal Distribution We say that a random variable $Z$ has the standard normal distribution, or Gaussian, written $Z \\sim N(0,1)$, if it has the density $$ \\phi(x)=\\frac{1}{\\sqrt{2 \\pi}} \\exp \\left(-\\frac{x^{2}}{2}\\right), \\quad-\\infty\u0026lt;x\u0026lt;\\infty $$ If $Z \\sim N(0, 1)$ and $X = \\mu + \\sigma Z$ for $\\mu \\in \\mathbb R$ and $\\sigma \\geq 0$, then $X$ has a univariate normal distribution, written $X \\sim N(\\mu, \\sigma^2)$. By change-of-variables X has the density $$ f(x)=\\frac{1}{\\sqrt{2 \\pi \\sigma^{2}}} \\exp \\left(-\\frac{(x-\\mu)^{2}}{2 \\sigma^{2}}\\right), \\quad-\\infty\u0026lt;x\u0026lt;\\infty $$\nMultinomial Normal Distribution We say that the k -vector Z has a multivariate standard normal distribution, written $Z \\sim N(0, I_k)$ if it has the joint density $$ f(x)=\\frac{1}{(2 \\pi)^{k / 2}} \\exp \\left(-\\frac{x^{\\prime} x}{2}\\right), \\quad x \\in \\mathbb{R}^{k} $$ If $Z \\sim N(0, I_k)$ and $X = \\mu + B Z$, then the k-vector $X$ has a multivariate normal distribution, written $X \\sim N(\\mu, \\Sigma)$ where $\\Sigma = BB' \\geq 0$. If $\\sigma \u0026gt; 0$, then by change-of-variables $X$ has the joint density function $$ f(x)=\\frac{1}{(2 \\pi)^{k / 2} \\operatorname{det}(\\Sigma)^{1 / 2}} \\exp \\left(-\\frac{(x-\\mu)^{\\prime} \\Sigma^{-1}(x-\\mu)}{2}\\right), \\quad x \\in \\mathbb{R}^{k} $$\nProperties  The expectation and covariance matrix of $X \\sim N(\\mu, \\Sigma)$ are $\\mathbb E = \\mu$ and $Var=\\Sigma$. If $(X,Y)$ are multivariate normal, $X$ and $Y$ are uncorrelated if and only if they are independent. If $X \\sim N(\\mu, \\Sigma)$ and $Y = a + bB$, then $X \\sim N(a + B\\mu, B \\Sigma B')$. If $X \\sim N(0, I_k)$, then $X\u0026rsquo;X \\sim \\chi^2_k$, chi-square with $k$ degrees of freedom. If $X \\sim N(0, \\Sigma)$ with $\\Sigma\u0026gt;0$, then $X' \\Sigma X \\sim \\chi_k$ where $k = \\dim (X)$. If $Z \\sim N(0,1)$ and $Q \\sim \\chi^2_k$ are independent then $\\frac{Z}{\\sqrt{Q/k}} \\sim t_k$, student t with k degrees of freedom.  Normal Distribution Relatives These distributions are relatives of the normal distribution\n $\\chi^2_q \\sim \\sum _ {i=1}^q Z_i^2$ where $Z_i \\sim N(0,1)$ $t_n \\sim \\frac{Z}{\\sqrt{\\chi^2 _ n}/n }$ $F(n_1 , n_2) \\sim \\frac{\\chi^2 _ {n_1} / n_1}{\\chi^2 _ {n_2}/n_2}$   The $t$ distribution is approximately standard normal but has heavier tails. The approximation is good for $n \\geq 30$: $t_{n\\geq 30} \\sim N(0,1)$\n ","date":1635465600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1635465600,"objectID":"a5c296515288d5e000ab88ebc0e2f779","permalink":"https://matteocourthoud.github.io/course/metrics/02_probability/","publishdate":"2021-10-29T00:00:00Z","relpermalink":"/course/metrics/02_probability/","section":"course","summary":"Probability Probability Space A probability space is a triple $(\\Omega, \\mathcal A, P)$ where\n $\\Omega$ is the sample space. $\\mathcal A$ is the $\\sigma$-algebra on $\\Omega$. $P$ is a probability measure.","tags":null,"title":"Probability Theory","type":"book"},{"authors":null,"categories":null,"content":"import numpy as np import pandas as pd  Setup For the scope of this tutorial we are going to use AirBnb Scraped data for the city of Bologna. The data is freely available at Inside AirBnb: http://insideairbnb.com/get-the-data.html.\nA description of all variables in all datasets is avaliable here.\nWe are going to use 2 datasets:\n listing dataset: contains listing-level information pricing dataset: contains pricing data, over time  # Import listings data url_listings = \u0026quot;http://data.insideairbnb.com/italy/emilia-romagna/bologna/2021-12-17/visualisations/listings.csv\u0026quot; df_listings = pd.read_csv(url_listings) # Import pricing data url_prices = \u0026quot;http://data.insideairbnb.com/italy/emilia-romagna/bologna/2021-12-17/data/calendar.csv.gz\u0026quot; df_prices = pd.read_csv(url_prices, compression=\u0026quot;gzip\u0026quot;)  Numerical Data  Methods\n +, -, *, / numpy functions pd.cut()   Standard mathematical operations between columns are done row-wise.\ndf_prices['maximum_nights'] - df_prices['minimum_nights']  0 148 1 357 2 357 3 357 4 357 ... 1260340 1124 1260341 1124 1260342 1124 1260343 1124 1260344 1124 Length: 1260345, dtype: int64  We can use most numpy operations element-wise on a single column.\nnp.log(df_listings['price'])  0 4.219508 1 3.367296 2 3.912023 3 4.836282 4 3.912023 ... 3448 3.465736 3449 3.806662 3450 3.912023 3451 4.897840 3452 4.744932 Name: price, Length: 3453, dtype: float64  We can create a categorical variables from a numerical one using the pd.cut() function.\npd.cut(df_listings['price'], bins = [0, 50, 100, np.inf], labels=['cheap', 'ok', 'expensive'])  0 ok 1 cheap 2 cheap 3 expensive 4 cheap ... 3448 cheap 3449 cheap 3450 cheap 3451 expensive 3452 expensive Name: price, Length: 3453, dtype: category Categories (3, object): ['cheap' \u0026lt; 'ok' \u0026lt; 'expensive']  String Data  Methods\n + .str.replace .str.contains .astype(str) -pd.get_dummies()   We can use the + operator between columns, to do pairwise append.\nNote: we cannot do it with strings.\ndf_listings['host_name'] + df_listings['neighbourhood']  0 CarloSanto Stefano 1 EleonoraPorto - Saragozza 2 PaoloSanto Stefano 3 Anna MariaSanto Stefano 4 ValerioPorto - Saragozza ... 3448 IleanaNavile 3449 FernandaPorto - Saragozza 3450 IleanaNavile 3451 Wonderful ItalySanto Stefano 3452 Wonderful ItalyPorto - Saragozza Length: 3453, dtype: object  Pandas Series have a lot of vectorized string functions. You can find a list here.\nFor example, we want to remove the dollar symbol from the price variable in the df_prices dataset.\ndf_prices['price'].str.replace('$', '', regex=False)  0 70.00 1 68.00 2 68.00 3 68.00 4 68.00 ... 1260340 115.00 1260341 115.00 1260342 115.00 1260343 115.00 1260344 115.00 Name: price, Length: 1260345, dtype: object  Some of these functions use regular expressions.\n match(): Call re.match() on each element, returning a boolean. extract(): Call re.match() on each element, returning matched groups as strings. findall(): Call re.findall() on each element replace(): Replace occurrences of pattern with some other string contains(): Call re.search() on each element, returning a boolean count(): Count occurrences of pattern split(): Equivalent to str.split(), but accepts regexps rsplit()  For example, the next code checks whether in the word centre or center are contained in the text description.\ndf_listings['name'].str.contains('centre|center')  0 True 1 False 2 True 3 False 4 False ... 3448 False 3449 False 3450 False 3451 False 3452 False Name: name, Length: 3453, dtype: bool  Lastly, we can (try to) convert string variables to numeric using astype(float).\ndf_prices['price'].str.replace('[$|,]', '', regex=True).astype(float)  0 70.0 1 68.0 2 68.0 3 68.0 4 68.0 ... 1260340 115.0 1260341 115.0 1260342 115.0 1260343 115.0 1260344 115.0 Name: price, Length: 1260345, dtype: float64  We can also use it to convert numerics to strings using astype(str).\ndf_listings['id'].astype(str)  0 42196 1 46352 2 59697 3 85368 4 145779 ... 3448 53810648 3449 53820830 3450 53837098 3451 53837654 3452 53854962 Name: id, Length: 3453, dtype: object  We can generate dummies from a categorical variable using pd.get_dummies()\npd.get_dummies(df_listings['neighbourhood']).head()   .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }  \n  Borgo Panigale - Reno Navile Porto - Saragozza San Donato - San Vitale Santo Stefano Savena     0 0 0 0 0 1 0   1 0 0 1 0 0 0   2 0 0 0 0 1 0   3 0 0 0 0 1 0   4 0 0 1 0 0 0     Time Data  Methods\n pd.to_datetime() .dt.year .df.to_period() pd.to_timedelta()   In the df_prices we have a date variable, date. Which format is it in? We can check it with the .dtypes attribute.\ndf_prices['date'].dtypes  dtype('O')  We can convert a variable into a date using the\ndf_prices['datetime'] = pd.to_datetime(df_prices['date'])  Indeed, if we now check the format of the datetime variable, it\u0026rsquo;s datetime.\ndf_prices['datetime'].dtypes  dtype('\u0026lt;M8[ns]')  Once we have a variable in datetime format, we gain plenty of datetime operations through the dt accessor object for datetime-like properties.\nFor example, we can extract the year using .dt.year. We can do the same with month, week and day.\ndf_prices['datetime'].dt.year  0 2021 1 2021 2 2021 3 2021 4 2021 ... 1260340 2022 1260341 2022 1260342 2022 1260343 2022 1260344 2022 Name: datetime, Length: 1260345, dtype: int64  We can change the level of aggregation of a date using .dt.to_period(). The option M converts to year-month level.\ndf_prices['datetime'].dt.to_period('M')  0 2021-12 1 2021-12 2 2021-12 3 2021-12 4 2021-12 ... 1260340 2022-12 1260341 2022-12 1260342 2022-12 1260343 2022-12 1260344 2022-12 Name: datetime, Length: 1260345, dtype: period[M]  We can add or subtract time periods from a date using the pd.to_timedelta() function. We need to specify the unit of measurement with the unit option.\ndf_prices['datetime'] - pd.to_timedelta(3, unit='d')  0 2021-12-14 1 2021-12-14 2 2021-12-15 3 2021-12-16 4 2021-12-17 ... 1260340 2022-12-09 1260341 2022-12-10 1260342 2022-12-11 1260343 2022-12-12 1260344 2022-12-13 Name: datetime, Length: 1260345, dtype: datetime64[ns]  Missing Data  Methods\n .isna() .dropna() .fillna()   The function isna() reports missing values.\ndf_listings.isna().head()   .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }  \n  id name host_id host_name neighbourhood_group neighbourhood latitude longitude room_type price minimum_nights number_of_reviews last_review reviews_per_month calculated_host_listings_count availability_365 number_of_reviews_ltm license     0 False False False False True False False False False False False False False False False False False True   1 False False False False True False False False False False False False False False False False False True   2 False False False False True False False False False False False False False False False False False True   3 False False False False True False False False False False False False False False False False False True   4 False False False False True False False False False False False False False False False False False True     To get a quick description of the amount of missing data in the dataset, we can use\ndf_listings.isna().sum()  id 0 name 0 host_id 0 host_name 9 neighbourhood_group 3453 neighbourhood 0 latitude 0 longitude 0 room_type 0 price 0 minimum_nights 0 number_of_reviews 0 last_review 409 reviews_per_month 409 calculated_host_listings_count 0 availability_365 0 number_of_reviews_ltm 0 license 3318 dtype: int64  We can drop missing values using dropna(). It drops all rows with at least one missing value.\ndf_listings.dropna().shape  (0, 18)  In this case unfortunately, it drops all the rows. If we wa to drop only rows with all missing values, we can use the parameter how='all'.\ndf_listings.dropna(how='all').shape  (3453, 18)  If we want to drop only missing values for one particular value, we can use the subset option.\ndf_listings.dropna(subset=['reviews_per_month']).shape  (3044, 18)  We can also fill the missing values instead of dropping them, using fillna().\ndf_listings.fillna(' -- This was NA -- ').head()   .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }  \n  id name host_id host_name neighbourhood_group neighbourhood latitude longitude room_type price minimum_nights number_of_reviews last_review reviews_per_month calculated_host_listings_count availability_365 number_of_reviews_ltm license     0 42196 50 sm Studio in the historic centre 184487 Carlo -- This was NA -- Santo Stefano 44.48507 11.34786 Entire home/apt 68 3 180 2021-11-12 1.32 1 161 6 -- This was NA --   1 46352 A room in Pasolini's house 467810 Eleonora -- This was NA -- Porto - Saragozza 44.49168 11.33514 Private room 29 1 300 2021-11-30 2.2 2 248 37 -- This was NA --   2 59697 COZY LARGE BEDROOM in the city center 286688 Paolo -- This was NA -- Santo Stefano 44.48817 11.34124 Private room 50 1 240 2020-10-04 2.18 2 327 0 -- This was NA --   3 85368 Garden House Bologna 467675 Anna Maria -- This was NA -- Santo Stefano 44.47834 11.35672 Entire home/apt 126 2 40 2019-11-03 0.34 1 332 0 -- This was NA --   4 145779 SINGLE ROOM 705535 Valerio -- This was NA -- Porto - Saragozza 44.49306 11.33786 Private room 50 10 69 2021-12-05 0.55 9 365 5 -- This was NA --     We can also make missing values if we want.\ndf_listings.iloc[2, 2] = np.nan df_listings.iloc[:3, :3]   .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }  \n  id name host_id     0 42196 50 sm Studio in the historic centre 184487.0   1 46352 A room in Pasolini's house 467810.0   2 59697 COZY LARGE BEDROOM in the city center NaN     ","date":1651363200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1651363200,"objectID":"ade16101536560376740ad034d1b702a","permalink":"https://matteocourthoud.github.io/course/data-science/03_data_types/","publishdate":"2022-05-01T00:00:00Z","relpermalink":"/course/data-science/03_data_types/","section":"course","summary":"import numpy as np import pandas as pd  Setup For the scope of this tutorial we are going to use AirBnb Scraped data for the city of Bologna. The data is freely available at Inside AirBnb: http://insideairbnb.","tags":null,"title":"Data Types","type":"book"},{"authors":null,"categories":null,"content":"# Setup %matplotlib inline from utils.lecture03 import *  Dataset For this session, we are mostly going to work with the wage dataset.\ndf = pd.read_csv('data/Wage.csv', index_col=0) df.head(3)   .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }  \n  year age maritl race education region jobclass health health_ins logwage wage     231655 2006 18 1. Never Married 1. White 1. \u0026lt; HS Grad 2. Middle Atlantic 1. Industrial 1. \u0026lt;=Good 2. No 4.318063 75.043154   86582 2004 24 1. Never Married 1. White 4. College Grad 2. Middle Atlantic 2. Information 2. \u0026gt;=Very Good 2. No 4.255273 70.476020   161300 2003 45 2. Married 1. White 3. Some College 2. Middle Atlantic 1. Industrial 1. \u0026lt;=Good 1. Yes 4.875061 130.982177     This dataset contains information on wages and individual characteristics.\nOur main objective is going to be to explain wages using the observables contained in the dataset.\nPolynomial Regression and Step Functions As we have seen in the first lecture, the most common way to introduce linearities is to replace the standard linear model\n$$ y_i = \\beta_0 + \\beta_1 x_i + \\varepsilon_i $$\nwith a polynomial function\n$$ y_i = \\beta_0 + \\beta_1 x_i + \\beta_2 x_i^2 + \\beta_2 x_i^3 + \u0026hellip; + \\varepsilon_i $$\nExplore the Data Suppose we want to investigate the relationship between wage and age. Let\u0026rsquo;s first plot the two variables.\n# Scatterplot of the data df.plot.scatter('age','wage',color='w', edgecolors='k', alpha=0.3);  Polynomials of different degrees The relationship is highly complex and non-linear. Let\u0026rsquo;s expand our linear regression polynomials of different degrees: 1 to 5.\nX_poly1 = PolynomialFeatures(1).fit_transform(df.age.values.reshape(-1,1)) X_poly2 = PolynomialFeatures(2).fit_transform(df.age.values.reshape(-1,1)) X_poly3 = PolynomialFeatures(3).fit_transform(df.age.values.reshape(-1,1)) X_poly4 = PolynomialFeatures(4).fit_transform(df.age.values.reshape(-1,1)) X_poly5 = PolynomialFeatures(5).fit_transform(df.age.values.reshape(-1,1))  Variables Our dependent varaible is going to be a dummy for income above 250.000 USD.\n# Get X and y X = df.age y = df.wage y01 = (df.wage \u0026gt; 250).map({False:0, True:1}).values  Polynomia Regression If we run a linear regression on a 4-degree polinomial expansion of age, this is what it looks like`:\n# Fit ols on 4th degree polynomial fit = sm.OLS(y, X_poly4).fit() fit.summary().tables[1]    coef std err t P|t| [0.025 0.975]   const  -184.1542  60.040  -3.067  0.002  -301.879  -66.430   x1  21.2455  5.887  3.609  0.000  9.703  32.788   x2  -0.5639  0.206  -2.736  0.006  -0.968  -0.160   x3  0.0068  0.003  2.221  0.026  0.001  0.013   x4 -3.204e-05  1.64e-05  -1.952  0.051 -6.42e-05  1.45e-07   Measures of Fit In this case, the single coefficients are not of particular interest. We are mostly interested in the best capturing the relationship between age and wage. How can we pick among thedifferent polynomials?\nWe compare different polynomial degrees. For each regression, we are going to look at a series of metrics:\n absolute residuals sum of squared residuals the difference in SSR w.r (SSR).t the 0-degree case F statistic  # Run regressions fit_1 = sm.OLS(y, X_poly1).fit() fit_2 = sm.OLS(y, X_poly2).fit() fit_3 = sm.OLS(y, X_poly3).fit() fit_4 = sm.OLS(y, X_poly4).fit() fit_5 = sm.OLS(y, X_poly5).fit() # Compare fit sm.stats.anova_lm(fit_1, fit_2, fit_3, fit_4, fit_5, typ=1)   .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }  \n  df_resid ssr df_diff ss_diff F Pr(\u0026gt;F)     0 2998.0 5.022216e+06 0.0 NaN NaN NaN   1 2997.0 4.793430e+06 1.0 228786.010128 143.593107 2.363850e-32   2 2996.0 4.777674e+06 1.0 15755.693664 9.888756 1.679202e-03   3 2995.0 4.771604e+06 1.0 6070.152124 3.809813 5.104620e-02   4 2994.0 4.770322e+06 1.0 1282.563017 0.804976 3.696820e-01     The polynomial degree 4 seems best.\n# Set polynomial X to 4th degree X_poly = X_poly4  Binary Dependent Variable Since we have a binary dependent variable, it would be best to account for it in our regression framework. One way to do so, is to run a logistic regression.\nHow to interpret a Logistic Regression?\n$$ y = \\mathbb I \\ \\Big( \\beta_0 + \\beta_1 x_i + \\beta_2 x_i^2 + \\beta_2 x_i^3 + \u0026hellip; + \\varepsilon_i \\Big) $$\nwhere $\\mathbb I(\\cdot)$ is an indicator function and now $\\varepsilon_i$ is the error term.\nBinomial Link Functions Depending on the assumed distribution of the error term, we get different results. I list below the error types supported by the Binomial family.\n# List link functions for the Binomial family sm.families.family.Binomial.links  [statsmodels.genmod.families.links.logit, statsmodels.genmod.families.links.probit, statsmodels.genmod.families.links.cauchy, statsmodels.genmod.families.links.log, statsmodels.genmod.families.links.cloglog, statsmodels.genmod.families.links.identity]  Logit Link Function We are going to pick the logit link, i.e. we are going to assume that the error term is Type 1 Extreme Value (or Gumbel) distributed. It instead we take the usual standard normal distribution assumption for $\\varepsilon_i$, we get probit regression.\n# Pick the logit link for the Binomial family logit_link = sm.families.Binomial(sm.genmod.families.links.logit())  Given the error distribution, we can write the probability that $y=1$ as\n$$ \\Pr(y=1) = \\frac{e^{ \\beta_0 + \\beta_1 x_i + \\beta_2 x_i^2 + \\beta_2 x_i^3 + \u0026hellip; + \\varepsilon_i }}{1 + e^{ \\beta_0 + \\beta_1 x_i + \\beta_2 x_i^2 + \\beta_2 x_i^3 + \u0026hellip; + \\varepsilon_i } } $$\nLogistic Regression We now estimate the regression and plot the estimated relationship between age and wage.\n# Run logistic regression logit_poly = sm.GLM(y01, X_poly, family=logit_link).fit() logit_poly.summary().tables[1]    coef std err z P|z| [0.025 0.975]   const  -109.5530  47.655  -2.299  0.022  -202.956  -16.150   x1  8.9950  4.187  2.148  0.032  0.789  17.201   x2  -0.2816  0.135  -2.081  0.037  -0.547  -0.016   x3  0.0039  0.002  2.022  0.043  0.000  0.008   x4 -1.949e-05  9.91e-06  -1.966  0.049 -3.89e-05 -6.41e-08   Linear Model Comparison What is the difference with the linear model?\n# Run OLS regression with binary outcome ols_poly = sm.OLS(y01, X_poly).fit() ols_poly.summary().tables[1]    coef std err t P|t| [0.025 0.975]   const  -0.1126  0.240  -0.468  0.640  -0.584  0.359   x1  0.0086  0.024  0.363  0.717  -0.038  0.055   x2  -0.0002  0.001  -0.270  0.787  -0.002  0.001   x3  3.194e-06  1.23e-05  0.260  0.795 -2.09e-05  2.73e-05   x4 -1.939e-08  6.57e-08  -0.295  0.768 -1.48e-07  1.09e-07   The magnitude of the coefficients is different, but the signs are the same.\nPlot data and predictions Let\u0026rsquo;s plot the estimated curves against the data distribution.\n# Generate predictions x_grid = np.arange(df.age.min(), df.age.max()).reshape(-1,1) X_poly_test = PolynomialFeatures(4).fit_transform(x_grid) y_hat1 = sm.OLS(y, X_poly).fit().predict(X_poly_test) y01_hat1 = logit_poly.predict(X_poly_test)  plot_predictions(X, y, x_grid, y01, y_hat1, y01_hat1, 'Figure 7.1: Degree-4 Polynomial')  Which is remindful of Using polynomial functions of the features as predictors in a linear model imposes a global structure on the non-linear function of age. We can instead use step functions in order to avoid imposing such a global structure.\nFor example, we could break the range of age into bins, and fit a different constant in each bin.\nStep Functions Building a step function means first picking $K$ cutpoints $c_1 , c_2 , . . . , c_K$ in the range of age, and then construct $K + 1$ new variables\n$$ C_0(age) = \\mathbb I ( age \u0026lt; c_1) \\ C_1(age) = \\mathbb I ( c_1 \u0026lt; age \u0026lt; c_2) \\ C_2(age) = \\mathbb I ( c_2 \u0026lt; age \u0026lt; c_3) \\ \u0026hellip; \\ C_{K-1}(age) = \\mathbb I ( c_{K-1} \u0026lt; age \u0026lt; c_K) \\ C_K(age) = \\mathbb I ( c_K \u0026lt; age) \\ $$\nwhere $\\mathbb I(\\cdot)$ is the indicator function.\nBinning First, we generate the cuts.\n# Generate cuts for the variable age df_cut, bins = pd.cut(df.age, 4, retbins=True, right=True) df_cut.value_counts(sort=False) type(df_cut)  pandas.core.series.Series  Let\u0026rsquo;s generate a DataFrame out of this series.\n# Generate bins for \u0026quot;age\u0026quot; from the cuts df_steps = pd.concat([df.age, df_cut, df.wage], keys=['age','age_cuts','wage'], axis=1) df_steps.head(5)   .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }  \n  age age_cuts wage     231655 18 (17.938, 33.5] 75.043154   86582 24 (17.938, 33.5] 70.476020   161300 45 (33.5, 49.0] 130.982177   155159 43 (33.5, 49.0] 154.685293   11443 50 (49.0, 64.5] 75.043154     Dummy Variables Now we can generate different dummy variables out of each bin.\n# Create dummy variables for the age groups df_steps_dummies = pd.get_dummies(df_steps['age_cuts']) # Statsmodels requires explicit adding of a constant (intercept) df_steps_dummies = sm.add_constant(df_steps_dummies) df_steps_dummies.head(5)   .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }  \n  const (17.938, 33.5] (33.5, 49.0] (49.0, 64.5] (64.5, 80.0]     231655 1.0 1 0 0 0   86582 1.0 1 0 0 0   161300 1.0 0 1 0 0   155159 1.0 0 1 0 0   11443 1.0 0 0 1 0     Stepwise Regression We are now ready to run our regression\n# Generate our new X variable X_step = df_steps_dummies.drop(df_steps_dummies.columns[1], axis=1) # OLS Regression on step functions ols_step = sm.OLS(y, X_step).fit() ols_step.summary().tables[1]    coef std err t P|t| [0.025 0.975]   const  94.1584  1.476  63.790  0.000  91.264  97.053   (33.5, 49.0]  24.0535  1.829  13.148  0.000  20.466  27.641   (49.0, 64.5]  23.6646  2.068  11.443  0.000  19.610  27.719   (64.5, 80.0]  7.6406  4.987  1.532  0.126  -2.139  17.420   From the regression outcome we can see that most bin coefficients are significant, except for the last one.\n# Put the test data in the same bins as the training data. bin_mapping = np.digitize(x_grid.ravel(), bins) bin_mapping  array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4])  # Get dummies, drop first dummy category, add constant X_step_test = sm.add_constant(pd.get_dummies(bin_mapping).drop(1, axis=1)) X_step_test.head()   .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }  \n  const 2 3 4     0 1.0 0 0 0   1 1.0 0 0 0   2 1.0 0 0 0   3 1.0 0 0 0   4 1.0 0 0 0     # Step prediction y_hat2 = ols_step.predict(X_step_test)  Logistic Step Regression We are going again to run a logistic regression, given that our outcome is binary.\n# Logistic regression on step functions logit_step = sm.GLM(y01, X_step, family=logit_link).fit() y01_hat2 = logit_step.predict(X_step_test) logit_step.summary().tables[1]    coef std err z P|z| [0.025 0.975]   const  -5.0039  0.449  -11.152  0.000  -5.883  -4.124   (33.5, 49.0]  1.5998  0.474  3.378  0.001  0.672  2.528   (49.0, 64.5]  1.7147  0.488  3.512  0.000  0.758  2.672   (64.5, 80.0]  0.7413  1.102  0.672  0.501  -1.420  2.902   Plotting How does the predicted function looks like?\nplot_predictions(X, y, x_grid, y01, y_hat2, y01_hat2, 'Figure 7.2: Piecewise Constant')  Regression Splines Spline regression, or piece-wise polynomial regression, involves fitting separate low-degree polynomials over different regions of $X$. The idea is to have one regression specification but with different coefficients in different parts of the $X$ range. The points where the coefficients change are called knots.\nFor example, we could have a third degree polynomial and splitting the sample in two.\n$$ y_{i}=\\left{\\begin{array}{ll} \\beta_{01}+\\beta_{11} x_{i}+\\beta_{21} x_{i}^{2}+\\beta_{31} x_{i}^{3}+\\epsilon_{i} \u0026amp; \\text { if } x_{i}\u0026lt;c \\ \\beta_{02}+\\beta_{12} x_{i}+\\beta_{22} x_{i}^{2}+\\beta_{32} x_{i}^{3}+\\epsilon_{i} \u0026amp; \\text { if } x_{i} \\geq c \\end{array}\\right. $$\nWe have now two sets of coefficients, one for each subsample.\nGenerally, using more knots leads to a more flexible piecewise polynomial. Also increasing the degree of the polynomial increases the degree of flexibility.\nExample We are now going to plot 4 different examples for the age wage relationship:\n Discontinuous piecewise cubic Continuous piecewise cubic Quadratic (continuous) Continuous piecewise linear  # Cut dataset df_short = df.iloc[:80,:] X_short = df_short.age y_short = df_short.wage x_grid_short = np.arange(df_short.age.min(), df_short.age.max()+1).reshape(-1,1) # 1. Discontinuous piecewise cubic spline1 = \u0026quot;bs(x, knots=(50,50,50,50), degree=3, include_intercept=False)\u0026quot; # 2. Continuous piecewise cubic spline2 = \u0026quot;bs(x, knots=(50,50,50), degree=3, include_intercept=False)\u0026quot; # 3. Quadratic (continuous) spline3 = \u0026quot;bs(x, knots=(%s,%s), degree=2, include_intercept=False)\u0026quot; % (min(df.age), min(df.age)) # 4. Continuous piecewise linear spline4 = \u0026quot;bs(x, knots=(%s,50), degree=1, include_intercept=False)\u0026quot; % min(df.age)  Generate Predictions # Generate spline predictions def fit_predict_spline(spline, X, y, x_grid): transformed_x = dmatrix(spline, {\u0026quot;x\u0026quot;: X}, return_type='dataframe') fit = sm.GLM(y, transformed_x).fit() y_hat = fit.predict(dmatrix(spline, {\u0026quot;x\u0026quot;: x_grid}, return_type='dataframe')) return y_hat y_hats = [fit_predict_spline(s, X_short, y_short, x_grid_short) for s in [spline1, spline2, spline3, spline4]]  Plotting plot_splines(df_short, x_grid_short, y_hats)  Comment The first example makes us think on why would we want out function to be discontinuous. Unless we expect a sudden wage jump at a certain age, we would like the function to be continuous. However, if for example we split age around the retirement age, we might expect a discontinuity.\nThe second example (top right) makes us think on why would we want out function not to be differentiable. Unless we have some specific mechanism in mind, ususally there is a trade-off between making the function non-differentiable or increasing the degree of the polynomial, as the last two examples show us. We get a similar fit with a quadratic fit or a discontinuous linear fit. The main difference is that in the second case we are picking the discontinuity point by hand instead of letting the data choose how to change the slope of the curve.\nThe Spline Basis Representation How can we fit a piecewise degree-d polynomial under the constraint that it (and possibly its first d − 1 derivatives) be continuous?\nThe most direct way to represent a cubic spline is to start off with a basis for a cubic polynomial—namely, x,x2,x3—and then add one truncated power basis function per knot. A truncated power basis function is defined as\n$$ h(x, c)=(x-c)_{+}^{3} = \\Bigg{\\begin{array}{cl} (x-c)^{3} \u0026amp; \\text { if } x\u0026gt;c \\ 0 \u0026amp; \\text { otherwise } \\end{array} $$\nOne can show that adding a term of the form $\\beta_4 h(x, c)$ to the model for a cubic polynomial will lead to a discontinuity in only the third derivative at $c$; the function will remain continuous, with continuous first and second derivatives, at each of the knots.\nCubic Splines One way to specify the spline is using nodes and degrees of freedom.\n# Specifying 3 knots and 3 degrees of freedom spline5 = \u0026quot;bs(x, knots=(25,40,60), degree=3, include_intercept=False)\u0026quot; pred5 = fit_predict_spline(spline5, X, y, x_grid)  No Knots When we fit a spline, where should we place the knots?\nThe regression spline is most flexible in regions that contain a lot of knots, because in those regions the polynomial coefficients can change rapidly. Hence, one option is to place more knots in places where we feel the function might vary most rapidly, and to place fewer knots where it seems more stable. While this option can work well, in practice it is common to place knots in a uniform fashion. One way to do this is to specify the desired degrees of freedom, and then have the software automatically place the corresponding number of knots at uniform quantiles of the data.\n# Specifying degree 3 and 6 degrees of freedom spline6 = \u0026quot;bs(x, df=6, degree=3, include_intercept=False)\u0026quot; pred6 = fit_predict_spline(spline6, X, y, x_grid)  Natural Splines A natural spline is a regression spline with additional boundary constraints: the function is required to be linear at the boundary (in the region where X is smaller than the smallest knot, or larger than the largest knot). This addi- tional constraint means that natural splines generally produce more stable estimates at the boundaries.\n# Natural spline with 4 degrees of freedom spline7 = \u0026quot;cr(x, df=4)\u0026quot; pred7 = fit_predict_spline(spline7, X, y, x_grid)  Comparison # Compare predictons preds = [pred5, pred6, pred7] labels = ['degree 3, knots 3', 'degree 3, degrees of freedom 3', 'natural, degrees of freedom 4'] compare_predictions(X, y, x_grid, preds, labels)  Comparison to Polynomial Regression Regression splines often give superior results to polynomial regression. This is because unlike polynomials, which must use a high degree to produce flexible fits, splines introduce flexibility by increasing the number of knots but keeping the degree fixed.\nWe are now fitting a polynomial of degree 15 and a spline with 15 degrees of freedom.\n# Polynomial of degree 15 X_poly15 = PolynomialFeatures(15).fit_transform(df.age.values.reshape(-1,1)) ols_poly_15 = sm.OLS(y, X_poly15).fit() pred8 = ols_poly_15.predict(PolynomialFeatures(15).fit_transform(x_grid)) # Spline with 15 degrees of freedon spline9 = \u0026quot;bs(x, df=15, degree=3, include_intercept=False)\u0026quot; pred9 = fit_predict_spline(spline9, X, y, x_grid)  Plotting # Compare predictons preds = [pred8, pred9] labels = ['Polynomial', 'Spline'] compare_predictions(X, y, x_grid, preds, labels)  As we can see, despite the two regressions having the same degrees of freedom, the polynomial fit is much more volatile. We can compare them along some dimensions.\nLocal Regression So far we have looked at so-called \u0026ldquo;global methods\u0026quot;: methods that try to fit a unique function specification over the whole data. The function specification can be complex, as in the case of splines, but can be expressed globally.\nLocal regression is a different approach for fitting flexible non-linear functions, which involves computing the fit at a target point $x_0$ using only the nearby training observations.\nDetails How does local regression work?\nIngredients: $X$, $y$.\nHow to you output a prediction $\\hat y_i$ at a new point $x_i$?\n Take a number of points in $X$ close to $x_i$: $X_{\\text{close-to-i}}$ Assign a weight to each of there points Fit a weigthed least squares regression of $X_{\\text{close-to-i}}$ on $y_{\\text{close-to-i}}$ Use the estimated coefficients $\\hat \\beta$ to predict $\\hat y_i = \\hat \\beta_0 + \\hat \\beta_1 x_i$  Generate Data # Set seed np.random.seed(1) # Generate data X_sim = np.sort(np.random.uniform(0,1,100)) e = np.random.uniform(-.5,.5,100) y_sim = -4*X_sim**2 + 3*X_sim + e # True Generating process without noise X_grid = np.linspace(0,1,100) y_grid = -4*X_grid**2 + 3*X_grid  Plotting Let\u0026rsquo;s visualize the simulated data and the curve without noise.\nplot_simulated_data(X_sim, y_sim, X_grid, y_grid);  Fit LL Regression Now we fit a local linear regression.\n# Settings spec = 'll' bandwidth = 0.1 kernel = 'gaussian' # Locally linear regression local_reg = KernelReg(y_sim, X_sim.reshape(-1,1), var_type='c', reg_type=spec, bw=[bandwidth]) y_hat = KernelReg.fit(local_reg)  What do the parameters mean?\n var_type: dependent variable type (c i.e. continuous) reg_type: local regression specification (ll i.e. locally linear) bw : bandwidth length (0.1) ckertype: kernel type (gaussian)  Prediction What does the prediction looks like?\nfig, ax = plot_simulated_data(X_sim, y_sim, X_grid, y_grid); make_figure_7_9a(fig, ax, X_sim, y_hat);  Details How exactly was the prediction generated? It was generated pointwise. We are now going to look at the prediction at one particular point: $x_i=0.5$.\nWe proceed as follows:\n We select the focal point: $x_i=0.5$ We select observations close to $\\ x_i$, i.e. $x_{\\text{close to i}} = { x \\in X : |x_i - x| \u0026lt; 0.1 } \\ $ and $ \\ y_{\\text{close to i}} = { y \\in Y : |x_i - x| \u0026lt; 0.1 }$ We apply gaussian weights We run a weighted linear regression of $y_{\\text{close to i}}$ on $x_{\\text{close to i}}$  # Get local X and y x_i = 0.5 close_to_i = (x_i-bandwidth \u0026lt; X_sim) \u0026amp; (X_sim \u0026lt; x_i+bandwidth) X_tilde = X_sim[close_to_i] y_tilde = y_sim[close_to_i] # Get local estimates local_estimate = KernelReg.fit(local_reg, data_predict=[x_i]) y_i_hat = local_estimate[0] beta_i_hat = local_estimate[1] alpha_i_hat = y_i_hat - beta_i_hat*x_i print('Estimates: alpha=%1.4f, beta=%1.4f' % (alpha_i_hat, beta_i_hat))  Estimates: alpha=0.7006, beta=-0.6141  Visualization Now we can use the locally estimated coefficients to predict the value of $\\hat y_i(x_i)$ for $x_i = 0.5$.\n# Build local predictions close_to_i_grid = (x_i-bandwidth \u0026lt; X_grid) \u0026amp; (X_grid \u0026lt; x_i+bandwidth) X_grid_tilde = X_grid[close_to_i_grid].reshape(-1,1) y_grid_tilde = alpha_i_hat + X_grid_tilde*beta_i_hat  fig, ax = plot_simulated_data(X_sim, y_sim, X_grid, y_grid); make_figure_7_9a(fig, ax, X_sim, y_hat); make_figure_7_9b(fig, ax, X_tilde, y_tilde, X_grid_tilde, y_grid_tilde, x_i, y_i_hat)  Zooming in We can zoom in and look only at the \u0026ldquo;close to i\u0026rdquo; sample.\nsns.regplot(X_tilde, y_tilde);  Why is the line upward sloped? We forgot the gaussian weights.\n# Weights w = norm.pdf((X_sim-x_i)/bandwidth) # Estimate LWS mod_wls = sm.WLS(y_sim, sm.add_constant(X_sim), weights=w) results = mod_wls.fit() print('Estimates: alpha=%1.4f, beta=%1.4f' % tuple(results.params))  Estimates: alpha=0.7006, beta=-0.6141  We indeed got the same estimates as before. Note two things:\n the badwidth defines the scale parameter of the gaussian weights our locally linear regression is acqually global  Plotting make_figure_7_9d(X_sim, y_sim, w, results, X_grid, x_i, y_i_hat)  Now the slope is indeed negative, as in the locally linear regression.\nGeneralized Additive Models Generalized additive models (GAMs) provide a general framework for extending a standard linear model by allowing non-linear functions of each of the variables, while maintaining additivity.\nGAM for Regression Problems Imagine to extend the general regression framework to some separabily additive model of the form\n$$ y_i = \\beta_0 + \\sum_{k=1}^K \\beta_k f_k(x_{ik}) + \\varepsilon_i $$\nIt is called an additive model because we calculate a separate $f_k$ for each $X_k$, and then add together all of their contributions.\nConsider for example the following model\n$$ \\text{wage} = \\beta_0 + f_1(\\text{year}) + f_2(\\text{age}) + f_3(\\text{education}) + \\varepsilon $$\nExample We are going to use the following functions:\n $f_1$: natural spline with 8 degrees of freedom $f_2$: natural spline with 10 degrees of freedom $f_3$: step function  # Set X and y df['education_'] = LabelEncoder().fit_transform(df[\u0026quot;education\u0026quot;]) X = df[['year','age','education_']].to_numpy() y = df[['wage']].to_numpy() ## model linear_gam = LinearGAM(s(0, n_splines=8) + s(1, n_splines=10) + f(2)) linear_gam.gridsearch(X, y);  100% (11 of 11) |########################| Elapsed Time: 0:00:00 Time: 0:00:00  Plotting plot_gam(linear_gam)  Pros and Cons Before we move on, let us summarize the advantages of a GAM.\n GAMs allow us to fit a non-linear $f_k$ to each $X_k$, so that we can automatically model non-linear relationships that standard linear regression will miss The non-linear fits can potentially make more accurate predictions Because the model is additive, we can still examine the effect of each $X_k$ on $Y$ separately The smoothness of the function $f_k$ for the variable $X_k$ can be summarized via degrees of freedom.  The main limitation of GAMs is that the model is restricted to be additive. With many variables, important interactions can be missed. However, as with linear regression, we can manually add interaction terms to the GAM model by including additional predictors of the form $X_j \\times X_k$.\nGAMs for Classification Problems We can use GAMs also with a binary dependent variable.\n# Binary dependent variable y_binary = (y\u0026gt;250) ## Logit link function logit_gam = LogisticGAM(s(0, n_splines=8) + s(1, n_splines=10) + f(2), fit_intercept=True) logit_gam.gridsearch(X, y_binary);  100% (11 of 11) |########################| Elapsed Time: 0:00:00 Time: 0:00:00  Plotting plot_gam(logit_gam)  The results are qualitatively similar to the non-binary case.\n","date":1646784000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1646784000,"objectID":"0f8ac3b5f6c7bc841b2b14305295ef5f","permalink":"https://matteocourthoud.github.io/course/ml-econ/03_nonparametric/","publishdate":"2022-03-09T00:00:00Z","relpermalink":"/course/ml-econ/03_nonparametric/","section":"course","summary":"# Setup %matplotlib inline from utils.lecture03 import *  Dataset For this session, we are mostly going to work with the wage dataset.\ndf = pd.read_csv('data/Wage.csv', index_col=0) df.head(3)   .","tags":null,"title":"Non-Parametric Regression","type":"book"},{"authors":null,"categories":null,"content":"Convergence Sequences A sequence of nonrandom numbers $\\lbrace a_n \\rbrace$ converges to $a$ (has limit $a$) if for all $\\varepsilon\u0026gt;0$, there exists $n _ \\varepsilon$ such that if $n \u0026gt; n_ \\varepsilon$, then $|a_n - a| \u0026lt; \\varepsilon$. We write $a_n \\to a$ as $n \\to \\infty$.\nA sequence of nonrandom numbers $\\lbrace a_n \\rbrace$ is bounded if and only if there is some $B \u0026lt; \\infty$ such that $|a_n| \\leq B$ for all $n=1,2,\u0026hellip;$ Otherwise, we say that $\\lbrace a_n \\rbrace$ is unbounded.\nBig-O and Small-o Notation A sequence of nonrandom numbers $\\lbrace a_n \\rbrace$ is $O(N^\\delta)$ (at most of order $N^\\delta$) if $N^{-\\delta} a_n$ is bounded. When $\\delta=0$, $a_n$ is bounded, and we also write $a_n = O(1)$ (big oh one).\nA sequence of nonrandom numbers $\\lbrace a_n \\rbrace$ is $o(N^\\delta)$ if $N^{-\\delta} a_n \\to 0$. When $\\delta=0$, $a_n$ converges to zero, and we also write $a_n = o(1)$ (little oh one).\n Properties\n if $a_n = o(N^{\\delta})$, then $a_n = O(N^\\delta)$ if $a_n = o(1)$, then $a_n = O(1)$ if each element of a sequence of vectors or matrices is $O(N^\\delta)$, we say the sequence of vectors or matrices is $O(N^\\delta)$ similarly for $o(N^\\delta)$.   Convergence in Probability A sequence of random variables $\\lbrace X_n \\rbrace$ converges in probability to a constant $c \\in \\mathbb R$ if for all $\\varepsilon\u0026gt;0$ $$ \\Pr \\big( |X_n - c| \u0026gt; \\varepsilon \\big) \\to 0 \\qquad \\text{ as } n \\to \\infty $$ We write $X_n \\overset{p}{\\to} c$ and say that $a$ is the probability limit (plim) of $X_n$: $\\mathrm{plim} X_n = c$. In the special case where $c=0$, we also say that $\\lbrace X_n \\rbrace$ is $o_p(1)$ (little oh p one). We also write $X_n = o_p(1)$ or $X_n \\overset{p}{\\to} 0$.\nA sequence of random variables $\\lbrace X_n \\rbrace$ is bounded in probability if for every $\\varepsilon\u0026gt;0$, there exists a $B _ \\varepsilon \u0026lt; \\infty$ and an integer $n_ \\varepsilon$ such that $$ \\Pr \\big( |x_ n| \u0026gt; B_ \\varepsilon \\big) \u0026lt; \\varepsilon \\qquad \\text{ for all } n \u0026gt; n_ \\varepsilon $$ We write $X_n = O_p(1)$ ($\\lbrace X_n \\rbrace$ is big oh p one).\nA sequence of random variables $\\lbrace X_n \\rbrace$ is $o_p(a_n)$ where $\\lbrace a_n \\rbrace$ is a nonrandom positive sequence, if $X_n/a_n = o_p(1)$. We write $X_n = o_p(a_n)$.\nA sequence of random variables $\\lbrace X_n \\rbrace$ is $O_p(a_n)$ where $\\lbrace a_n \\rbrace$ is a nonrandom positive sequence, if $X_n/a_n = O_p(1)$. We write $X_n = O_p(a_n)$.\nOther Convergences A sequence of random variables $\\lbrace X_n \\rbrace$ converges almost surely to a constant $c \\in \\mathbb R$ if $$ \\Pr \\big( X_n \\overset{p}{\\to} c \\big) = 1 $$ We write $X_n \\overset{as}{\\to} c$.\nA sequence of random variables $\\lbrace X_n \\rbrace$ converges in mean square to a constant $c \\in \\mathbb R$ if $$ \\mathbb E [(X_n - c)^2] \\to 0 \\qquad \\text{ as } n \\to \\infty $$ We write $X_n \\overset{ms}{\\to} c$.\nLet $\\lbrace X_n \\rbrace$ be a sequence of random variables and $F_n$ be the cumulative distribution function (cdf) of $X_n$. We say that $X_n$ converges in distribution to a random variable $x$ with cdf $F$ if the cdf $F_n$ of $X_n$ converges to the cdf $F$ of $x$ at every continuity point of $F$. We write $X_n \\overset{d}{\\to} x$ and we call $F$ the asymptotic distribution of $X_n$.\nCompare Convergences Lemma: Let $\\lbrace X_n \\rbrace$ be a sequence of random variables and $c \\in \\mathbb R$\n $X_n \\overset{ms}{\\to} c \\ \\Rightarrow \\ X_n \\overset{p}{\\to} c$ $X_n \\overset{as}{\\to} c \\ \\Rightarrow \\ X_n \\overset{p}{\\to} c$ $X_n \\overset{p}{\\to} c \\ \\Rightarrow \\ X_n \\overset{d}{\\to} c$   Note that all the above definitions naturally extend to a sequence of random vectors by requiring element-by-element convergence. For example, a sequence of $K \\times 1$ random vectors $\\lbrace X_n \\rbrace$ converges in probability to a constant $c \\in \\mathbb R^K$ if for all $\\varepsilon\u0026gt;0$ $$ \\Pr \\big( |X _ {nk} - c_k| \u0026gt; \\varepsilon \\big) \\to 0 \\qquad \\text{ as } n \\to \\infty \\quad \\forall k = 1\u0026hellip;K $$\n Theorems Slutsky Theorem Theorem\nLet $\\lbrace X_n \\rbrace$ and $\\lbrace Y_n \\rbrace$ be two sequences of random variables, $x$ a random variable and $c \\in \\mathbb R$ a constant such that $\\lbrace X_n \\rbrace \\overset{d}{\\to} X$ and $\\lbrace Y_n \\rbrace \\overset{p}{\\to} c$. Then\n $X_n + Y_n \\overset{d}{\\to} X + c$ $X_n \\cdot Y_n \\overset{d}{\\to} X \\cdot c$  Continuous Mapping Theorem Theorem\nLet $\\lbrace X_n \\rbrace$ be sequence of $K \\times 1$ random vectors and $g: \\mathbb{R}^K \\to \\mathbb{R}^J$ a continuous function that does not depend on $n$.Then\n $x _n \\overset{as}{\\to} x \\ \\Rightarrow \\ g(X_n) \\overset{as}{\\to} g(x)$ $x _n \\overset{p}{\\to} x \\ \\Rightarrow \\ g(X_n) \\overset{p}{\\to} g(x)$ $x _n \\overset{d}{\\to} x \\ \\Rightarrow \\ g(X_n) \\overset{d}{\\to} g(x)$  Weak Law of Large Numbers Theorem\nLet $\\lbrace x_i \\rbrace _ {i=1}^n$ be a sequence of independent, identically distributed random variables such that $\\mathbb{E}[|x_i|] \u0026lt; \\infty$. Then the sequence satisfies the weak law of large numbers (WLLN): $$ \\mathbb{E}_n[x_i] = \\frac{1}{n} \\sum _ {i=1}^n x_i \\overset{p}{\\to} \\mu \\qquad \\text{ where } \\mu \\equiv \\mathbb{E}[x_i] $$\n Intuitions for the law of large numbers:\n Cancellation with high probability. Re-visiting regions of the sample space over and over again.   WLLN Proof The independence of the random variables implies no correlation between them, and we have that $$ Var \\left( \\mathbb{E}_n[x_i] \\right) = Var \\left( \\frac{1}{n} \\sum _ {i=1}^n x_i \\right) = \\frac{1}{n^2} Var\\left( \\sum _ {i=1}^n x_i \\right) = \\frac{n \\sigma^2}{n^2} = \\frac{\\sigma^2}{n} $$ Using Chebyshev’s inequality on $\\mathbb{E}_n[x_i]$ results in $$ \\Pr \\big( \\left|\\mathbb{E}_n[x_i]-\\mu \\right| \u0026gt; \\varepsilon \\big) \\leq {\\frac {\\sigma ^{2}}{n\\varepsilon ^{2}}} $$ As $n$ approaches infinity, the right hand side approaches $0$. And by definition of convergence in probability, we have obtained $\\mathbb{E}_n[x_i] \\overset{p}{\\to} \\mu$ as $n \\to \\infty$. $$\\tag*{$\\blacksquare$}$$\nCentral Limit Theorem Lindberg-Levy Central Limit Theorem\nLet $\\lbrace x_i \\rbrace _ {i=1}^n$ be a sequence of independent, identically distributed random variables such that $\\mathbb{E}[x_i^2] \u0026lt; \\infty$, and $\\mathbb{E}[x_i] = \\mu$. Then $\\lbrace x_i \\rbrace$ satisfies the central limit theorem (CLT); that is, $$ \\frac{1}{\\sqrt{n}} \\sum _ {i=1}^{n} (x_i - \\mu) \\overset{d}{\\to} N(0,\\sigma^2) $$ where $\\sigma^2 = Var(x_i) = \\mathbb{E}[x_i x_i']$ is necessarily positive semidefinite.\nCLT Proof (1) Suppose $\\lbrace x_i \\rbrace$ are independent and identically distributed random variables, each with mean $\\mu$ and finite variance $\\sigma^2$. The sum $x_1 + \u0026hellip; + X_n$ has mean $n \\mu$ and variance $n \\sigma^2$.\nConsider the random variable $$ Z_n = \\frac{x_1 + \u0026hellip; + X_n - n\\mu}{\\sqrt{n \\sigma^2}} = \\sum _ {i=1}^n \\frac{x_i - \\mu}{\\sqrt{n \\sigma^2}} = \\sum _ {i=1}^n \\frac{1}{\\sqrt{n}} \\tilde x_i $$\nwhere in the last step we defined the new random variables $\\tilde x_i = \\frac{x_i - \\mu}{\\sigma}$ each with zero mean and unit variance. The characteristic function of $Z_n$ is given by $$ \\varphi _ {Z_n} (t) = \\varphi _ { \\sum _ {i=1}^n \\frac{1}{\\sqrt{n} } \\tilde{x}_i}(t) = \\varphi _ {\\tilde x_1} \\left( \\frac{t}{\\sqrt{n}} \\right) \\times \u0026hellip; \\times \\varphi _ {Y_n} \\left( \\frac{t}{\\sqrt{n}} \\right) = \\left[ \\varphi _ {\\tilde x_1} \\left( \\frac{t}{\\sqrt{n}} \\right) \\right]^n $$\nwhere in the last step we used the fact that all of the $\\tilde{x}_i$ are identically distributed.\nCLT Proof (2) The characteristic function of $\\tilde{x}_1$ is, by Taylor’s theorem, $$ \\varphi _ {\\tilde{x}_1} \\left( \\frac{t}{\\sqrt{n}} \\right) = 1 - \\frac{t^2}{2n} + o \\left( \\frac{t^2}{n} \\right) \\qquad \\text{ for } n \\to \\infty $$\nwhere $o(t^2)$ is “little o notation” for some function of $t$ that goes to zero more rapidly than $t^2$. By the limit of the exponential function, the characteristic function of $Z_n$ equals $$ \\varphi _ {Z_ n}(t) = \\left[ 1 - \\frac{t^2}{2n} + o \\left( \\frac{t^2}{n} \\right) \\right]^n \\to e^{ -\\frac{1}{2}t^2 } \\qquad \\text{ for } n \\to \\infty $$\nNote that all of the higher order terms vanish in the limit $n \\to \\infty$. The right hand side equals the characteristic function of a standard normal distribution $N(0,1)$, which implies through Lévy’s continuity theorem that the distribution of $Z_ n$ will approach $N(0,1)$ as $n \\to \\infty$. Therefore, the sum $x_1 + \u0026hellip; + x_n$ will approach that of the normal distribution $N(n_{\\mu}, n\\sigma^2)$, and the sample average $$ \\mathbb{E}_n [x_i] = \\frac{1}{n} \\sum _ {i=1}^n x_i $$\nconverges to the normal distribution $N(\\mu, \\sigma^2)$, from which the central limit theorem follows. $$\\tag*{$\\blacksquare$}$$\nDelta Method Let $\\lbrace X_n \\rbrace$ be a sequence of independent, identically distributed $K \\times 1$ random vectors such that\n $\\sqrt{n} (X_n - c) \\overset{d}{\\to} Z$ for some fixed $c \\in \\mathbb{R}^K$ and $\\Sigma$ a $K \\times K$ positive definite matrix.  Suppose $g : \\mathbb{R}^K \\to \\mathbb{R}^J$ with $J \\leq K$ is continuously differentiable and full rank at $c$, then $$ \\sqrt{n} \\Big[ g(X_n) - g( c ) \\Big] \\overset{d}{\\to} G Z $$\nwhere $G = \\frac{\\partial g( c )}{\\partial x}$ is the $J \\times K$ matrix of partial derivatives evaluated at $c$.\n Note that the most common utilization is with the random variable $\\mathbb E_n [x_i]$. In fact, under the assumptions of the CLT, we have that $$ \\sqrt{n} \\Big[ g \\big( \\mathbb E_n [x_i] \\big) - g(\\mu) \\Big] \\overset{d}{\\to} N(0, G \\Sigma G') $$\n Ergodic Theory PPT Let $(\\Omega, \\mathcal{B}, P)$ be a probability space and $T: \\Omega \\rightarrow \\Omega$ a measurable map. $T$ is a probability preserving transformation if the probability of the pre-image of every set is the same as the probability of the set itself, i.e. $\\forall G, \\Pr(T^{-1}(G)) = \\Pr(G)$.\nLet $(\\Omega, \\mathcal{B}, P)$ be a probability space and $T: \\Omega \\rightarrow \\Omega$ a PPT. A set $G \\in \\mathcal{B}$ is invariant if $T^{-1}(G)=G$.\n Note that it does not have to work the other way around: $G \\neq T(G)$.\n Let $(\\Omega, \\mathcal{B}, P)$ be a probability space and $T: \\Omega \\rightarrow \\Omega$ a PPT. $T$ is ergodic if every invariant set $G \\in \\mathcal{B}$ has probability zero or one, i.e. $\\Pr(G) = 0 \\lor \\Pr(G) = 1$.\nPoincarè Recurrence Theorem\nLet $(\\Omega, \\mathcal{B}, P)$ be a probability space and $T: \\Omega \\rightarrow \\Omega$ a PPT. Suppose $A \\in \\mathcal{B}$ is measurable. Then, for almost every $\\omega \\in A$, $T^n(\\omega)\\in A$ for infinitely many $n$.\nProof\nWe follow 5 steps:\n Let $G = \\lbrace \\omega \\in A : T^K(\\omega) \\notin A \\quad \\forall k \u0026gt;0 \\rbrace$: the set of all points of A that never ``return” in A. Note that $\\forall j \\geq 1$, $T^{-j}(G) \\cap G = \\emptyset$. In fact, suppose $\\omega \\in T^{-j}(G)$. Then $\\omega \\notin G$ since otherwise we would have $\\omega \\in G \\subseteq A$ and $\\omega \\in T^J(G) \\subseteq A$ which contradicts the definition of $G$. It follows that $\\forall l,n \\geq 1$, $T^{-l}(G) \\cap T^{-n}(G) = \\emptyset$ Since $T$ is a PPT, $\\Pr(T^{-j}(G)) = \\Pr(G)$ $\\forall j$ Then $$ \\Pr (T^{-1}(G) \\cup T^{-2}(G) \\cup \u0026hellip; \\cup T^{-l}(G)) = l \\cdot \\Pr(G) \\leq 1 \\Rightarrow \\Pr(G) \\leq \\frac{1}{l} \\quad \\Rightarrow \\quad \\lim_ {l \\to \\infty} \\Pr(G) = 0 $$ $$\\tag*{$\\blacksquare$}$$  Comment Halmos: “The recurrence theorem says that under the appropriate conditions on a transformation T almost every point of each measurable set $A$ returns to $A$ infinitely often. It is natural to ask: exactly how long a time do the images of such recurrent points spend in $A$? The precise formulation of the problem runs as follows: given a point $x$ (for present purposes it does not matter whether $x$ is in $A$ or not), and given a positive integer $n$, form the ratio of the number of these points that belong to $A$ to the total number (i.e., to $n$), and evaluate the limit of these ratios as $n$ tends to infinity. It is, of course, not at all obvious in what sense, if any, that limit exists. If $f$ is the characteristic function of $A$ then the ratio just discussed is” $$ \\frac{1}{n} \\sum _ {i=1}^n f(T^{i}x) = \\frac{1}{n} \\sum _ {i=1}^n x_i $$\nErgodic Theorem Theorem\nLet $T$ be an ergodic PPT on $\\Omega$. Let $x$ be a random variable on $\\Omega$ with $\\mathbb{E}[x] \u0026lt; \\infty$. Let $x_i = x \\circ T^i$. Then, $$ \\frac{1}{n} \\sum _ {i=1}^n x_i \\overset{as}{\\to} \\mathbb{E}[x] $$\n To figure out whether a PPT is ergodic, it’s useful to draw a graph with $T^{-1}(G)$ on the y-axis and $G$ on the x-axis.\n Comment From the ergodic theorem, we have that $$ \\lim _ {n \\to \\infty} \\frac{1}{n} \\sum _ {i=1}^n f(T^{i}x) g(x) = f^* (x)g(x) \\quad \\Rightarrow \\quad \\lim _ {n \\to \\infty} \\Pr(T^{-n}G \\cap H) = \\Pr(G)\\Pr(H) $$ where $f^* (x) = \\int f(x) dx = \\mathbb{E}[f]$.\n[Halmos]: We have seen that if a transformation $T$ is ergodic, then $\\Pr(T^{-n}G \\cap H)$ converges in the sense of Cesaro to $\\Pr(G)\\Pr(H)$. The validity of this condition for all $G$ and $H$ is, in fact, equivalent to ergodicity. To prove this, suppose that $A$ is a measurable invariant set, and take both $G$ and $H$ equal to $A$. It follows that $\\Pr(A) = (\\Pr(A))^2$, and hence that $\\Pr(A)$ is either 0 or 1.\nComment 2 The Cesaro convergence condition has a natural intuitive interpretation. We may visualize the transformation $T$ as a particular way of stirring the contents of a vessel (of total volume 1) full of an incompressible fluid, which may be thought of as 90 per cent gin ($G$) and 10 per cent vermouth ($H$). If $H$ is the region originally occupied by the vermouth, then, for any part $G$ of the vessel, the relative amount of vermouth in $G$, after $n$ repetitions of the act of stirring, is given by $\\Pr(T^{-n}G \\cap H)/\\Pr(H)$. The ergodicity of $T$ implies therefore that on the average this relative amount is exactly equal to 10 per cent. In general, in physical situations like this one, one expects to be justified in making a much stronger statement, namely that, after the liquid has been stirred sufficiently often ($n \\to \\infty$), every part $G$ of the container will contain approximately 10 per cent vermouth. In mathematical language this expectation amounts to replacing Cesaro convergence by ordinary convergence, i.e., to the condition $\\lim_ {n\\to \\infty} \\Pr(T^{-n}G \\cap H) = \\Pr(G)\\Pr(H)$. If a transformation $T$ satisfies this condition for every pair $G$ and $H$ of measurable sets, it is called mixing, or, in distinction from a related but slightly weaker concept, strongly mixing.”\nMixing Let $\\lbrace\\Omega, \\mathcal{B}, P \\rbrace$ be a probability space. Let $T$ be a probability preserving transform. Then $T$ is strongly mixing if for every invariant sets $G$,$H \\in \\mathcal{B}$ $$ P(G \\cap T^{-k}H) \\to P(G)P(H) \\quad \\text{ as } k \\to \\infty $$ where $T^{-k}H$ is defined as $T^{-k}H = T^{-1}(\u0026hellip;T^{-1}(T^{-1} H)\u0026hellip;)$ repeated $k$ times.\nLet $\\lbrace X_i\\rbrace _ {i=-\\infty}^{\\infty}$ be a two sided sequence of random variables. Let $\\mathcal{B}_ {-\\infty}^n$ be the sigma algebra generated by $\\lbrace X_i\\rbrace _ {i=-\\infty}^{n}$ and $\\mathcal{B}_ {n+k}^\\infty$ the sigma algebra generated by $\\lbrace X_i \\rbrace _ {i=n+k}^{\\infty}$. Define the mixing coefficient $$ \\alpha(k) = \\sup_ {n \\in \\mathbb{Z}} \\sup_ {G \\in \\mathcal{B}_ {-\\infty}^n} \\sup_ {H \\in \\mathcal{B}_ {n+k}^\\infty} | \\Pr(G \\cap H) - \\Pr(G) \\Pr(H)| $$ $\\lbrace X_i \\rbrace$ is $\\mathbb{\\alpha}$-mixing if $\\alpha(k) \\to 0$ if $k \\to \\infty$.\n Note that mixing implies ergodicity.\n Stationarity Let $X_i : \\Omega \\to \\mathbb{R}$ be a (two sided) sequence of random variables with $i \\in \\mathbb{Z}$. $X_i$ is strongly stationary or simply stationary if $$ \\Pr (X _ {i_ 1} \\leq a_ 1 , \u0026hellip; , X _ {i_ k} \\leq a_ k ) = \\Pr (X _ { i _ {1-s}} \\leq a_ 1 , \u0026hellip; , X _ {i _ {k-s}} \\leq a_ k) \\quad \\text{ for every } i_ 1, \u0026hellip;, i_ k, a_ 1, \u0026hellip;, a_ k, s \\in \\mathbb{R}. $$\nLet $X_i : \\Omega \\to \\mathbb{R}$ be a (two sided) sequence of random variables with $i \\in \\mathbb{Z}$. $X_i$ is covariance stationary if $\\mathbb{E}[X_i] = \\mathbb{E}[X_j]$ for every $i,j$ and $\\mathbb{E}[X_i X_j] = \\mathbb{E}[X _ {i+k} X _ {j+k}]$ for all $i,j,k$. All of the second moments above are assumed to exist.\nLet $X_t : \\Omega \\to \\mathbb{R}$ be a sequence of random variables indexed by $t \\in \\mathbb{Z}$ such that $\\mathbb{E}[|X_t|] \u0026lt; 1$ for each $t$. $X_t$ is a martingale if $\\mathbb{E} [X _ t |X _ {t-1} , X _ {t-2} , \u0026hellip;] = X _ t$. $X_t$ is a martingale difference if $\\mathbb{E} [X _ t | X _ {t-1} , X _ {t-2} ,\u0026hellip;] = 0$.\nGordin’s Central Limit Theorem Theorem\nLet $\\lbrace z_i \\rbrace$ be a stationary, $\\alpha$-mixing sequence of random variables. If moreover\n $\\sum_ {m=1}^\\infty \\alpha(m)^{\\frac{\\delta}{2 + \\delta}} \u0026lt; \\infty$ $\\mathbb{E}[z_i] = 0$ $\\mathbb{E}\\Big[ ||z_i || ^ {2+\\delta} \\Big] \u0026lt; \\infty$  Then $$ \\sqrt{n} \\mathbb{E}_n [z_i] \\overset{d}{\\to} N(0,\\Omega) \\quad \\text{ where } \\quad \\Omega = \\lim _ {n \\to \\infty} Var(\\sqrt{n} \\mathbb{E}_n [z_i]) $$\nLet $\\Omega_k = \\mathbb{E}[ z_i z _ {i+k}']$. Then a necessary condition for Gordin’s CLT is covariance summability: $\\sum _ {k=1}^\\infty \\Omega_k \u0026lt; \\infty$.\nErgodic Central Limit Theorem Theorem\nLet $\\lbrace z_i \\rbrace$ be a stationary, ergodic, martingale difference sequence. Then $$ \\sqrt{n} \\mathbb{E}_n [z_i] \\overset{d}{\\to} N(0,\\Omega) \\quad \\text{ where } \\quad \\Omega = \\lim _ {n \\to \\infty} Var(\\sqrt{n}\\mathbb{E}_n[z_i]) $$\n","date":1635465600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1635465600,"objectID":"70668fa894234c30126070b28f67c601","permalink":"https://matteocourthoud.github.io/course/metrics/03_asymptotics/","publishdate":"2021-10-29T00:00:00Z","relpermalink":"/course/metrics/03_asymptotics/","section":"course","summary":"Convergence Sequences A sequence of nonrandom numbers $\\lbrace a_n \\rbrace$ converges to $a$ (has limit $a$) if for all $\\varepsilon\u0026gt;0$, there exists $n _ \\varepsilon$ such that if $n \u0026gt; n_ \\varepsilon$, then $|a_n - a| \u0026lt; \\varepsilon$.","tags":null,"title":"Asymptotic Theory","type":"book"},{"authors":null,"categories":null,"content":"Setup For the scope of this tutorial we are going to use AirBnb Scraped data for the city of Bologna. The data is freely available at Inside AirBnb: http://insideairbnb.com/get-the-data.html.\nA description of all variables in all datasets is avaliable here.\nWe are going to use 2 datasets:\n listing dataset: contains listing-level information pricing dataset: contains pricing data, over time  import numpy as np import pandas as pd  # Import listings data url_listings = \u0026quot;http://data.insideairbnb.com/italy/emilia-romagna/bologna/2021-12-17/visualisations/listings.csv\u0026quot; df_listings = pd.read_csv(url_listings) # Import pricing data url_prices = \u0026quot;http://data.insideairbnb.com/italy/emilia-romagna/bologna/2021-12-17/data/calendar.csv.gz\u0026quot; df_prices = pd.read_csv(url_prices, compression=\u0026quot;gzip\u0026quot;)  Sorting and Renaming You can sort the data using the sort_values function.\nOptions\n ascending: bool or list of bool, default True na_position: {‘first’, ‘last’}, default ‘last’  df_listings.sort_values(by=['name', 'price'], ascending=[False, True], na_position='last').head()   .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }  \n  id name host_id host_name neighbourhood_group neighbourhood latitude longitude room_type price minimum_nights number_of_reviews last_review reviews_per_month calculated_host_listings_count availability_365 number_of_reviews_ltm license     2280 38601411 🏡Giardino di Annabella-relax in città-casa intera 240803020 Annabella NaN Porto - Saragozza 44.49303 11.31986 Entire home/apt 90 2 53 2021-12-13 1.96 1 76 27 392901   2988 48177313 ❤ Romantic Suite with SPA Bath ❤ 4starbologna.com 239491712 4 Star Bologna NaN Santo Stefano 44.50271 11.34998 Entire home/apt 309 1 1 2021-03-14 0.11 14 344 1 NaN   3302 52367336 ✨House of Alchemy✨ 140013413 Greta NaN Porto - Saragozza 44.49072 11.30890 Entire home/apt 96 2 7 2021-11-28 3.18 1 88 7 NaN   2039 34495335 ♥ Romantic for Couple in Love ♥ | 4 Star Boutique 239491712 4 Star Bologna NaN Santo Stefano 44.50368 11.34972 Entire home/apt 143 1 25 2021-08-20 0.79 14 262 6 NaN   2964 47866124 ♡Amazing Suite with Private SPA ♡ 4starbologna... 239491712 4 Star Bologna NaN Santo Stefano 44.50381 11.34951 Entire home/apt 347 1 2 2021-10-17 0.72 14 337 2 NaN     You can remane columns using the rename() function. It takes a dictionary as column argument in the form {\u0026quot;old_name\u0026quot;: \u0026quot;new_name\u0026quot;}.\ndf_listings.rename(columns={'name': 'listing_name', 'id': 'listing_id'}).head()   .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }  \n  listing_id listing_name host_id host_name neighbourhood_group neighbourhood latitude longitude room_type price minimum_nights number_of_reviews last_review reviews_per_month calculated_host_listings_count availability_365 number_of_reviews_ltm license     0 42196 50 sm Studio in the historic centre 184487 Carlo NaN Santo Stefano 44.48507 11.34786 Entire home/apt 68 3 180 2021-11-12 1.32 1 161 6 NaN   1 46352 A room in Pasolini's house 467810 Eleonora NaN Porto - Saragozza 44.49168 11.33514 Private room 29 1 300 2021-11-30 2.20 2 248 37 NaN   2 59697 COZY LARGE BEDROOM in the city center 286688 Paolo NaN Santo Stefano 44.48817 11.34124 Private room 50 1 240 2020-10-04 2.18 2 327 0 NaN   3 85368 Garden House Bologna 467675 Anna Maria NaN Santo Stefano 44.47834 11.35672 Entire home/apt 126 2 40 2019-11-03 0.34 1 332 0 NaN   4 145779 SINGLE ROOM 705535 Valerio NaN Porto - Saragozza 44.49306 11.33786 Private room 50 10 69 2021-12-05 0.55 9 365 5 NaN     Aggregating If we want to count observations across 2 categorical variables, we can use pd.crosstab().\npd.crosstab(df_listings['neighbourhood'], df_listings['room_type'])   .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }  \n room_type Entire home/apt Hotel room Private room Shared room   neighbourhood         Borgo Panigale - Reno 107 0 39 0   Navile 250 3 149 1   Porto - Saragozza 842 16 299 10   San Donato - San Vitale 280 1 134 4   Santo Stefano 924 29 237 5   Savena 73 0 48 2     We can compute statistics by group using groupby().\ndf_listings.groupby('neighbourhood')[['price', 'reviews_per_month']].mean()   .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }  \n  price reviews_per_month   neighbourhood       Borgo Panigale - Reno 83.020548 0.983488   Navile 142.200993 1.156745   Porto - Saragozza 129.908312 1.340325   San Donato - San Vitale 91.618138 0.933011   Santo Stefano 119.441841 1.344810   Savena 69.626016 0.805888     If you want to perform more than one function, maybe on different columns, you can use aggregate() which can be shortened to agg(). The sintax is agg(output_var = (\u0026quot;input_var\u0026quot;, function)) and it accepts also numpy functions.\ndf_listings.groupby('neighbourhood').agg(mean_reviews=(\u0026quot;reviews_per_month\u0026quot;, \u0026quot;mean\u0026quot;), min_price=(\u0026quot;price\u0026quot;, \u0026quot;min\u0026quot;), max_price=(\u0026quot;price\u0026quot;, np.max)).reset_index()   .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }  \n  neighbourhood mean_reviews min_price max_price     0 Borgo Panigale - Reno 0.983488 9 1429   1 Navile 1.156745 14 5000   2 Porto - Saragozza 1.340325 7 9999   3 San Donato - San Vitale 0.933011 10 1600   4 Santo Stefano 1.344810 11 9999   5 Savena 0.805888 9 680     If we want to build a new column by group, we can use transform() on the grouped data. Unfortunately, it does not work as nicely as aggregate() and we have to do one column at the time.\ndf_listings.groupby('neighbourhood')[['price', 'reviews_per_month']].transform('mean').head()   .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }  \n  price reviews_per_month     0 119.441841 1.344810   1 129.908312 1.340325   2 119.441841 1.344810   3 119.441841 1.344810   4 129.908312 1.340325     Combining Datasets We can concatenate datasets using pd.concat(). It takes as argument a list of dataframes. By default, pd.concat() performs the outer join. We can change it using the join option (in this case, it makes no difference).\ndf_listings1 = df_listings[:2000] np.shape(df_listings1)  (2000, 18)  df_listings2 = df_listings[1000:] np.shape(df_listings2)  (2453, 18)  np.shape( pd.concat([df_listings1, df_listings2]) )  (4453, 18)  To instead merge dataframes, we can use the pd.merge function.\nOptions\n how: {‘left’, ‘right’, ‘outer’, ‘inner’, ‘cross’}, default ‘inner’ on: label or list  df_merged = pd.merge(df_listings, df_prices, left_on='id', right_on='listing_id', how='inner') df_merged.head()   .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }  \n  id name host_id host_name neighbourhood_group neighbourhood latitude longitude room_type price_x ... availability_365 number_of_reviews_ltm license listing_id date available price_y adjusted_price minimum_nights_y maximum_nights     0 42196 50 sm Studio in the historic centre 184487 Carlo NaN Santo Stefano 44.48507 11.34786 Entire home/apt 68 ... 161 6 NaN 42196 2021-12-17 f $68.00 $68.00 3 360   1 42196 50 sm Studio in the historic centre 184487 Carlo NaN Santo Stefano 44.48507 11.34786 Entire home/apt 68 ... 161 6 NaN 42196 2021-12-18 f $68.00 $68.00 3 360   2 42196 50 sm Studio in the historic centre 184487 Carlo NaN Santo Stefano 44.48507 11.34786 Entire home/apt 68 ... 161 6 NaN 42196 2021-12-19 f $68.00 $68.00 3 360   3 42196 50 sm Studio in the historic centre 184487 Carlo NaN Santo Stefano 44.48507 11.34786 Entire home/apt 68 ... 161 6 NaN 42196 2021-12-20 f $68.00 $68.00 3 360   4 42196 50 sm Studio in the historic centre 184487 Carlo NaN Santo Stefano 44.48507 11.34786 Entire home/apt 68 ... 161 6 NaN 42196 2021-12-21 f $68.00 $68.00 3 360    5 rows × 25 columns\n As you can see, since the variable price was present in both datasets, we now have a price.x and a price.y.\nReshaping First, let\u0026rsquo;s compute average prices by neighbourhood and date using the merged dataset.\ndf_long = df_merged.groupby(['neighbourhood', 'date'])['price_x'].agg('mean').reset_index() df_long.head()   .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }  \n  neighbourhood date price_x     0 Borgo Panigale - Reno 2021-12-17 83.020548   1 Borgo Panigale - Reno 2021-12-18 83.020548   2 Borgo Panigale - Reno 2021-12-19 83.020548   3 Borgo Panigale - Reno 2021-12-20 83.020548   4 Borgo Panigale - Reno 2021-12-21 83.020548     This is what is called long format since it has one or more variables (price_x in this case) stacked vertically along a categorical variable (neighborhood and date here), which acts as index.\nThe alternative is the wide format where we have one separate column for each neighborhood.\nWe can reshape the dataset from long to wide using the pd.pivot() command. d\ndf_wide = pd.pivot(data=df_long, index='date', columns='neighbourhood').reset_index() df_wide.head()   .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead tr th { text-align: left; }  \n   date price_x   neighbourhood  Borgo Panigale - Reno Navile Porto - Saragozza San Donato - San Vitale Santo Stefano Savena     0 2021-12-17 83.020548 142.200993 129.908312 91.618138 119.441841 69.626016   1 2021-12-18 83.020548 142.200993 129.908312 91.618138 119.441841 69.626016   2 2021-12-19 83.020548 142.200993 129.908312 91.618138 119.441841 69.626016   3 2021-12-20 83.020548 142.200993 129.908312 91.618138 119.441841 69.626016   4 2021-12-21 83.020548 142.200993 129.908312 91.618138 119.441841 69.626016     We can reshape the dataset from wide to long using the pd.melt() command. It takes the following arguments\n data: the dataframe id_vars: the variable that was indexing the old dataset  pd.melt(df_wide, id_vars='date', value_name='price').head()   .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }  \n  date None neighbourhood price     0 2021-12-17 price_x Borgo Panigale - Reno 83.020548   1 2021-12-18 price_x Borgo Panigale - Reno 83.020548   2 2021-12-19 price_x Borgo Panigale - Reno 83.020548   3 2021-12-20 price_x Borgo Panigale - Reno 83.020548   4 2021-12-21 price_x Borgo Panigale - Reno 83.020548     If we do not have MultiIndex columns, but just a common prefix, we can reshape the dataset from wide to long using the pd.wide_to_long() command.\ndf_wide2 = df_wide.copy() df_wide2.columns = [''.join(col) for col in df_wide2.columns] df_wide2.head()   .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }  \n  date price_xBorgo Panigale - Reno price_xNavile price_xPorto - Saragozza price_xSan Donato - San Vitale price_xSanto Stefano price_xSavena     0 2021-12-17 83.020548 142.200993 129.908312 91.618138 119.441841 69.626016   1 2021-12-18 83.020548 142.200993 129.908312 91.618138 119.441841 69.626016   2 2021-12-19 83.020548 142.200993 129.908312 91.618138 119.441841 69.626016   3 2021-12-20 83.020548 142.200993 129.908312 91.618138 119.441841 69.626016   4 2021-12-21 83.020548 142.200993 129.908312 91.618138 119.441841 69.626016     The pd.wide_to_long() command takes the following arguments\n data: the dataframe stubnames: the prefixes of the variables that we want to reshape into one i: the variable that was indexing the old dataset j: the name of the new categorical variable that we extract from stubnames suffix: regular expression of the suffix, the default is \\d+, i.e. digits  pd.wide_to_long(df_wide2, stubnames='price_x', i='date', j='neighborhood', suffix='\\D+').head()   .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }  \n   price_x   date neighborhood      2021-12-17 Borgo Panigale - Reno 83.020548   2021-12-18 Borgo Panigale - Reno 83.020548   2021-12-19 Borgo Panigale - Reno 83.020548   2021-12-20 Borgo Panigale - Reno 83.020548   2021-12-21 Borgo Panigale - Reno 83.020548     Note that we had to change the suffix to \\D+, i.e. not digits.\nWindow Functions  Methods\n shift() expanding() rolling()   When we have time series data, we might want to do operations across time. First, let\u0026rsquo;s aggregate the df_price dataset at the year-month level.\ntemp = df_prices.copy() temp['price'] = temp['price'].str.replace('[$|,]', '', regex=True).astype(float) temp['date'] = pd.to_datetime(temp['date']).dt.to_period('M') temp = temp.groupby(['listing_id', 'date'])['price'].mean().reset_index()\\ .sort_values(by=['listing_id', 'date'], ascending=[False, True]) temp.head()   .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }  \n  listing_id date price     44876 53854962 2021-12 147.400000   44877 53854962 2022-01 137.645161   44878 53854962 2022-02 124.642857   44879 53854962 2022-03 285.096774   44880 53854962 2022-04 115.000000     We can lead or lag one variable using shift().\ntemp['price1'] = temp['price'].shift(1) temp.head(15)   .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }  \n  listing_id date price price1     44876 53854962 2021-12 147.400000 NaN   44877 53854962 2022-01 137.645161 147.400000   44878 53854962 2022-02 124.642857 137.645161   44879 53854962 2022-03 285.096774 124.642857   44880 53854962 2022-04 115.000000 285.096774   44881 53854962 2022-05 115.000000 115.000000   44882 53854962 2022-06 115.000000 115.000000   44883 53854962 2022-07 115.000000 115.000000   44884 53854962 2022-08 115.000000 115.000000   44885 53854962 2022-09 115.000000 115.000000   44886 53854962 2022-10 115.000000 115.000000   44887 53854962 2022-11 115.000000 115.000000   44888 53854962 2022-12 115.000000 115.000000   44863 53837654 2021-12 184.133333 115.000000   44864 53837654 2022-01 148.741935 184.133333     If we want to lead or lag a variable within a group, we can combine shift() with groupby()\ntemp['price1'] = temp.groupby('listing_id')['price'].shift(1) temp.head(15)   .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }  \n  listing_id date price price1     44876 53854962 2021-12 147.400000 NaN   44877 53854962 2022-01 137.645161 147.400000   44878 53854962 2022-02 124.642857 137.645161   44879 53854962 2022-03 285.096774 124.642857   44880 53854962 2022-04 115.000000 285.096774   44881 53854962 2022-05 115.000000 115.000000   44882 53854962 2022-06 115.000000 115.000000   44883 53854962 2022-07 115.000000 115.000000   44884 53854962 2022-08 115.000000 115.000000   44885 53854962 2022-09 115.000000 115.000000   44886 53854962 2022-10 115.000000 115.000000   44887 53854962 2022-11 115.000000 115.000000   44888 53854962 2022-12 115.000000 115.000000   44863 53837654 2021-12 184.133333 NaN   44864 53837654 2022-01 148.741935 184.133333     We can perform cumulative operations using the expanding() function\ntemp['avg_cum_price'] = temp['price'].expanding().mean() temp.head(15)   .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }  \n  listing_id date price price1 avg_cum_price     44876 53854962 2021-12 147.400000 NaN 147.400000   44877 53854962 2022-01 137.645161 147.400000 142.522581   44878 53854962 2022-02 124.642857 137.645161 136.562673   44879 53854962 2022-03 285.096774 124.642857 173.696198   44880 53854962 2022-04 115.000000 285.096774 161.956959   44881 53854962 2022-05 115.000000 115.000000 154.130799   44882 53854962 2022-06 115.000000 115.000000 148.540685   44883 53854962 2022-07 115.000000 115.000000 144.348099   44884 53854962 2022-08 115.000000 115.000000 141.087199   44885 53854962 2022-09 115.000000 115.000000 138.478479   44886 53854962 2022-10 115.000000 115.000000 136.344072   44887 53854962 2022-11 115.000000 115.000000 134.565399   44888 53854962 2022-12 115.000000 115.000000 133.060369   44863 53837654 2021-12 184.133333 NaN 136.708438   44864 53837654 2022-01 148.741935 184.133333 137.510671     To perform cumulative operations within a group, we can combine expanding() with groupby(). Since groups with not enough observations get dropped, we need to merge the dataset back.\ntemp.groupby('listing_id')['price'].expanding().mean().reset_index(level=0).head(15)   .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }  \n  listing_id price     0 42196 68.000000   1 42196 68.000000   2 42196 68.000000   3 42196 68.000000   4 42196 68.000000   5 42196 68.000000   6 42196 68.000000   7 42196 68.000000   8 42196 68.000000   9 42196 68.000000   10 42196 68.000000   11 42196 68.000000   12 42196 68.000000   13 46352 29.333333   14 46352 29.311828     If we want to perform an operation over a rolling window, we can use the rolling() function\ntemp['avg3_price'] = temp['price'].rolling(3).mean() temp.head(15)   .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }  \n  listing_id date price price1 avg_cum_price avg3_price     44876 53854962 2021-12 147.400000 NaN 147.400000 NaN   44877 53854962 2022-01 137.645161 147.400000 142.522581 NaN   44878 53854962 2022-02 124.642857 137.645161 136.562673 136.562673   44879 53854962 2022-03 285.096774 124.642857 173.696198 182.461598   44880 53854962 2022-04 115.000000 285.096774 161.956959 174.913210   44881 53854962 2022-05 115.000000 115.000000 154.130799 171.698925   44882 53854962 2022-06 115.000000 115.000000 148.540685 115.000000   44883 53854962 2022-07 115.000000 115.000000 144.348099 115.000000   44884 53854962 2022-08 115.000000 115.000000 141.087199 115.000000   44885 53854962 2022-09 115.000000 115.000000 138.478479 115.000000   44886 53854962 2022-10 115.000000 115.000000 136.344072 115.000000   44887 53854962 2022-11 115.000000 115.000000 134.565399 115.000000   44888 53854962 2022-12 115.000000 115.000000 133.060369 115.000000   44863 53837654 2021-12 184.133333 NaN 136.708438 138.044444   44864 53837654 2022-01 148.741935 184.133333 137.510671 149.291756     ","date":1651363200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1651363200,"objectID":"842c28fe2413586e18023093b371e682","permalink":"https://matteocourthoud.github.io/course/data-science/04_data_wrangling/","publishdate":"2022-05-01T00:00:00Z","relpermalink":"/course/data-science/04_data_wrangling/","section":"course","summary":"Setup For the scope of this tutorial we are going to use AirBnb Scraped data for the city of Bologna. The data is freely available at Inside AirBnb: http://insideairbnb.com/get-the-data.html.\nA description of all variables in all datasets is avaliable here.","tags":null,"title":"Data Wrangling","type":"book"},{"authors":null,"categories":null,"content":"# Remove warnings import warnings warnings.filterwarnings('ignore')  # Import import pandas as pd import numpy as np import seaborn as sns import time from numpy.linalg import inv from numpy.random import normal from sklearn.linear_model import LinearRegression from sklearn.metrics import mean_squared_error from sklearn.model_selection import train_test_split, LeaveOneOut, KFold, cross_val_score from sklearn.preprocessing import PolynomialFeatures from sklearn.utils import resample  # Import matplotlib for graphs import matplotlib.pyplot as plt from mpl_toolkits.mplot3d import axes3d # Set global parameters %matplotlib inline plt.style.use('seaborn-white') plt.rcParams['lines.linewidth'] = 3 plt.rcParams['figure.figsize'] = (10,6) plt.rcParams['figure.titlesize'] = 20 plt.rcParams['axes.titlesize'] = 18 plt.rcParams['axes.labelsize'] = 14 plt.rcParams['legend.fontsize'] = 14  Resampling methods involve repeatedly drawing samples from a training set and refitting a model of interest on each sample in order to obtain additional information about the fitted model.\n4.1 Cross-Validation Cross-validation can be used to estimate the test error associated with a given statistical learning method in order to evaluate its performance, or to select the appropriate level of flexibility. The process of evaluating a model’s performance is known as model assessment, whereas the process of selecting the proper level of flexibility for a model is known as model selection.\nLet\u0026rsquo;s use the auto dataset we have used for nonparametric models.\n# Load car dataset df1 = pd.read_csv('data/Auto.csv', na_values='?').dropna() df1.head()   .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }  \n  mpg cylinders displacement horsepower weight acceleration year origin name     0 18.0 8 307.0 130 3504 12.0 70 1 chevrolet chevelle malibu   1 15.0 8 350.0 165 3693 11.5 70 1 buick skylark 320   2 18.0 8 318.0 150 3436 11.0 70 1 plymouth satellite   3 16.0 8 304.0 150 3433 12.0 70 1 amc rebel sst   4 17.0 8 302.0 140 3449 10.5 70 1 ford torino     The Validation Set Approach Suppose that we would like to estimate the test error associated with fitting a particular statistical learning method on a set of observations. The validation set approach is a very simple strategy for this task. It involves randomly dividing the available set of observations into two parts\n a training set and a validation set or hold-out set  The model is fit on the training set, and the fitted model is used to predict the responses for the observations in the validation set. The resulting validation set error rate-typically assessed using MSE in the case of a quantitative response—provides an estimate of the test error rate.\nIn the following example we are are going to compute the MSE fit polynomial of different order (one to ten). We are going to split the data 50-50 across training and test set.\n# Cross-validation function for polynomials def cv_poly(X, y, p_order, r_states, t_prop): start = time.time() # Init scores scores = np.zeros((p_order.size,r_states.size)) # Generate 10 random splits of the dataset for j in r_states: # Split sample in train and test X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=t_prop, random_state=j) # For every polynomial degree for i in p_order: # Generate polynomial X_train_poly = PolynomialFeatures(i+1).fit_transform(X_train) X_test_poly = PolynomialFeatures(i+1).fit_transform(X_test) # Fit regression ols = LinearRegression().fit(X_train_poly, y_train) pred = ols.predict(X_test_poly) scores[i,j]= mean_squared_error(y_test, pred) print('Time elapsed: %.4f seconds' % (time.time()-start)) return scores  # Init t_prop = 0.5 p_order = np.arange(10) r_states = np.arange(10) # Get X,y X = df1.horsepower.values.reshape(-1,1) y = df1.mpg.ravel() # Compute scores cv_scores = cv_poly(X, y, p_order, r_states, t_prop)  Time elapsed: 0.0277 seconds  Let\u0026rsquo;s test the score for polynomials of different orders.\n# Figure 5.2 def make_figure_5_2(): # Init fig, (ax1, ax2) = plt.subplots(1,2,figsize=(14,6)) fig.suptitle('Figure 5.2') # Left plot (first split) ax1.plot(p_order+1,cv_scores[:,0], '-o') ax1.set_title('Random split of the data set') # Right plot (all splits) ax2.plot(p_order+1,cv_scores) ax2.set_title('10 random splits of the data set') for ax in fig.axes: ax.set_ylabel('Mean Squared Error') ax.set_ylim(15,30) ax.set_xlabel('Degree of Polynomial') ax.set_xlim(0.5,10.5) ax.set_xticks(range(2,11,2));  make_figure_5_2()  This figure illustrates a first drawback of the validation approach: the estimate of the test error rate can be highly variable, depending on precisely which observations are included in the training set and which observations are included in the validation set.\nThe second drawback of the validation approach is that only a subset of the observations—those that are included in the training set rather than in the validation set—are used to fit the model. Since statistical methods tend to per- form worse when trained on fewer observations, this suggests that the validation set error rate may tend to overestimate the test error rate for the model fit on the entire data set.\nLeave-One-Out Cross-Validation Leave-one-out cross-validation (LOOCV) attempts to address that method’s drawbacks.\nLike the validation set approach, LOOCV involves splitting the set of observations into two parts. However, instead of creating two subsets of comparable size, a single observation $i$ is used for the validation set, and the remaining $n-1$ observations make up the training set. The statistical learning method is fit on the $n−1$ training observations and the MSE is computed using the excluded observation $i$. The procedure is repeated $n$ times, for $i=1,\u0026hellip;,n$.\nThe LOOCV estimate for the test MSE is the average of these $n$ test error estimates:\n$$ \\mathrm{CV}{(n)}=\\frac{1}{n} \\sum{i=1}^{n} \\mathrm{MSE}_{i} $$\nLOOCV has a couple of major advantages over the validation set approach.\nFirst, it has far less bias. In LOOCV, we repeatedly fit the statistical learning method using training sets that contain $n − 1$ observations, almost as many as are in the entire data set. However, this also means that LOOCV is more computationally intense.\nSecond, in contrast to the validation approach which will yield different results when applied repeatedly due to randomness in the training/validation set splits, performing LOOCV multiple times will always yield the same results: there is no randomness in the training/validation set splits.\n# LeaveOneOut CV function for polynomials def loo_cv_poly(X, y, p_order): start = time.time() # Init loo = LeaveOneOut().get_n_splits(y) loo_scores = np.zeros((p_order.size,1)) # For every polynomial degree for i in p_order: # Generate polynomial X_poly = PolynomialFeatures(i+1).fit_transform(X) # Get score loo_scores[i] = cross_val_score(LinearRegression(), X_poly, y, cv=loo, scoring='neg_mean_squared_error').mean() print('Time elapsed: %.4f seconds' % (time.time()-start)) return loo_scores  Let\u0026rsquo;s compare the validation set approach against LOO in terms of computational time.\n# Validation set approach cv_scores = cv_poly(X, y, p_order, r_states, t_prop) # Leave One Out CV loo_scores = loo_cv_poly(X, y, p_order)  Time elapsed: 0.0270 seconds Time elapsed: 1.1495 seconds  As expected, LOOCV is much more computationally intense. Even accounting for the fact that we repeat every the validation set approach 10 times.\nLet\u0026rsquo;s now compare them in terms of accuracy in minimizing the MSE.\n# Make new figure 1 def make_new_figure_1(): # Init fig, ax = plt.subplots(1,1, figsize=(7,6)) # Left plot ax.plot(p_order+1, np.array(loo_scores)*-1, '-o', label='LOOCV') ax.plot(p_order+1, np.mean(cv_scores, axis=1), '-o', c='orange', label='Standard CV') ax.set_ylabel('Mean Squared Error'); ax.set_xlabel('Degree of Polynomial'); ax.set_ylim(15,30); ax.set_xlim(0.5,10.5); ax.set_xticks(range(2,11,2)); ax.legend();  make_new_figure_1()  With least squares linear or polynomial regression, an amazing shortcut makes the cost of LOOCV the same as that of a single model fit! The following formula holds:\n$$ \\mathrm{CV}{(n)}=\\frac{1}{n} \\sum{i=1}^{n}\\left(\\frac{y_{i}-\\hat{y}{i}}{1-h{i}}\\right)^{2} $$\nwhere $\\hat y_i$ is the $i^{th}$ fitted value from the original least squares fit, and $h_i$ is the leverage of observation $i$.\nk-Fold Cross-Validation An alternative to LOOCV is k-fold CV. This approach involves the following steps:\n Randomly dividing the set of observations into $k$ groups, or folds, of approximately equal size. The first fold is treated as a validation set, and the method is fit on the remaining $k − 1$ folds. The mean squared error, MSE1, is then computed on the observations in the held-out fold. Steps (1)-(3) are repeated $k$ times; each time, a different group of observations is treated as a validation set.  The k-fold CV estimate is computed by averaging these values\n$$ \\mathrm{CV}{(k)}=\\frac{1}{k} \\sum{i=1}^{k} \\mathrm{MSE}_{i} $$\nLOOCV is a special case of k-fold CV in which $k$ is set to equal $n$. In practice, one typically performs k-fold CV using $k = 5$ or $k = 10$.\nThe most obvious advantage is computational. LOOCV requires fitting the statistical learning method $n$ times, while k-fold CV only requires $k$ splits.\n# 10fold CV function for polynomials def k10_cv_poly(X, y, p_order, r_states, folds): start = time.time() # Init k10_scores = np.zeros((p_order.size,r_states.size)) # Generate 10 random splits of the dataset for j in r_states: # For every polynomial degree for i in p_order: # Generate polynomial X_poly = PolynomialFeatures(i+1).fit_transform(X) # Split sample in train and test kf10 = KFold(n_splits=folds, shuffle=True, random_state=j) k10_scores[i,j] = cross_val_score(LinearRegression(), X_poly, y, cv=kf10, scoring='neg_mean_squared_error').mean() print('Time elapsed: %.4f seconds' % (time.time()-start)) return k10_scores  Let\u0026rsquo;s now compare 10 fold cross-validation with LOO in terms of computational time.\n# Leave One Out CV loo_scores = loo_cv_poly(X, y, p_order) # 10-fold CV folds = 10 k10_scores = k10_cv_poly(X, y, p_order, r_states, folds)  Time elapsed: 1.1153 seconds Time elapsed: 0.3078 seconds  Indeed we see that the LOOCV approach is more computationally intense. Even accounting for the fact that we repeat every 10-fold cross-validation 10 times.\nWe can now compare all the methods in terms of accuracy.\n# Figure 5.4 def make_figure_5_4(): fig, (ax1, ax2, ax3) = plt.subplots(1,3,figsize=(17,5)) fig.suptitle('Figure 5.4') # Left plot ax1.plot(p_order+1, np.array(loo_scores)*-1, '-o') ax1.set_title('LOOCV', fontsize=12) # Center plot ax2.plot(p_order+1,k10_scores*-1) ax2.set_title('10-fold CV', fontsize=12) # Right plot ax3.plot(p_order+1, np.array(loo_scores)*-1, '-o', label='LOOCV') ax3.plot(p_order+1, np.mean(cv_scores, axis=1), label='Standard CV') ax3.plot(p_order+1,np.mean(k10_scores,axis=1)*-1, label='10-fold CV') ax3.set_title('Comparison', fontsize=12); ax3.legend(); for ax in fig.axes: ax.set_ylabel('Mean Squared Error') ax.set_ylim(15,30) ax.set_xlabel('Degree of Polynomial') ax.set_xlim(0.5,10.5) ax.set_xticks(range(2,11,2));  make_figure_5_4()  10-fold cross-validation outputs a very similar MSE with respect to LOOCV, but with considerably less computational time.\n4.2 The Bootstrap The bootstrap is a widely applicable and extremely powerful statistical tool that can be used to quantify the uncertainty associated with a given estimator or statistical learning method. In the specific case of linear regression, this is not particularly useful since there exist a formula for the standard errors. However, there are many models (almost all actually) for which there exists no closed for solution to the estimator variance.\nIn pricinple, we would like to draw independent samples from the true data generating process and assessing the uncertainty of an estimator by comparing its values across the different samples. However, this is clearly unfeasible since we do not know the true data generating process.\nWith the bootstrap, rather than repeatedly obtaining independent data sets from the population, we instead obtain distinct data sets by repeatedly sampling observations from the original data set. The power of the bootstrap lies in the fact that it can be easily applied to a wide range of statistical learning methods, including some for which a measure of variability is otherwise difficult to obtain and is not automatically output by statistical software.\nWe are now going to assess its usefulness through simulation. Take the following model:\n$$ y_i = \\beta_0 \\cdot x_i + \\varepsilon_i $$\nwhere $\\beta_0 = 0.6$ and $\\varepsilon \\sim N(0,1)$. We are now going to assess the variance of the OLS estimator $\\hat \\beta$ with the standard formula, simulating different samples and with bootstrap.\n# Set seed np.random.seed(1) # Init simulations = 1000 N = 1000 beta_0 = 0.6 beta_sim = np.zeros((simulations,1)) # Generate X X = normal(0,3,N).reshape(-1,1) # Loop over simulations for i in range(simulations): # Generate y e = normal(0,1,N).reshape(-1,1) y = beta_0*X + e # Estimate beta OLS beta_sim[i] = inv(X.T @ X) @ X.T @ y  # Init Bootstrap beta_boot = np.zeros((simulations,1)) # Loop over simulations for i in range(simulations): # Sample y X_sample, y_sample = resample(X, y, random_state=i) # Estimate beta OLS beta_boot[i] = inv(X_sample.T @ X_sample) @ X_sample.T @ y_sample  We can first compare the means.\n# Print means print('True value : %.4f' % beta_0) print('Mean Simulations: %.4f' % np.mean(beta_sim)) print('Mean One Sample : %.4f' % beta_sim[-1]) print('Mean Boostrap : %.4f' % np.mean(beta_boot))  True value : 0.6000 Mean Simulations: 0.6003 Mean One Sample : 0.5815 Mean Boostrap : 0.5816  The mean of the bootstrap estimtor is quite off. But this is not its actual purpose: it is designed to assess the uncertainty of an estimator, not its value.\nNow we compare the variances.\n# Print variances print('True std : %.6f' % np.sqrt(inv(X.T @ X))) print('Std Simulations: %.6f' % np.std(beta_sim)) print('Std One Sample : %.6f' % np.sqrt(inv(X.T @ X) * np.var(y - beta_sim[-1]*X))) print('Std Boostrap : %.6f' % np.std(beta_boot))  True std : 0.010737 Std Simulations: 0.010830 Std One Sample : 0.010536 Std Boostrap : 0.010812  Bootstrap gets as close to the true standard deviation of the estimator as the simulation with the true data generating process. Impressive!\nWe can now have a visual inspection.\n# Figure 5.10 def make_figure_5_10(): fig, (ax1, ax2, ax3) = plt.subplots(1,3,figsize=(14,6)) fig.suptitle('Figure 5.10') # Left plot ax1.hist(beta_sim, bins=10, edgecolor='black'); ax1.axvline(x=beta_0, color='r', label='beta_0') ax1.set_xlabel('beta simulated'); # Center plot ax2.hist(beta_boot, bins=10, color='orange', edgecolor='black'); ax2.axvline(x=beta_0, color='r', label='beta_0') ax2.set_xlabel('beta bootstrap'); # Right plot df_bootstrap = pd.DataFrame({'simulated': beta_sim.ravel(), 'bootstrap':beta_boot.ravel()}, index=range(simulations)) ax3 = sns.boxplot(data=df_bootstrap, width=0.5, linewidth=2); ax3.axhline(y=beta_0, color='r', label='beta_0');  make_figure_5_10()  As we can see, the bootstrap is a powerful tool to assess the uncertainty of an estimator.\n","date":1646784000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1646784000,"objectID":"55557b8d76a9ab6ec9e78a74df31ab53","permalink":"https://matteocourthoud.github.io/course/ml-econ/04_crossvalidation/","publishdate":"2022-03-09T00:00:00Z","relpermalink":"/course/ml-econ/04_crossvalidation/","section":"course","summary":"# Remove warnings import warnings warnings.filterwarnings('ignore')  # Import import pandas as pd import numpy as np import seaborn as sns import time from numpy.linalg import inv from numpy.random import normal from sklearn.","tags":null,"title":"Resampling Methods","type":"book"},{"authors":null,"categories":null,"content":"Statistical Models Definition A statistical model is a set of probability distributions $\\lbrace P \\rbrace$.\nMore precisely, a statistical model over data $D \\in \\mathcal{D}$ is a set of probability distribution over datasets $D$ which takes values in $\\mathcal{D}$.\nSuppose you have regression data $\\lbrace x_i , y_i \\rbrace _ {i=1}^N$ with $x_i \\in \\mathbb{R}^p$ and $y_i \\in \\mathbb{R}$. The statistical model is\n$$ \\Big\\lbrace P : y_i = f(x_i) + \\varepsilon_i, \\ x_i \\sim F_x , \\ \\varepsilon_i \\sim F _\\varepsilon , \\ \\varepsilon_i \\perp x_i , \\ f \\in C^2 (\\mathbb{R}^p) \\Big\\rbrace $$\n In words: the statistical model is the set of distributions $P$ such that an additive decomposition of $y_i$ as $f(x_i) + \\varepsilon_i$ exists for some $x_i$; where $f$ is twice continuously differentiable.\n A data generating process (DGP) is a single statistical distribution over\nParametrization A statistical model parameterized by $\\theta \\in \\Theta$ is well specified if the data generating process corresponds to some $\\theta_0$ and $\\theta_0 \\in \\Theta$. Otherwise, the statistical model is misspecified.\nA statistical model can be parametrized as $\\mathcal{F} = \\lbrace P_\\theta \\rbrace _ {\\lbrace \\theta \\in \\Theta \\rbrace }$.\nWe can divide statistical models into 3 classes\n  Parametric: the stochastic features of the model are completly specified up to a finite dimensional parameter: $\\lbrace P_\\theta \\rbrace _ { \\lbrace \\theta \\in \\Theta \\rbrace }$ with $\\Theta \\subseteq \\mathbb{R}^k, k\u0026lt;\\infty$;   Semiparametric: it is a partially specified model, e.g., $\\lbrace P_\\theta \\rbrace _ { \\lbrace \\theta \\in \\Theta, \\gamma \\in \\Gamma \\rbrace }$ with $\\Theta$ of finite dimension and $\\Gamma$ of infinite dimension;\n  Non parametric: there is no finite dimensional component of the model.\n  Estimation Let $\\mathcal{D}$ be the set of possible data realizations. Let $D \\in \\mathcal{D}$ be your data. Let $\\mathcal{F}$ be a statistical model indexed by some parameter $\\theta \\in \\Theta$. An estimator is a map $$ \\mathcal{D} \\to \\mathcal{F} \\quad , \\quad D \\mapsto \\hat{\\theta} $$\n In words:\n An estimator is a map from the set of data realizations to the set of statistical models. It takes as inputs a dataset $D$ and outputs a parameter estimate $\\hat \\theta$.   Inference Let $\\alpha \u0026gt; 0$ be a small tolerance. Statistical inference is a map into subsets of $\\mathcal{F}$ given by $$ \\mathcal{D} \\to \\mathcal{G} \\subseteq \\mathcal{F}: \\min _ \\theta P_\\theta (\\mathcal{G} | \\theta \\in \\mathcal{G}) \\geq 1-\\alpha $$\n In words\n Inference maps datasets into sets of models The set contains only models that generate the observed data with high probability I.e. at least $1-\\alpha$   Hypotesis Testing Hypothesis A statistical hypothesis $H_0$, is a subset of a statistical model, $\\mathcal K \\subset \\mathcal F$.\nIf $\\mathcal F$ is the statistical model and $\\mathcal K$ is the statistical hypothesis, we use the notation $H_0 : P \\in \\mathcal K$.\n Example\nCommon hypothesis are\n A single coefficient being equal to zero, $\\beta_k = c \\in \\mathbb R$ Multiple linear combination of coefficients being equal to some values: $\\boldsymbol R' \\beta = r \\in \\mathbb R^p$   Test A hypothesis test $T$ is a map from the space of datasets to a decision, rejection (0) or acceptance (1) $$ \\mathcal D \\to \\lbrace 0, 1 \\rbrace \\quad, \\quad D \\mapsto T $$\n Generally, we are interested in understanding whether it is likely that data $D$ are drawn from a model $\\mathcal K$ or not.\nA hypothesis test, $T$ is our tool for deciding whether the hypothesis is consistent with the data.\n $T(D) = 0 \\to$ fail to reject $H_0$ and test inconclusive $T (D) = 1 \\to$ reject $H_0$ and D is inconsistent with any $P \\in \\mathcal K$   Errors Let $\\mathcal K \\subset \\mathcal F$ be a statistical hypothesis and $T$ a hypothesis test.\n A Type I error is an event $T(D)=1$ under $P \\in \\mathcal K$.  In words: rejecting the null hypothesis, when it is is true   A Type II error is an event $T(D)=0$ under $P \\in \\mathcal K^C$.  In words: not rejecting the null hypothesis, when it is false    The corresponding probability of a type I error is called size.\nThe corresponding probability of a type II error is called power (against the alternative P).\nType I Error and Test Size Test size is the probability of a Type I error, i.e. $$ \\Pr \\Big[ \\text{ Reject } H_0 \\Big| H_0 \\text{ is true } \\Big] = \\Pr \\Big[ T(D)=1 \\Big| P \\in \\mathcal K \\Big] $$ A primary goal of test construction is to limit the incidence of Type I error by bounding the size of the test.\n In the dominant approach to hypothesis testing the researcher pre-selects a significance level $\\alpha \\in (0,1)$ and then selects the test so that its size is no larger than $\\alpha$.\n Type II Error and Power Test power is the probability of a Type II error, i.e. $$ \\Pr \\Big[ \\text{ Not Reject } H_0 \\Big| H_0 \\text{ is false } \\Big] = \\Pr \\Big[ T(D)=0 \\Big| P \\in \\mathcal K^C \\Big] $$\n In the dominant approach to hypothesis testing the goal of test construction is to have high power subject to the constraint that the size of the test is lower than the pre-specified significance level.\n Statistical Significance TBD\nP-Values Recap We now summarize the main features of hypothesis testing.\n Select a significance level $\\alpha$. Select a test statistic $T$ with asymptotic distribution $T\\to \\xi$ under $H_0$. Set the asymptotic critical value $c$ so that 1−G(c)=α, where G is the distribution function of $\\xi$. Calculate the asymptotic p-value p=1−G(T). Reject $H_0$ if T \u0026gt; c, or equivalently p \u0026lt; α. Accept $H_0$ if T ≤ c, or equivalently p ≥ α. Report $p$ to summarize the evidence concerning $H_0$ versus $H_1$.  Examples Let’s focus two hypotheses:\n $\\beta_k = c \\in \\mathbb R$ $\\boldsymbol R' \\beta = r \\in \\mathbb R^p$  t-test with Known Variance Consider the testing problem $H_0 : \\beta_k = c$, where $c$ is a pre-specified value under the null. Suppose the variance of the esimator $\\hat \\beta_k$ is known.\nThe t-statistic for this problem is defined by $$ n_{k}:=\\frac{\\hat \\beta_{k} - c}{\\sigma_{\\hat \\beta_{k}}} $$ In the testing procedure above, the sampling distribution under the null $H_0$ is given by $$ n_k \\sim N(0,1) $$ Where $N(0,1)$ the standard normal distribution.\nt-test with Unknown Variance Consider the testing problem $H_0 : \\beta_k = c$, where $c$ is a presepecified value under the null. In case the variance of the estimator $\\hat \\beta_k$ is not known, we have to replace it with a consistent estimate $\\hat \\sigma^2_{\\hat \\beta}$\nThe t-statistic for this problem is defined by $$ t_{k}:=\\frac{\\hat \\beta_{k} - c}{\\hat \\sigma_{\\hat \\beta_{k}}} $$ In the testing procedure above, the sampling distribution under the null $H_0$ is given by $$ t_k \\sim t_{n-K} $$ Where $t_{n-K}$ denotes the t-distribution with $n-K$ degress of freedom.\nWald-test Consider the testing problem $\\boldsymbol R' \\beta = r$, where $\\boldsymbol R \\in \\mathbb R^{p+K}$ is a pre-specified set of linear combinations and $r \\in \\mathbb R^p$ is a restriction vector. Suppose the variance of the esimator $\\hat \\beta$ is known.\nThe Wald statistic for this problem is given by $$ W := \\frac{(R \\hat \\beta-r)^{\\prime}(R \\hat \\beta-r) }{R' \\sigma^{2}{\\hat \\beta} R} $$ In the testing procedure above, the sampling distribution under the null $H_0$ is given by $$ W \\sim \\chi^2{n-K} $$ Where $\\chi^2_{n-K}$ denotes the chi-squared distribution with $n-K$ degress of freedom.\nComments on the Wald test  The Wald statistic $W$ is a weighted Euclidean measure of the length of the vector $R \\hat \\beta-r$ The Wald test is intrinsecally 2-sided When $p=1$ then $W = |T|$ , the square of the t-statistic, so hypothesis tests based on $W$ and $|T|$ are equivalent.  F-test Consider the testing problem $\\boldsymbol R' \\beta = r$, where $\\boldsymbol R \\in \\mathbb R^{p+K}$ is a pre-specified set of linear combinations and $r \\in \\mathbb R^p$ is a restriction vector. In case the variance of the estimator $\\hat \\beta$ is not known, we have to replace it with a consistent estimate $\\hat \\sigma^2_{\\hat \\beta}$.\nThe F-statistic for this problem is given by $$ F := \\frac{(R \\hat \\beta-r)^{\\prime}(R \\hat \\beta-r) / p }{R' \\hat \\sigma^{2} _ {\\hat \\beta} R} $$ In the testing procedure above, the sampling distribution under the null $H_0$ is given by $$ F \\sim F_{p, n-K} $$ Where $F_{p, n-K}$ denotes the F-distribution with $n-K$ degress of freedom, with $p$ restrictions.\nF-test Equivalence Consider the testing problem $\\boldsymbol R' \\beta = r$, where $\\boldsymbol R \\in \\mathbb R^{p+K}$ is a pre-specified set of linear combinations and $r \\in \\mathbb R^p$ is a restriction vector. Consider two estimators\n $\\hat \\beta_U = \\arg \\min_b \\frac{1}{n} (y - X \\beta)' (y - X\\beta)$ $\\hat \\beta_R = \\arg \\min_{b : \\boldsymbol R' \\beta = r} \\frac{1}{n} (y - X \\beta)' (y - X\\beta)$  Then the F statistic is numerically equivalent to the following expression $$ F = \\frac{\\left(S S R_{R}-S S R_{U}\\right) / p}{S S R_{U} /(n-K)} $$ where SSR is the sum of squared residuals.\nConfidence Intervals TBD\nMinimum Distance Tests TBD\nAsymptotics Estimator Properties Given a sequence of well specified data generating processes $\\mathcal F_n$, each indexed by the same parameter space $\\Theta$, with $\\theta_0$ a component of the true parameter for each $n$.\nThen estimator $\\hat \\theta$ is\n unbiased if $\\mathbb E [\\hat \\theta] = \\theta_0$ consistent if $\\hat \\theta \\overset{p}{\\to} \\theta_0$ asymptotically normal $\\sqrt{n} (\\hat \\theta - \\theta_0) \\overset{d}{\\to} N(0, V)$ for some positive definite $V$  Test Consistency The asymptotic size of a testing procedure is defined as the limiting probability of rejecting $H_0$ when $H_0$ is true. Mathematically, we can write this as $\\lim _ {n \\to \\infty} \\Pr_n ( \\text{reject } H_0 | H_0)$, where the $n$ subscript indexes the sample size.\nA test is said to be consistent against the alternative $H_1$ if the null hypothesis is rejected with probability approaching $1$ when $H_1$ is true: $\\lim _ {N \\to \\infty} \\Pr_N (\\text{reject } H_0 | H_1) \\overset{p}{\\to} 1$.\nConvergence Theorem: Suppose that $\\sqrt{n}(\\hat{\\theta} - \\theta_0) \\overset{d}{\\to} N(0, V)$, where $V$ is positive definite. Then for any non-stochastic $Q\\times P$ matrix $R$, $Q \\leq P$, with rank$( R ) = Q$ $$ \\sqrt{n} R (\\hat{\\theta} - \\theta_0) \\sim N(0, R VR') $$ and $$ [\\sqrt{n}R(\\hat{\\theta} - \\theta_0)]'[RVR']^{-1}[\\sqrt{n}R(\\hat{\\theta} - \\theta_0)] \\overset{d}{\\to} \\chi^2_Q $$ In addition, if $\\text{plim} \\hat{V} _n = V$, then $$ (\\hat{\\theta} - \\theta_0)' R'[R (\\hat{V} _n/n) R']^{-1}R (\\hat{\\theta} - \\theta_0) \\overset{d}{\\to} \\chi^2_Q $$\nWald Statistic For testing the null hypothesis $H_0: R\\theta_0 = r$, where $r$ is a $Q\\times1$ random vector, define the Wald statistic for testing $H_0$ against $H_1 : R\\theta_0 \\neq r$ as $$ W_n = (R\\hat{\\theta} - r)'[R (\\hat{V} _n/n) R']^{-1} (R\\hat{\\theta} - r) $$ Under $H_0$, $W_n \\overset{d}{\\to} \\chi^2_Q$. If we abuse the asymptotics and we treat $\\hat{\\theta}$ as being distributed as Normal we get the equation exactly.\n","date":1635465600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1635465600,"objectID":"9b5c711497ddb91c20e1b4cb62202ed9","permalink":"https://matteocourthoud.github.io/course/metrics/04_inference/","publishdate":"2021-10-29T00:00:00Z","relpermalink":"/course/metrics/04_inference/","section":"course","summary":"Statistical Models Definition A statistical model is a set of probability distributions $\\lbrace P \\rbrace$.\nMore precisely, a statistical model over data $D \\in \\mathcal{D}$ is a set of probability distribution over datasets $D$ which takes values in $\\mathcal{D}$.","tags":null,"title":"Inference","type":"book"},{"authors":null,"categories":null,"content":"import numpy as np import pandas as pd import folium import geopandas import contextily import matplotlib as mpl import matplotlib.pyplot as plt import seaborn as sns from src.import_data import import_data  For the scope of this tutorial we are going to use AirBnb Scraped data for the city of Bologna. The data is freely available at Inside AirBnb: http://insideairbnb.com/get-the-data.html.\nA description of all variables in all datasets is avaliable here.\nWe are going to use 2 datasets:\n listing dataset: contains listing-level information pricing dataset: contains pricing data, over time  We import and clean them with a script. If you want more details, have a look at the data exploration and data wrangling sections.\ndf_listings, df_prices, df = import_data()  Intro The default library for plotting in python is matplotlib. However, a more modern package that builds on top of it, is seaborn.\nWe start by telling the notebook to display the plots inline.\n%matplotlib inline  Another important configuration is the plot resulution. We set it to retina to have high resolution plots.\n%config InlineBackend.figure_format = 'retina'  You can choose set a general theme using plt.style.use(). The list of themes is available here.\nplt.style.use('seaborn')  If you want to further customize some aspects of a theme, you can set some global paramters for all plots. You can find a list of all the options here. If you want to customize all plots in a project in the samy way, you can create a filename.mplstyle file and call it at the beginning of each file as plt.style.use('filename.mplstyle').\nmpl.rcParams['figure.figsize'] = (10,6) mpl.rcParams['axes.labelsize'] = 16 mpl.rcParams['axes.titlesize'] = 18 mpl.rcParams['axes.titleweight'] = 'bold' mpl.rcParams['figure.titlesize'] = 18 mpl.rcParams['figure.titleweight'] = 'bold' mpl.rcParams['axes.titlepad'] = 20 mpl.rcParams['legend.facecolor'] = 'w'  Distributions Suppose you have a numerical variable and you want to see how it\u0026rsquo;s distributed. The best option is to use an histogram. Seaborn function is sns.histplot.\ndf_listings['log_price'] = np.log(1+df_listings['mean_price'])  sns.histplot(df_listings['log_price'], bins=50)\\ .set(title='Distribution of log-prices');  We can add a smooth kernel density approximation with the kde option.\nsns.histplot(df_listings['log_price'], bins=50, kde=True)\\ .set(title='Distribution of log-prices with density');  If we have a categorical variable, we might want to plot the distribution of the data across its values. We can use a barplot. Seaborn function is sns.countplot() for count data.\nsns.countplot(x=\u0026quot;neighborhood\u0026quot;, data=df_listings)\\ .set(title='Number of observations by neighborhood');  If instead we want to see the distribution of another variable across some group, we can use the sns.barplot() function.\nsns.barplot(x=\u0026quot;neighborhood\u0026quot;, y=\u0026quot;mean_price\u0026quot;, data=df_listings)\\ .set(title='Average price by neighborhood');  We can also use other metrics besides the mean with the estimator option.\nsns.barplot(x=\u0026quot;neighborhood\u0026quot;, y=\u0026quot;mean_price\u0026quot;, data=df_listings, estimator=np.median)\\ .set(title='Median price by neighborhood');  We can also plot the full distribution using, for example boxplots with sns.boxplot(). Boxplots display quartiles and outliers.\nsns.boxplot(x=\u0026quot;neighborhood\u0026quot;, y=\u0026quot;log_price\u0026quot;, data=df_listings)\\ .set(title='Price distribution across neighborhoods');  If we want to see the full distribution, we can use the sns.violinplot() function.\nsns.violinplot(x=\u0026quot;neighborhood\u0026quot;, y=\u0026quot;log_price\u0026quot;, data=df_listings)\\ .set(title='Price distribution across neighborhoods');  Time Series If the dataset has a time dimension, we might want to explore how a variable evolves over time. Seaborn function is sns.lineplot(). If the data has multiple observations for each time period, it will also display a 95% confidence interval around the mean.\nsns.lineplot(data=df, x='date', y='price')\\ .set(title=\u0026quot;Price distribution over time\u0026quot;);  We can do the samy by group, with the hue option. We can suppress confidence intervals setting ci=None (making the code much faster).\nsns.lineplot(data=df, x='date', y='price', hue='neighborhood', ci=None)\\ .set(title=\u0026quot;Price distribution over time\u0026quot;);  Correlations df_listings[\u0026quot;log_reviews\u0026quot;] = np.log(1 + df_listings[\u0026quot;number_of_reviews\u0026quot;]) df_listings[\u0026quot;log_rpm\u0026quot;] = np.log(1 + df_listings[\u0026quot;reviews_per_month\u0026quot;])  The most intuitive way to plot a correlation between two variables is a scatterplot. Seaborn function is sns.scatterplot()\nsns.scatterplot(data=df_listings, x=\u0026quot;log_rpm\u0026quot;, y=\u0026quot;log_price\u0026quot;, alpha=0.3)\\ .set(title='Prices and Reviews');  We can highlight the best linear approximation adding a line of fit using sns.regplot().\nsns.regplot(x=\u0026quot;log_rpm\u0026quot;, y=\u0026quot;log_price\u0026quot;, data=df_listings, scatter_kws={'alpha':.1}, line_kws={'color':'C1'})\\ .set(title='Price and Reviews');  If we want a more flexible representation of the data, we can use the binscatter package. binscatter splits the data into equally sized bins and displays a scatterplot of the averages.\nThe main difference between a binscatterplot and an histogram is that in a histogram bins have the same width while in a binscatterplot bins have the same number of observations.\nAn advantage of binscatter is that it makes the nature of the data much more transparent, at the cost of hiding some of the background noise.\nimport binscatter # Remove nans temp = df_listings[[\u0026quot;log_rpm\u0026quot;, \u0026quot;log_price\u0026quot;]].dropna() # Binned scatter plot of Wage vs Tenure fig, ax = plt.subplots() ax.binscatter(temp[\u0026quot;log_rpm\u0026quot;], temp[\u0026quot;log_price\u0026quot;]); ax.set_title('Price and Reviews');  As usual, we can split the data by group with the hue option.\nsns.scatterplot(data=df_listings, x=\u0026quot;log_rpm\u0026quot;, y=\u0026quot;log_price\u0026quot;, hue=\u0026quot;room_type\u0026quot;, alpha=0.3)\\ .set(title=\u0026quot;Prices and Ratings, by room type\u0026quot;);  We can also add the marginal distributions using the sns.jointplot() function.\nsns.jointplot(data=df_listings, x=\u0026quot;log_rpm\u0026quot;, y=\u0026quot;log_price\u0026quot;, kind=\u0026quot;hex\u0026quot;)\\ .fig.suptitle(\u0026quot;Prices and Reviews, with marginals\u0026quot;) plt.subplots_adjust(top=0.9);  If we want to plot correlations (and marginals) of multiple variables, we can use the sns.pairplot() function.\nsns.pairplot(data=df_listings, vars=[\u0026quot;log_rpm\u0026quot;, \u0026quot;log_reviews\u0026quot;, \u0026quot;log_price\u0026quot;], plot_kws={'s':2})\\ .fig.suptitle(\u0026quot;Correlations\u0026quot;); plt.subplots_adjust(top=0.9)  We can distinguish across groups with the hue option.\nsns.pairplot(data=df_listings, vars=[\u0026quot;log_rpm\u0026quot;, \u0026quot;log_reviews\u0026quot;, \u0026quot;log_price\u0026quot;], hue='room_type', plot_kws={'s':2})\\ .fig.suptitle(\u0026quot;Correlations, by room type\u0026quot;); plt.subplots_adjust(top=0.9)  If we want to plot all the correlations in the data, we can use the sns.heatmap() function on top of a correlation matrix generated by .corr().\n# Plot sns.heatmap(df.corr(), vmin=-1, vmax=1, linewidths=.5, cmap=\u0026quot;RdBu\u0026quot;)\\ .set(title=\u0026quot;Correlations\u0026quot;);  Geographical data We can in principle plot geographical data as a simple scatterplot.\nsns.scatterplot(data=df_listings, x=\u0026quot;longitude\u0026quot;, y=\u0026quot;latitude\u0026quot;)\\ .set(title='Listing coordinates');  However, we can do better and do the scatterplot over a map layer.\nFirst, we neeed to convert the latitude and longitude variables into coordinates. We use the library geopandas. Note that the original coordinate system is 4326 (3D) and we need to 3857 (2D).\ngeom = geopandas.points_from_xy(df_listings.longitude, df_listings.latitude) gdf = geopandas.GeoDataFrame( df_listings, geometry=geom, crs=4326).to_crs(3857)  We import a map of Bologna using the library contextily.\nbologna = contextily.Place(\u0026quot;Bologna\u0026quot;, source=contextily.providers.Stamen.TonerLite)  We are now ready to plot it with the airbnb listings.\nax = bologna.plot() ax.set_ylim([5530000, 5555000]) gdf.plot(ax=ax, c=df_listings['mean_price'], cmap='viridis', alpha=0.8);  ","date":1651363200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1651363200,"objectID":"bb056caedc679d8e403fac05680718fb","permalink":"https://matteocourthoud.github.io/course/data-science/05_plotting/","publishdate":"2022-05-01T00:00:00Z","relpermalink":"/course/data-science/05_plotting/","section":"course","summary":"import numpy as np import pandas as pd import folium import geopandas import contextily import matplotlib as mpl import matplotlib.pyplot as plt import seaborn as sns from src.import_data import import_data  For the scope of this tutorial we are going to use AirBnb Scraped data for the city of Bologna.","tags":null,"title":"Plotting","type":"book"},{"authors":null,"categories":null,"content":"# Remove warnings import warnings warnings.filterwarnings('ignore')  # Import import pandas as pd import numpy as np import time import itertools import statsmodels.api as sm import seaborn as sns from numpy.random import normal, uniform from itertools import combinations from statsmodels.api import add_constant from statsmodels.regression.linear_model import OLS from sklearn.linear_model import LinearRegression, Ridge, Lasso, RidgeCV, LassoCV from sklearn.cross_decomposition import PLSRegression, PLSSVD from sklearn.model_selection import KFold, cross_val_score, train_test_split, LeaveOneOut, ShuffleSplit from sklearn.preprocessing import scale from sklearn.decomposition import PCA from sklearn.metrics import mean_squared_error  # Import matplotlib for graphs import matplotlib.pyplot as plt from mpl_toolkits.mplot3d import axes3d # Set global parameters %matplotlib inline plt.style.use('seaborn-white') plt.rcParams['lines.linewidth'] = 3 plt.rcParams['figure.figsize'] = (12,5) plt.rcParams['figure.titlesize'] = 20 plt.rcParams['axes.titlesize'] = 18 plt.rcParams['axes.labelsize'] = 14 plt.rcParams['legend.fontsize'] = 14  When we talk about big data, we do not only talk about bigger sample size, $n$, but also about a larger number of explanatory variables, $p$. However, with ordinary least squares, we are limited by the identification constraint that $p \u0026lt; n$. Moreover, for inference and prediction accuracy, we would actually like to have $k \u0026laquo; n$.\nThis session adresses methods to use a least squares fit in a setting in which the number of regressors, $p$, is large with respect to the sample size, $n$\n5.1 Subset Selection The Subset Selection approach involves identifying a subset of the $p$ predictors that we believe to be related to the response. We then fit a model using least squares on the reduced set of variables.\nLet\u0026rsquo;s load the credit rating dataset.\n# Credit ratings dataset credit = pd.read_csv('data/Credit.csv', usecols=list(range(1,12))) credit.head()   .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }  \n  Income Limit Rating Cards Age Education Gender Student Married Ethnicity Balance     0 14.891 3606 283 2 34 11 Male No Yes Caucasian 333   1 106.025 6645 483 3 82 15 Female Yes Yes Asian 903   2 104.593 7075 514 4 71 11 Male No No Asian 580   3 148.924 9504 681 3 36 11 Female No No Asian 964   4 55.882 4897 357 2 68 16 Male No Yes Caucasian 331     We are going to look at the relationship between individual characteristics and account Balance in the Credit dataset.\n# X and y X = credit.loc[:, credit.columns != 'Balance'] y = credit.loc[:,'Balance']  Best Subset Selection To perform best subset selection, we fit a separate least squares regression for each possible combination of the $p$ predictors. That is, we fit all $p$ models that contain exactly one predictor, all $p = p(p−1)/2$ models that contain 2 exactly two predictors, and so forth. We then look at all of the resulting models, with the goal of identifying the one that is best.\nClearly the main disadvantage of best subset selection is computational power.\ndef model_selection(X, y, *args): # Init scores = list(itertools.repeat(np.zeros((0,2)), len(args))) # Categorical variables categ_cols = {\u0026quot;Gender\u0026quot;, \u0026quot;Student\u0026quot;, \u0026quot;Married\u0026quot;, \u0026quot;Ethnicity\u0026quot;} # Loop over all admissible number of regressors K = np.shape(X)[1] for k in range(K+1): print(\u0026quot;Computing k=%1.0f\u0026quot; % k, end =\u0026quot;\u0026quot;) # Loop over all combinations for i in combinations(range(K), k): # Subset X X_subset = X.iloc[:,list(i)] # Get dummies for categorical variables if k\u0026gt;0: categ_subset = list(categ_cols \u0026amp; set(X_subset.columns)) X_subset = pd.get_dummies(X_subset, columns=categ_subset, drop_first=True) # Regress reg = OLS(y,add_constant(X_subset)).fit() # Metrics for i,metric in enumerate(args): score = np.reshape([k,metric(reg)], (1,-1)) scores[i] = np.append(scores[i], score, axis=0) print(\u0026quot;\u0026quot;, end=\u0026quot;\\r\u0026quot;) return scores  We are going to consider 10 variables and two difference metrics: the Sum of Squares Residuals and $R^2$.\n# Set metrics rss = lambda reg : reg.ssr r2 = lambda reg : reg.rsquared # Compute scores scores = model_selection(X, y, rss, r2) ms_RSS = scores[0] ms_R2 = scores[1]  Computing k=10  # Save best scores K = np.shape(X)[1] ms_RSS_best = [np.min(ms_RSS[ms_RSS[:,0]==k,1]) for k in range(K+1)] ms_R2_best = [np.max(ms_R2[ms_R2[:,0]==k,1]) for k in range(K+1)]  Let\u0026rsquo;s plot the best scores.\n# Figure 6.1 def make_figure_6_1(): fig, (ax1,ax2) = plt.subplots(1,2) fig.suptitle('Figure 6.1: Best Model Selection') # RSS ax1.scatter(x=ms_RSS[:,0], y=ms_RSS[:,1], facecolors='None', edgecolors='k', alpha=0.5); ax1.plot(range(K+1), ms_RSS_best, c='r'); ax1.scatter(np.argmin(ms_RSS_best), np.min(ms_RSS_best), marker='x', s=300) ax1.set_ylabel('RSS'); # R2 ax2.scatter(x=ms_R2[:,0], y=ms_R2[:,1], facecolors='None', edgecolors='k', alpha=0.5); ax2.plot(range(K+1), ms_R2_best, c='r'); ax2.scatter(np.argmax(ms_R2_best), np.max(ms_R2_best), marker='x', s=300) ax2.set_ylabel('R2'); # All axes; for ax in fig.axes: ax.set_xlabel('Number of Predictors'); ax.set_yticks([]);  make_figure_6_1()  The figure shows that, as expected, both metrics improve as the number of variables increases; however, from the three-variable model on, there is little improvement in RSS and $R^2$ as a result of including additional predictors.\nForward Stepwise Selection For computational reasons, best subset selection cannot be applied with very large $p$.\nWhile the best subset selection procedure considers all $2^p$ possible models containing subsets of the p predictors, forward step-wise considers a much smaller set of models. Forward stepwise selection begins with a model containing no predictors, and then adds predictors to the model, one-at-a-time, until all of the predictors are in the model. In particular, at each step the variable that gives the greatest additional improvement to the fit is added to the model.\ndef forward_selection(X, y, f): # Init RSS and R2 K = np.shape(X)[1] fms_scores = np.zeros((K,1)) # Categorical variables categ_cols = {\u0026quot;Gender\u0026quot;, \u0026quot;Student\u0026quot;, \u0026quot;Married\u0026quot;, \u0026quot;Ethnicity\u0026quot;} # Loop over p selected_cols = [] for k in range(1,K+1): # Loop over selected columns remaining_cols = [col for col in X.columns if col not in selected_cols] temp_scores = np.zeros((0,1)) # Loop on remaining columns for col in remaining_cols: # Subset X X_subset = X.loc[:,selected_cols + [col]] if k\u0026gt;0: categ_subset = list(categ_cols \u0026amp; set(X_subset.columns)) X_subset = pd.get_dummies(X_subset, columns=categ_subset, drop_first=True) # Regress reg = OLS(y,add_constant(X_subset).values).fit() # Metrics temp_scores = np.append(temp_scores, f(reg)) # Pick best variable best_col = remaining_cols[np.argmin(temp_scores)] print(best_col) selected_cols += [best_col] fms_scores[k-1] = np.min(temp_scores) return fms_scores  Let\u0026rsquo;s select the best model according, using the sum of squared residuals as a metric.\nWhat are the most important variables?\n# Forward selection by RSS rss = lambda reg : reg.ssr fms_RSS = forward_selection(X, y, rss)  Rating Income Student Limit Cards Age Ethnicity Gender Married Education  What happens if we use $R^2$ instead?\n# Forward selection by R2 r2 = lambda reg : -reg.rsquared fms_R2 = -forward_selection(X, y, r2)  Rating Income Student Limit Cards Age Ethnicity Gender Married Education  Unsurprisingly, both methods select the same models. Why? In the end $R^2$ is just a normalized version of RSS.\nLet\u0026rsquo;s plot the scores of the two methods, for different number of predictors.\n# New figure 1 def make_new_figure_1(): # Init fig, (ax1,ax2) = plt.subplots(1,2) fig.suptitle('Forward Model Selection') # RSS ax1.plot(range(1,K+1), fms_RSS, c='r'); ax1.scatter(np.argmin(fms_RSS)+1, np.min(fms_RSS), marker='x', s=300) ax1.set_ylabel('RSS'); # R2 ax2.plot(range(1,K+1), fms_R2, c='r'); ax2.scatter(np.argmax(fms_R2)+1, np.max(fms_R2), marker='x', s=300) ax2.set_ylabel('R2'); # All axes; for ax in fig.axes: ax.set_xlabel('Number of Predictors'); ax.set_yticks([]);  make_new_figure_1()  Backward Stepwise Selection Like forward stepwise selection, backward stepwise selection provides an efficient alternative to best subset selection. However, unlike forward stepwise selection, it begins with the full least squares model containing all p predictors, and then iteratively removes the least useful predictor, one-at-a-time.\ndef backward_selection(X, y, f): # Init RSS and R2 K = np.shape(X)[1] fms_scores = np.zeros((K,1)) # Categorical variables categ_cols = {\u0026quot;Gender\u0026quot;, \u0026quot;Student\u0026quot;, \u0026quot;Married\u0026quot;, \u0026quot;Ethnicity\u0026quot;} # Loop over p selected_cols = list(X.columns) for k in range(K,0,-1): # Loop over selected columns temp_scores = np.zeros((0,1)) # Loop on remaining columns for col in selected_cols: # Subset X X_subset = X.loc[:,[x for x in selected_cols if x != col]] if k\u0026gt;1: categ_subset = list(categ_cols \u0026amp; set(X_subset.columns)) X_subset = pd.get_dummies(X_subset, columns=categ_subset, drop_first=True) # Regress reg = OLS(y,add_constant(X_subset).values).fit() # Metrics temp_scores = np.append(temp_scores, f(reg)) # Pick best variable worst_col = selected_cols[np.argmin(temp_scores)] print(worst_col) selected_cols.remove(worst_col) fms_scores[k-1] = np.min(temp_scores) return fms_scores  Let\u0026rsquo;s select the best model according, using the sum of squared residuals as a metric.\nWhat are the most important variables?\n# Backward selection by RSS rss = lambda reg : reg.ssr bms_RSS = backward_selection(X, y, rss)  Education Married Gender Ethnicity Age Rating Cards Student Income Limit  What if we use $R^2$ instead?\n# Backward selection by R2 r2 = lambda reg : -reg.rsquared bms_R2 = -backward_selection(X, y, r2)  Education Married Gender Ethnicity Age Rating Cards Student Income Limit  The interesting part here is that the the variable Rating that was selected first by forward model selection, is now dropped $5^{th}$ to last. Why? It\u0026rsquo;s probably because it contains a lot of information by itself (hence first in FMS) but it\u0026rsquo;s highly correlated with Student, Income and Limit while these variables are more ortogonal to each other, and hence it gets dropped before them in BMS.\n# Plot correlations sns.pairplot(credit[['Rating','Student','Income','Limit']], height=1.8);  If is indeed what we see: Rating and Limit are highly correlated.\nLet\u0026rsquo;s plot the scores for different number of predictors.\n# New figure 2 def make_new_figure_2(): # Init fig, (ax1,ax2) = plt.subplots(1,2) fig.suptitle('Backward Model Selection') # RSS ax1.plot(range(1,K+1), bms_RSS, c='r'); ax1.scatter(np.argmin(bms_RSS)+1, np.min(bms_RSS), marker='x', s=300) ax1.set_ylabel('RSS'); # R2 ax2.plot(range(1,K+1), bms_R2, c='r'); ax2.scatter(np.argmax(bms_R2)+1, np.max(bms_R2), marker='x', s=300) ax2.set_ylabel('R2'); # All axes; for ax in fig.axes: ax.set_xlabel('Number of Predictors'); ax.set_yticks([]);  make_new_figure_2()  Choosing the Optimal Model So far we have use the trainint error in order to select the model. However, the training error can be a poor estimate of the test error. Therefore, RSS and R2 are not suitable for selecting the best model among a collection of models with different numbers of predictors.\nIn order to select the best model with respect to test error, we need to estimate this test error. There are two common approaches:\n We can indirectly estimate test error by making an adjustment to the training error to account for the bias due to overfitting. We can directly estimate the test error, using either a validation set approach or a cross-validation approach.  Some metrics that account for the trainint error are\n Akaike Information Criterium (AIC) Bayesian Information Criterium (BIC) Adjusted $R^2$  The idea behind all these varaibles is to insert a penalty for the number of parameters used in the model. All these measure have theoretical fundations which are beyond the scope of this session.\nWe are now going to test the three metrics\n# Set metrics aic = lambda reg : reg.aic bic = lambda reg : reg.bic r2a = lambda reg : reg.rsquared_adj # Compute best model selection scores scores = model_selection(X, y, aic, bic, r2a) ms_AIC = scores[0] ms_BIC = scores[1] ms_R2a = scores[2]  Computing k=10  # Save best scores ms_AIC_best = [np.min(ms_AIC[ms_AIC[:,0]==k,1]) for k in range(K+1)] ms_BIC_best = [np.min(ms_BIC[ms_BIC[:,0]==k,1]) for k in range(K+1)] ms_R2a_best = [np.max(ms_R2a[ms_R2a[:,0]==k,1]) for k in range(K+1)]  We plot the scores for different model selection methods.\n# Figure 6.2 def make_figure_6_2(): # Init fig, (ax1,ax2,ax3) = plt.subplots(1,3, figsize=(16,5)) fig.suptitle('Figure 6.2') # AIC ax1.scatter(x=ms_AIC[:,0], y=ms_AIC[:,1], facecolors='None', edgecolors='k', alpha=0.5); ax1.plot(range(K+1),ms_AIC_best, c='r'); ax1.scatter(np.argmin(ms_AIC_best), np.min(ms_AIC_best), marker='x', s=300) ax1.set_ylabel('AIC'); # BIC ax2.scatter(x=ms_BIC[:,0], y=ms_BIC[:,1], facecolors='None', edgecolors='k', alpha=0.5); ax2.plot(range(K+1), ms_BIC_best, c='r'); ax2.scatter(np.argmin(ms_BIC_best), np.min(ms_BIC_best), marker='x', s=300) ax2.set_ylabel('BIC'); # R2 adj ax3.scatter(x=ms_R2a[:,0], y=ms_R2a[:,1], facecolors='None', edgecolors='k', alpha=0.5); ax3.plot(range(K+1), ms_R2a_best, c='r'); ax3.scatter(np.argmax(ms_R2a_best), np.max(ms_R2a_best), marker='x', s=300) ax3.set_ylabel('R2_adj'); # All axes; for ax in fig.axes: ax.set_xlabel('Number of Predictors'); ax.set_yticks([]);  make_figure_6_2()  As we can see, all three metrics select more parsimonious models, with BIC being particularly conservative with only 4 variables and $R^2_{adj}$ selecting the larger model with 7 variables.\nValidation and Cross-Validation As an alternative to the approaches just discussed, we can directly estimate the test error using the validation set and cross-validation methods discussed in the previous session.\nThe main problem with cross-validation is the computational burden. We are now going to perform best model selection using the following cross-validation algorithms:\n Validation set approach, 50-50 split, repeated 10 times 5-fold cross-validation 10-fold cross-validation  We are not going to perform Leave-One-Out cross-validation for computational reasons.\ndef cv_scores(X, y, *args): # Init scores = list(itertools.repeat(np.zeros((0,2)), len(args))) # Categorical variables categ_cols = {\u0026quot;Gender\u0026quot;, \u0026quot;Student\u0026quot;, \u0026quot;Married\u0026quot;, \u0026quot;Ethnicity\u0026quot;} # Loop over all possible combinations of regressions K = np.shape(X)[1] for k in range(K+1): print(\u0026quot;Computing k=%1.0f\u0026quot; % k, end =\u0026quot;\u0026quot;) for i in combinations(range(K), k): # Subset X X_subset = X.iloc[:,list(i)] # Get dummies for categorical variables if k\u0026gt;0: categ_subset = list(categ_cols \u0026amp; set(X_subset.columns)) X_subset = pd.get_dummies(X_subset, columns=categ_subset, drop_first=True) # Metrics for i,cv_method in enumerate(args): score = cross_val_score(LinearRegression(), add_constant(X_subset), y, cv=cv_method, scoring='neg_mean_squared_error').mean() score_pair = np.reshape([k,score], (1,-1)) scores[i] = np.append(scores[i], score_pair, axis=0) print(\u0026quot;\u0026quot;, end=\u0026quot;\\r\u0026quot;) return scores  Let\u0026rsquo;s compute the scores for different model selection methods.\n# Define cv methods vset = ShuffleSplit(n_splits=10, test_size=0.5) kf5 = KFold(n_splits=5, shuffle=True) kf10 = KFold(n_splits=10, shuffle=True) # Get best model selection scores scores = cv_scores(X, y, vset, kf5, kf10) ms_vset = scores[0] ms_kf5 = scores[1] ms_kf10 = scores[2]  Computing k=10  # Save best scores ms_vset_best = [np.max(ms_vset[ms_vset[:,0]==k,1]) for k in range(K+1)] ms_kf5_best = [np.max(ms_kf5[ms_kf5[:,0]==k,1]) for k in range(K+1)] ms_kf10_best = [np.max(ms_kf10[ms_kf10[:,0]==k,1]) for k in range(K+1)]  We not plot the scores.\n# Figure 6.3 def make_figure_6_3(): # Init fig, (ax1,ax2,ax3) = plt.subplots(1,3, figsize=(16,5)) fig.suptitle('Figure 6.3') # Validation Set ax1.scatter(x=ms_vset[:,0], y=ms_vset[:,1], facecolors='None', edgecolors='k', alpha=0.5); ax1.plot(range(K+1),ms_vset_best, c='r'); ax1.scatter(np.argmax(ms_vset_best), np.max(ms_vset_best), marker='x', s=300) ax1.set_ylabel('Validation Set'); # 5-Fold Cross Validation ax2.scatter(x=ms_kf5[:,0], y=ms_kf5[:,1], facecolors='None', edgecolors='k', alpha=0.5); ax2.plot(range(K+1), ms_kf5_best, c='r'); ax2.scatter(np.argmax(ms_kf5_best), np.max(ms_kf5_best), marker='x', s=300) ax2.set_ylabel('5-Fold Cross Validation'); # 10-Fold Cross-Validation ax3.scatter(x=ms_kf10[:,0], y=ms_kf10[:,1], facecolors='None', edgecolors='k', alpha=0.5); ax3.plot(range(K+1), ms_kf10_best, c='r'); ax3.scatter(np.argmax(ms_kf10_best), np.max(ms_kf10_best), marker='x', s=300) ax3.set_ylabel('10-Fold Cross-Validation'); # All axes; for ax in fig.axes: ax.set_xlabel('Number of Predictors'); ax.set_yticks([]);  make_figure_6_3()  From the figure we see that each cross-validation method selects a different model and the most accurate one, K-fold CV, select 5 predictors.\n5.2 Shrinkage Methods Model selection methods constrained the number of varaibles before running a linear regression. Shrinkage methods attempt to do the two things simultaneously. In particular they constrain or shrink coefficients by imposing penalties in the objective functions for high values of the parameters.\nRidge Regression The Least Squares Regression minimizes the Residual Sum of Squares\n$$ \\mathrm{RSS}=\\sum_{i=1}^{n}\\left(y_{i}-\\beta_{0}-\\sum_{j=1}^{p} \\beta_{j} x_{i j}\\right)^{2} $$\nThe Ridge Regression objective function instead is\n$$ \\sum_{i=1}^{n}\\left(y_{i}-\\beta_{0}-\\sum_{j=1}^{p} \\beta_{j} x_{i j}\\right)^{2}+\\lambda \\sum_{j=1}^{p} \\beta_{j}^{2}=\\mathrm{RSS}+\\lambda \\sum_{j=1}^{p} \\beta_{j}^{2} $$\nwhere $\\lambda\u0026gt;0$ is a tuning parameter that regulates the extent to which large parameters are penalized.\nIn matrix notation, the objective function is\n$$ ||X\\beta - y||^2_2 + \\alpha ||\\beta||^2_2 $$\nwhich is equivalent to optimizing\n$$ \\frac{1}{N}||X\\beta - y||^2_2 + \\frac{\\alpha}{N} ||\\beta||^2_2 $$\nWe are now going to run Ridge Regression on the Credit dataset trying to explain account Balance with a set of observable individual characteristics.\n# X and y categ_cols = [\u0026quot;Gender\u0026quot;, \u0026quot;Student\u0026quot;, \u0026quot;Married\u0026quot;, \u0026quot;Ethnicity\u0026quot;] X = credit.loc[:, credit.columns != 'Balance'] X = pd.get_dummies(X, columns=categ_cols, drop_first=True) y = credit.loc[:,'Balance'] n = len(credit)  We run ridge regression over a range of values for the penalty paramenter $\\lambda$.\n# Init alpha grid n_grid = 100 alphas = 10**np.linspace(-2,5,n_grid).reshape(-1,1) ridge = Ridge() ridge_coefs = [] # Loop over values of alpha for a in alphas: ridge.set_params(alpha=a) ridge.fit(scale(X), y) ridge_coefs.append(ridge.coef_) ridge_coefs = np.reshape(ridge_coefs,(n_grid,-1))  We use linear regression as a comparison.\n# OLS regression ols = LinearRegression().fit(scale(X),y) ols_coefs = ols.coef_; mod_ols = np.linalg.norm(ols_coefs) # Relative magnitude rel_beta = [np.linalg.norm(ridge_coefs[k,:])/mod_ols for k in range(n_grid)] rel_beta = np.reshape(rel_beta, (-1,1))  We plot the results\n# Figure 6.4 def make_figure_6_4(): # Init fig, (ax1,ax2) = plt.subplots(1,2) fig.suptitle('Figure 6.4: Ridge Regression Coefficients') highlight = [0,1,2,7]; # Plot coefficients - absolute ax1.plot(alphas, ridge_coefs[:,highlight], alpha=1) ax1.plot(alphas, ridge_coefs, c='grey', alpha=0.3) ax1.set_xscale('log') ax1.set_xlabel('lambda'); ax1.set_ylabel('Standardized coefficients'); ax1.legend(['Income', 'Limit', 'Rating', 'Student']) # Plot coefficients - relative ax2.plot(rel_beta, ridge_coefs[:,highlight], alpha=1) ax2.plot(rel_beta, ridge_coefs, c='grey', alpha=0.3) ax2.set_xlabel('Relative Beta'); ax2.set_ylabel('Standardized coefficients');  make_figure_6_4()  As we decrease $\\lambda$, the Ridge coefficients get larger. Moreover, the variables with the consistently largest coefficients are Income, Limit, Rating and Student.\nBias-Variance Trade-off Ridge regression’s advantage over least squares is rooted in the bias-variance trade-off. As $\\lambda$ increases, the flexibility of the ridge regression fit decreases, leading to decreased variance but increased bias.\n$$ y_0 = f(x_0) + \\varepsilon $$\nRecap: we can decompose the Mean Squared Error of an estimator into two components: the variance and the squared bias:\n$$ \\mathbb E\\left(y_{0}-\\hat{f}\\left(x_{0}\\right)\\right)^{2} = \\mathbb E\\left(f(x_0) + \\varepsilon - \\hat f(x_{0})\\right)^{2} = \\ = \\mathbb E\\left(f(x_0) - \\mathbb E[\\hat f(x_{0})] + \\varepsilon - \\hat f(x_{0}) + \\mathbb E[\\hat f(x_{0})] \\right)^{2} = \\ = \\mathbb E \\left[ \\mathbb E [\\hat{f} (x_{0}) ] - f(x_0) \\right]^2 + \\mathbb E \\left[ \\left( \\hat{f} (x_{0}) - \\mathbb E [\\hat{f} (x_{0})] \\right)^2 \\right] + \\mathbb E[\\varepsilon^2] \\ = \\operatorname{Bias} \\left( \\hat{f} (x_{0}) \\right)^2 + \\operatorname{Var}\\left(\\hat{f}\\left(x_{0}\\right)\\right) + \\operatorname{Var}(\\varepsilon) $$\nThe last term is the variance of the error term, sometimes also called the irreducible error since it\u0026rsquo;s pure noise, and we cannot account for it.\n# Compute var-bias def compute_var_bias(X_train, b0, x0, a, k, n, sim, f): # Init y_hat = np.zeros(sim) coefs = np.zeros((sim, k)) # Loop over simulations for s in range(sim): e_train = normal(0,1,(n,1)) y_train = X_train @ b0 + e_train fit = f(a).fit(X_train, y_train) y_hat[s] = fit.predict(x0) coefs[s,:] = fit.coef_ # Compute MSE, Var and Bias2 e_test = normal(0,1,(sim,1)) y_test = x0 @ b0 + e_test mse = np.mean((y_test - y_hat)**2) var = np.var(y_hat) bias2 = np.mean(x0 @ b0 - y_hat)**2 return [mse, var, bias2], np.mean(coefs, axis=0)  np.random.seed(1) # Generate random data n = 50 k = 45 N = 50000 X_train = normal(0.2,1,(n,k)) x0 = normal(0.2,1,(1,k)) e_train = normal(0,1,(n,1)) b0 = uniform(0,1,(k,1))  # Init alpha grid sim = 1000 n_grid = 30 df = pd.DataFrame({'alpha':10**np.linspace(-5,5,n_grid)}) ridge_coefs2 = [] # Init simulations sim = 1000 ridge = lambda a: Ridge(alpha=a, fit_intercept=False) # Loop over values of alpha for i in range(len(df)): print(\u0026quot;Alpha %1.0f/%1.0f\u0026quot; % (i+1,len(df)), end =\u0026quot;\u0026quot;) a = df.loc[i,'alpha'] df.loc[i,['mse','var','bias2']], c = compute_var_bias(X_train, b0, x0, a, k, n, sim, ridge) ridge_coefs2.append(c) print(\u0026quot;\u0026quot;, end=\u0026quot;\\r\u0026quot;) ridge_coefs2 = np.reshape(ridge_coefs2,(n_grid,-1))  Alpha 30/30  # OLS regression y_train = X_train @ b0 + e_train ols = LinearRegression().fit(X_train,y_train) ols_coefs = ols.coef_; mod_ols = np.linalg.norm(ols_coefs) # Relative magnitude rel_beta = [np.linalg.norm(ridge_coefs2[i,:])/mod_ols for i in range(n_grid)] rel_beta = np.reshape(rel_beta, (-1,1))  # Figure 6.5 def make_figure_6_5(): # Init fig, (ax1,ax2) = plt.subplots(1,2) fig.suptitle('Figure 6.5: Ridge Bias-Var decomposition') # MSE ax1.plot(df['alpha'], df[['bias2','var','mse']]); ax1.set_xscale('log'); ax1.set_xlabel('lambda'); ax1.set_ylabel('Mean Squared Error'); ax1.legend(['Bias2','Variance','MSE'], fontsize=12); # MSE ax2.plot(rel_beta, df[['bias2','var','mse']]); ax2.set_xlabel('Relative Beta'); ax2.set_ylabel('Mean Squared Error'); ax2.legend(['Bias2','Variance','MSE'], fontsize=12);  make_figure_6_5()  Ridge regression has the advantage of shrinking coefficients. However, unlike best subset, forward stepwise, and backward stepwise selection, which will generally select models that involve just a subset of the variables, ridge regression will include all $p$ predictors in the final model.\nLasso solves that problem by using a different penalty function.\nLasso The lasso coefficients minimize the following objective function:\n$$ \\sum_{i=1}^{n}\\left(y_{i}-\\beta_{0}-\\sum_{j=1}^{p} \\beta_{j} x_{i j}\\right)^{2}+\\lambda \\sum_{j=1}^{p}\\left|\\beta_{j}\\right| = \\mathrm{RSS} + \\lambda \\sum_{j=1}^p|\\beta_j| $$\nso that the main difference with respect to ridge regression is the penalty function $\\lambda \\sum_{j=1}^{p}\\left|\\beta_{j}\\right|$ instead of $\\lambda \\sum_{j=1}^p (\\beta_j)^2$.\nA consequence of this objective function is that Lasso is much more likely to shrink coefficients to exactly zero, while Ridge only decreases their magnitude. The reason why lies in the shape of the objective function. You can rewrite the Ridge and Lasso minimization problems as constrained optimization:\n  Ridge $$ \\underset{\\beta}{\\operatorname{min}} \\ \\left{\\sum_{i=1}^{n}\\left(y_{i}-\\beta_{0}-\\sum_{j=1}^{p} \\beta_{j} x_{i j}\\right)^{2}\\right} \\quad \\text { subject to } \\quad \\sum_{j=1}^{p}\\left|\\beta_{j}\\right| \\leq s $$\n  Lasso $$ \\underset{\\beta}{\\operatorname{min}} \\ \\left{\\sum_{i=1}^{n}\\left(y_{i}-\\beta_{0}-\\sum_{j=1}^{p} \\beta_{j} x_{i j}\\right)^{2}\\right} \\quad \\text { subject to } \\quad \\sum_{j=1}^{p} \\beta_{j}^{2} \\leq s $$\n  In pictures, constrained optimization problem lookes like this.\nThe red curves represents the contour sets of the RSS. They are elliptical since the objective function is quadratic. The blue area represents the admissible set, i.e. the constraints. As we can see, it is much easier with Lasso to have the constrained optimum on one of the edges of the rhombus.\nWe are now going to repeat the same exercise on the Credit dataset, trying to predict account Balance with a set of obsevable induvidual characteristics, for different values of the penalty paramenter $\\lambda$.\n# X and y categ_cols = [\u0026quot;Gender\u0026quot;, \u0026quot;Student\u0026quot;, \u0026quot;Married\u0026quot;, \u0026quot;Ethnicity\u0026quot;] X = credit.loc[:, credit.columns != 'Balance'] X = pd.get_dummies(X, columns=categ_cols, drop_first=True) y = credit.loc[:,'Balance']  The $\\lambda$ grid is going to be slightly different now.\n# Init alpha grid n_grid = 100 alphas = 10**np.linspace(0,3,n_grid).reshape(-1,1) lasso = Lasso() lasso_coefs = [] # Loop over values of alpha for a in alphas: lasso.set_params(alpha=a) lasso.fit(scale(X), y) lasso_coefs.append(lasso.coef_) lasso_coefs = np.reshape(lasso_coefs,(n_grid,-1))  We run OLS to plot the relative magnitude of the Lasso coefficients.\n# Relative magnitude mod_ols = np.linalg.norm(ols_coefs) rel_beta = [np.linalg.norm(lasso_coefs[i,:])/mod_ols for i in range(n_grid)] rel_beta = np.reshape(rel_beta, (-1,1))  We plot the magnitude of the coefficients $\\beta$\n for different values of $\\lambda$ for different values of of $||\\beta||$  # Figure 6.6 def make_figure_6_6(): # Init fig, (ax1,ax2) = plt.subplots(1,2) fig.suptitle('Figure 6.6') highlight = [0,1,2,7]; # Plot coefficients - absolute ax1.plot(alphas, lasso_coefs[:,highlight], alpha=1) ax1.plot(alphas, lasso_coefs, c='grey', alpha=0.3) ax1.set_xscale('log') ax1.set_xlabel('lambda'); ax1.set_ylabel('Standardized coefficients'); ax1.legend(['Income', 'Limit', 'Rating', 'Student'], fontsize=12) # Plot coefficients - relative ax2.plot(rel_beta, lasso_coefs[:,highlight], alpha=1) ax2.plot(rel_beta, lasso_coefs, c='grey', alpha=0.3) ax2.set_xlabel('relative mod beta'); ax2.set_ylabel('Standardized coefficients');  make_figure_6_6()  Rating seems to be the most important variable, followed by Limit and Student.\nAs with ridge regression, the lasso shrinks the coefficient estimates towards zero. However, in the case of the lasso, the $l_1$ penalty has the effect of forcing some of the coefficient estimates to be exactly equal to zero when the tuning parameter $\\lambda$ is sufficiently large. Hence, much like best subset selection, the lasso performs variable selection.\nWe say that the lasso yields sparse models — that is, models that involve only a subset of the variable\nWe now plot how the choice of $\\lambda$ affects the bias-variance trade-off.\n# Init alpha grid sim = 1000 n_grid = 30 df = pd.DataFrame({'alpha':10**np.linspace(-1,1,n_grid)}) lasso_coefs2 = [] # Init simulations sim = 1000 lasso = lambda a: Lasso(alpha=a, fit_intercept=False) # Loop over values of alpha for i in range(len(df)): print(\u0026quot;Alpha %1.0f/%1.0f\u0026quot; % (i+1,len(df)), end =\u0026quot;\u0026quot;) a = df.loc[i,'alpha'] df.loc[i,['mse','var','bias2']], c = compute_var_bias(X_train, b0, x0, a, k, n, sim, lasso) lasso_coefs2.append(c) print(\u0026quot;\u0026quot;, end=\u0026quot;\\r\u0026quot;) lasso_coefs2 = np.reshape(lasso_coefs2,(n_grid,-1))  Alpha 30/30  # Relative magnitude mod_ols = np.linalg.norm(ols_coefs) rel_beta = [np.linalg.norm(lasso_coefs2[k,:])/mod_ols for k in range(n_grid)] rel_beta = np.reshape(rel_beta, (-1,1))  # OLS regression y_train = X_train @ b0 + e_train ols = LinearRegression().fit(X_train,y_train) ols_coefs = ols.coef_; mod_ols = np.linalg.norm(ols_coefs) # Relative magnitude mod_ols = np.linalg.norm(ols_coefs) rel_beta = [np.linalg.norm(lasso_coefs2[k,:])/mod_ols for k in range(n_grid)] rel_beta = np.reshape(rel_beta, (-1,1))  # Figure 6.8 def make_figure_6_8(): fig, (ax1,ax2) = plt.subplots(1,2, figsize=(12,5)) fig.suptitle('Figure 6.8: Lasso Bias-Var decomposition') # MSE ax1.plot(df['alpha'], df[['bias2','var','mse']]); ax1.set_xscale('log'); ax1.set_xlabel('lambda'); ax1.set_ylabel('Mean Squared Error'); ax1.legend(['Bias2','Variance','MSE'], fontsize=12); # MSE ax2.plot(rel_beta, df[['bias2','var','mse']]); ax2.set_xlabel('Relative Beta'); ax1.set_ylabel('Mean Squared Error'); ax2.legend(['Bias2','Variance','MSE'], fontsize=12);  make_figure_6_8()  As $\\lambda$ increases the squared bias increases and the variance decreases.\nComparing the Lasso and Ridge Regression In order to obtain a better intuition about the behavior of ridge regression and the lasso, consider a simple special case with $n = p$, and $X$ a diagonal matrix with $1$’s on the diagonal and $0$’s in all off-diagonal elements. To simplify the problem further, assume also that we are performing regression without an intercept.\nWith these assumptions, the usual least squares problem simplifies to the coefficients that minimize\n$$ \\sum_{j=1}^{p}\\left(y_{j}-\\beta_{j}\\right)^{2} $$\nIn this case, the least squares solution is given by\n$$ \\hat \\beta_j = y_j $$\nOne can show that in this setting, the ridge regression estimates take the form\n$$ \\hat \\beta_j^{RIDGE} = \\frac{y_j}{1+\\lambda} $$\nand the lasso estimates take the form\n$$ \\hat{\\beta}{j}^{LASSO}=\\left{\\begin{array}{ll} y{j}-\\lambda / 2 \u0026amp; \\text { if } y_{j}\u0026gt;\\lambda / 2 \\ y_{j}+\\lambda / 2 \u0026amp; \\text { if } y_{j}\u0026lt;-\\lambda / 2 \\ 0 \u0026amp; \\text { if }\\left|y_{j}\\right| \\leq \\lambda / 2 \\end{array}\\right. $$\nWe plot the relationship visually.\nnp.random.seed(3) # Generate random data n = 100 k = n X = np.eye(k) e = normal(0,1,(n,1)) b0 = uniform(-1,1,(k,1)) y = X @ b0 + e  # OLS regression reg = LinearRegression().fit(X,y) ols_coefs = reg.coef_; # Ridge regression ridge = Ridge(alpha=1).fit(X,y) ridge_coefs = ridge.coef_; # Ridge regression lasso = Lasso(alpha=0.01).fit(X,y) lasso_coefs = lasso.coef_.reshape(1,-1); # sort order = np.argsort(y.reshape(1,-1), axis=1) y_sorted = np.take_along_axis(ols_coefs, order, axis=1) ols_coefs = np.take_along_axis(ols_coefs, order, axis=1) ridge_coefs = np.take_along_axis(ridge_coefs, order, axis=1) lasso_coefs = np.take_along_axis(lasso_coefs, order, axis=1)  # Figure 6.10 def make_figure_6_10(): # Init fig, (ax1,ax2) = plt.subplots(1,2) fig.suptitle('Figure 6.10') # Ridge ax1.plot(y_sorted.T, ols_coefs.T) ax1.plot(y_sorted.T, ridge_coefs.T) ax1.set_xlabel('True Coefficient'); ax1.set_ylabel('Estimated Coefficient'); ax1.legend(['OLS','Ridge'], fontsize=12); # Lasso ax2.plot(y_sorted.T, ols_coefs.T) ax2.plot(y_sorted.T, lasso_coefs.T) ax2.set_xlabel('True Coefficient'); ax2.set_ylabel('Estimated Coefficient'); ax2.legend(['OLS','Lasso'], fontsize=12);  make_figure_6_10()  We see that ridge regression shrinks every dimension of the data by the same proportion, whereas the lasso hrinks all coefficients toward zero by a similar amount, and sufficiently small coefficients are shrunken all the way to zero.\nSelecting the Tuning Parameter Implementing ridge regression and the lasso requires a method for selecting a value for the tuning parameter $\\lambda$.\nCross-validation provides a simple way to tackle this problem. We choose a grid of $\\lambda$ values, and compute the cross-validation error for each value of $\\lambda$. We then select the tuning parameter value for which the cross-validation error is smallest. Finally, the model is re-fit using all of the available observations and the selected value of the tuning parameter.\n# X and y categ_cols = [\u0026quot;Gender\u0026quot;, \u0026quot;Student\u0026quot;, \u0026quot;Married\u0026quot;, \u0026quot;Ethnicity\u0026quot;] X = credit.loc[:, credit.columns != 'Balance'] X = pd.get_dummies(X, columns=categ_cols, drop_first=True).values y = credit.loc[:,'Balance'] n = len(credit)  We are going to use 10-fold CV as cross-validation algorithm.\n# Get MSE def cv_lasso(X,y,a): # Init mse mse = [] # Generate splits kf10 = KFold(n_splits=10, random_state=None, shuffle=False) kf10.get_n_splits(X) # Loop over splits for train_index, test_index in kf10.split(X): X_train, X_test = X[train_index], X[test_index] y_train, y_test = y[train_index], y[test_index] lasso = Lasso(alpha=a).fit(X_train, y_train) y_hat = lasso.predict(X_test) mse.append(mean_squared_error(y_test, y_hat)) return np.mean(mse)  # Compute MSE over grid of alphas n_grid = 30 alphas = 10**np.linspace(0,3,n_grid).reshape(-1,1) MSE = [cv_lasso(X,y,a) for a in alphas]  What is the optimal $\\lambda$?\n# Find minimum alpha alpha_min = alphas[np.argmin(MSE)] print('Best alpha by 10fold CV:',alpha_min[0])  Best alpha by 10fold CV: 2.592943797404667  We now plot the objective function and the implied coefficients at the optimal $\\lambda$.\n# Get coefficients coefs = [] # Loop over values of alpha for a in alphas: lasso = Lasso(alpha=a).fit(scale(X), y) coefs.append(lasso.coef_) coefs = np.reshape(coefs,(n_grid,-1))  np.shape(coefs)  (30, 11)  # Figure 6.12 def make_figure_6_12(): # Init fig, (ax1,ax2) = plt.subplots(1,2) fig.suptitle('Figure 6.12: Lasso 10-fold CV') # MSE by LOO CV ax1.plot(alphas, MSE, alpha=1); ax1.axvline(alpha_min, c='k', ls='--') ax1.set_xscale('log') ax1.set_xlabel('lambda'); ax1.set_ylabel('MSE'); highlight = [0,1,2,7]; # Plot coefficients - absolute ax2.plot(alphas, coefs[:,highlight], alpha=1) ax2.plot(alphas, coefs, c='grey', alpha=0.3) ax2.axvline(alpha_min, c='k', ls='--') ax2.set_xscale('log') ax2.set_xlabel('lambda'); ax2.set_ylabel('Standardized coefficients'); ax2.legend(['Income', 'Limit', 'Rating', 'Student'], fontsize=10);  make_figure_6_12()  ","date":1646784000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1646784000,"objectID":"2f1f6f919e48a777cab5eae3dcdcbc48","permalink":"https://matteocourthoud.github.io/course/ml-econ/05_regularization/","publishdate":"2022-03-09T00:00:00Z","relpermalink":"/course/ml-econ/05_regularization/","section":"course","summary":"# Remove warnings import warnings warnings.filterwarnings('ignore')  # Import import pandas as pd import numpy as np import time import itertools import statsmodels.api as sm import seaborn as sns from numpy.","tags":null,"title":"Model Selection and Regularization","type":"book"},{"authors":null,"categories":null,"content":"The Gauss Markov Model Definition A statistical model for regression data is the Gauss Markov Model if each of its distributions satisfies the conditions\n  Linearity: a statistical model $\\mathcal{F}$ over data $\\mathcal{D}$ satisfies linearity if for each element of $\\mathcal{F}$, the data can be decomposed in $$ \\begin{aligned} y_ i \u0026amp;= \\beta_ 1 x _ {i1} + \\dots + \\beta_ k x _ {ik} + \\varepsilon_ i = x_ i'\\beta + \\varepsilon_ i \\newline \\underset{n \\times 1}{\\vphantom{\\beta_ \\beta} y} \u0026amp;= \\underset{n \\times k}{\\vphantom{\\beta}X} \\cdot \\underset{k \\times 1}{\\beta} + \\underset{n \\times 1}{\\vphantom{\\beta}\\varepsilon} \\end{aligned} $$\n  Strict Exogeneity: $\\mathbb E [\\varepsilon_i|x_1, \\dots, x_n] = 0, \\forall i$.\n  No Multicollinerity: $\\mathbb E_n [x_i x_i']$ is strictly positive definite almost surely. Equivalent to require $rank(X)=k$ with probability $p \\to 1$. Intuition: no regressor is a linear combination of other regressors.\n  Spherical Error Variance: -$\\mathbb E[\\varepsilon_i^2 | x] = \\sigma^2 \u0026gt; 0, \\ \\forall i$ -$\\mathbb E [\\varepsilon_i \\varepsilon_j |x ] = 0, \\ \\forall$ $1 \\leq i \u0026lt; j \\leq n$\n  The Extended Gauss Markov Model also satisfies assumption\n Normal error term: $\\varepsilon|X \\sim N(0, \\sigma^2 I_n)$ and $\\varepsilon \\perp X$.  Implications  Note that by (2) and (4) you get homoskedasticity:  $$ Var(\\varepsilon_i|x) = \\mathbb E[\\varepsilon_i^2|x]- \\mathbb E[\\varepsilon_i|x]^2 = \\sigma^2 I \\qquad \\forall i $$\n Strict exogeneity is not restrictive since it is sufficient to include a constant in the regression to enforce it $$ y_i = \\alpha + x_i'\\beta + (\\varepsilon_i - \\alpha) \\quad \\Rightarrow \\quad \\mathbb E[\\varepsilon_i] = \\mathbb E_x [ \\mathbb E[ \\varepsilon_i | x]] = 0 $$ This implies $\\mathbb E[x _ {jk} \\varepsilon_i ] = 0$ by the LIE. These two conditions together imply $Cov (x _ {jk} \\varepsilon_i ) = 0$.  Projection A map $\\Pi: V \\to V$ is a projection if $\\Pi \\circ \\Pi = \\Pi$.\nThe Gauss Markov Model assumes that the conditional expectation function (CEF) $f(X) = \\mathbb E[Y|X]$ and the linear projection $g(X) = X \\beta$ coincide.\nCode - DGP This code draws 100 observations from the model $y = 2 x_1 - x_2 + \\varepsilon$ where $x_1, x_2 \\sim U[0,1]$ and $\\varepsilon \\sim N(0,1)$.\n# Set seed Random.seed!(123); # Set the number of observations n = 100; # Set the dimension of X k = 2; # Draw a sample of explanatory variables X = rand(Uniform(0,1), n, k); # Draw the error term σ = 1; ε = rand(Normal(0,1), n, 1) * sqrt(σ); # Set the parameters β = [2; -1]; # Calculate the dependent variable y = X*β + ε;  The OLS estimator Definition The sum of squared residuals (SSR) is given by $$ Q_n (\\beta) \\equiv \\frac{1}{n} \\sum _ {i=1}^n \\left( y_i - x_i'\\beta \\right)^2 = \\frac{1}{n} (y - X\\beta)' (y - X \\beta) $$\nConsider a dataset $\\mathcal{D}$ and define $Q_n(\\beta) = \\mathbb E_n[(y_i - x_i'\\beta )^2 ]$. Then the ordinary least squares (OLS) estimator $\\hat \\beta _ {OLS}$ is the value of $\\beta$ that minimizes $Q_n(\\beta)$.\nWhen we can write $D = (y, X)$ in matrix form, then $$ \\hat \\beta _ {OLS} = \\arg \\min_\\beta \\frac{1}{n} (y - X \\beta)' (y - X\\beta) $$\nDerivation Theorem\nUnder the assumption that $X$ has full rank, the OLS estimator is unique and it is determined by the normal equations. More explicitly, $\\hat \\beta$ is the OLS estimate precisely when $X\u0026rsquo;X \\hat \\beta = X\u0026rsquo;y$.\nProof\nTaking the FOC: $$ \\frac{\\partial Q_n (\\beta)}{\\partial \\beta} = -\\frac{2}{n} X' y + \\frac{2}{n} X\u0026rsquo;X\\beta = 0 \\quad \\Leftrightarrow \\quad X\u0026rsquo;X \\beta = X\u0026rsquo;y $$ Since $(X\u0026rsquo;X)^{-1}$ exists by assumption,\nFinally, $\\frac{\\partial^2 Q_n (\\beta)}{\\partial \\beta \\partial \\beta'} = X\u0026rsquo;X/n$ is positive definite since $X\u0026rsquo;X$ is positive semi-definite and $(X\u0026rsquo;X)^{-1}$ exists because $X$ is full rank. Therefore, $Q_n(\\beta)$ minimized at $\\hat \\beta_n$. $$\\tag*{$\\blacksquare$}$$\nThe $k$ equations $X\u0026rsquo;X \\hat \\beta = X\u0026rsquo;y$ are called normal equations.\nFuther Objects  Fitted coefficient: $\\hat \\beta _ {OLS} = (X\u0026rsquo;X)^{-1} X\u0026rsquo;y = \\mathbb E_n [x_i x_i'] \\mathbb E_n [x_i y_i]$ Fitted residual: $\\hat \\varepsilon_i = y_i - x_i'\\hat \\beta$ Fitted value: $\\hat y_i = x_i' \\hat \\beta$ Predicted coefficient: $\\hat \\beta _ {-i} = \\mathbb E_n [x _ {-i} x' _ {-i}] \\mathbb E_n [x _ {-i} y _ {-i}]$ Prediction error: $\\hat \\varepsilon _ {-i} = y_i - x_i'\\hat \\beta _ {-i}$ Predicted value: $\\hat y_i = x_i' \\hat \\beta _ {-i}$  Notes on Orthogonality Conditions  The normal equations are equivalent to the moment condition $\\mathbb E_n [x_i \\varepsilon_i]= 0$. The algebraic result $\\mathbb E_n [x_i \\hat \\varepsilon_i]= 0$ is called ortogonality property of the OLS residual $\\hat \\varepsilon_i$. If we have included a constant in the regression, $\\mathbb E_n [\\hat \\varepsilon_i] = 0$. $\\mathbb E \\Big[\\mathbb E_n [x_i \\varepsilon_i ] \\Big] = 0$ by strict exogeneity (assumed in GM), but $\\mathbb E_n [x_i \\varepsilon_i] \\ne \\mathbb E [x_i \\varepsilon_i] = 0$. This is why $\\hat \\beta _ {OLS}$ is just an estimate of $\\beta_0$. Calculating OLS is like replacing the $j$ equations $\\mathbb E [x _ {ij} \\varepsilon_i] = 0$ $\\forall j$ with $\\mathbb E_n [x _ {ij} \\varepsilon_i] = 0$ $\\forall j$ and forcing them to hold (remindful of GMM).  The Projection Matrix The projection matrix is given by $P = X(X\u0026rsquo;X)^{-1} X'$. It has the following properties: - $PX = X$ - $P \\hat \\varepsilon = 0 \\quad$ ($P$, $\\varepsilon$ orthogonal) - $P y = X(X\u0026rsquo;X)^{-1} X\u0026rsquo;y = X\\hat \\beta = \\hat y$ - Symmetric: $P=P'$, Idempotent: $PP = P$ - $tr(P) = tr( X(X\u0026rsquo;X)^{-1} X') = tr( X\u0026rsquo;X(X\u0026rsquo;X)^{-1}) = tr(I_k) = k$ - Its diagonal elements are $h_{ii} = x_i (X\u0026rsquo;X)^{-1} x_i'$ and are called leverage.\n $h _ {ii} \\in [0,1]$ is a normalized length of the observed regressor vector $x_i$. In the OLS regression framework it captures the relative influence of observation $i$ on the estimated coefficient. Note that $\\sum _ n h_{ii} = k$.\n The Annihilator Matrix The annihilator matrix is given by $M = I_n - P$. It has the following properties: - $MX = 0 \\quad$ ($M$, $X$ orthogonal) - $M \\hat \\varepsilon = \\hat \\varepsilon$ - $M y = \\hat \\varepsilon$ - Symmetric: $M=M'$, idempotent: $MM = M$ - $tr(M) = n - k$ - Its diagonal elements are $1 - h_{ii} \\in [0,1]$\n Then we can equivalently write $\\hat y$ (defined by stacking $\\hat y_i$ into a vector) as $\\hat y = Py$.\n Estimating Beta # Estimate beta β_hat = inv(X'*X)*(X'*y)  ## 2×1 Array{Float64,2}: ## 1.8821600407711814 ## -0.9429354944506099  # Equivalent but faster formulation β_hat = (X'*X)\\(X'*y)  ## 2×1 Array{Float64,2}: ## 1.8821600407711816 ## -0.9429354944506098  # Even faster (but less intuitive) formulation β_hat = X\\y  ## 2×1 Array{Float64,2}: ## 1.8821600407711807 ## -0.9429354944506088  Equivalent Formulation? Generally it’s not true that $$ \\hat \\beta_{OLS} = \\frac{Var(X)}{Cov(X,y)} $$\n# Wrong formulation β_wrong = inv(cov(X)) * cov(X, y)  ## 2×1 Array{Float64,2}: ## 1.8490257777704475 ## -0.9709213554007003  Equivalent Formulation (correct) But it’s true if you include a constant, $\\alpha$ $$ y = \\alpha + X \\beta + \\varepsilon $$\n# Correct, with constant α = 3; y1 = α .+ X*β + ε; β_hat1 = [ones(n,1) X] \\ y1  ## 3×1 Array{Float64,2}: ## 3.0362313477745615 ## 1.8490257777704477 ## -0.9709213554007007  β_correct1 = inv(cov(X)) * cov(X, y1)  ## 2×1 Array{Float64,2}: ## 1.8490257777704477 ## -0.9709213554007006  Some More Objects # Predicted y y_hat = X*β_hat; # Residuals ε_hat = y - X*β_hat; # Projection matrix P = X * inv(X'*X) * X'; # Annihilator matrix M = I - P; # Leverage h = diag(P);  OLS Residuals Homoskedasticity The error is homoskedastic if $\\mathbb E [\\varepsilon^2 | x] = \\sigma^2$ does not depend on $x$. $$ Var(\\varepsilon) = I \\sigma^2 = \\begin{bmatrix} \\sigma^2 \u0026amp; \\dots \u0026amp; 0 \\newline\\newline\n\\vdots \u0026amp; \\ddots \u0026amp; \\vdots \\newline 0 \u0026amp; \\dots \u0026amp; \\sigma^2 \\end{bmatrix} $$\nThe error is heteroskedastic if $\\mathbb E [\\varepsilon^2 | x] = \\sigma^2(x)$ does depend on $x$. $$ Var(\\varepsilon) = I \\sigma_i^2 = \\begin{bmatrix} \\sigma_1^2 \u0026amp; \\dots \u0026amp; 0 \\newline \\vdots \u0026amp; \\ddots \u0026amp; \\vdots \\newline 0 \u0026amp; \\dots \u0026amp; \\sigma_n^2 \\end{bmatrix} $$\nResidual Variance The OLS residual variance can be an object of interest even in a heteroskedastic regression. Its method of moments estimator is given by $$ \\hat \\sigma^2 = \\frac{1}{n} \\sum _ {i=1}^n \\hat \\varepsilon_i^2 $$\n Note that $\\hat \\sigma^2$ can be rewritten as $$ \\hat \\sigma^2 = \\frac{1}{n} \\varepsilon' M' M \\varepsilon = \\frac{1}{n} tr(\\varepsilon' M \\varepsilon) = \\frac{1}{n} tr(M \\varepsilon' \\varepsilon) $$\n However, the method of moments estimator is a biesed estimator. In fact $$ \\mathbb E[\\hat \\sigma^2 | X] = \\frac{1}{n} \\mathbb E [ tr(M \\varepsilon' \\varepsilon) | X] = \\frac{1}{n} tr( M\\mathbb E[\\varepsilon' \\varepsilon |X]) = \\frac{1}{n} \\sum _ {i=1}^n (1-h_{ii}) \\sigma^2_i $$\nUnder conditional homoskedasticity, the above expression simplifies to $$ \\mathbb E[\\hat \\sigma^2 | X] = \\frac{1}{n} tr(M) \\sigma^2 = \\frac{n-k}{n} \\sigma^2 $$\nSample Variance The OLS residual sample variance is denoted by $s^2$ and is given by $$ s^2 = \\frac{SSR}{n-k} = \\frac{\\hat \\varepsilon'\\hat \\varepsilon}{n-k} = \\frac{1}{n-k}\\sum _ {i=1}^n \\hat \\varepsilon_i^2 $$ Furthermore, the square root of $s^2$, denoted $s$, is called the standard error of the regression (SER) or the standard error of the equation (SEE). Not to be confused with other notions of standard error to be defined later in the course.\n The sum of squared residuals can be rewritten as: $SSR = \\hat \\varepsilon' \\hat \\varepsilon = \\varepsilon' M \\varepsilon$.\n The OLS residual sample variance is an unbiased estimator of the error variance $\\sigma^2$.\nAnother unbiased estimator of $\\sigma^2$ is given by $$ \\bar \\sigma^2 = \\frac{1}{n} \\sum _ {i=1}^n (1-h_{ii})^{-1} \\hat \\varepsilon_i^2 $$\nUncentered R^2 One measure of the variability of the dependent variable $y_i$ is the sum of squares $\\sum _ {i=1}^n y_i^2 = y\u0026rsquo;y$. There is a decomposition: $$ \\begin{aligned} y\u0026rsquo;y \u0026amp;= (\\hat y + e)' (\\hat y + \\hat \\varepsilon) \\newline \u0026amp;= \\hat y' \\hat y + 2 \\hat y' \\hat \\varepsilon + \\hat \\varepsilon' \\hat \\varepsilon e \\newline \u0026amp;= \\hat y' \\hat y + 2 b\u0026rsquo;X'\\hat \\varepsilon + \\hat \\varepsilon' \\hat \\varepsilon \\ \\ (\\text{since} \\ \\hat y = Xb) \\newline \u0026amp;= \\hat y' \\hat y + \\hat \\varepsilon'\\hat \\varepsilon \\ \\ (\\text{since} \\ X'\\hat \\varepsilon =0) \\end{aligned} $$\nThe uncentered $\\mathbf{R^2}$ is defined as: $$ R^2 _ {uc} \\equiv 1 - \\frac{\\hat \\varepsilon'\\hat \\varepsilon}{y\u0026rsquo;y} = 1 - \\frac{\\mathbb E_n[\\hat \\varepsilon_i^2]}{\\mathbb E_n[y_i^2]} = \\frac{ \\mathbb E [\\hat y_i^2]}{ \\mathbb E [y_i^2]} $$\nCentered R^2 A more natural measure of variability is the sum of centered squares $\\sum _ {i=1}^n (y_i - \\bar y)^2,$ where $\\bar y := \\frac{1}{n}\\sum _ {i=1}^n y_i$. If the regressors include a constant, it can be decomposed as $$ \\sum _ {i=1}^n (y_i - \\bar y)^2 = \\sum _ {i=1}^n (\\hat y_i - \\bar y)^2 + \\sum _ {i=1}^n \\hat \\varepsilon_i^2 $$\nThe coefficient of determination, $\\mathbf{R^2}$, is defined as $$ R^2 \\equiv 1 - \\frac{\\sum _ {i=1}^n \\hat \\varepsilon_i^2}{\\sum _ {i=1}^n (y_i - \\bar y)^2 }= \\frac{ \\sum _ {i=1}^n (\\hat y_i - \\bar y)^2 } { \\sum _ {i=1}^n (y_i - \\bar y)^2} = \\frac{\\mathbb E_n[(\\hat y_i - \\bar y)^2]}{\\mathbb E_n[(y_i - \\bar y)^2]} $$\n Always use the centered $R^2$ unless you really know what you are doing.\n Code - Variance # Biased variance estimator σ_hat = ε_hat'*ε_hat / n; # Unbiased estimator 1 σ_hat_2 = ε_hat'*ε_hat / (n-k); # Unbiased estimator 2 σ_hat_3 = mean( ε_hat.^2 ./ (1 .- h) );  Code - R^2 # R squared - uncentered R2_uc = (y_hat'*y_hat)/ (y'*y); # R squared y_bar = mean(y); R2 = ((y_hat .- y_bar)'*(y_hat .- y_bar))/ ((y .- y_bar)'*(y .- y_bar));  Finite Sample Properties of OLS Conditional Unbiasedness Theorem\nUnder the GM assumptions (1)-(3), the OLS estimator is conditionally unbiased, i.e. the distribution of $\\hat \\beta _ {OLS}$ is centered at $\\beta_0$: $\\mathbb E [\\hat \\beta | X] = \\beta_0$.\nProof $$ \\begin{aligned} \\mathbb E [\\hat \\beta | X] \u0026amp;= \\mathbb E [ (X\u0026rsquo;X)^{-1} X\u0026rsquo;y | X] = \\newline \u0026amp;= (X\u0026rsquo;X)^{-1} X ' \\mathbb E [y | X] = \\newline \u0026amp;= (X\u0026rsquo;X)^{-1} X' \\mathbb E [X \\beta + \\varepsilon | X] = \\newline \u0026amp;= (X\u0026rsquo;X)^{-1} X\u0026rsquo;X \\beta + (X\u0026rsquo;X)^{-1} X' \\mathbb E [\\varepsilon | X] = \\newline \u0026amp;= \\beta \\end{aligned} $$ $$\\tag*{$\\blacksquare$}$$\nOLS Variance Theorem\nUnder the GM assumptions (1)-(3), $Var(\\hat \\beta |X) = \\sigma^2 (X\u0026rsquo;X)^{-1}$.\nProof: $$ \\begin{aligned} Var(\\hat \\beta |X) \u0026amp;= Var( (X\u0026rsquo;X)^{-1} X\u0026rsquo;y|X) = \\newline \u0026amp;= ((X\u0026rsquo;X)^{-1} X' ) Var(y|X) ((X\u0026rsquo;X)^{-1} X' )' = \\newline \u0026amp;= ((X\u0026rsquo;X)^{-1} X' ) Var(X\\beta + \\varepsilon|X) ((X\u0026rsquo;X)^{-1} X' )' = \\newline \u0026amp;= ((X\u0026rsquo;X)^{-1} X' ) Var(\\varepsilon|X) ((X\u0026rsquo;X)^{-1} X' )' = \\newline \u0026amp;= ((X\u0026rsquo;X)^{-1} X' ) \\sigma^2 I ((X\u0026rsquo;X)^{-1} X' )' = \\newline \u0026amp;= \\sigma^2 (X\u0026rsquo;X)^{-1} \\end{aligned} $$ $$\\tag*{$\\blacksquare$}$$\nHigher correlation of the $X$ implies higher variance of the OLS estimator.\n Intuition: individual observations carry less information. You are exploring a smaller region of the $X$ space.\n BLUE Theorem\nUnder the GM assumptions (1)-(3), $Cov (\\hat \\beta, \\hat \\varepsilon ) = 0$.\nTheorem\nUnder the GM assumptions (1)-(3), $\\hat \\beta _ {OLS}$ is the best (most efficient) linear, unbiased estimator (BLUE), i.e., for any unbiased linear estimator $b$: $Var (b|X) \\geq Var (\\hat \\beta |X)$.\nBLUE Proof Consider four steps:\n Define three objects: (i) $b= Cy$, (ii) $A = (X\u0026rsquo;X)^{-1} X'$ such that $\\hat \\beta = A y$, and (iii) $D = C-A$. Decompose $b$ as $$ \\begin{aligned} b \u0026amp;= (D + A) y = \\newline \u0026amp;= Dy + Ay = \\newline\n\u0026amp;= D (X\\beta + \\varepsilon) + \\hat \\beta = \\newline \u0026amp;= DX\\beta + D \\varepsilon + \\hat \\beta \\end{aligned} $$ By assumption, $b$ must be unbiased: $$ \\begin{aligned} \\mathbb E [b|X] \u0026amp;= \\mathbb E [D(X\\beta + \\varepsilon) + Ay |X] = \\newline \u0026amp;= \\mathbb E [DX\\beta|X] + \\mathbb E [D\\varepsilon |X] + \\mathbb E [\\hat \\beta |X] = \\newline \u0026amp;= DX\\beta + D \\mathbb E [\\varepsilon |X] +\\beta \\newline\n\u0026amp;= DX\\beta + \\beta \\end{aligned} $$ Hence, it must be that $DX = 0$  BLUE Proof (2)  We know by (2)-(3) that $b = D \\varepsilon + \\hat \\beta$. We can now calculate its variance. $$ \\begin{aligned} Var (b|X) \u0026amp;= Var (\\hat \\beta + D\\varepsilon|X) = \\newline \u0026amp;= Var (Ay + D\\varepsilon|X) = \\newline \u0026amp;= Var (AX\\beta + (D + A)\\varepsilon|X) = \\newline \u0026amp;= Var((D+A)\\varepsilon |X) = \\newline \u0026amp;= (D+A)\\sigma^2 I (D+A)' = \\newline \u0026amp;= \\sigma^2 I (DD' + AA' + DA' + AD') = \\newline \u0026amp;= \\sigma^2 I (DD' + AA') \\geq \\newline \u0026amp;\\geq \\sigma^2 AA'= \\newline \u0026amp;= \\sigma^2 (X\u0026rsquo;X)^{-1} = \\newline \u0026amp;= Var (\\hat \\beta|X) \\end{aligned} $$ since $DA'= AD' = 0$, $DX = 0$ and $AA' = (X\u0026rsquo;X)^{-1}$. $$\\tag*{$\\blacksquare$}$$   $Var(b | X) \\geq Var (\\hat{\\beta} | X)$ is meant in a positive definite sense.\n Code - Variance # Ideal variance of the OLS estimator var_β = σ * inv(X'*X)  ## 2×2 Array{Float64,2}: ## 0.0609402 -0.0467732 ## -0.0467732 0.0656808  # Standard errors std_β = sqrt.(diag(var_β))  ## 2-element Array{Float64,1}: ## 0.24686077212177054 ## 0.25628257446345265  ","date":1635465600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1635465600,"objectID":"ff820c54188a6875670d383b4784b4c5","permalink":"https://matteocourthoud.github.io/course/metrics/05_ols_algebra/","publishdate":"2021-10-29T00:00:00Z","relpermalink":"/course/metrics/05_ols_algebra/","section":"course","summary":"The Gauss Markov Model Definition A statistical model for regression data is the Gauss Markov Model if each of its distributions satisfies the conditions\n  Linearity: a statistical model $\\mathcal{F}$ over data $\\mathcal{D}$ satisfies linearity if for each element of $\\mathcal{F}$, the data can be decomposed in $$ \\begin{aligned} y_ i \u0026amp;= \\beta_ 1 x _ {i1} + \\dots + \\beta_ k x _ {ik} + \\varepsilon_ i = x_ i'\\beta + \\varepsilon_ i \\newline \\underset{n \\times 1}{\\vphantom{\\beta_ \\beta} y} \u0026amp;= \\underset{n \\times k}{\\vphantom{\\beta}X} \\cdot \\underset{k \\times 1}{\\beta} + \\underset{n \\times 1}{\\vphantom{\\beta}\\varepsilon} \\end{aligned} $$","tags":null,"title":"OLS Algebra","type":"book"},{"authors":null,"categories":null,"content":"In this notebook, we are going to build a pipeline for a general prediction problem.\n# Standard Imports from src.utils import * from src.get_feature_names import get_feature_names  # Set inline graphs plt.style.use('seaborn') %matplotlib inline %config InlineBackend.figure_format = 'retina'  Introduction Usually, in machine learning prediction tasks, the data consists in 3 files:\n X_train.csv y_train.csv X_test.csv  The purpose of the exercise is to produce a y_test.csv file, with the predicted values corresponding to the X_test.csv observations.\nThe functions we will write are going to be general and will adapt to any type of dataset, and we will test them on the House Prices Dataset which is a standard dataset for these kind of tasks. The data consists of 2 files:\n train.csv test.csv  The target variable that we want to predict is SalePrice.\nSetup First we want to import the data.\n# Import data df_train = pd.read_csv(\u0026quot;data/train.csv\u0026quot;) df_test = pd.read_csv(\u0026quot;data/test.csv\u0026quot;) print(f\u0026quot;Training data: {np.shape(df_train)} \\n Testing data: {np.shape(df_test)}\u0026quot;)  Training data: (1460, 81) Testing data: (1459, 80)  The training data also includes the target variable SalePrice, while, as usual, the testing data does not. We need to separate the training data into two parts:\n X: the features y: the target  # Select the features X_train = df_train.drop(['SalePrice'], axis=1) X_test = df_test # Check size print(f\u0026quot;Training features: {np.shape(X_train)} \\n Testing features: {np.shape(X_test)}\u0026quot;)  Training features: (1460, 80) Testing features: (1459, 80)  # Select the target y_train = df_train['SalePrice'] # Check size print(f\u0026quot;Training target: {np.shape(y_train)}\u0026quot;)  Training target: (1460,)  It\u0026rsquo;s good practice to immediately set aside a validation sample with 20% of the observations. The purpose of the validation sample is to give us unbiased estimate of the prediction score. Therefore, we want to set it aside as soon as possible, not to be conditioned in any way by it. Possibly, set it away even before data exploration.\nThe more we tune the algorithm based on the feedback received from the validation sample, the more biased our estimate is going to be. Ideally, one would use only cross-validation on the training data and tune only a couple of times using the validation data.\n# Set aside the validation sample X_train, X_validation, y_train, y_validation = train_test_split(X_train, y_train, test_size=0.2)  Now we are ready to build and test our pipeline.\nData Exploration First, let\u0026rsquo;s have a quick look at the data.\nX_train.head()   .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }  \n  Id MSSubClass MSZoning LotFrontage LotArea Street Alley LotShape LandContour Utilities ... ScreenPorch PoolArea PoolQC Fence MiscFeature MiscVal MoSold YrSold SaleType SaleCondition     822 823 60 RL NaN 12394 Pave NaN IR1 Lvl AllPub ... 0 0 NaN NaN NaN 0 10 2007 WD Family   648 649 60 RL 70.0 7700 Pave NaN Reg Lvl AllPub ... 0 0 NaN NaN NaN 0 6 2010 WD Normal   897 898 90 RL 64.0 7018 Pave NaN Reg Lvl AllPub ... 0 0 NaN NaN NaN 0 6 2009 WD Alloca   1131 1132 20 RL 63.0 10712 Pave NaN Reg Lvl AllPub ... 0 0 NaN MnPrv NaN 0 9 2007 Oth Abnorml   1038 1039 160 RM 21.0 1533 Pave NaN Reg Lvl AllPub ... 0 0 NaN NaN NaN 0 5 2009 WD Normal    5 rows × 80 columns\n The Id column is clearly not useful for prediction, let\u0026rsquo;s drop it from both datasets.\n# Drop Id X_train.drop([\u0026quot;Id\u0026quot;], axis=1, inplace=True) X_test.drop([\u0026quot;Id\u0026quot;], axis=1, inplace=True)  Now we want to identify categorical and numerical variables.\n# Save column types numerical_cols = list(X_train.describe().columns) categorical_cols = list(X_train.describe(include=object).columns) print(\u0026quot;There are %i numerical and %i categorical variables\u0026quot; % (len(numerical_cols), len(categorical_cols)))  There are 36 numerical and 43 categorical variables  Let\u0026rsquo;s start by analyzing the numerical variables.\nX_numerical = X_train.loc[:, numerical_cols] corr = X_numerical.corr()  fig, ax = plt.subplots(1, 1, figsize=(10,10)) fig.suptitle(\u0026quot;Correlation between categorical variables\u0026quot;, fontsize=16) cbar_ax = fig.add_axes([.95, .12, .05, .76]) sns.heatmap(corr, vmin=-1, vmax=1, center=0, cmap=sns.diverging_palette(20, 220, n=20), square=True, ax=ax, cbar_ax = cbar_ax) plt.show()  For the non/numeric columns, we need a further option.\nunique_values = X_train.describe(include=object).T.unique  # Plot fig, ax = plt.subplots(1, 1, figsize=(10,6)) fig.suptitle(\u0026quot;Distribution of unique values for categorical variables\u0026quot;, fontsize=16) sns.histplot(data=unique_values) plt.show();  Let\u0026rsquo;s save the identity of the numerical and categorical columns.\n# Save column types numerical_cols = list(X_train.describe().columns) categorical_cols = list(X_train.describe(include=object).columns)  How many missing values are there in the dataset?\nmissing_values = X_train.isnull().sum().sort_values(ascending=True)[-20:] / len(X_train)  fig, ax = plt.subplots(figsize=(10,8)) ax.set_title(\u0026quot;Variables with most missing values\u0026quot;, fontsize=16) ax.barh(np.arange(len(missing_values)), missing_values) ax.set_yticks(np.arange(len(missing_values))) ax.set_yticklabels(missing_values.index) ax.set_xlabel('Percentage of missig values') plt.show()  Around 10% of each feature is missing. We will have to deal with that.\nPre-processing First, let\u0026rsquo;s process numerical variables. We want to do two things:\n inpute missing values standardize all variables  Which imputer should to use? It depends on the type of missing data:\n  Missing absolutely at random: as the name says, in this case we believe that missing values are distributed uniformly at random, independently across variables.\n In this case, the only information on missing values comes from the distribution of non-missing values of the same variable. No information on missing values is contained in other variables.    Missing at random: in this case, missing values are random, conditional on values of other observed variables.\n In this case, information in other variables might help filling missing values.    Missing non at random: in this last case, missing values depend on information that we do not observe.\n This is the most tricky category of missing values since data alone does not tell us which values might be missing. For example, we might have that older women might be less likely to report the age. If we consider the data missing at random (absolutely or not), we would underestimate the missing ages. External information such as the sample population might help. For example, we could estimate the probability of not reporting the age and fill the missing values with the expected age, conditional on age not being reported.    So, which imputers are readily available in sklearn for numerical data?\nFor data missing absolutely at random, there is one standard sklearn library: SimpleImputer(). It allows different strategy options such as\n \u0026quot;mean\u0026quot; \u0026quot;median\u0026quot; \u0026quot;most_frequent\u0026quot;  For data missing at random, there are multiple sklearn libraries:\n KNNImputer(): uses KNN IterativeImputer(): uses a variety of ML algorithms  see comparison here    After we have inputed missing values, we want to standardize numerical variables to make the algorithm more efficient and robust to outliers.\nThe two main options for standardization are:\n StandardScaler(): which normalizes each variable to mean zero and unit variance MinMaxScaler(): which normalizes each variable to an interval between zero an one  # Inputer for numerical variables num = Pipeline(steps=[ ('ii', IterativeImputer()), ('ss', StandardScaler()) ])  For categorical variables, we do not have to worry about scaling. However, we still need to impute missing values and, crucially, we need to transform them into numerical variables. This process is called encoding.\nWhich imputer should to use?\nFor data missing absolutely at random, the only available strategy option for SimpleImputer() is\n \u0026quot;most_frequent\u0026quot;  For data missing at random, we can still use both\n KNNImputer() IterativeImputer()  For encoding categorical variables, the standard option is OneHotEncoder() which generates unique binary variables out of every values of the categorical variable.\n# One Hot Encoder for categorical data cat = Pipeline(steps=[ ('si', SimpleImputer(strategy=\u0026quot;most_frequent\u0026quot;)), ('ohe', OneHotEncoder(handle_unknown=\u0026quot;ignore\u0026quot;)), ])  # Preprocess column transformer for preprocessing data preprocess = ColumnTransformer( transformers=[ ('num', num, numerical_cols), ('cat', cat, categorical_cols), ])  Information and components How much information is contained in our dataset? It is a dense or sparse dataset?\nX_clean = num.fit_transform(X_numerical) pca = PCA().fit(X_clean) explained_variance = pca.explained_variance_ratio_  fig, (ax1, ax2) = plt.subplots(1,2, figsize=(15,6)) fig.suptitle('Principal Component Analysis', fontsize=16); # Relative ax1.plot(range(len(explained_variance)), explained_variance) ax1.set_ylabel('Prop. Variance Explained') ax1.set_xlabel('Principal Component'); # Cumulative ax2.plot(range(len(explained_variance)), np.cumsum(explained_variance)) ax2.set_ylabel('Cumulative Variance Explained'); ax2.set_xlabel('Principal Component');  Feature Importance Before starting our prediction analysis, we would like to understand which variables are most important for our prediction problem.\ndef plot_featureimportance(importance, preprocess): df = pd.DataFrame({\u0026quot;names\u0026quot;: get_feature_names(preprocess), \u0026quot;values\u0026quot;: importance}) df = df.sort_values(\u0026quot;values\u0026quot;).iloc[:20, :] # plot fig, ax = plt.subplots(figsize=(10,8)) ax.set_title(\u0026quot;Feature importance\u0026quot;, fontsize=16) sns.barplot(y=\u0026quot;names\u0026quot;, x=\u0026quot;values\u0026quot;, data=df) ax.barh(np.arange(len(df)), df[\u0026quot;values\u0026quot;]) plt.show()  We start with linear regression feature importance: we standardize all variables to be mean vero and unit variance, and we run a linear regression over the test set.\ndef featureimportance_lr(X, y): X_clean = preprocess.fit_transform(X) # fit the model model = LinearRegression() model.fit(X_clean, y) # get importance importance = np.abs(model.coef_) plot_featureimportance(importance, preprocess)  # Plot linear feature importance featureimportance_lr(X_train, y_train)  We now look at regression tree feature importance.\ndef featureimportance_forest(X, y): X_clean = preprocess.fit_transform(X) # fit the model model = RandomForestRegressor() model.fit(X_clean, y) # get importance importance = model.feature_importances_ plot_featureimportance(importance, preprocess)  # Plot tree feature importance featureimportance_forest(X_train, y_train)  Weighting Another important check to perform concerns weighting. Is the distribution of our objective variable the same in the training and in the test sample? If it is not the case, we might get a poor performance just because our training sample is not representative of our testing sample.\nThis is something that usually we cannot test, since we do not have access to the distribution of the target variable in the test data. However, we might be given the information ex-ante as a warning.\nIn this case, we perform the analysis on the validation set. Since we have selected the validation set at random, we do not expect significant differences.\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15,6)) # Plot 1 sns.histplot(data=y_train, kde=True, ax=ax1) sns.histplot(data=y_validation, kde=True, ax=ax1, color='orange') ax1.set_title(\u0026quot;Density Function of y\u0026quot;, fontsize=16); ax1.legend(['y train', 'y validation']) # Plot 2 sns.histplot(data=y_train, element=\u0026quot;step\u0026quot;, fill=False, cumulative=True, stat=\u0026quot;density\u0026quot;, common_norm=False, ax=ax2) sns.histplot(data=y_validation, element=\u0026quot;step\u0026quot;, fill=False, cumulative=True, stat=\u0026quot;density\u0026quot;, common_norm=False, ax=ax2, color='orange') ax2.set_title(\u0026quot;Cumulative Distribution of y\u0026quot;, fontsize=16); ax2.legend(['y train', 'y validation']);  Since the size of the test sample is smaller than the size of the training sample, the two densities are different. However, the distributions indicate that the standardized distributions are the same.\nModel There are many models to choose among.\n# prepare models models = {\u0026quot;Lasso\u0026quot;: Lasso(alpha=100), \u0026quot;Ridge\u0026quot;: BayesianRidge(), \u0026quot;KNN\u0026quot;: KNeighborsRegressor(), \u0026quot;Kernel\u0026quot;: KernelRidge(), \u0026quot;Naive\u0026quot;: GaussianNB(), \u0026quot;SVM\u0026quot;: SVR(), \u0026quot;Ada\u0026quot;: AdaBoostRegressor(), \u0026quot;Tree\u0026quot;: DecisionTreeRegressor(), \u0026quot;Forest\u0026quot;: RandomForestRegressor(), \u0026quot;GBoost\u0026quot;: GradientBoostingRegressor(), \u0026quot;XGBoost\u0026quot;: XGBRegressor(), \u0026quot;LGBoost\u0026quot;: LGBMRegressor()}  def evaluate_model(model, name, X, y, cv, scoring): X_clean = preprocess.fit_transform(X) start = time.perf_counter() cv_results = cross_val_score(model, X_clean, y, cv=cv, scoring=scoring) t = time.perf_counter()-start score = {\u0026quot;model\u0026quot;:name, \u0026quot;mean\u0026quot;:-np.mean(cv_results), \u0026quot;std\u0026quot;:np.std(cv_results), \u0026quot;time\u0026quot;:t} print(\u0026quot;%s: %f (%f) in %f seconds\u0026quot; % (name, -np.mean(cv_results), np.std(cv_results), t)) return score  def plot_model_scores(scores): fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15,6)) fig.suptitle(\u0026quot;Comparing algorithms\u0026quot;, fontsize=16) # Plot 1 scores.sort_values(\u0026quot;mean\u0026quot;, ascending=False, inplace=True) ax1.set_title(\u0026quot;Mean squared error\u0026quot;, fontsize=16) ax1.barh(range(len(scores)), scores[\u0026quot;mean\u0026quot;], xerr=scores[\u0026quot;std\u0026quot;]) ax1.set_yticks(range(len(scores))) ax1.set_yticklabels([s for s in scores[\u0026quot;model\u0026quot;]]) # Plot 2 scores.sort_values(\u0026quot;time\u0026quot;, ascending=False, inplace=True) ax2.set_title(\u0026quot;Time\u0026quot;, fontsize=16) ax2.barh(range(len(scores)), scores[\u0026quot;time\u0026quot;], color='tab:orange') ax2.set_yticks(range(len(scores))) ax2.set_yticklabels([s for s in scores[\u0026quot;model\u0026quot;]]) plt.show()  def compare_models(models): scores = pd.DataFrame() cv = KFold(n_splits=5) scoring = 'neg_mean_squared_error' for name, model in models.items(): score = evaluate_model(model, name, X_validation, y_validation, cv, scoring) scores = scores.append(score, ignore_index=True) return scores  scores = compare_models(models)  Lasso: 747411443.913101 (462917309.181485) in 0.109821 seconds Ridge: 718774315.061634 (487089023.387329) in 0.472070 seconds KNN: 1756639001.600806 (1476470798.673143) in 0.019063 seconds Kernel: 844681295.934677 (476183041.447080) in 0.085055 seconds Naive: 5254835359.080946 (2916476370.114636) in 0.045415 seconds SVM: 6141030577.726756 (3241262535.954060) in 0.046852 seconds Ada: 1513638885.120911 (1332241015.479751) in 0.306255 seconds Tree: 3258264310.733547 (2139525308.773295) in 0.018476 seconds Forest: 1324403652.968275 (1246235286.003631) in 1.105161 seconds GBoost: 1200654655.518314 (1053677796.098979) in 0.494536 seconds XGBoost: 1819197282.034136 (1587393748.901112) in 0.692401 seconds LGBoost: 1318077152.379926 (1278188928.507894) in 0.157495 seconds  plot_model_scores(scores)  Pipeline We are now ready to pick a model.\n# Set model model = LGBMRegressor()  We need to choose a cross-validation procedure to test our model.\ncv = KFold()  Finally, we can combine all the parts into a single pipeline.\nfinal_pipeline = Pipeline(steps=[ ('preprocess', preprocess), ('model', model) ])  Now we can decide which parts of the pipeline to test.\n# Select parameters to explore param_grid = {'preprocess__num__ii': [SimpleImputer(), KNNImputer(), IterativeImputer()], 'preprocess__cat__si__strategy': [\u0026quot;most_frequent\u0026quot;, \u0026quot;constant\u0026quot;], 'model__learning_rate': [0.1, 0.2], 'model__subsample': [1.0, 0.5], 'model__max_depth': [30, -1]}  We now generate a grid of parameters we want to search over.\n# Save pipeline grid_search = GridSearchCV(final_pipeline, param_grid, cv=cv, n_jobs=-1, scoring='neg_mean_squared_error', verbose=3)  We fit the pipeline and pick the best estimator, from the cross-validation score.\n# Fit pipeline grid_search.fit(X_train, y_train) grid_search.best_estimator_  Fitting 5 folds for each of 48 candidates, totalling 240 fits Pipeline(steps=[('preprocess', ColumnTransformer(transformers=[('num', Pipeline(steps=[('ii', KNNImputer()), ('ss', StandardScaler())]), ['MSSubClass', 'LotFrontage', 'LotArea', 'OverallQual', 'OverallCond', 'YearBuilt', 'YearRemodAdd', 'MasVnrArea', 'BsmtFinSF1', 'BsmtFinSF2', 'BsmtUnfSF', 'TotalBsmtSF', '1stFlrSF', '2ndFlrSF', 'LowQualFinSF', 'GrLivArea', 'BsmtFullBat... 'LotConfig', 'LandSlope', 'Neighborhood', 'Condition1', 'Condition2', 'BldgType', 'HouseStyle', 'RoofStyle', 'RoofMatl', 'Exterior1st', 'Exterior2nd', 'MasVnrType', 'ExterQual', 'ExterCond', 'Foundation', 'BsmtQual', 'BsmtCond', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2', 'Heating', 'HeatingQC', 'CentralAir', 'Electrical', ...])])), ('model', LGBMRegressor(max_depth=30))])  We have three ways of testing the quality of fit of our model:\n score on the training data score on the validation data score on the test data  Score on the training data: this is a biased score since we have picked the model that was best fitting the training data. Kfold cross-validation is efficient in terms of data use, but still evaluates the model over the same data it was trained.\n# Cross/validation score y_train_hat = grid_search.best_estimator_.predict(X_train) train_rmse = mean_squared_error(y_train, y_train_hat, squared=False) print('RMSE on training data :', train_rmse)  RMSE on training data : 12151.309344378069  Score on the validation data: this is an unbiased score since we have left out this sample exactly for this purpose. However, be aware that the validation score is unbiased on on the first run. Once we change the grid and pick the algorithm based on previous validation data scores, also this score becomes biased.\n# Validation set score y_validation_hat = grid_search.best_estimator_.predict(X_validation) validation_rmse = mean_squared_error(y_validation, y_validation_hat, squared=False) print('RMSE on validation data :', validation_rmse)  RMSE on validation data : 27676.358798908263  Final predictions: we can now use our model to output the predictions.\n# Validation score y_test_hat = grid_search.best_estimator_.predict(X_test)  [CV 2/5] END model__learning_rate=0.1, model__max_depth=30, model__subsample=1.0, preprocess__cat__si__strategy=most_frequent, preprocess__num__ii=KNNImputer();, score=-945800410.003 total time= 0.2s [CV 4/5] END model__learning_rate=0.1, model__max_depth=30, model__subsample=1.0, preprocess__cat__si__strategy=most_frequent, preprocess__num__ii=IterativeImputer();, score=-570100776.470 total time= 0.4s [CV 5/5] END model__learning_rate=0.1, model__max_depth=30, model__subsample=1.0, preprocess__cat__si__strategy=constant, preprocess__num__ii=KNNImputer();, score=-1239206018.769 total time= 0.2s [CV 2/5] END model__learning_rate=0.1, model__max_depth=30, model__subsample=0.5, preprocess__cat__si__strategy=most_frequent, preprocess__num__ii=SimpleImputer();, score=-997691407.890 total time= 0.1s [CV 5/5] END model__learning_rate=0.1, model__max_depth=30, model__subsample=0.5, preprocess__cat__si__strategy=most_frequent, preprocess__num__ii=SimpleImputer();, score=-1226710083.913 total time= 0.2s [CV 2/5] END model__learning_rate=0.1, model__max_depth=30, model__subsample=0.5, preprocess__cat__si__strategy=most_frequent, preprocess__num__ii=IterativeImputer();, score=-1024476538.628 total time= 0.4s [CV 3/5] END model__learning_rate=0.1, model__max_depth=30, model__subsample=0.5, preprocess__cat__si__strategy=constant, preprocess__num__ii=KNNImputer();, score=-541737588.393 total time= 0.2s [CV 3/5] END model__learning_rate=0.1, model__max_depth=-1, model__subsample=1.0, preprocess__cat__si__strategy=most_frequent, preprocess__num__ii=SimpleImputer();, score=-513492880.363 total time= 0.1s [CV 1/5] END model__learning_rate=0.1, model__max_depth=-1, model__subsample=1.0, preprocess__cat__si__strategy=most_frequent, preprocess__num__ii=KNNImputer();, score=-1085304295.641 total time= 0.2s [CV 2/5] END model__learning_rate=0.1, model__max_depth=-1, model__subsample=1.0, preprocess__cat__si__strategy=most_frequent, preprocess__num__ii=IterativeImputer();, score=-1024476538.628 total time= 0.4s [CV 2/5] END model__learning_rate=0.1, model__max_depth=-1, model__subsample=1.0, preprocess__cat__si__strategy=constant, preprocess__num__ii=KNNImputer();, score=-1033297772.352 total time= 0.2s [CV 5/5] END model__learning_rate=0.1, model__max_depth=-1, model__subsample=1.0, preprocess__cat__si__strategy=constant, preprocess__num__ii=IterativeImputer();, score=-1260432508.211 total time= 0.4s [CV 1/5] END model__learning_rate=0.1, model__max_depth=-1, model__subsample=0.5, preprocess__cat__si__strategy=most_frequent, preprocess__num__ii=IterativeImputer();, score=-1038744584.272 total time= 0.6s [CV 5/5] END model__learning_rate=0.1, model__max_depth=-1, model__subsample=0.5, preprocess__cat__si__strategy=constant, preprocess__num__ii=IterativeImputer();, score=-1260432508.211 total time= 0.4s [CV 2/5] END model__learning_rate=0.2, model__max_depth=30, model__subsample=1.0, preprocess__cat__si__strategy=most_frequent, preprocess__num__ii=IterativeImputer();, score=-1045009894.967 total time= 0.4s [CV 2/5] END model__learning_rate=0.2, model__max_depth=30, model__subsample=1.0, preprocess__cat__si__strategy=constant, preprocess__num__ii=KNNImputer();, score=-1064162610.680 total time= 0.2s [CV 4/5] END model__learning_rate=0.2, model__max_depth=30, model__subsample=1.0, preprocess__cat__si__strategy=constant, preprocess__num__ii=IterativeImputer();, score=-600837629.536 total time= 0.4s [CV 5/5] END model__learning_rate=0.2, model__max_depth=30, model__subsample=0.5, preprocess__cat__si__strategy=most_frequent, preprocess__num__ii=KNNImputer();, score=-1243376201.323 total time= 0.2s [CV 3/5] END model__learning_rate=0.2, model__max_depth=30, model__subsample=0.5, preprocess__cat__si__strategy=constant, preprocess__num__ii=SimpleImputer();, score=-539187722.319 total time= 0.2s [CV 1/5] END model__learning_rate=0.2, model__max_depth=30, model__subsample=0.5, preprocess__cat__si__strategy=constant, preprocess__num__ii=KNNImputer();, score=-1101298878.719 total time= 0.2s [CV 3/5] END model__learning_rate=0.2, model__max_depth=30, model__subsample=0.5, preprocess__cat__si__strategy=constant, preprocess__num__ii=IterativeImputer();, score=-533278666.873 total time= 0.4s [CV 4/5] END model__learning_rate=0.2, model__max_depth=-1, model__subsample=1.0, preprocess__cat__si__strategy=most_frequent, preprocess__num__ii=KNNImputer();, score=-544430870.856 total time= 0.2s [CV 2/5] END model__learning_rate=0.2, model__max_depth=-1, model__subsample=1.0, preprocess__cat__si__strategy=constant, preprocess__num__ii=SimpleImputer();, score=-1024576434.974 total time= 0.1s [CV 5/5] END model__learning_rate=0.2, model__max_depth=-1, model__subsample=1.0, preprocess__cat__si__strategy=constant, preprocess__num__ii=SimpleImputer();, score=-1271758627.677 total time= 0.2s [CV 1/5] END model__learning_rate=0.2, model__max_depth=-1, model__subsample=1.0, preprocess__cat__si__strategy=constant, preprocess__num__ii=IterativeImputer();, score=-1064924532.138 total time= 0.5s [CV 4/5] END model__learning_rate=0.2, model__max_depth=-1, model__subsample=0.5, preprocess__cat__si__strategy=most_frequent, preprocess__num__ii=IterativeImputer();, score=-643767514.223 total time= 0.4s [CV 4/5] END model__learning_rate=0.2, model__max_depth=-1, model__subsample=0.5, preprocess__cat__si__strategy=constant, preprocess__num__ii=KNNImputer();, score=-590633071.038 total time= 0.2s [CV 3/5] END model__learning_rate=0.1, model__max_depth=30, model__subsample=1.0, preprocess__cat__si__strategy=most_frequent, preprocess__num__ii=KNNImputer();, score=-515317030.231 total time= 0.2s [CV 1/5] END model__learning_rate=0.1, model__max_depth=30, model__subsample=1.0, preprocess__cat__si__strategy=constant, preprocess__num__ii=SimpleImputer();, score=-1102984449.306 total time= 0.1s [CV 4/5] END model__learning_rate=0.1, model__max_depth=30, model__subsample=1.0, preprocess__cat__si__strategy=constant, preprocess__num__ii=SimpleImputer();, score=-543635446.429 total time= 0.2s [CV 4/5] END model__learning_rate=0.1, model__max_depth=30, model__subsample=1.0, preprocess__cat__si__strategy=constant, preprocess__num__ii=KNNImputer();, score=-513174790.496 total time= 0.2s [CV 1/5] END model__learning_rate=0.1, model__max_depth=30, model__subsample=0.5, preprocess__cat__si__strategy=most_frequent, preprocess__num__ii=SimpleImputer();, score=-1094302605.510 total time= 0.2s [CV 4/5] END model__learning_rate=0.1, model__max_depth=30, model__subsample=0.5, preprocess__cat__si__strategy=most_frequent, preprocess__num__ii=SimpleImputer();, score=-530900512.729 total time= 0.1s [CV 4/5] END model__learning_rate=0.1, model__max_depth=30, model__subsample=0.5, preprocess__cat__si__strategy=most_frequent, preprocess__num__ii=KNNImputer();, score=-518644185.752 total time= 0.2s [CV 2/5] END model__learning_rate=0.1, model__max_depth=30, model__subsample=0.5, preprocess__cat__si__strategy=constant, preprocess__num__ii=SimpleImputer();, score=-995475493.332 total time= 0.2s [CV 5/5] END model__learning_rate=0.1, model__max_depth=30, model__subsample=0.5, preprocess__cat__si__strategy=constant, preprocess__num__ii=SimpleImputer();, score=-1261991041.562 total time= 0.2s [CV 1/5] END model__learning_rate=0.1, model__max_depth=30, model__subsample=0.5, preprocess__cat__si__strategy=constant, preprocess__num__ii=IterativeImputer();, score=-1047546222.332 total time= 0.6s [CV 5/5] END model__learning_rate=0.1, model__max_depth=-1, model__subsample=1.0, preprocess__cat__si__strategy=most_frequent, preprocess__num__ii=IterativeImputer();, score=-1219549323.929 total time= 0.4s [CV 3/5] END model__learning_rate=0.1, model__max_depth=-1, model__subsample=1.0, preprocess__cat__si__strategy=constant, preprocess__num__ii=IterativeImputer();, score=-532594533.605 total time= 0.4s [CV 3/5] END model__learning_rate=0.1, model__max_depth=-1, model__subsample=0.5, preprocess__cat__si__strategy=most_frequent, preprocess__num__ii=KNNImputer();, score=-515317030.231 total time= 0.2s [CV 1/5] END model__learning_rate=0.1, model__max_depth=-1, model__subsample=0.5, preprocess__cat__si__strategy=constant, preprocess__num__ii=SimpleImputer();, score=-1102984449.306 total time= 0.1s [CV 4/5] END model__learning_rate=0.1, model__max_depth=-1, model__subsample=0.5, preprocess__cat__si__strategy=constant, preprocess__num__ii=SimpleImputer();, score=-543635446.429 total time= 0.2s [CV 3/5] END model__learning_rate=0.1, model__max_depth=-1, model__subsample=0.5, preprocess__cat__si__strategy=constant, preprocess__num__ii=KNNImputer();, score=-541737588.393 total time= 0.2s [CV 1/5] END model__learning_rate=0.2, model__max_depth=30, model__subsample=1.0, preprocess__cat__si__strategy=most_frequent, preprocess__num__ii=SimpleImputer();, score=-1174830723.077 total time= 0.1s [CV 4/5] END model__learning_rate=0.2, model__max_depth=30, model__subsample=1.0, preprocess__cat__si__strategy=most_frequent, preprocess__num__ii=SimpleImputer();, score=-589395877.632 total time= 0.1s [CV 5/5] END model__learning_rate=0.2, model__max_depth=30, model__subsample=1.0, preprocess__cat__si__strategy=most_frequent, preprocess__num__ii=KNNImputer();, score=-1243376201.323 total time= 0.2s [CV 2/5] END model__learning_rate=0.2, model__max_depth=30, model__subsample=1.0, preprocess__cat__si__strategy=constant, preprocess__num__ii=SimpleImputer();, score=-1024576434.974 total time= 0.1s [CV 5/5] END model__learning_rate=0.2, model__max_depth=30, model__subsample=1.0, preprocess__cat__si__strategy=constant, preprocess__num__ii=SimpleImputer();, score=-1271758627.677 total time= 0.1s [CV 1/5] END model__learning_rate=0.2, model__max_depth=30, model__subsample=1.0, preprocess__cat__si__strategy=constant, preprocess__num__ii=IterativeImputer();, score=-1064924532.138 total time= 0.6s [CV 5/5] END model__learning_rate=0.2, model__max_depth=30, model__subsample=0.5, preprocess__cat__si__strategy=most_frequent, preprocess__num__ii=IterativeImputer();, score=-1249207388.654 total time= 0.3s [CV 1/5] END model__learning_rate=0.2, model__max_depth=30, model__subsample=0.5, preprocess__cat__si__strategy=constant, preprocess__num__ii=IterativeImputer();, score=-1064924532.138 total time= 0.6s [CV 4/5] END model__learning_rate=0.2, model__max_depth=-1, model__subsample=1.0, preprocess__cat__si__strategy=most_frequent, preprocess__num__ii=IterativeImputer();, score=-643767514.223 total time= 0.4s [CV 5/5] END model__learning_rate=0.2, model__max_depth=-1, model__subsample=1.0, preprocess__cat__si__strategy=constant, preprocess__num__ii=KNNImputer();, score=-1210584444.764 total time= 0.2s [CV 3/5] END model__learning_rate=0.2, model__max_depth=-1, model__subsample=0.5, preprocess__cat__si__strategy=most_frequent, preprocess__num__ii=SimpleImputer();, score=-524058509.993 total time= 0.1s [CV 1/5] END model__learning_rate=0.2, model__max_depth=-1, model__subsample=0.5, preprocess__cat__si__strategy=most_frequent, preprocess__num__ii=KNNImputer();, score=-1079192998.183 total time= 0.2s [CV 3/5] END model__learning_rate=0.2, model__max_depth=-1, model__subsample=0.5, preprocess__cat__si__strategy=most_frequent, preprocess__num__ii=IterativeImputer();, score=-576066791.159 total time= 0.4s [CV 3/5] END model__learning_rate=0.2, model__max_depth=-1, model__subsample=0.5, preprocess__cat__si__strategy=constant, preprocess__num__ii=KNNImputer();, score=-564186901.908 total time= 0.2s [CV 3/5] END model__learning_rate=0.1, model__max_depth=30, model__subsample=1.0, preprocess__cat__si__strategy=most_frequent, preprocess__num__ii=SimpleImputer();, score=-513492880.363 total time= 0.2s [CV 1/5] END model__learning_rate=0.1, model__max_depth=30, model__subsample=1.0, preprocess__cat__si__strategy=most_frequent, preprocess__num__ii=IterativeImputer();, score=-1038744584.272 total time= 0.7s [CV 3/5] END model__learning_rate=0.1, model__max_depth=30, model__subsample=0.5, preprocess__cat__si__strategy=most_frequent, preprocess__num__ii=SimpleImputer();, score=-513492880.363 total time= 0.2s [CV 1/5] END model__learning_rate=0.1, model__max_depth=30, model__subsample=0.5, preprocess__cat__si__strategy=most_frequent, preprocess__num__ii=KNNImputer();, score=-1085304295.641 total time= 0.2s [CV 5/5] END model__learning_rate=0.1, model__max_depth=30, model__subsample=0.5, preprocess__cat__si__strategy=most_frequent, preprocess__num__ii=IterativeImputer();, score=-1219549323.929 total time= 0.4s [CV 2/5] END model__learning_rate=0.1, model__max_depth=30, model__subsample=0.5, preprocess__cat__si__strategy=constant, preprocess__num__ii=IterativeImputer();, score=-1016845641.469 total time= 0.4s [CV 2/5] END model__learning_rate=0.1, model__max_depth=-1, model__subsample=1.0, preprocess__cat__si__strategy=most_frequent, preprocess__num__ii=KNNImputer();, score=-945800410.003 total time= 0.2s [CV 4/5] END model__learning_rate=0.1, model__max_depth=-1, model__subsample=1.0, preprocess__cat__si__strategy=most_frequent, preprocess__num__ii=IterativeImputer();, score=-570100776.470 total time= 0.4s [CV 2/5] END model__learning_rate=0.1, model__max_depth=-1, model__subsample=1.0, preprocess__cat__si__strategy=constant, preprocess__num__ii=IterativeImputer();, score=-1016845641.469 total time= 0.3s [CV 2/5] END model__learning_rate=0.1, model__max_depth=-1, model__subsample=0.5, preprocess__cat__si__strategy=most_frequent, preprocess__num__ii=KNNImputer();, score=-945800410.003 total time= 0.2s [CV 4/5] END model__learning_rate=0.1, model__max_depth=-1, model__subsample=0.5, preprocess__cat__si__strategy=most_frequent, preprocess__num__ii=IterativeImputer();, score=-570100776.470 total time= 0.4s [CV 5/5] END model__learning_rate=0.1, model__max_depth=-1, model__subsample=0.5, preprocess__cat__si__strategy=constant, preprocess__num__ii=KNNImputer();, score=-1239206018.769 total time= 0.2s [CV 3/5] END model__learning_rate=0.2, model__max_depth=30, model__subsample=1.0, preprocess__cat__si__strategy=most_frequent, preprocess__num__ii=SimpleImputer();, score=-524058509.993 total time= 0.1s [CV 1/5] END model__learning_rate=0.2, model__max_depth=30, model__subsample=1.0, preprocess__cat__si__strategy=most_frequent, preprocess__num__ii=KNNImputer();, score=-1079192998.183 total time= 0.2s [CV 3/5] END model__learning_rate=0.2, model__max_depth=30, model__subsample=1.0, preprocess__cat__si__strategy=most_frequent, preprocess__num__ii=IterativeImputer();, score=-576066791.159 total time= 0.4s [CV 4/5] END model__learning_rate=0.2, model__max_depth=30, model__subsample=1.0, preprocess__cat__si__strategy=constant, preprocess__num__ii=KNNImputer();, score=-590633071.038 total time= 0.2s [CV 2/5] END model__learning_rate=0.2, model__max_depth=30, model__subsample=0.5, preprocess__cat__si__strategy=most_frequent, preprocess__num__ii=SimpleImputer();, score=-1031476671.446 total time= 0.2s [CV 5/5] END model__learning_rate=0.2, model__max_depth=30, model__subsample=0.5, preprocess__cat__si__strategy=most_frequent, preprocess__num__ii=SimpleImputer();, score=-1267733161.543 total time= 0.1s [CV 1/5] END model__learning_rate=0.2, model__max_depth=30, model__subsample=0.5, preprocess__cat__si__strategy=most_frequent, preprocess__num__ii=IterativeImputer();, score=-1051746575.599 total time= 0.6s [CV 4/5] END model__learning_rate=0.2, model__max_depth=30, model__subsample=0.5, preprocess__cat__si__strategy=constant, preprocess__num__ii=IterativeImputer();, score=-600837629.536 total time= 0.4s [CV 1/5] END model__learning_rate=0.2, model__max_depth=-1, model__subsample=1.0, preprocess__cat__si__strategy=most_frequent, preprocess__num__ii=IterativeImputer();, score=-1051746575.599 total time= 0.6s [CV 3/5] END model__learning_rate=0.2, model__max_depth=-1, model__subsample=1.0, preprocess__cat__si__strategy=constant, preprocess__num__ii=IterativeImputer();, score=-533278666.873 total time= 0.4s [CV 3/5] END model__learning_rate=0.2, model__max_depth=-1, model__subsample=0.5, preprocess__cat__si__strategy=most_frequent, preprocess__num__ii=KNNImputer();, score=-540602961.526 total time= 0.2s [CV 2/5] END model__learning_rate=0.2, model__max_depth=-1, model__subsample=0.5, preprocess__cat__si__strategy=constant, preprocess__num__ii=SimpleImputer();, score=-1024576434.974 total time= 0.2s [CV 5/5] END model__learning_rate=0.2, model__max_depth=-1, model__subsample=0.5, preprocess__cat__si__strategy=constant, preprocess__num__ii=SimpleImputer();, score=-1271758627.677 total time= 0.1s [CV 5/5] END model__learning_rate=0.2, model__max_depth=-1, model__subsample=0.5, preprocess__cat__si__strategy=constant, preprocess__num__ii=KNNImputer();, score=-1210584444.764 total time= 0.2s [CV 1/5] END model__learning_rate=0.1, model__max_depth=30, model__subsample=1.0, preprocess__cat__si__strategy=most_frequent, preprocess__num__ii=KNNImputer();, score=-1085304295.641 total time= 0.2s [CV 5/5] END model__learning_rate=0.1, model__max_depth=30, model__subsample=1.0, preprocess__cat__si__strategy=most_frequent, preprocess__num__ii=IterativeImputer();, score=-1219549323.929 total time= 0.4s [CV 2/5] END model__learning_rate=0.1, model__max_depth=30, model__subsample=1.0, preprocess__cat__si__strategy=constant, preprocess__num__ii=IterativeImputer();, score=-1016845641.469 total time= 0.4s [CV 2/5] END model__learning_rate=0.1, model__max_depth=30, model__subsample=0.5, preprocess__cat__si__strategy=most_frequent, preprocess__num__ii=KNNImputer();, score=-945800410.003 total time= 0.2s [CV 1/5] END model__learning_rate=0.1, model__max_depth=30, model__subsample=0.5, preprocess__cat__si__strategy=constant, preprocess__num__ii=SimpleImputer();, score=-1102984449.306 total time= 0.1s [CV 4/5] END model__learning_rate=0.1, model__max_depth=30, model__subsample=0.5, preprocess__cat__si__strategy=constant, preprocess__num__ii=SimpleImputer();, score=-543635446.429 total time= 0.3s [CV 5/5] END model__learning_rate=0.1, model__max_depth=30, model__subsample=0.5, preprocess__cat__si__strategy=constant, preprocess__num__ii=IterativeImputer();, score=-1260432508.211 total time= 0.4s [CV 3/5] END model__learning_rate=0.1, model__max_depth=-1, model__subsample=1.0, preprocess__cat__si__strategy=most_frequent, preprocess__num__ii=IterativeImputer();, score=-519293006.931 total time= 0.4s [CV 4/5] END model__learning_rate=0.1, model__max_depth=-1, model__subsample=1.0, preprocess__cat__si__strategy=constant, preprocess__num__ii=KNNImputer();, score=-513174790.496 total time= 0.2s [CV 2/5] END model__learning_rate=0.1, model__max_depth=-1, model__subsample=0.5, preprocess__cat__si__strategy=most_frequent, preprocess__num__ii=SimpleImputer();, score=-997691407.890 total time= 0.1s [CV 5/5] END model__learning_rate=0.1, model__max_depth=-1, model__subsample=0.5, preprocess__cat__si__strategy=most_frequent, preprocess__num__ii=SimpleImputer();, score=-1226710083.913 total time= 0.2s [CV 5/5] END model__learning_rate=0.1, model__max_depth=-1, model__subsample=0.5, preprocess__cat__si__strategy=most_frequent, preprocess__num__ii=KNNImputer();, score=-1224677483.241 total time= 0.2s [CV 3/5] END model__learning_rate=0.1, model__max_depth=-1, model__subsample=0.5, preprocess__cat__si__strategy=constant, preprocess__num__ii=SimpleImputer();, score=-529011858.629 total time= 0.1s [CV 1/5] END model__learning_rate=0.1, model__max_depth=-1, model__subsample=0.5, preprocess__cat__si__strategy=constant, preprocess__num__ii=KNNImputer();, score=-1060914076.633 total time= 0.2s [CV 3/5] END model__learning_rate=0.1, model__max_depth=-1, model__subsample=0.5, preprocess__cat__si__strategy=constant, preprocess__num__ii=IterativeImputer();, score=-532594533.605 total time= 0.4s [CV 3/5] END model__learning_rate=0.2, model__max_depth=30, model__subsample=1.0, preprocess__cat__si__strategy=most_frequent, preprocess__num__ii=KNNImputer();, score=-540602961.526 total time= 0.2s [CV 1/5] END model__learning_rate=0.2, model__max_depth=30, model__subsample=1.0, preprocess__cat__si__strategy=constant, preprocess__num__ii=SimpleImputer();, score=-1088255251.747 total time= 0.1s [CV 4/5] END model__learning_rate=0.2, model__max_depth=30, model__subsample=1.0, preprocess__cat__si__strategy=constant, preprocess__num__ii=SimpleImputer();, score=-599626460.012 total time= 0.1s [CV 5/5] END model__learning_rate=0.2, model__max_depth=30, model__subsample=1.0, preprocess__cat__si__strategy=constant, preprocess__num__ii=KNNImputer();, score=-1210584444.764 total time= 0.2s [CV 3/5] END model__learning_rate=0.2, model__max_depth=30, model__subsample=0.5, preprocess__cat__si__strategy=most_frequent, preprocess__num__ii=SimpleImputer();, score=-524058509.993 total time= 0.1s [CV 1/5] END model__learning_rate=0.2, model__max_depth=30, model__subsample=0.5, preprocess__cat__si__strategy=most_frequent, preprocess__num__ii=KNNImputer();, score=-1079192998.183 total time= 0.2s [CV 3/5] END model__learning_rate=0.2, model__max_depth=30, model__subsample=0.5, preprocess__cat__si__strategy=most_frequent, preprocess__num__ii=IterativeImputer();, score=-576066791.159 total time= 0.4s [CV 4/5] END model__learning_rate=0.2, model__max_depth=30, model__subsample=0.5, preprocess__cat__si__strategy=constant, preprocess__num__ii=KNNImputer();, score=-590633071.038 total time= 0.2s [CV 2/5] END model__learning_rate=0.2, model__max_depth=-1, model__subsample=1.0, preprocess__cat__si__strategy=most_frequent, preprocess__num__ii=SimpleImputer();, score=-1031476671.446 total time= 0.2s [CV 5/5] END model__learning_rate=0.2, model__max_depth=-1, model__subsample=1.0, preprocess__cat__si__strategy=most_frequent, preprocess__num__ii=SimpleImputer();, score=-1267733161.543 total time= 0.1s [CV 5/5] END model__learning_rate=0.2, model__max_depth=-1, model__subsample=1.0, preprocess__cat__si__strategy=most_frequent, preprocess__num__ii=KNNImputer();, score=-1243376201.323 total time= 0.2s [CV 3/5] END model__learning_rate=0.2, model__max_depth=-1, model__subsample=1.0, preprocess__cat__si__strategy=constant, preprocess__num__ii=SimpleImputer();, score=-539187722.319 total time= 0.2s [CV 1/5] END model__learning_rate=0.2, model__max_depth=-1, model__subsample=1.0, preprocess__cat__si__strategy=constant, preprocess__num__ii=KNNImputer();, score=-1101298878.719 total time= 0.2s [CV 4/5] END model__learning_rate=0.2, model__max_depth=-1, model__subsample=1.0, preprocess__cat__si__strategy=constant, preprocess__num__ii=IterativeImputer();, score=-600837629.536 total time= 0.4s [CV 5/5] END model__learning_rate=0.2, model__max_depth=-1, model__subsample=0.5, preprocess__cat__si__strategy=most_frequent, preprocess__num__ii=KNNImputer();, score=-1243376201.323 total time= 0.2s [CV 1/5] END model__learning_rate=0.2, model__max_depth=-1, model__subsample=0.5, preprocess__cat__si__strategy=constant, preprocess__num__ii=SimpleImputer();, score=-1088255251.747 total time= 0.1s [CV 4/5] END model__learning_rate=0.2, model__max_depth=-1, model__subsample=0.5, preprocess__cat__si__strategy=constant, preprocess__num__ii=SimpleImputer();, score=-599626460.012 total time= 0.2s [CV 2/5] END model__learning_rate=0.2, model__max_depth=-1, model__subsample=0.5, preprocess__cat__si__strategy=constant, preprocess__num__ii=IterativeImputer();, score=-1034331933.536 total time= 0.3s [CV 4/5] END model__learning_rate=0.1, model__max_depth=30, model__subsample=1.0, preprocess__cat__si__strategy=most_frequent, preprocess__num__ii=SimpleImputer();, score=-530900512.729 total time= 0.1s [CV 3/5] END model__learning_rate=0.1, model__max_depth=30, model__subsample=1.0, preprocess__cat__si__strategy=most_frequent, preprocess__num__ii=IterativeImputer();, score=-519293006.931 total time= 0.3s [CV 2/5] END model__learning_rate=0.1, model__max_depth=30, model__subsample=1.0, preprocess__cat__si__strategy=constant, preprocess__num__ii=KNNImputer();, score=-1033297772.352 total time= 0.2s [CV 4/5] END model__learning_rate=0.1, model__max_depth=30, model__subsample=1.0, preprocess__cat__si__strategy=constant, preprocess__num__ii=IterativeImputer();, score=-551768174.727 total time= 0.4s [CV 5/5] END model__learning_rate=0.1, model__max_depth=30, model__subsample=0.5, preprocess__cat__si__strategy=most_frequent, preprocess__num__ii=KNNImputer();, score=-1224677483.241 total time= 0.2s [CV 3/5] END model__learning_rate=0.1, model__max_depth=30, model__subsample=0.5, preprocess__cat__si__strategy=constant, preprocess__num__ii=SimpleImputer();, score=-529011858.629 total time= 0.2s [CV 1/5] END model__learning_rate=0.1, model__max_depth=30, model__subsample=0.5, preprocess__cat__si__strategy=constant, preprocess__num__ii=KNNImputer();, score=-1060914076.633 total time= 0.2s [CV 4/5] END model__learning_rate=0.1, model__max_depth=30, model__subsample=0.5, preprocess__cat__si__strategy=constant, preprocess__num__ii=IterativeImputer();, score=-551768174.727 total time= 0.3s [CV 4/5] END model__learning_rate=0.1, model__max_depth=-1, model__subsample=1.0, preprocess__cat__si__strategy=most_frequent, preprocess__num__ii=KNNImputer();, score=-518644185.752 total time= 0.2s [CV 2/5] END model__learning_rate=0.1, model__max_depth=-1, model__subsample=1.0, preprocess__cat__si__strategy=constant, preprocess__num__ii=SimpleImputer();, score=-995475493.332 total time= 0.1s [CV 5/5] END model__learning_rate=0.1, model__max_depth=-1, model__subsample=1.0, preprocess__cat__si__strategy=constant, preprocess__num__ii=SimpleImputer();, score=-1261991041.562 total time= 0.1s [CV 5/5] END model__learning_rate=0.1, model__max_depth=-1, model__subsample=1.0, preprocess__cat__si__strategy=constant, preprocess__num__ii=KNNImputer();, score=-1239206018.769 total time= 0.2s [CV 3/5] END model__learning_rate=0.1, model__max_depth=-1, model__subsample=0.5, preprocess__cat__si__strategy=most_frequent, preprocess__num__ii=SimpleImputer();, score=-513492880.363 total time= 0.2s [CV 1/5] END model__learning_rate=0.1, model__max_depth=-1, model__subsample=0.5, preprocess__cat__si__strategy=most_frequent, preprocess__num__ii=KNNImputer();, score=-1085304295.641 total time= 0.2s [CV 3/5] END model__learning_rate=0.1, model__max_depth=-1, model__subsample=0.5, preprocess__cat__si__strategy=most_frequent, preprocess__num__ii=IterativeImputer();, score=-519293006.931 total time= 0.4s [CV 1/5] END model__learning_rate=0.1, model__max_depth=-1, model__subsample=0.5, preprocess__cat__si__strategy=constant, preprocess__num__ii=IterativeImputer();, score=-1047546222.332 total time= 0.6s [CV 4/5] END model__learning_rate=0.2, model__max_depth=30, model__subsample=1.0, preprocess__cat__si__strategy=most_frequent, preprocess__num__ii=IterativeImputer();, score=-643767514.223 total time= 0.3s [CV 3/5] END model__learning_rate=0.2, model__max_depth=30, model__subsample=1.0, preprocess__cat__si__strategy=constant, preprocess__num__ii=KNNImputer();, score=-564186901.908 total time= 0.2s [CV 1/5] END model__learning_rate=0.2, model__max_depth=30, model__subsample=0.5, preprocess__cat__si__strategy=most_frequent, preprocess__num__ii=SimpleImputer();, score=-1174830723.077 total time= 0.1s [CV 4/5] END model__learning_rate=0.2, model__max_depth=30, model__subsample=0.5, preprocess__cat__si__strategy=most_frequent, preprocess__num__ii=SimpleImputer();, score=-589395877.632 total time= 0.1s [CV 4/5] END model__learning_rate=0.2, model__max_depth=30, model__subsample=0.5, preprocess__cat__si__strategy=most_frequent, preprocess__num__ii=KNNImputer();, score=-544430870.856 total time= 0.2s [CV 4/5] END model__learning_rate=0.2, model__max_depth=30, model__subsample=0.5, preprocess__cat__si__strategy=most_frequent, preprocess__num__ii=IterativeImputer();, score=-643767514.223 total time= 0.4s [CV 2/5] END model__learning_rate=0.2, model__max_depth=30, model__subsample=0.5, preprocess__cat__si__strategy=constant, preprocess__num__ii=IterativeImputer();, score=-1034331933.536 total time= 0.4s [CV 3/5] END model__learning_rate=0.2, model__max_depth=-1, model__subsample=1.0, preprocess__cat__si__strategy=most_frequent, preprocess__num__ii=KNNImputer();, score=-540602961.526 total time= 0.2s [CV 1/5] END model__learning_rate=0.2, model__max_depth=-1, model__subsample=1.0, preprocess__cat__si__strategy=constant, preprocess__num__ii=SimpleImputer();, score=-1088255251.747 total time= 0.1s [CV 4/5] END model__learning_rate=0.2, model__max_depth=-1, model__subsample=1.0, preprocess__cat__si__strategy=constant, preprocess__num__ii=SimpleImputer();, score=-599626460.012 total time= 0.2s [CV 4/5] END model__learning_rate=0.2, model__max_depth=-1, model__subsample=1.0, preprocess__cat__si__strategy=constant, preprocess__num__ii=KNNImputer();, score=-590633071.038 total time= 0.2s [CV 2/5] END model__learning_rate=0.2, model__max_depth=-1, model__subsample=0.5, preprocess__cat__si__strategy=most_frequent, preprocess__num__ii=SimpleImputer();, score=-1031476671.446 total time= 0.2s [CV 5/5] END model__learning_rate=0.2, model__max_depth=-1, model__subsample=0.5, preprocess__cat__si__strategy=most_frequent, preprocess__num__ii=SimpleImputer();, score=-1267733161.543 total time= 0.1s [CV 1/5] END model__learning_rate=0.2, model__max_depth=-1, model__subsample=0.5, preprocess__cat__si__strategy=most_frequent, preprocess__num__ii=IterativeImputer();, score=-1051746575.599 total time= 0.5s [CV 3/5] END model__learning_rate=0.2, model__max_depth=-1, model__subsample=0.5, preprocess__cat__si__strategy=constant, preprocess__num__ii=IterativeImputer();, score=-533278666.873 total time= 0.3s [CV 2/5] END model__learning_rate=0.1, model__max_depth=30, model__subsample=1.0, preprocess__cat__si__strategy=most_frequent, preprocess__num__ii=SimpleImputer();, score=-997691407.890 total time= 0.2s [CV 5/5] END model__learning_rate=0.1, model__max_depth=30, model__subsample=1.0, preprocess__cat__si__strategy=most_frequent, preprocess__num__ii=KNNImputer();, score=-1224677483.241 total time= 0.2s [CV 3/5] END model__learning_rate=0.1, model__max_depth=30, model__subsample=1.0, preprocess__cat__si__strategy=constant, preprocess__num__ii=SimpleImputer();, score=-529011858.629 total time= 0.1s [CV 1/5] END model__learning_rate=0.1, model__max_depth=30, model__subsample=1.0, preprocess__cat__si__strategy=constant, preprocess__num__ii=KNNImputer();, score=-1060914076.633 total time= 0.2s [CV 3/5] END model__learning_rate=0.1, model__max_depth=30, model__subsample=1.0, preprocess__cat__si__strategy=constant, preprocess__num__ii=IterativeImputer();, score=-532594533.605 total time= 0.3s [CV 3/5] END model__learning_rate=0.1, model__max_depth=30, model__subsample=0.5, preprocess__cat__si__strategy=most_frequent, preprocess__num__ii=KNNImputer();, score=-515317030.231 total time= 0.1s [CV 3/5] END model__learning_rate=0.1, model__max_depth=30, model__subsample=0.5, preprocess__cat__si__strategy=most_frequent, preprocess__num__ii=IterativeImputer();, score=-519293006.931 total time= 0.4s [CV 2/5] END model__learning_rate=0.1, model__max_depth=30, model__subsample=0.5, preprocess__cat__si__strategy=constant, preprocess__num__ii=KNNImputer();, score=-1033297772.352 total time= 0.2s [CV 3/5] END model__learning_rate=0.1, model__max_depth=30, model__subsample=0.5, preprocess__cat__si__strategy=constant, preprocess__num__ii=IterativeImputer();, score=-532594533.605 total time= 0.4s [CV 1/5] END model__learning_rate=0.1, model__max_depth=-1, model__subsample=1.0, preprocess__cat__si__strategy=most_frequent, preprocess__num__ii=IterativeImputer();, score=-1038744584.272 total time= 0.6s [CV 4/5] END model__learning_rate=0.1, model__max_depth=-1, model__subsample=1.0, preprocess__cat__si__strategy=constant, preprocess__num__ii=IterativeImputer();, score=-551768174.727 total time= 0.4s [CV 2/5] END model__learning_rate=0.1, model__max_depth=-1, model__subsample=0.5, preprocess__cat__si__strategy=most_frequent, preprocess__num__ii=IterativeImputer();, score=-1024476538.628 total time= 0.4s [CV 2/5] END model__learning_rate=0.1, model__max_depth=-1, model__subsample=0.5, preprocess__cat__si__strategy=constant, preprocess__num__ii=KNNImputer();, score=-1033297772.352 total time= 0.2s [CV 4/5] END model__learning_rate=0.1, model__max_depth=-1, model__subsample=0.5, preprocess__cat__si__strategy=constant, preprocess__num__ii=IterativeImputer();, score=-551768174.727 total time= 0.4s [CV 4/5] END model__learning_rate=0.2, model__max_depth=30, model__subsample=1.0, preprocess__cat__si__strategy=most_frequent, preprocess__num__ii=KNNImputer();, score=-544430870.856 total time= 0.2s [CV 3/5] END model__learning_rate=0.2, model__max_depth=30, model__subsample=1.0, preprocess__cat__si__strategy=constant, preprocess__num__ii=SimpleImputer();, score=-539187722.319 total time= 0.1s [CV 1/5] END model__learning_rate=0.2, model__max_depth=30, model__subsample=1.0, preprocess__cat__si__strategy=constant, preprocess__num__ii=KNNImputer();, score=-1101298878.719 total time= 0.2s [CV 3/5] END model__learning_rate=0.2, model__max_depth=30, model__subsample=1.0, preprocess__cat__si__strategy=constant, preprocess__num__ii=IterativeImputer();, score=-533278666.873 total time= 0.4s [CV 3/5] END model__learning_rate=0.2, model__max_depth=30, model__subsample=0.5, preprocess__cat__si__strategy=most_frequent, preprocess__num__ii=KNNImputer();, score=-540602961.526 total time= 0.2s [CV 1/5] END model__learning_rate=0.2, model__max_depth=30, model__subsample=0.5, preprocess__cat__si__strategy=constant, preprocess__num__ii=SimpleImputer();, score=-1088255251.747 total time= 0.1s [CV 4/5] END model__learning_rate=0.2, model__max_depth=30, model__subsample=0.5, preprocess__cat__si__strategy=constant, preprocess__num__ii=SimpleImputer();, score=-599626460.012 total time= 0.1s [CV 2/5] END model__learning_rate=0.2, model__max_depth=30, model__subsample=0.5, preprocess__cat__si__strategy=constant, preprocess__num__ii=KNNImputer();, score=-1064162610.680 total time= 0.2s [CV 5/5] END model__learning_rate=0.2, model__max_depth=30, model__subsample=0.5, preprocess__cat__si__strategy=constant, preprocess__num__ii=IterativeImputer();, score=-1226474594.655 total time= 0.4s [CV 2/5] END model__learning_rate=0.2, model__max_depth=-1, model__subsample=1.0, preprocess__cat__si__strategy=most_frequent, preprocess__num__ii=IterativeImputer();, score=-1045009894.967 total time= 0.4s [CV 2/5] END model__learning_rate=0.2, model__max_depth=-1, model__subsample=1.0, preprocess__cat__si__strategy=constant, preprocess__num__ii=KNNImputer();, score=-1064162610.680 total time= 0.2s [CV 5/5] END model__learning_rate=0.2, model__max_depth=-1, model__subsample=1.0, preprocess__cat__si__strategy=constant, preprocess__num__ii=IterativeImputer();, score=-1226474594.655 total time= 0.4s [CV 2/5] END model__learning_rate=0.2, model__max_depth=-1, model__subsample=0.5, preprocess__cat__si__strategy=most_frequent, preprocess__num__ii=IterativeImputer();, score=-1045009894.967 total time= 0.4s [CV 1/5] END model__learning_rate=0.2, model__max_depth=-1, model__subsample=0.5, preprocess__cat__si__strategy=constant, preprocess__num__ii=KNNImputer();, score=-1101298878.719 total time= 0.2s [CV 4/5] END model__learning_rate=0.2, model__max_depth=-1, model__subsample=0.5, preprocess__cat__si__strategy=constant, preprocess__num__ii=IterativeImputer();, score=-600837629.536 total time= 0.3s [CV 1/5] END model__learning_rate=0.1, model__max_depth=30, model__subsample=1.0, preprocess__cat__si__strategy=most_frequent, preprocess__num__ii=SimpleImputer();, score=-1094302605.510 total time= 0.2s [CV 4/5] END model__learning_rate=0.1, model__max_depth=30, model__subsample=1.0, preprocess__cat__si__strategy=most_frequent, preprocess__num__ii=KNNImputer();, score=-518644185.752 total time= 0.2s [CV 2/5] END model__learning_rate=0.1, model__max_depth=30, model__subsample=1.0, preprocess__cat__si__strategy=constant, preprocess__num__ii=SimpleImputer();, score=-995475493.332 total time= 0.1s [CV 5/5] END model__learning_rate=0.1, model__max_depth=30, model__subsample=1.0, preprocess__cat__si__strategy=constant, preprocess__num__ii=SimpleImputer();, score=-1261991041.562 total time= 0.1s [CV 1/5] END model__learning_rate=0.1, model__max_depth=30, model__subsample=1.0, preprocess__cat__si__strategy=constant, preprocess__num__ii=IterativeImputer();, score=-1047546222.332 total time= 0.6s [CV 4/5] END model__learning_rate=0.1, model__max_depth=30, model__subsample=0.5, preprocess__cat__si__strategy=most_frequent, preprocess__num__ii=IterativeImputer();, score=-570100776.470 total time= 0.3s [CV 4/5] END model__learning_rate=0.1, model__max_depth=30, model__subsample=0.5, preprocess__cat__si__strategy=constant, preprocess__num__ii=KNNImputer();, score=-513174790.496 total time= 0.2s [CV 2/5] END model__learning_rate=0.1, model__max_depth=-1, model__subsample=1.0, preprocess__cat__si__strategy=most_frequent, preprocess__num__ii=SimpleImputer();, score=-997691407.890 total time= 0.1s [CV 5/5] END model__learning_rate=0.1, model__max_depth=-1, model__subsample=1.0, preprocess__cat__si__strategy=most_frequent, preprocess__num__ii=SimpleImputer();, score=-1226710083.913 total time= 0.1s [CV 5/5] END model__learning_rate=0.1, model__max_depth=-1, model__subsample=1.0, preprocess__cat__si__strategy=most_frequent, preprocess__num__ii=KNNImputer();, score=-1224677483.241 total time= 0.2s [CV 3/5] END model__learning_rate=0.1, model__max_depth=-1, model__subsample=1.0, preprocess__cat__si__strategy=constant, preprocess__num__ii=SimpleImputer();, score=-529011858.629 total time= 0.1s [CV 1/5] END model__learning_rate=0.1, model__max_depth=-1, model__subsample=1.0, preprocess__cat__si__strategy=constant, preprocess__num__ii=KNNImputer();, score=-1060914076.633 total time= 0.2s [CV 1/5] END model__learning_rate=0.1, model__max_depth=-1, model__subsample=1.0, preprocess__cat__si__strategy=constant, preprocess__num__ii=IterativeImputer();, score=-1047546222.332 total time= 0.6s [CV 5/5] END model__learning_rate=0.1, model__max_depth=-1, model__subsample=0.5, preprocess__cat__si__strategy=most_frequent, preprocess__num__ii=IterativeImputer();, score=-1219549323.929 total time= 0.4s [CV 2/5] END model__learning_rate=0.1, model__max_depth=-1, model__subsample=0.5, preprocess__cat__si__strategy=constant, preprocess__num__ii=IterativeImputer();, score=-1016845641.469 total time= 0.4s [CV 2/5] END model__learning_rate=0.2, model__max_depth=30, model__subsample=1.0, preprocess__cat__si__strategy=most_frequent, preprocess__num__ii=KNNImputer();, score=-1017028960.525 total time= 0.2s [CV 5/5] END model__learning_rate=0.2, model__max_depth=30, model__subsample=1.0, preprocess__cat__si__strategy=most_frequent, preprocess__num__ii=IterativeImputer();, score=-1249207388.654 total time= 0.4s [CV 2/5] END model__learning_rate=0.2, model__max_depth=30, model__subsample=1.0, preprocess__cat__si__strategy=constant, preprocess__num__ii=IterativeImputer();, score=-1034331933.536 total time= 0.4s [CV 2/5] END model__learning_rate=0.2, model__max_depth=30, model__subsample=0.5, preprocess__cat__si__strategy=most_frequent, preprocess__num__ii=KNNImputer();, score=-1017028960.525 total time= 0.2s [CV 2/5] END model__learning_rate=0.2, model__max_depth=30, model__subsample=0.5, preprocess__cat__si__strategy=constant, preprocess__num__ii=SimpleImputer();, score=-1024576434.974 total time= 0.1s [CV 5/5] END model__learning_rate=0.2, model__max_depth=30, model__subsample=0.5, preprocess__cat__si__strategy=constant, preprocess__num__ii=SimpleImputer();, score=-1271758627.677 total time= 0.2s [CV 5/5] END model__learning_rate=0.2, model__max_depth=30, model__subsample=0.5, preprocess__cat__si__strategy=constant, preprocess__num__ii=KNNImputer();, score=-1210584444.764 total time= 0.2s [CV 3/5] END model__learning_rate=0.2, model__max_depth=-1, model__subsample=1.0, preprocess__cat__si__strategy=most_frequent, preprocess__num__ii=SimpleImputer();, score=-524058509.993 total time= 0.2s [CV 1/5] END model__learning_rate=0.2, model__max_depth=-1, model__subsample=1.0, preprocess__cat__si__strategy=most_frequent, preprocess__num__ii=KNNImputer();, score=-1079192998.183 total time= 0.2s [CV 3/5] END model__learning_rate=0.2, model__max_depth=-1, model__subsample=1.0, preprocess__cat__si__strategy=most_frequent, preprocess__num__ii=IterativeImputer();, score=-576066791.159 total time= 0.4s [CV 3/5] END model__learning_rate=0.2, model__max_depth=-1, model__subsample=1.0, preprocess__cat__si__strategy=constant, preprocess__num__ii=KNNImputer();, score=-564186901.908 total time= 0.2s [CV 1/5] END model__learning_rate=0.2, model__max_depth=-1, model__subsample=0.5, preprocess__cat__si__strategy=most_frequent, preprocess__num__ii=SimpleImputer();, score=-1174830723.077 total time= 0.1s [CV 4/5] END model__learning_rate=0.2, model__max_depth=-1, model__subsample=0.5, preprocess__cat__si__strategy=most_frequent, preprocess__num__ii=SimpleImputer();, score=-589395877.632 total time= 0.1s [CV 4/5] END model__learning_rate=0.2, model__max_depth=-1, model__subsample=0.5, preprocess__cat__si__strategy=most_frequent, preprocess__num__ii=KNNImputer();, score=-544430870.856 total time= 0.3s [CV 3/5] END model__learning_rate=0.2, model__max_depth=-1, model__subsample=0.5, preprocess__cat__si__strategy=constant, preprocess__num__ii=SimpleImputer();, score=-539187722.319 total time= 0.2s [CV 2/5] END model__learning_rate=0.2, model__max_depth=-1, model__subsample=0.5, preprocess__cat__si__strategy=constant, preprocess__num__ii=KNNImputer();, score=-1064162610.680 total time= 0.2s [CV 5/5] END model__learning_rate=0.2, model__max_depth=-1, model__subsample=0.5, preprocess__cat__si__strategy=constant, preprocess__num__ii=IterativeImputer();, score=-1226474594.655 total time= 0.3s [CV 5/5] END model__learning_rate=0.1, model__max_depth=30, model__subsample=1.0, preprocess__cat__si__strategy=most_frequent, preprocess__num__ii=SimpleImputer();, score=-1226710083.913 total time= 0.2s [CV 2/5] END model__learning_rate=0.1, model__max_depth=30, model__subsample=1.0, preprocess__cat__si__strategy=most_frequent, preprocess__num__ii=IterativeImputer();, score=-1024476538.628 total time= 0.4s [CV 3/5] END model__learning_rate=0.1, model__max_depth=30, model__subsample=1.0, preprocess__cat__si__strategy=constant, preprocess__num__ii=KNNImputer();, score=-541737588.393 total time= 0.2s [CV 5/5] END model__learning_rate=0.1, model__max_depth=30, model__subsample=1.0, preprocess__cat__si__strategy=constant, preprocess__num__ii=IterativeImputer();, score=-1260432508.211 total time= 0.4s [CV 1/5] END model__learning_rate=0.1, model__max_depth=30, model__subsample=0.5, preprocess__cat__si__strategy=most_frequent, preprocess__num__ii=IterativeImputer();, score=-1038744584.272 total time= 0.4s [CV 5/5] END model__learning_rate=0.1, model__max_depth=30, model__subsample=0.5, preprocess__cat__si__strategy=constant, preprocess__num__ii=KNNImputer();, score=-1239206018.769 total time= 0.1s [CV 1/5] END model__learning_rate=0.1, model__max_depth=-1, model__subsample=1.0, preprocess__cat__si__strategy=most_frequent, preprocess__num__ii=SimpleImputer();, score=-1094302605.510 total time= 0.1s [CV 4/5] END model__learning_rate=0.1, model__max_depth=-1, model__subsample=1.0, preprocess__cat__si__strategy=most_frequent, preprocess__num__ii=SimpleImputer();, score=-530900512.729 total time= 0.2s [CV 3/5] END model__learning_rate=0.1, model__max_depth=-1, model__subsample=1.0, preprocess__cat__si__strategy=most_frequent, preprocess__num__ii=KNNImputer();, score=-515317030.231 total time= 0.2s [CV 1/5] END model__learning_rate=0.1, model__max_depth=-1, model__subsample=1.0, preprocess__cat__si__strategy=constant, preprocess__num__ii=SimpleImputer();, score=-1102984449.306 total time= 0.1s [CV 4/5] END model__learning_rate=0.1, model__max_depth=-1, model__subsample=1.0, preprocess__cat__si__strategy=constant, preprocess__num__ii=SimpleImputer();, score=-543635446.429 total time= 0.1s [CV 3/5] END model__learning_rate=0.1, model__max_depth=-1, model__subsample=1.0, preprocess__cat__si__strategy=constant, preprocess__num__ii=KNNImputer();, score=-541737588.393 total time= 0.2s [CV 1/5] END model__learning_rate=0.1, model__max_depth=-1, model__subsample=0.5, preprocess__cat__si__strategy=most_frequent, preprocess__num__ii=SimpleImputer();, score=-1094302605.510 total time= 0.1s [CV 4/5] END model__learning_rate=0.1, model__max_depth=-1, model__subsample=0.5, preprocess__cat__si__strategy=most_frequent, preprocess__num__ii=SimpleImputer();, score=-530900512.729 total time= 0.2s [CV 4/5] END model__learning_rate=0.1, model__max_depth=-1, model__subsample=0.5, preprocess__cat__si__strategy=most_frequent, preprocess__num__ii=KNNImputer();, score=-518644185.752 total time= 0.2s [CV 2/5] END model__learning_rate=0.1, model__max_depth=-1, model__subsample=0.5, preprocess__cat__si__strategy=constant, preprocess__num__ii=SimpleImputer();, score=-995475493.332 total time= 0.1s [CV 5/5] END model__learning_rate=0.1, model__max_depth=-1, model__subsample=0.5, preprocess__cat__si__strategy=constant, preprocess__num__ii=SimpleImputer();, score=-1261991041.562 total time= 0.1s [CV 4/5] END model__learning_rate=0.1, model__max_depth=-1, model__subsample=0.5, preprocess__cat__si__strategy=constant, preprocess__num__ii=KNNImputer();, score=-513174790.496 total time= 0.2s [CV 2/5] END model__learning_rate=0.2, model__max_depth=30, model__subsample=1.0, preprocess__cat__si__strategy=most_frequent, preprocess__num__ii=SimpleImputer();, score=-1031476671.446 total time= 0.1s [CV 5/5] END model__learning_rate=0.2, model__max_depth=30, model__subsample=1.0, preprocess__cat__si__strategy=most_frequent, preprocess__num__ii=SimpleImputer();, score=-1267733161.543 total time= 0.1s [CV 1/5] END model__learning_rate=0.2, model__max_depth=30, model__subsample=1.0, preprocess__cat__si__strategy=most_frequent, preprocess__num__ii=IterativeImputer();, score=-1051746575.599 total time= 0.6s [CV 5/5] END model__learning_rate=0.2, model__max_depth=30, model__subsample=1.0, preprocess__cat__si__strategy=constant, preprocess__num__ii=IterativeImputer();, score=-1226474594.655 total time= 0.4s [CV 2/5] END model__learning_rate=0.2, model__max_depth=30, model__subsample=0.5, preprocess__cat__si__strategy=most_frequent, preprocess__num__ii=IterativeImputer();, score=-1045009894.967 total time= 0.4s [CV 3/5] END model__learning_rate=0.2, model__max_depth=30, model__subsample=0.5, preprocess__cat__si__strategy=constant, preprocess__num__ii=KNNImputer();, score=-564186901.908 total time= 0.2s [CV 1/5] END model__learning_rate=0.2, model__max_depth=-1, model__subsample=1.0, preprocess__cat__si__strategy=most_frequent, preprocess__num__ii=SimpleImputer();, score=-1174830723.077 total time= 0.1s [CV 4/5] END model__learning_rate=0.2, model__max_depth=-1, model__subsample=1.0, preprocess__cat__si__strategy=most_frequent, preprocess__num__ii=SimpleImputer();, score=-589395877.632 total time= 0.2s [CV 2/5] END model__learning_rate=0.2, model__max_depth=-1, model__subsample=1.0, preprocess__cat__si__strategy=most_frequent, preprocess__num__ii=KNNImputer();, score=-1017028960.525 total time= 0.2s [CV 5/5] END model__learning_rate=0.2, model__max_depth=-1, model__subsample=1.0, preprocess__cat__si__strategy=most_frequent, preprocess__num__ii=IterativeImputer();, score=-1249207388.654 total time= 0.4s [CV 2/5] END model__learning_rate=0.2, model__max_depth=-1, model__subsample=1.0, preprocess__cat__si__strategy=constant, preprocess__num__ii=IterativeImputer();, score=-1034331933.536 total time= 0.4s [CV 2/5] END model__learning_rate=0.2, model__max_depth=-1, model__subsample=0.5, preprocess__cat__si__strategy=most_frequent, preprocess__num__ii=KNNImputer();, score=-1017028960.525 total time= 0.2s [CV 5/5] END model__learning_rate=0.2, model__max_depth=-1, model__subsample=0.5, preprocess__cat__si__strategy=most_frequent, preprocess__num__ii=IterativeImputer();, score=-1249207388.654 total time= 0.4s [CV 1/5] END model__learning_rate=0.2, model__max_depth=-1, model__subsample=0.5, preprocess__cat__si__strategy=constant, preprocess__num__ii=IterativeImputer();, score=-1064924532.138 total time= 0.5s  ","date":1651363200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1651363200,"objectID":"e8c0f32bd58d0f20f450a3facf21546b","permalink":"https://matteocourthoud.github.io/course/data-science/06_ml_pipeline/","publishdate":"2022-05-01T00:00:00Z","relpermalink":"/course/data-science/06_ml_pipeline/","section":"course","summary":"In this notebook, we are going to build a pipeline for a general prediction problem.\n# Standard Imports from src.utils import * from src.get_feature_names import get_feature_names  # Set inline graphs plt.","tags":null,"title":"Machine Learning Pipeline","type":"book"},{"authors":null,"categories":null,"content":"# Remove warnings import warnings warnings.filterwarnings('ignore')  # Import import autograd.numpy as np from autograd import grad import seaborn as sns  # Import matplotlib for graphs import matplotlib.pyplot as plt from mpl_toolkits.mplot3d import axes3d # Set global parameters %matplotlib inline plt.style.use('seaborn-white') plt.rcParams['lines.linewidth'] = 3 plt.rcParams['figure.figsize'] = (10,6) plt.rcParams['figure.titlesize'] = 20 plt.rcParams['axes.titlesize'] = 18 plt.rcParams['axes.labelsize'] = 14 plt.rcParams['legend.fontsize'] = 14  # Function to plot errors def error_plot(ys, yscale='log'): plt.figure() plt.xlabel('Step') plt.ylabel('Error') plt.yscale(yscale) plt.plot(range(len(ys)), ys)  6.1 Gradient Descent We start with a basic implementation of projected gradient descent.\ndef gradient_descent(init, steps, grad, proj=lambda x: x): \u0026quot;\u0026quot;\u0026quot;Projected gradient descent. Inputs: initial: starting point steps: list of scalar step sizes grad: function mapping points to gradients proj (optional): function mapping points to points Returns: List of all points computed by projected gradient descent. \u0026quot;\u0026quot;\u0026quot; xs = [init] for step in steps: xs.append(proj(xs[-1] - step * grad(xs[-1]))) return xs  Note that this implementation keeps around all points computed along the way. This is clearly not what you would do on large instances. We do this for illustrative purposes to be able to easily inspect the computed sequence of points.\nWarm-up: Optimizing a quadratic As a toy example, let\u0026rsquo;s optimize $$f(x)=\\frac12|x|^2,$$ which has the gradient map $\\nabla f(x)=x.$\ndef quadratic(x): return 0.5*x.dot(x) def quadratic_gradient(x): return x  Note the function is $1$-smooth and $1$-strongly convex. Our theorems would then suggest that we use a constant step size of $1.$ If you think about it, for this step size the algorithm will actually find the optimal solution in just one step.\nx0 = np.random.normal(0, 1, (1000)) _, x1 = gradient_descent(x0, [1.0], quadratic_gradient)  Indeed, it does.\nx1.all() == 0  True  Let\u0026rsquo;s say we don\u0026rsquo;t have the right learning rate.\nxs = gradient_descent(x0, [0.1]*50, quadratic_gradient)  # Plot errors along steps error_plot([quadratic(x) for x in xs])  Constrained Optimization Let\u0026rsquo;s say we want to optimize the function inside some affine subspace. Recall that affine subspaces are convex sets. Below we pick a random low dimensional affine subspace $b+U$ and define the corresponding linear projection operator.\n# U is an orthonormal basis of a random 100-dimensional subspace. U = np.linalg.qr(np.random.normal(0, 1, (1000, 100)))[0] b = np.random.normal(0, 1, 1000) def proj(x): \u0026quot;\u0026quot;\u0026quot;Projection of x onto an affine subspace\u0026quot;\u0026quot;\u0026quot; return b + U.dot(U.T).dot(x-b)  x0 = np.random.normal(0, 1, (1000)) xs = gradient_descent(x0, [0.1]*50, quadratic_gradient, proj) # the optimal solution is the projection of the origin x_opt = proj(0)  Let\u0026rsquo;s plot the results.\nerror_plot([quadratic(x) for x in xs]) plt.plot(range(len(xs)), [quadratic(x_opt)]*len(xs), label='$\\\\frac{1}{2}|\\!|x_{\\mathrm{opt}}|\\!|^2$') plt.legend();  The orangle line shows the optimal error, which the algorithm reaches quickly.\nThe iterates also converge to the optimal solution in domain as the following plot shows.\nerror_plot([np.linalg.norm(x_opt-x)**2 for x in xs])  Least Squares One of the most fundamental data analysis tools is linear least squares. Given an $m\\times n$ matrix $A$ and a vector $b$ our goal is to find a vector $x\\in\\mathbb{R}^n$ that minimizes the following objective:\n $$f(x) = \\frac 1{2m}\\sum_{i=1}^m (a_i^\\top x - b_j)^2 =\\frac1{2m}\\|Ax-b\\|^2$$ We can verify that $\\nabla f(x) = A^\\top(Ax-b)$ and $\\nabla^2 f(x) = A^\\top A.$\nHence, the objective is $\\beta$-smooth with $\\beta=\\lambda_{\\mathrm{max}}(A^\\top A)$, and $\\alpha$-strongly convex with $\\alpha=\\lambda_{\\mathrm{min}}(A^\\top A)$.\ndef least_squares(A, b, x): \u0026quot;\u0026quot;\u0026quot;Least squares objective.\u0026quot;\u0026quot;\u0026quot; return (0.5/m) * np.linalg.norm(A.dot(x)-b)**2 def least_squares_gradient(A, b, x): \u0026quot;\u0026quot;\u0026quot;Gradient of least squares objective at x.\u0026quot;\u0026quot;\u0026quot; return A.T.dot(A.dot(x)-b)/m  Overdetermined case $m\\ge n$ m, n = 1000, 100 A = np.random.normal(0, 1, (m, n)) x_opt = np.random.normal(0, 1, n) noise = np.random.normal(0, 0.1, m) b = A.dot(x_opt) + noise objective = lambda x: least_squares(A, b, x) gradient = lambda x: least_squares_gradient(A, b, x)  Convergence in Objective x0 = np.random.normal(0, 1, n) xs = gradient_descent(x0, [0.1]*100, gradient)  error_plot([objective(x) for x in xs]) plt.plot(range(len(xs)), [np.linalg.norm(noise)**2]*len(xs), label='noise level') plt.plot(range(len(xs)), [least_squares(A,b,x_opt)]*len(xs), label='optimal') plt.legend();  Convergence in Domain error_plot([np.linalg.norm(x-x_opt)**2 for x in xs])  Underdetermined Case $m \u0026lt; n$ In the underdetermined case, the least squares objective is inevitably not strongly convex, since $A^\\top A$ is a rank deficient matrix and hence $\\lambda_{\\mathrm{min}}(A^\\top A)=0.$\nm, n = 100, 1000 A = np.random.normal(0, 1, (m, n)) b = np.random.normal(0, 1, m) # The least norm solution is given by the pseudo-inverse x_opt = np.linalg.pinv(A).dot(b) objective = lambda x: least_squares(A, b, x) gradient = lambda x: least_squares_gradient(A, b, x) x0 = np.random.normal(0, 1, n) xs = gradient_descent(x0, [0.1]*100, gradient)  Results.\nerror_plot([objective(x) for x in xs]) plt.plot(range(len(xs)), [least_squares(A,b,x_opt)]*len(xs), label='optimal') plt.legend();  While we quickly reduce the error, we don\u0026rsquo;t actually converge in domain to the least norm solution. This is just because the function is no longer strongly convex in the underdetermined case.\nerror_plot([np.linalg.norm(x-x_opt)**2 for x in xs], yscale='linear') plt.plot(range(len(xs)), [np.linalg.norm(x_opt)**2]*len(xs), label='$|\\!|x_{\\mathrm{opt}}|\\!|^2$') plt.legend();  $\\ell_2$-regularized least squares In the underdetermined case, it is often desirable to restore strong convexity of the objective function by adding an $\\ell_2^2$-penality, also known as Tikhonov regularization, $\\ell_2$-regularization, or weight decay.\n $$\\frac1{2m}\\|Ax-b\\|^2 + \\frac{\\alpha}2\\|x\\|^2$$ Note: With this modification the objective is $\\alpha$-strongly convex again.\ndef least_squares_l2(A, b, x, alpha=0.1): return least_squares(A, b, x) + (alpha/2) * x.dot(x) def least_squares_l2_gradient(A, b, x, alpha=0.1): return least_squares_gradient(A, b, x) + alpha * x  Let\u0026rsquo;s create a least squares instance.\nm, n = 100, 1000 A = np.random.normal(0, 1, (m, n)) b = A.dot(np.random.normal(0, 1, n)) objective = lambda x: least_squares_l2(A, b, x) gradient = lambda x: least_squares_l2_gradient(A, b, x)  Note that we can find the optimal solution to the optimization problem in closed form without even running gradient descent by computing $x_{\\mathrm{opt}}=(A^\\top+\\alpha I)^{-1}A^\\top b.$ Please verify that this point is indeed optimal.\nx_opt = np.linalg.inv(A.T.dot(A) + 0.1*np.eye(1000)).dot(A.T).dot(b)  Here\u0026rsquo;s how gradient descent fares.\nx0 = np.random.normal(0, 1, n) xs = gradient_descent(x0, [0.1]*500, gradient)  We plot the descent.\nerror_plot([objective(x) for x in xs]) plt.plot(range(len(xs)), [least_squares_l2(A,b,x_opt)]*len(xs), label='optimal') plt.legend();  You see that the error doesn\u0026rsquo;t decrease below a certain level due to the regularization term. This is not a bad thing. In fact, the regularization term gives as strong convexity which leads to convergence in domain again:\nxs = gradient_descent(x0, [0.1]*500, gradient)  error_plot([np.linalg.norm(x-x_opt)**2 for x in xs]) plt.plot(range(len(xs)), [np.linalg.norm(x_opt)**2]*len(xs), label='squared norm of $x_{\\mathrm{opt}}$') plt.legend();  The Magic of Implicit Regularization Sometimes simply running gradient descent from a suitable initial point has a regularizing effect on its own without introducing an explicit regularization term.\nWe will see this below where we revisit the unregularized least squares objective, but initialize gradient descent from the origin rather than a random gaussian point.\n# We initialize from 0 x0 = np.zeros(n) # Note this is the gradient w.r.t. the unregularized objective! gradient = lambda x: least_squares_gradient(A, b, x) xs = gradient_descent(x0, [0.1]*50, gradient) error_plot([np.linalg.norm(x_opt-x)**2 for x in xs], yscale='linear') plt.plot(range(len(xs)), [np.linalg.norm(x_opt)**2]*len(xs), label='$|\\!|x_{\\mathrm{opt}}|\\!|^2$') plt.legend();  Incredible! We converge to the minimum norm solution!\nImplicit regularization is a deep phenomenon that\u0026rsquo;s an active research topic in learning and optimization. It\u0026rsquo;s exciting that we see it play out in this simple least squares problem already!\nLASSO LASSO is the name for $\\ell_1$-regularized least squares regression:\n $$\\frac1{2m}\\|Ax-b\\|^2 + \\alpha\\|x\\|_1$$ We will see that LASSO is able to fine sparse solutions if they exist. This is a common motivation for using an $\\ell_1$-regularizer.\ndef lasso(A, b, x, alpha=0.1): return least_squares(A, b, x) + alpha * np.linalg.norm(x, 1) def ell1_subgradient(x): \u0026quot;\u0026quot;\u0026quot;Subgradient of the ell1-norm at x.\u0026quot;\u0026quot;\u0026quot; g = np.ones(x.shape) g[x \u0026lt; 0.] = -1.0 return g def lasso_subgradient(A, b, x, alpha=0.1): \u0026quot;\u0026quot;\u0026quot;Subgradient of the lasso objective at x\u0026quot;\u0026quot;\u0026quot; return least_squares_gradient(A, b, x) + alpha*ell1_subgradient(x)  m, n = 100, 1000 A = np.random.normal(0, 1, (m, n)) x_opt = np.zeros(n) x_opt[:10] = 1.0 b = A.dot(x_opt) x0 = np.random.normal(0, 1, n) xs = gradient_descent(x0, [0.1]*500, lambda x: lasso_subgradient(A, b, x))  error_plot([lasso(A, b, x) for x in xs])  plt.figure() plt.title('Comparison of initial, optimal, and computed point') idxs = range(50) plt.plot(idxs, x0[idxs], '--', color='#aaaaaa', label='initial') plt.plot(idxs, x_opt[idxs], 'r-', label='optimal') plt.plot(idxs, xs[-1][idxs], 'g-', label='final') plt.xlabel('Coordinate') plt.ylabel('Value') plt.legend();  As promised, LASSO correctly identifies the significant coordinates of the optimal solution. This is why, in practice, LASSO is a popular tool for feature selection.\nPlay around with this plot to inspect other points along the way, e.g., the point that achieves lowest objective value. Why does the objective value go up even though we continue to get better solutions?\nSupport Vector Machines In a linear classification problem, we\u0026rsquo;re given $m$ labeled points $(a_i, y_i)$ and we wish to find a hyperplane given by a point $x$ that separates them so that\n $\\langle a_i, x\\rangle \\ge 1$ when $y_i=1$, and $\\langle a_i, x\\rangle \\le -1$ when $y_i = -1$  The smaller the norm $|x|$ the larger the margin between positive and negative instances. Therefore, it makes sense to throw in a regularizer that penalizes large norms. This leads to the objective.\n $$\\frac 1m \\sum_{i=1}^m \\max\\{1-y_i(a_i^\\top x), 0\\} + \\frac{\\alpha}2\\|x\\|^2$$ def hinge_loss(z): return np.maximum(1.-z, np.zeros(z.shape)) def svm_objective(A, y, x, alpha=0.1): \u0026quot;\u0026quot;\u0026quot;SVM objective.\u0026quot;\u0026quot;\u0026quot; m, _ = A.shape return np.mean(hinge_loss(np.diag(y).dot(A.dot(x))))+(alpha/2)*x.dot(x)  z = np.linspace(-2, 2, 100)  plt.figure() plt.plot(z, hinge_loss(z));  def hinge_subgradient(z): g = np.zeros(z.shape) g[z \u0026lt; 1] = -1. return g def svm_subgradient(A, y, x, alpha=0.1): g1 = hinge_subgradient(np.diag(y).dot(A.dot(x))) g2 = np.diag(y).dot(A) return g1.dot(g2) + alpha*x  plt.figure() plt.plot(z, hinge_subgradient(z));  m, n = 1000, 100 A = np.vstack([np.random.normal(0.1, 1, (m//2, n)), np.random.normal(-0.1, 1, (m//2, n))]) y = np.hstack([np.ones(m//2), -1.*np.ones(m//2)]) x0 = np.random.normal(0, 1, n) xs = gradient_descent(x0, [0.01]*100, lambda x: svm_subgradient(A, y, x, 0.05))  error_plot([svm_objective(A, y, x) for x in xs])  Let\u0026rsquo;s see if averaging out the solutions gives us an improved function value.\nxavg = 0.0 for x in xs: xavg += x svm_objective(A, y, xs[-1]), svm_objective(A, y, xavg/len(xs))  (1.0710162653835846, 0.9069593413738611)  We can also look at the accuracy of our linear model for predicting the labels. From how we defined the data, we can see that the all ones vector is the highest accuracy classifier in the limit of infinite data (very large $m$). For a finite data set, the accuracy could be even higher due to random fluctuations.\ndef accuracy(A, y, x): return np.mean(np.diag(y).dot(A.dot(x))\u0026gt;0)  plt.figure() plt.ylabel('Accuracy') plt.xlabel('Step') plt.plot(range(len(xs)), [accuracy(A, y, x) for x in xs]) plt.plot(range(len(xs)), [accuracy(A, y, np.ones(n))]*len(xs), label='Population optimum') plt.legend();  We see that the accuracy spikes pretty early and drops a bit as we train for too long.\nSparse Inverse Covariance Estimation Given a positive semidefinite matrix $S\\in\\mathbb{R}^{n\\times n}$ the objective function in sparse inverse covariance estimation is as follows:\n $$ \\min_{X\\in\\mathbb{R}^{n\\times n}, X\\succeq 0} \\langle S, X\\rangle - \\log\\det(X) + \\alpha\\|X\\|_1$$ Here, we define $$\\langle S, X\\rangle = \\mathrm{trace}(S^\\top X)$$ and $$|X|1 = \\sum{ij}|X_{ij}|.$$\nTypically, we think of the matrix $S$ as a sample covariance matrix of a set of vectors $a_1,\\dots, a_m,$ defined as: $$ S = \\frac1{m-1}\\sum_{i=1}^n a_ia_i^\\top $$ The example also highlights the utility of automatic differentiation as provided by the autograd package that we\u0026rsquo;ll regularly use. In a later lecture we will understand exactly how automatic differentiation works. For now we just treat it as a blackbox that gives us gradients.\nnp.random.seed(1337)  def sparse_inv_cov(S, X, alpha=0.1): return (np.trace(S.T.dot(X)) - np.log(np.linalg.det(X)) + alpha * np.sum(np.abs(X)))  n = 5 A = np.random.normal(0, 1, (n, n)) S = A.dot(A.T) objective = lambda X: sparse_inv_cov(S, X) # autograd provides a \u0026quot;gradient\u0026quot;, yay! gradient = grad(objective)  We also need to worry about the projection onto the positive semidefinite cone, which corresponds to truncating eigenvalues.\ndef projection(X): \u0026quot;\u0026quot;\u0026quot;Projection onto positive semidefinite cone.\u0026quot;\u0026quot;\u0026quot; es, U = np.linalg.eig(X) es[es\u0026lt;0] = 0. return U.dot(np.diag(es).dot(U.T))  A0 = np.random.normal(0, 1, (n,n)) X0 = A0.dot(A0.T) Xs = gradient_descent(X0, [0.01]*500, gradient, projection) error_plot([objective(X) for X in Xs])  Going crazy with autograd Just for fun, we\u0026rsquo;ll go through a crazy example below. We can use autograd not just for getting gradients for natural objectives, we can in principle also use it to tune hyperparameters of our optimizer, like the step size schedulde.\nBelow we see how we can find a better 10-step learning rate schedules for optimizing a quadratic. This is mostly just for illustrative purposes (although some researchers are exploring these kinds of ideas more seriously).\nx0 = np.random.normal(0, 1, 1000)  def f(x): return 0.5*np.dot(x,x) def optimizer(steps): \u0026quot;\u0026quot;\u0026quot;Optimize a quadratic with the given steps.\u0026quot;\u0026quot;\u0026quot; xs = gradient_descent(x0, steps, grad(f)) return f(xs[-1])  The function optimizer is a non-differentiable function of its input steps. Nontheless, autograd will provide a gradient that we can stick into gradient descent. That is, we\u0026rsquo;re tuning gradient descent with gradient descent.\ngrad_optimizer = grad(optimizer)  initial_steps = np.abs(np.random.normal(0, 0.1, 10)) better_steps = gradient_descent(initial_steps, [0.001]*500, grad_optimizer)  error_plot([optimizer(steps) for steps in better_steps])  As we can see, the learning rate schedules improve dramatically over time. Of course, we already know from the first example that there is a step size schedule that converges in one step. Interestingly, the last schedule we find here doesn\u0026rsquo;t look at all like what we might expect:\nplt.figure() plt.xticks(range(len(better_steps[-1]))) plt.ylabel('Step size') plt.xlabel('Step number') plt.plot(range(len(better_steps[-1])), better_steps[-1]);  ","date":1646784000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1646784000,"objectID":"95b7f39b86cd1dae5bb9b292f50b65a4","permalink":"https://matteocourthoud.github.io/course/ml-econ/06_convexity/","publishdate":"2022-03-09T00:00:00Z","relpermalink":"/course/ml-econ/06_convexity/","section":"course","summary":"# Remove warnings import warnings warnings.filterwarnings('ignore')  # Import import autograd.numpy as np from autograd import grad import seaborn as sns  # Import matplotlib for graphs import matplotlib.pyplot as plt from mpl_toolkits.","tags":null,"title":"Convexity and Optimization","type":"book"},{"authors":null,"categories":null,"content":"Instrumental Variables Endogeneity We say that there is endogeneity in the linear regression model if $\\mathbb E[x_i \\varepsilon_i] \\neq 0$.\nThe random vector $z_i$ is an instrumental variable in the linear regression model if the following conditions are met.\n Exclusion restriction: the instruments are uncorrelated with the regression error $$ \\mathbb E_n[z_i \\varepsilon_i] = 0 $$ almost surely, i.e. with probability $p \\to 1$. Rank condition: no linearly redundant instruments $$ \\mathbb E_n[z_i z_i'] \\neq 0 $$ almost surely, i.e. with probability $p \\to 1$. Relevance condition (need $L \u0026gt; K$): $$ rank \\ (\\mathbb E_n[z_i x_i']) = K $$ almost surely, i.e. with probability $p \\to 1$.  IV and 2SLS Let $K = dim(x_i)$ and $L = dim(z_i)$. We say that the model is just-identified if $L = K$ (method: IV) and over-identified if $L \u0026gt; K$ (method: 2SLS).\nAssume $z_i$ satisfies the instrumental variable assumptions above and $dim(z_i) = dim(x_i)$, then the instrumental variables (IV) estimator $\\hat{\\beta} _ {IV}$ is given by $$ \\begin{aligned} \\hat{\\beta} _ {IV} \u0026amp;= \\mathbb E_n[z_i x_i']^{-1} \\mathbb E_n[z_i y_i] = \\newline \u0026amp;= \\left( \\frac{1}{n} \\sum _ {i=1}^n z_i x_i\\right)^{-1} \\left( \\frac{1}{n} \\sum _ {i=1}^n z_i y_i\\right) = \\newline \u0026amp;= (Z\u0026rsquo;X)^{-1} (Z\u0026rsquo;y) \\end{aligned} $$\nAssume $z_i$ satisfies the instrumental variable assumptions above and $dim(z_i) \u0026gt; dim(x_i)$, then the two-stage-least squares (2SLS) estimator $\\hat{\\beta} _ {2SLS}$ is given by $$ \\hat{\\beta} _ {2SLS} = \\Big( X\u0026rsquo;Z (Z\u0026rsquo;Z)^{-1} Z\u0026rsquo;X \\Big)^{-1} \\Big( X\u0026rsquo;Z (Z\u0026rsquo;Z)^{-1} Z\u0026rsquo;y \\Big) $$ Where $\\hat{x}_i$ is the predicted $x_i$ from the first stage regression of $x_i$ on $z_i$. This is equivalent to the IV estimator using $\\hat{x}_i$ as an instrument for $x_i$.\n2SLS Algebra   The estimator is called two-stage-least squares since it can be rewritten as an IV estimator that uses $\\hat{X}$ as instrument: $$ \\begin{aligned} \\hat{\\beta} _ {\\text{2SLS}} \u0026amp;= \\Big( X\u0026rsquo;Z (Z\u0026rsquo;Z)^{-1} Z\u0026rsquo;X \\Big)^{-1} \\Big( X\u0026rsquo;Z (Z\u0026rsquo;Z)^{-1} Z\u0026rsquo;y \\Big) = \\newline \u0026amp;= (\\hat{X}' X)^{-1} \\hat{X}' y = \\newline \u0026amp;= \\mathbb E_n[\\hat{x}_i x_i']^{-1} \\mathbb E_n[\\hat{x}_i y_i] \\end{aligned} $$\n  Moreover it can be rewritten as $$ \\begin{aligned} \\hat{\\beta} _ {\\text{2SLS}} \u0026amp;= (\\hat{X}' X)^{-1} \\hat{X}' y = \\newline \u0026amp;= (X' P_Z X)^{-1} X' P_Z y = \\newline \u0026amp;= (X' P_Z P_Z X)^{-1} X' P_Z y = \\newline \u0026amp;= (\\hat{X}' \\hat{X})^{-1} \\hat{X}' y = \\newline \u0026amp;= \\mathbb E_n [\\hat{x}_i \\hat{x}_i]^{-1} \\mathbb E_n[\\hat{x}_i y_i] \\end{aligned} $$\n  Rule of Thumb How to the test the relevance condition? Rule of thumb: $F$-test in the first stage $\u0026gt;10$ (joint test on $z_i$).\n Problem: as $n \\to \\infty$, with finite $L$, $F \\to \\infty$ (bad rule of thumb).\n Equivalence Theorem\nIf $K=L$, $\\hat{\\beta} _ {\\text{2SLS}} = \\hat{\\beta} _ {\\text{IV}}$.\nProof\nIf $K=L$, $X\u0026rsquo;Z$ and $Z\u0026rsquo;X$ are squared matrices and, by the relevance condition, non-singular (invertible). $$ \\begin{aligned} \\hat{\\beta} _ {\\text{2SLS}} \u0026amp;= \\Big( X\u0026rsquo;Z (Z\u0026rsquo;Z)^{-1} Z\u0026rsquo;X \\Big)^{-1} \\Big( X\u0026rsquo;Z (Z\u0026rsquo;Z)^{-1} Z\u0026rsquo;y \\Big) = \\newline \u0026amp;= (Z\u0026rsquo;X)^{-1} (Z\u0026rsquo;Z) (X\u0026rsquo;Z)^{-1} X\u0026rsquo;Z (Z\u0026rsquo;Z)^{-1} Z\u0026rsquo;y = \\newline \u0026amp;= (Z\u0026rsquo;X)^{-1} (Z\u0026rsquo;Z) (Z\u0026rsquo;Z)^{-1} Z\u0026rsquo;y = \\newline \u0026amp;= (Z\u0026rsquo;X)^{-1} (Z\u0026rsquo;y) = \\newline \u0026amp;= \\hat{\\beta} _ {\\text{IV}} \\end{aligned} $$ $$\\tag*{$\\blacksquare$}$$\nDemand Example Example from Hayiashi (2000) page 187: demand and supply simultaneous equations. $$ \\begin{aligned} \u0026amp; q_i^D(p_i) = \\alpha_0 + \\alpha_1 p_i + u_i \\newline \u0026amp; q_i^S(p_i) = \\beta_0 + \\beta_1 p_i + v_i \\end{aligned} $$\nWe have an endogeneity problem. To see why, we solve the system of equations for $(p_i, q_i)$: $$ \\begin{aligned} \u0026amp; p_i = \\frac{\\beta_0 - \\alpha_0}{\\alpha_1 - \\beta_1} + \\frac{v_i - u_i}{\\alpha_1 - \\beta_1 } \\newline \u0026amp; q_i = \\frac{\\alpha_1\\beta_0 - \\alpha_0 \\beta_1}{\\alpha_1 - \\beta_1} + \\frac{\\alpha_1 v_i - \\beta_1 u_i}{\\alpha_1 - \\beta_1 } \\end{aligned} $$\nDemand Example (2) Then the price variable is not independent from the error term in neither equation: $$ \\begin{aligned} \u0026amp; Cov(p_i, u_i) = - \\frac{Var(u_i)}{\\alpha_1 - \\beta_1 } \\newline \u0026amp; Cov(p_i, v_i) = \\frac{Var(v_i)}{\\alpha_1 - \\beta_1 } \\end{aligned} $$\nAs a consequence, the OLS estimators are not consistent: $$ \\begin{aligned} \u0026amp; \\hat{\\alpha} _ {1, OLS} \\overset{p}{\\to} \\alpha_1 + \\frac{Cov(p_i, u_i)}{Var(p_i)} \\newline \u0026amp; \\hat{\\beta} _ {1, OLS} \\overset{p}{\\to} \\beta_1 + \\frac{Cov(p_i, v_i)}{Var(p_i)} \\end{aligned} $$\nDemand Example (3) In general, running regressing $q$ on $p$ you estimate $$ \\begin{aligned} \\hat{\\gamma} _ {OLS} \u0026amp;\\overset{p}{\\to} \\frac{Cov(p_i, q_i)}{Var(p_i)} = \\newline \u0026amp;= \\frac{\\alpha_1 Var(v_i) + \\beta_1 Var(u_i)}{(\\alpha_1 - \\beta_1)^2} \\left( \\frac{Var(v_i) + Var(u_i)}{(\\alpha_1 - \\beta_1)^2} \\right)^{-1} = \\newline \u0026amp;= \\frac{\\alpha_1 Var(v_i) + \\beta_1 Var(u_i)}{Var(v_i) + Var(u_i)} \\end{aligned} $$ Which is neither $\\alpha_1$ nor $\\beta_1$ but a variance weighted average of the two.\nDemand Example (4) Suppose we have a supply shifter $z_i$ such that\n $\\mathbb E[z_i v_i] \\neq 0$ $\\mathbb E[z_i u_i] = 0$.  We combine the second condition and $\\mathbb E[u_i] = 0$ to get a system of 2 equations in 2 unknowns: $\\alpha_0$ and $\\alpha_1$. $$ \\begin{aligned} \u0026amp; \\mathbb E[z_i u_i] = \\mathbb E[ z_i (q_i^D(p_i) - \\alpha_0 - \\alpha_1 p_i) ] = 0 \\newline \u0026amp; \\mathbb E[u_i] = \\mathbb E[q_i^D(p_i) - \\alpha_0 - \\alpha_1 p_i] = 0\n\\end{aligned} $$\nWe could try to solve for the vector $\\alpha$ that solves $$ \\begin{aligned} \u0026amp; \\mathbb E_n[z_i (q_i^D - x_i\\alpha)] = 0 \\newline \u0026amp; \\mathbb E_n[z_i q_i^D] - \\mathbb E_n[z_ix_i\\alpha] = 0 \\end{aligned} $$\nIf $\\mathbb E_n[z_ix_i]$ is invertible, we get $\\hat{\\alpha} = \\mathbb E_n[z_ix_i]^{-1} \\mathbb E_n[z_i q^D_i]$ which is indeed the IV estimator of $\\alpha$ using $z_i$ as an instrument for the endogenous variable $p_i$.\nCode - DGP This code draws 100 observations from the model $y = 2 x_1 - x_2 + \\varepsilon$ where $x_1, x_2 \\sim U[0,1]$ and $\\varepsilon \\sim N(0,1)$.\n# Set seed Random.seed!(123); # Set the number of observations n = 100; # Set the dimension of Z l = 3; # Draw instruments Z = rand(Uniform(0,1), n, l); # Correlation matrix for error terms S = [1 0.8; 0.8 1]; # Endogenous X γ = [2 0; 0 -1; -1 3]; ε = rand(Normal(0,1), n, 2) * cholesky(S).U; X = Z*γ .+ ε[:,1]; # Calculate y y = X*β .+ ε[:,2];  Code - IV # Estimate beta OLS β_OLS = (X'*X)\\(X'*y)  ## 2-element Array{Float64,1}: ## 2.335699233358403 ## -0.8576266209987325  # IV: l=k=2 instruments Z_IV = Z[:,1:k]; β_IV = (Z_IV'*X)\\(Z_IV'*y)  ## 2-element Array{Float64,1}: ## 1.6133344277861439 ## -0.6678537395714547  # Calculate standard errors ε_hat = y - X*β_IV; V_NHC_IV = var(ε_hat) * inv(Z_IV'*X)*Z_IV'*Z_IV*inv(Z_IV'*X); V_HC0_IV = inv(Z_IV'*X)*Z_IV' * (I(n) .* ε_hat.^2) * Z_IV*inv(Z_IV'*X);  Code - 2SLS # 2SLS: l=3 instruments Pz = Z*inv(Z'*Z)*Z'; β_2SLS = (X'*Pz*X)\\(X'*Pz*y)  ## 2-element Array{Float64,1}: ## 1.904553638377971 ## -0.8810907510370429  # Calculate standard errors ε_hat = y - X*β_2SLS; V_NCH_2SLS = var(ε_hat) * inv(X'*Pz*X); V_HC0_2SLS = inv(X'*Pz*X)*X'*Pz * (I(n) .* ε_hat.^2) *Pz*X*inv(X'*Pz*X);  GMM Setting We have a system of $L$ moment conditions $$ \\begin{aligned} \u0026amp; \\mathbb E[g_1(\\omega_i, \\delta_0)] = 0 \\newline \u0026amp; \\vdots \\newline \u0026amp; \\mathbb E[g_L(\\omega_i, \\delta_0)] = 0 \\end{aligned} $$\nIf $L = \\dim (\\delta_0)$, no problem. If $L \u0026gt; \\dim (\\delta_0)$, there may be no solution to the system of equations.\nOptions There are two possibilities.\n First Solution: add moment conditions until the system is identified $$ \\mathbb E[ a' g(\\omega_i, \\delta_0)] = 0 $$ Solve $\\mathbb E[Ag(\\omega_i, \\delta)] = 0$ for $\\hat{\\delta}$. How to choose $A$? Such that it minimizes $Var(\\hat{\\delta})$. Second Solution: generalized method of moments (GMM) $$ \\begin{aligned} \\hat{\\delta} _ {GMM} \u0026amp;= \\arg \\min _ \\delta \\quad \\Big| \\Big| \\mathbb E_n [ g(\\omega_i, \\delta) ] \\Big| \\Big| = \\newline \u0026amp;= \\arg \\min _ \\delta \\quad n \\mathbb E_n[g(\\omega_i, \\delta)]' W \\mathbb E_n [g(\\omega_i, \\delta)] \\end{aligned} $$   The choice of $A$ and $W$ are closely related to each other.\n 1-step GMM Since $J(\\delta,W)$ is a quadratic form, a closed form solution exists: $$ \\hat{\\delta}(W) = \\Big(\\mathbb E_n[z_i x_i'] W \\mathbb E_n[z_i x_i'] \\Big)^{-1}\\mathbb E_n[z_i x_i'] W \\mathbb E_n[z_i y_i] $$\nAssumptions for consistency of the GMM estimator given data $\\mathcal D = \\lbrace y_i, x_i, z_i \\rbrace _ {i=1}^n$:\n Linearity: $y_i = x_i\\gamma_0 + \\varepsilon_i$ IID: $(y_i, x_i, z_i)$ iid Orthogonality: $\\mathbb E [z_i(y_i - x_i\\gamma_0)] = \\mathbb E[z_i \\varepsilon_i] = 0$ Rank identification: $\\Sigma_{xz} = \\mathbb E[z_i x_i']$ has full rank  Convergence Theorem\nUnder linearity, independence, orthogonality and rank conditions, if $\\hat{W} \\overset{p}{\\to} W$ positive definite, then $$ \\hat{\\delta}(\\hat{W}) \\to \\delta(W) $$ If in addition to the above assumption, $\\sqrt{n} \\mathbb E_n [g(\\omega_i, \\delta_0)] \\overset{d}{\\to} N(0,S)$ for a fixed positive definite $S$, then $$ \\sqrt{n} (\\hat{\\delta} (\\hat{W}) - \\delta(W)) \\overset{d}{\\to} N(0,V) $$ where $V = (\\Sigma' _ {xz} W \\Sigma _ {xz})^{-1} \\Sigma _ {xz} W S W \\Sigma _ {xz}(\\Sigma' _ {xz} W \\Sigma _ {xz})^{-1}$.\nFinally, if a consistent estimator $\\hat{S}$ of $S$ is available, then using sample analogues $\\hat{\\Sigma}_{xz}$ it follows that $$ \\hat{V} \\overset{p}{\\to} V $$\n If $W = S^{-1}$ then $V$ reduces to $V = (\\Sigma' _ {xz} W \\Sigma _ {xz})^{-1}$. Moreover, $(\\Sigma' _ {xz} W \\Sigma _ {xz})^{-1}$ is the smallest possible form of $V$, in a positive definite sense.\n Therefore, to have an efficient estimator, you want to construct $\\hat{W}$ such that $\\hat{W} \\overset{p}{\\to} S^{-1}$.\n2-step GMM Estimation steps:\n Choose an arbitrary weighting matrix $\\hat{W}_{init}$ (usually the identity matrix $I_K$) Estimate $\\hat{\\delta} _ {init}(\\hat{W} _ {init})$ Estimate $\\hat{S}$ (asymptotic variance of the moment condition) Estimate $\\hat{\\delta}(\\hat{S}^{-1})$   On the procedure:\n This estimator achieves the semiparametric efficiency bound. This strategy works only if $\\hat{S} \\overset{p}{\\to} S$ exists. For iid cases: we can use $\\hat{\\delta} = \\mathbb E_n[(\\hat{\\varepsilon}_i z_i)(\\hat{\\varepsilon}_i z_i) ' ]$ where $\\hat{\\varepsilon}_i = y_i - x_i \\hat{\\delta}(\\hat{W} _ {init})$.   Code - 1-step GMM # GMM 1-step: inefficient weighting matrix W_1 = I(l); # Objective function gmm_1(b) = ( y - X*b )' * Z * W_1 * Z' * ( y - X*b ); # Estimate GMM β_gmm_1 = optimize(gmm_1, β_OLS).minimizer  ## 2-element Array{Float64,1}: ## 1.91556882526808 ## -0.8769689391885799  # Standard errors GMM ε_hat = y - X*β_gmm_1; S_hat = Z' * (I(n) .* ε_hat.^2) * Z; d_hat = -X'*Z; V_gmm_1 = inv(d_hat * inv(S_hat) * d_hat')  ## 2×2 Array{Float64,2}: ## 0.0158497 -0.00346601 ## -0.00346601 0.00616531  Code - 2-step GMM # GMM 2-step: efficient weighting matrix W_2 = inv(S_hat); # Objective function gmm_2(b) = ( y - X*b )' * Z * W_2 * Z' * ( y - X*b ); # Estimate GMM β_gmm_2 = optimize(gmm_2, β_OLS).minimizer  ## 2-element Array{Float64,1}: ## 1.905326742963115 ## -0.881808949213345  # Standard errors GMM ε_hat = y - X*β_gmm_2; S_hat = Z' * (I(n) .* ε_hat.^2) * Z; d_hat = -X'*Z; V_gmm_2 = inv(d_hat * inv(S_hat) * d_hat')  ## 2×2 Array{Float64,2}: ## 0.0162603 -0.00357632 ## -0.00357632 0.00631259  Testing Overidentifying Restrictions If the equations are exactly identified, then it is possible to choose $\\delta$ so that all the elements of the sample moments $\\mathbb E_n[g(\\omega_i; \\delta)]$ are zero and thus that the distance $$ J(\\delta, \\hat{W}) = n \\mathbb E_n[g(\\omega_i, \\delta)]' \\hat{W} \\mathbb E_n[g(\\omega_i, \\delta)] $$ is zero. (The $\\delta$ that does it is the IV estimator.)\nIf the equations are overidentified, i.e. $L$ (number of instruments) $\u0026gt; K$ (number of equations), then the distance cannot be zero exactly in general, but we would expect the minimized distance to be close to zero.\nNaive Test Suppose your model is overidentified ($L \u0026gt; K$) and you use the following naive testing procedure:\n Estimate $\\hat{\\delta}$ using a subset of dimension $K$ of instruments $\\lbrace z_1 , .. , z_K\\rbrace$ for $\\lbrace x_1 , \u0026hellip; , x_K\\rbrace$ Set $\\hat{\\varepsilon}_i = y_i - x_i \\hat{\\delta} _ {\\text{GMM}}$ Infer the size of the remaining $L-K$ moment conditions $\\mathbb E[z _{i, K+1} \\varepsilon_i], \u0026hellip;, \\mathbb E[z _{i, L} \\varepsilon_i]$ looking at their empirical counterparts $\\mathbb E_n[z _{i, K+1} \\hat{\\varepsilon}_i], \u0026hellip;, \\mathbb E_n[z _{i, L} \\hat{\\varepsilon}_i]$ Reject exogeneity if the empirical expectations are high. How high? Calculate p-values.  Example If you have two invalid instruments and you use one to test the validity of the other, it might happen by chance that you don’t reject it.\n  Model: $y_i = x_i + \\varepsilon_i$ and $x_i = \\frac{1}{2} z _{i1} - \\frac{1}{2} z _{i2} + u_i$\n  Have $$ Cov (z _{i1}, z _{i2}, \\varepsilon_i, u_i) = \\begin{bmatrix} 1 \u0026amp; 0 \u0026amp; 0 \u0026amp; 0 \\newline 0 \u0026amp; 1 \u0026amp; 0 \u0026amp; 0 \\newline 0 \u0026amp; 0 \u0026amp; 1 \u0026amp; 0.5 \\newline 0 \u0026amp; 0 \u0026amp; 0.5 \u0026amp; 1 \\end{bmatrix} $$\n  You want to test whether the second instrument is valid (is not since $\\mathbb E[z_2 \\varepsilon] \\neq 0$). You use $z_1$ and estimate $\\hat{\\beta} \\to$ the estimator is consistent.\n  You obtain $\\mathbb E_n[z _{i2} \\hat{\\varepsilon}_i] \\simeq 0$ even if $z_2$ is invalid\n  Problem: you are using an invalid instrument in the first place.\n  Hansen’s Test Theorem: We are interested in testing $H_0: \\mathbb E[z_i \\varepsilon_i] = 0$ against $H_1: \\mathbb E[z_i \\varepsilon_i] \\neq 0$. Suppose $\\hat{S} \\overset{p}{\\to} S$. Then $$ J(\\hat{\\delta}(\\hat{S}^{-1}) , \\hat{S}^{-1}) \\overset{d}{\\to} \\chi^2 _ {L-K} $$ For $c$ satisfying $\\alpha = 1- G_{L - K} ( c )$, $\\Pr(J\u0026gt;c | H_0) \\to \\alpha$ so the test reject $H_0$ if $J \u0026gt; c$ has asymptotic size $\\alpha$.\nComments  The degrees of freedom of the asymptotic distribution are the number of overidentifying restrictions. This is a specification test, testing whether all model assumptions are true jointly. Only when we are confident that about the other assumptions, can we interpret a large $J$ statistic as evidence for the endogeneity of some of the $L$ instruments included in $x$. Unlike the tests we have encountered so far, the test is not consistent against some failures of the orthogonality conditions (that is, it is not consistent against some fixed elements of the alternative). Several papers in the July 1996 issue of JBES report that the finite-sample null rejection probability of the test can far exceed the nominal significance level $\\alpha$.  Special Case: Conditional Homoskedasticity The main implication of conditional homoskedasticity is that efficient GMM becomes 2SLS. With efficient GMM estimation, the weighting matrix is $\\hat{S}^{-1} = \\mathbb En [z_i z_i' \\varepsilon_i^2]^{-1}$. With conditional homoskedasticity, the efficient weighting matrix is $\\mathbb E_n[z_iz_i']^{-1} \\sigma^{-2}$, or equivalently $\\mathbb E_n[z_iz_i']^{-1}$. Then, the GMM estimator becomes $$ \\hat{\\delta}(\\hat{S}^{-1}) = \\Big(\\mathbb E_n[z_i x_i']' \\underbrace{\\mathbb E_n[z_iz_i']^{-1} \\mathbb E[z_i x_i']} _ {\\text{ols of } x_i \\text{ on }z_i} \\Big)^{-1}\\mathbb E_n[z_i x_i']' \\underbrace{\\mathbb E_n[z_iz_i']^{-1} \\mathbb E[z_i y_i']} _ {\\text{ols of } y_i \\text{ on }z_i}= \\hat{\\delta} _ {2SLS} $$\nProof: Consider the matrix notation. $$ \\begin{aligned} \\hat{\\delta} \\left( \\frac{Z\u0026rsquo;Z}{n}\\right) \u0026amp;= \\left( \\frac{X\u0026rsquo;Z}{n} \\left( \\frac{Z\u0026rsquo;Z}{n}\\right)^{-1} \\frac{Z\u0026rsquo;X}{n} \\right)^{-1} \\frac{X\u0026rsquo;Z}{n} \\left( \\frac{Z\u0026rsquo;Z}{n}\\right)^{-1} \\frac{Z\u0026rsquo;Y}{n} = \\newline \u0026amp;= \\left( X\u0026rsquo;Z(Z\u0026rsquo;Z)^{-1} Z\u0026rsquo;X \\right)^{-1} X\u0026rsquo;Z(Z\u0026rsquo;Z)^{-1} Z\u0026rsquo;Y = \\newline \u0026amp;= \\left(X\u0026rsquo;P_ZX\\right)^{-1} X\u0026rsquo;P_ZY = \\newline \u0026amp;= \\left(X\u0026rsquo;P_ZP_ZX\\right)^{-1} X\u0026rsquo;P_ZY = \\newline \u0026amp;= \\left(\\hat{X}'_Z \\hat{X}_Z\\right)^{-1} \\hat{X}'_ZY = \\newline \u0026amp;= \\hat{\\delta} _ {2SLS} \\end{aligned} $$ $$\\tag*{$\\blacksquare$}$$\nSmall-Sample Properties of 2SLS Theorem: When the number of instruments is equal to the sample size ($L = n$), then $\\hat{\\delta} _ {2SLS} = \\hat{\\delta} _ {OLS}$\nProof: We have a perfect prediction problem. The first stage estimated coefficient $\\hat{\\gamma}$ is such that it solves the normal equations: $\\hat{\\gamma} = z_i^{-1} x_i$. Then $$ \\begin{aligned} \\hat{\\delta} _ {2SLS} \u0026amp;= \\mathbb E_n[\\hat{x}_i x'_i]^{-1} \\mathbb E_n[\\hat{x}_i y_i] = \\newline \u0026amp;= \\mathbb E_n[z_i z_i^{-1} x_i x'_i]^{-1} \\mathbb E_n[z_i z_i^{-1} x_i y_i] = \\newline \u0026amp;= \\mathbb E_n[x_i x'_i]^{-1} \\mathbb E_n[x_i y_i] = \\newline \u0026amp;= \\hat{\\delta} _ {OLS} \\end{aligned} $$ $$\\tag*{$\\blacksquare$}$$\n You have this overfitting problem in general when the number of instruments is large relative to the sample size. This problem arises even if the instruments are valid.\n Example from Angrist (1992)  They regress wages on years of schooling. Problem: endogeneity: both variables are correlated with skills which are unobserved. Solution: instrument years of schooling with the quarter of birth.  Idea: if born in the first three quarters, can attend school from the year of your sixth birthday. Otherwise, you have to wait one more year.   Problem: quarters of birth are three dummies.  In order to ``improve the first stage fit” they interact them with year of birth (180 effective instruments) and also with the state (1527 effective instruments). This mechanically increases the $R^2$ but also increases the bias of the 2SLS estimator.   Solutions: LIML, JIVE, RJIVE (Hansen et al., 2014), Post-Lasso (Belloni et al., 2012).  Example from Angrist (1992) Many Instrument Robust Estimation Issue Why having too many instruments is problematic? As the number of instruments increases, the estimated coefficient gets closer to OLS which is biased. As seen in the theorem above, for $L=n$, the two estimators coincide.\nLIML An alternative method to estimate the parameters of the structural equation is by maximum likelihood. Anderson and Rubin (1949) derived the maximum likelihood estimator for the joint distribution of $(y_i, x_i)$. The estimator is known as limited information maximum likelihood, or LIML.\nThis estimator is called “limited information” because it is based on the structural equation for $(y_i, x_i)$ combined with the reduced form equation for $x_i$. If maximum likelihood is derived based on a structural equation for $x_i$ as well, then this leads to what is known as full information maximum likelihood (FIML). The advantage of the LIML approach relative to FIML is that the former does not require a structural model for $x_i$, and thus allows the researcher to focus on the structural equation of interest - that for $y_i$.\nK-class Estimators The k-class estimators have the form $$ \\hat{\\delta}(\\alpha) = (X' P_Z X - \\alpha X' X)^{-1} (X' P_Z Y - \\alpha X' Y) $$\nThe limited information maximum likelihood estimator LIML is the k-class estimator $\\hat{\\delta}(\\alpha)$ where $$ \\alpha = \\lambda_{min} \\Big( ([X' , Y]^{-1} [X' , Y])^{-1} [X' , Y]^{-1} P_Z [X' , Y] \\Big) $$\nIf $\\alpha = 0$ then $\\hat{\\delta} _ {\\text{LIML}} = \\hat{\\delta} _ {\\text{2SLS}}$ while for $\\alpha \\to \\infty$, $\\hat{\\delta} _ {\\text{LIML}} \\to \\hat{\\delta} _ {\\text{OLS}}$.\nComments on LIML  The particular choice of $\\alpha$ gives a many instruments robust estimate The LIML estimator has no finite sample moments. $\\mathbb E[\\delta(\\alpha_{LIML})]$ does not exist in general In simulation studies performs well Has good asymptotic properties  Asymptotically the LIML estimator has the same distribution as 2SLS. However, they can have quite different behaviors in finite samples. There is considerable evidence that the LIML estimator has superior finite sample performance to 2SLS when there are many instruments or the reduced form is weak. However, on the other hand there is worry that since the LIML estimator is derived under normality it may not be robust in non-normal settings.\nJIVE The Jacknife IV procedure is the following\n Regress $\\lbrace x_j \\rbrace _ {j \\neq i}$ on $\\lbrace z_j \\rbrace _ {j \\neq i}$ and estimate $\\pi_{-i}$ (leave the $i^{th}$ observation out). Form $\\hat{x}_i = \\hat{\\pi} _ {-i} z_i$. Run IV using $\\hat{x}_i$ as instruments. $$ \\hat{\\delta} _ {JIVE} = \\mathbb E_n[\\hat{x}_i x_i']^{-1} \\mathbb E_n[\\hat{x}_i y_i'] $$  Comments on JIVE:  Prevents overfitting. With many instruments you get bad out of sample prediction which implies low correlation between $\\hat{x}_i$ and $x_i$: $\\mathbb E_n[\\hat{x}_i x_i'] \\simeq 0$. Use lasso/ridge regression in the first stage in case of too many instruments.  Hausman Test Here we consider testing the validity of OLS. OLS is generally preferred to IV in terms of precision. Many researchers only doubt the (joint) validity of the regressor $z_i$ instead of being certain that it is invalid (in the sense of not being predetermined). So then they wish to choose between OLS and 2SLS, assuming that they have an instrument vector $x_i$ whose validity is not in question. Further, assume for simplicity that $L = K$ so that the efficient GMM estimator is the IV estimator.\nThe Hausman test statistic $$ H \\equiv n (\\hat{\\delta} _ {IV} - \\hat{\\delta} _ {OLS})' [\\hat{Avar} (\\hat{\\delta} _ {IV} - \\hat{\\delta} _ {OLS})]^{-1} (\\hat{\\delta} _ {IV} - \\hat{\\delta} _ {OLS}) $$ is asymptotically distributed as a $\\chi^2_{L-s}$ under the null where $s = | z_i \\cup x_i |$: the number of regressors that are retained as instruments in $x_i$.\nComments In general, the idea of the Hausman test is the following. If you have two estimators, one which is efficient under $H_0$ but inconsistent under $H_1$ (in this case, OLS), and another which is consistent under $H_1$ (in this case, IV), then construct a test as a quadratic form in the differences of the estimators. Another classic example arises in panel data with the hypothesis $H_0$ of unconditional strict exogeneity. In that case, under $H_0$ Random Effects estimators are efficient but under $H_1$ they are inconsistent. Fixed Effects estimators instead are consistent under $H_1$.\nThe Hausman test statistic can be used as a pretest procedure: select either OLS or IV according to the outcome of the test. Although widely used, this pretest procedure is not advisable. When the null is false, it is still possible that the test accepts the null (committing a Type 2 error). In particular, this can happen with a high probability when the sample size is small and/or when the regressor $z_i$ is almost valid. In such an instance, estimation and also inference will be based on incorrect methods. Therefore, the overall properties of the Hausman pretest procedure are undesirable.\nThe Hausman test is an example of a specification test. There are many other specification tests. One could for example test for conditional homoskedasticity. Unlike for the OLS case, there does not exist a convenient test for conditional homoskedasticity for the GMM case. A test statistic that is asymptotically chi-squared under the null is available but is extremely cumbersome; see White (1982, note 2). If in doubt, it is better to use the more generally valid inference methods that allow for conditional heteroskedasticity. Similarly, there does not exist a convenient test for serial correlation for the GMM case. If in doubt, it is better to use the more generally valid inference methods that allow for serial correlation; for example, when data are collected over time (that is, time-series data).\n","date":1635465600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1635465600,"objectID":"07162f3680824234e3d24605eb09ffd5","permalink":"https://matteocourthoud.github.io/course/metrics/06_endogeneity/","publishdate":"2021-10-29T00:00:00Z","relpermalink":"/course/metrics/06_endogeneity/","section":"course","summary":"Instrumental Variables Endogeneity We say that there is endogeneity in the linear regression model if $\\mathbb E[x_i \\varepsilon_i] \\neq 0$.\nThe random vector $z_i$ is an instrumental variable in the linear regression model if the following conditions are met.","tags":null,"title":"Endogeneity","type":"book"},{"authors":null,"categories":null,"content":"Asymptotic Theory of the OLS Estimator OLS Consistency Theorem: Assume that $(x_i, y_i) _ {i=1}^n$ i.i.d. , $\\mathbb E[x_i x_i'] = Q$ positive definite, $\\mathbb E[x_i x_i'] \u0026lt; \\infty$ and $\\mathbb E [y_i^2] \u0026lt; \\infty$, then $\\hat \\beta _ {OLS}$ is a consistent estimator of $\\beta_0$, i.e. $\\hat \\beta = \\mathbb E_n [x_i x_i'] \\mathbb E_n [x_i y_i]\\overset{p}{\\to} \\beta_0$.\nProof:\nWe consider 4 steps:\n $\\mathbb E_n [x_i x_i'] \\xrightarrow{p} \\mathbb E [x_i x_i']$ by WLLN since $x_i x_i'$ iid and $\\mathbb E[x_i x_i'] \u0026lt; \\infty$. $\\mathbb E_n [x_i y_i] \\xrightarrow{p} \\mathbb E [x_i y_i]$ by WLLN, due to $x_i y_i$ iid, Cauchy-Schwarz and finite second moments of $x_i$ and $y_i$ $$ \\mathbb E \\left[ x_i y_i \\right] \\leq \\sqrt{ \\mathbb E[x_i^2] \\mathbb E[y_i^2]} \u0026lt; \\infty $$ $\\mathbb E_n [x_i x_i']^{-1} \\xrightarrow{p} \\mathbb E [x_i x_i']^{-1}$ by CMT. $\\mathbb E_n [x_i x_i']^{-1} \\mathbb E_n [x_i y_i] \\xrightarrow{p} \\mathbb E [x_i x_i']^{-1} \\mathbb E [x_i y_i] = \\beta$ by CMT. $$\\tag*{$\\blacksquare$}$$  Variance and Assumptions Now we are going to investigate the variance of $\\hat \\beta _ {OLS}$ progressively relaxing the underlying assumptions.\n Gaussian error term. Homoskedastic error term. Heteroskedastic error term. Heteroskedastic and autocorrelated error term.  Gaussian Error Term Theorem: Under the GM assumption (1)-(5), $\\hat \\beta - \\beta |X \\sim N(0, \\sigma^2 (X\u0026rsquo;X)^{-1})$\nProof:\nWe follow 2 steps:\n We can rewrite $\\hat \\beta$ as $$ \\begin{aligned} \\hat \\beta \u0026amp; = (X\u0026rsquo;X)^{-1} X\u0026rsquo;y = (X\u0026rsquo;X)^{-1} X'(X\\beta + \\varepsilon) \\newline \u0026amp;= \\beta + (X\u0026rsquo;X)^{-1} X' \\varepsilon = \\newline \u0026amp;= \\beta + \\mathbb E_n [x_i x_i']^{-1} \\mathbb E_n [x_i \\varepsilon_i] \\end{aligned} $$ Therefore: $\\hat \\beta-\\beta = \\mathbb E_n [x_i x_i']^{-1} \\mathbb E_n [x_i \\varepsilon_i]$. $$ \\begin{aligned} \\hat \\beta-\\beta |X \u0026amp; \\sim (X\u0026rsquo;X)^{-1} X' N(0, \\sigma^2 I_n) = \\newline \u0026amp;= N(0, \\sigma^2 (X\u0026rsquo;X)^{-1} X\u0026rsquo;X (X\u0026rsquo;X)^{-1}) = \\newline \u0026amp;= N(0, \\sigma^2 (X\u0026rsquo;X)^{-1}) \\end{aligned} $$ $$\\tag*{$\\blacksquare$}$$   Does it make sense to assume that $\\varepsilon$ is gaussian? Not much. But does it make sense that $\\hat \\beta$ is gaussian? Yes, because it’s an average.\n Homoskedastic Error Term Theorem: Under the assumptions of the previous theorem, plus $\\mathbb E[x^4] \u0026lt; \\infty$, the OLS estimate has an asymptotic normal distribution: $\\hat \\beta|X \\overset{d}{\\to} N(\\beta, \\sigma^2 (X\u0026rsquo;X)^{-1})$.\nProof: $$ \\sqrt{n} (\\hat \\beta - \\beta ) = \\underbrace{\\mathbb E_n [x_i x_i']^{-1}} _ {\\xrightarrow{p} Q^{-1} } \\underbrace{\\sqrt{n} \\mathbb E_n [x_i \\varepsilon_i ]} _ {\\xrightarrow{d} N(0, \\Omega)} \\rightarrow N(0, \\Sigma ) $$ where in general $\\Omega = Var (x_i \\varepsilon_i) = \\mathbb E [(x_i \\varepsilon_i)^2]$ and $\\Sigma = Q^{-1} \\Omega Q^{-1}$. $$\\tag*{$\\blacksquare$}$$\n Given that $Q = \\mathbb E [x_i x_i']$ is unobserved, we estimate it with $\\hat{Q} = \\mathbb E_n [x_i x_i']$. Since we have assumed homoskedastic error term, we have $\\Omega = \\sigma^2 (X\u0026rsquo;X)^{-1}$. Since we do not observe $\\sigma^2$ we estimate it as $\\hat{\\sigma}^2 = \\mathbb E_n[\\hat{\\varepsilon}_i^2]$.\n The terms $x_i \\varepsilon_i$ are called scores and we can already see their central importance for inference.\nHeteroskedastic Error Term Assumption: $\\mathbb E [\\varepsilon_i x_i \\varepsilon_j' x_j'] = 0$, for all $j \\ne i$ and $\\mathbb E [\\varepsilon_i^4] \\leq \\infty$, $\\mathbb E [|| x_i||^4] \\leq C \u0026lt; \\infty$ a.s.\nTheorem: Under GM assumptions (1)-(4) plus heteroskedastic error term, the following estimators are consistent, i.e. $\\hat{\\Sigma}\\xrightarrow{p} \\Sigma$.\n Note that we are only looking at $\\Omega$ of the $\\Sigma = Q^{-1} \\Omega Q^{-1}$ matrix.\n  HC0: use the observed residual $\\hat{\\varepsilon}_i$ $$ \\Omega _ {HC0} = \\mathbb E_n [x_i x_i' \\hat{\\varepsilon}_i^2] $$ When $k$ is too big relative to $n$ – i.e., $k/n \\rightarrow c \u0026gt;0$ – $\\hat{\\varepsilon}_i^2$ are too small ($\\Omega _ {HC0}$ biased towards zero). $\\Omega _ {HC1}$, $\\Omega _ {HC2}$ and $\\Omega _ {HC3}$ try to correct this small sample bias. HC1: degree of freedom correction (default robust in Stata) $$ \\Omega _ {HC1} = \\frac{1}{n - k }\\mathbb E_n [x_i x_i' \\hat{\\varepsilon}_i^2] $$ HC2: use standardized residuals $$ \\Omega _ {HC2} = \\mathbb E_n [x_i x_i' \\hat{\\varepsilon}_i^2 (1-h _ {ii})^{-1}] $$ where $h _ {ii} = [X(X\u0026rsquo;X)^{-1} X'] _ {ii}$ is the leverage of the $i^{th}$ observation. A large $h _ {ii}$ means that observation $i$ is unusual in the sense that the regressor $x_i$ is far from its sample mean. HC3: use prediction error, equivalent to Jack-knife estimator, i.e., $\\mathbb E_n [x_i x_i' \\hat{\\varepsilon} _ {(-i)}^2]$ $$ \\Omega _ {HC3} = \\mathbb E_n [x_i x_i' \\hat{\\varepsilon}_i^2 (1-h _ {ii})^{-2}] $$ This estimator does not overfit when $k$ is relatively big with respect to $n$. Idea: you exclude the corresponding observation when estimating a particular $\\varepsilon_i$: $\\hat{\\varepsilon}_i = y_i - x_i' \\hat \\beta _ {-i}$.  HC0 Consistency Theorem\nUnder regularity conditions HC0 is consistent, i.e. $\\hat{\\Sigma} _ {HC0} \\overset{p}{\\to} \\Sigma$. $$ \\hat{\\Sigma} = \\hat{Q}^{-1} \\hat{\\Omega} \\hat{Q}^{-1} \\xrightarrow{p} \\Sigma \\qquad \\text{ with } \\hat{\\Omega} = \\mathbb E_n [x_i x_i' \\hat{\\varepsilon}_i^2] \\quad \\text{ and } \\hat{Q} = \\mathbb E_n [x_i x_i']^{-1} $$\n Why is the proof relevant? You cannot directly apply the WLLN to $\\hat \\Sigma$.\n Proof\nFor the case $\\mathrm{dim}(x_i) =1$.\n $\\hat{Q}^{-1} \\xrightarrow{p} Q^{-1}$ by WLLN since $x_i$ is iid, $\\mathbb E[x_i^4] \u0026lt; \\infty$ $\\bar{\\Omega} = \\mathbb E_n [\\varepsilon_i^2 x_i x_i'] \\xrightarrow{p} \\Omega$ by WLLN since $\\mathbb E_n [\\varepsilon_i^4] \u0026lt; c$ and $x_i$ bounded. By the triangle inequality, $$ | \\hat{\\Omega} - \\hat{\\Omega}| \\leq \\underbrace{|\\Omega - \\bar{\\Omega}|} _ {\\overset{p}{\\to} 0} + \\underbrace{|\\bar{\\Omega} - \\hat{\\Omega}|} _ {\\text{WTS:} \\overset{p}{\\to} 0} $$ We want to show $|\\bar{\\Omega} - \\hat{\\Omega}| \\overset{p}{\\to} 0$ $$ \\begin{aligned} |\\bar{\\Omega} - \\hat{\\Omega}| \u0026amp;= \\mathbb E_n [\\varepsilon_i^2 x_i^2] - \\mathbb E_n [\\hat{\\varepsilon}_i^2 x_i^2] = \\newline \u0026amp;= \\mathbb E_n [\\left( \\varepsilon_i^2 - \\hat{\\varepsilon}_i^2 \\right) x_i^2] \\leq \\newline \u0026amp; \\leq \\mathbb E_n \\left[ \\left( \\varepsilon_i^2 - \\hat{\\varepsilon}_i^2 \\right)^2\\right]^{\\frac{1}{2}} \\mathbb E_n [x_i^4]^{\\frac{1}{2}} \\end{aligned} $$ where $\\mathbb E_n [x_i^4]^{\\frac{1}{2}} \\xrightarrow{p} \\mathbb E [x_i^4]^{\\frac{1}{2}}$ by $x_i$ bounded, iid and CMT. We want to show that $\\mathbb E_n \\left[ \\left( \\varepsilon_i^2 - \\hat{\\varepsilon}_i^2 \\right)^2\\right] \\leq \\eta$ with $\\eta \\rightarrow 0$. Let $L = \\max_i |\\hat{\\varepsilon}_i - \\varepsilon_i|$ (RV depending on $n$), with $L \\xrightarrow{p} 0$ since $$ |\\hat{\\varepsilon}_i - \\varepsilon_i| = |x_i \\hat \\beta - x_i \\beta| \\leq |x_i||\\hat \\beta - \\beta|\\xrightarrow{p} c \\cdot 0 $$ We can depompose $$ \\begin{aligned} \\left(\\varepsilon_i^2 - \\hat{\\varepsilon}_i^2 \\right)^2 \u0026amp; = \\left(\\varepsilon_i - \\hat{\\varepsilon}_i \\right)^2 \\left(\\varepsilon_i + \\hat{\\varepsilon}_i \\right)^2 \\leq \\newline\n\u0026amp; \\leq \\left(\\varepsilon_i + \\hat{\\varepsilon}_i \\right)^2 L^2 = \\newline \u0026amp;= \\left(2\\varepsilon_i - \\varepsilon_i + \\hat{\\varepsilon}_i \\right)^2 L^2\\leq \\newline \u0026amp; \\leq \\left( 2(2\\varepsilon_i)^2 + 2(\\hat{\\varepsilon}_i - \\varepsilon_i)^2 \\right)^2 L^2 \\leq \\newline \u0026amp; \\leq (8 \\varepsilon_i^2 + 2 L^2) L^2 \\end{aligned} $$ Hence $$ \\mathbb E \\left[ \\left(\\varepsilon_i^2 - \\hat{\\varepsilon}_i^2 \\right)^2 \\right] \\leq L^2 \\left( 8 \\mathbb E_n [ \\varepsilon_i^2] + 2 \\mathbb E_n [L^2] \\right) \\xrightarrow{p}0 $$ $$\\tag*{$\\blacksquare$}$$  Heteroskedastic and Autocorrelated Error Term Assumption\nThere esists a $\\bar{d}$ such that:\n $\\mathbb E[\\varepsilon_i x_i \\varepsilon' _ {i-d} x' _ {i-d}] \\neq 0 \\quad$ for $d \\leq \\bar{d}$ $\\mathbb E[\\varepsilon_i x_i \\varepsilon' _ {i-d} x' _ {i-d}] = 0 \\quad$ for $d \u0026gt; \\bar{d}$   Intuition: observations far enough from each other are not correlated.\n We can express the variance of the score as $$ \\begin{aligned} \\Omega_n \u0026amp;= Var(\\sqrt{n} \\mathbb E_n[x_i \\varepsilon_i]) = \\newline \u0026amp;= \\mathbb E \\left[ \\left( \\frac{1}{n} \\sum _ {i=1}^n x_i \\varepsilon_i \\right) \\left( \\frac{1}{n} \\sum _ {j=1}^n x_j \\varepsilon_j \\right) \\right] = \\newline \u0026amp;= \\frac{1}{n} \\sum _ {i=1}^n \\sum _ {j=1}^n \\mathbb E[x_i \\varepsilon_i x_j' \\varepsilon_j'] = \\newline \u0026amp;= \\frac{1}{n} \\sum _ {i=1}^n \\sum _ {j : |i-j|\\leq \\bar{d}} \\mathbb E[x_i \\varepsilon_i x_j' \\varepsilon_j'] = \\newline \u0026amp;= \\frac{1}{n} \\sum _ {d=0}^{\\bar{d}} \\sum _ {i = d}^{n} \\mathbb E[x_i \\varepsilon_i x _ {i-d}' \\varepsilon _ {i-d}'] \\end{aligned} $$\nWe estimate $\\Omega_n$ by $$ \\hat{\\Omega}_n = \\frac{1}{n} \\sum _ {d=0}^{\\bar{d}} \\sum _ {i = d}^{n} x_i \\hat{\\varepsilon}_i x _ {i-d}' \\hat{\\varepsilon} _ {i-d}' $$\nTheorem\nIf $\\bar{d}$ is a fixed integer, then $$ \\hat{\\Omega}_n - \\Omega_n \\overset{p}{\\to} 0 $$\n What if $\\bar{d}$ does not exist (all $x_i, x_j$ are correlated)? $$ \\hat{\\Omega}_n = \\frac{1}{n} \\sum _ {d=0}^{n} \\sum _ {i = d}^{n} x_i \\hat{\\varepsilon}_i x _ {i-d}' \\hat{\\varepsilon} _ {i-d}' = n \\mathbb E_n[x_i \\hat{\\varepsilon}_i]^2 = 0 $$ By the orthogonality property of the OLS residual.\n HAC with Uniform Kernel $$ \\hat{\\Omega}_h = \\frac{1}{n} \\sum _ {i,j} x_i \\hat{\\varepsilon}_i x_j' \\hat{\\varepsilon}_j' \\mathbb{I} \\lbrace |i-j| \\leq h \\rbrace $$ where $h$ is the bandwidth of the kernel. The bandwidth is chosen such that $\\mathbb E[x_i \\varepsilon_i x _ {i-d}' \\varepsilon _ {i-d}' ]$ is small for $d \u0026gt; h$. How small? Small enough for the estimates to be consistent.\nHAC with General Kernel $$ \\hat{\\Omega}^{HAC} _ {k,h} = \\frac{1}{n} \\sum _ {i,j} x_i \\hat{\\varepsilon}_i x_j' \\hat{\\varepsilon}_j' k \\left( \\frac{|i-j|}{n} \\right) $$\nHAC Consistency Theorem If the joint distribution is stationary and $\\alpha$-mixing with $\\sum _ {k=1}^\\infty k^2 \\alpha(k) \u0026lt; \\infty$ and\n $\\mathbb E[ | x _ {ij} \\varepsilon_i |^\\nu ] \u0026lt; \\infty$ $\\forall \\nu$ $\\hat{\\varepsilon}_i = y_i - x_i' \\hat \\beta$ for some $\\hat \\beta \\overset{p}{\\to} \\beta_0$ $k$ smooth, symmetric, $k(0) \\to \\infty$ as $z \\to \\infty$, $\\int k^2 \u0026lt; \\infty$ $\\frac{h}{n} \\to 0$ $h \\to \\infty$  Then the HAC estimator is consistent. $$ \\hat{\\Omega}^{HAC} _ {k,h} - \\Omega_n \\overset{p}{\\to} 0 $$\nComments We want to choose $h$ small relative to $n$ in order to avoid estimation problems. But we also want to choose $h$ large so that the remainder is small: $$ \\begin{aligned} \\Omega_n \u0026amp;= Var(\\sqrt{n} \\mathbb E_n[x_i \\varepsilon_i]) = \\newline \u0026amp;= \\underbrace{\\frac{1}{n} \\sum _ {i,j : |i-j|\\leq h} \\mathbb E[x_i \\varepsilon_i x_j' \\varepsilon_j']} _ {\\Omega^h_n} + \\underbrace{\\frac{1}{n} \\sum _ {i,j : |i-j|\u0026gt; h} \\mathbb E[x_i \\varepsilon_i x_j' \\varepsilon_j']} _ {\\text{remainder: } R_n} = \\newline \u0026amp;= \\Omega_n^h + R_n \\end{aligned} $$\nIn particular, HAC theory requires: $$ \\hat{\\Omega}^{HAC} \\overset{p}{\\to} \\Omega \\quad \\text{ if } \\quad \\begin{cases} \u0026amp; \\frac{h}{n} \\to 0 \\newline \u0026amp; h \\to \\infty \\end{cases} $$\nBut in practice, long-run estimation implies $\\frac{h}{n} \\simeq 0$ which is not ``safe” in the sense that it does not imply $R_n \\simeq 0$. On the other hand, if $h \\simeq n$, $\\hat{\\Omega}^{HAC}$ does not converge in probability because it’s too noisy.\nChoice of h How to choose $h$? Look at the score autocorrelation function (ACF).\nIt looks like after 10 periods the empirical autocorrelation is quite small but still not zero.\nFixed b Asymptotics [Neave, 1970]: “When proving results on the asymptotic behavior of estimates of the spectrum of a stationary time series, it is invariably assumed that as the sample size $n$ tends to infinity, so does the truncation point $h$, but at a slower rate, so that $\\frac{h}{n}$ tends to zero. This is a convenient assumption mathematically in that, in particular, it ensures consistency of the estimates, but it is unrealistic when such results are used as approximations to the finite case where the value of $\\frac{h}{n}$ cannot be zero.””\nFixed b Theorem Theorem\nUnder regularity conditions, $$ \\sqrt{n} \\Big( V^{HAC} _ {k,h} \\Big)(\\hat \\beta - \\beta_0) \\overset{d}{\\to} F $$\nThe asymptotic critical values of the $F$ statistic depend on the choice of the kernel. In order to do hypothesis testing, Kiefer and Vogelsang(2005) provide critical value functions for the t-statistic for each kernel-confidence level combination using a cubic equation: $$ cv(b) = a_0 + a_1 b + a_2 b^2 + a_3 b^3 $$\nExample Example for the Bartlett kernel:\nFixed G Asymptotics [Bester, 2013]: “Cluster covariance estimators are routinely used with data that has a group structure with independence assumed across groups. Typically, inference is conducted in such settings under the assumption that there are a large number of these independent groups.””\n“However, with enough weakly dependent data, we show that groups can be chosen by the researcher so that group-level averages are approximately independent. Intuitively, if groups are large enough and well shaped (e.g. do not have gaps), the majority of points in a group will be far from other groups, and hence approximately independent of observations from other groups provided the data are weakly dependent. The key prerequisite for our methods is the researcher’s ability to construct groups whose averages are approximately independent. As we show later, this often requires that the number of groups be kept relatively small, which is why our main results explicitly consider a fixed (small) number of groups.””\nAssumption Assumption Suppose you have data $D = (y _ {it} , x _ {it}) _ {i=1, t=1}^{N, T}$ where $y _ {it} = x _ {it}' \\beta + \\alpha_i + \\varepsilon _ {it}$ where $i$ indexes the observational unit and $t$ indexes time (could also be space).\nLet $$ \\begin{aligned} \u0026amp; \\tilde{y} _ {it} = y _ {it} - \\frac{1}{T} \\sum _ {t=1}^T y _ {it} \\newline \u0026amp; \\tilde{x} _ {it} = x _ {it} - \\frac{1}{T} \\sum _ {t=1}^T x _ {it} \\newline \u0026amp; \\tilde{\\varepsilon} _ {it} = \\varepsilon _ {it} - \\frac{1}{T} \\sum _ {t=1}^T \\varepsilon _ {it} \\end{aligned} $$ Then $$ \\tilde{y} _ {it} = \\tilde{x} _ {it}' \\beta + \\tilde{\\varepsilon} _ {it} $$\nThe $\\tilde{\\varepsilon} _ {it}$ are by construction correlated between each other even if the original $\\varepsilon$ was iid. The cluster score variance estimator is given by: $$ \\hat{\\Omega}^{CL} = \\frac{1}{T-1} \\sum _ {i=1}^n \\sum _ {t=1}^T \\sum _ {s=1}^T \\tilde{x} _ {it} \\hat{\\tilde{\\varepsilon}} _ {it} \\tilde{x} _ {is} \\hat{\\tilde{\\varepsilon}} _ {is} $$\n It’s very similar too the HAC estimator since we have dependent cross-products here as well. However, here we do not consider the $i \\times j$ cross-products. We only have time-dependency (state).\n Comments (1) On $T$ and $n$:\n If $T$ is fixed and $n \\to \\infty$, then the number of cross-products considered is much smaller than the total number of cross-products. If $T \u0026raquo; n$ issues arise since the number of cross products considered is close to the total number of cross products. As in HAC estimation, this is a problem because it implies that the algebraic estimate of the cluster score variance gets close to zero because of the orthogonality property of the residuals. The panel assumption is that observations across individuals are not correlated.   Strategy: as in HAC, we want to limit the correlation across clusters (individuals). We hope that observations are negligibly dependent between cluster sufficiently distant from each other.\n Comments (2) Classical cluster robust estimator: $$ \\hat{\\Omega}^{CL} = \\frac{1}{n} \\sum _ {i=1}^n x_i \\varepsilon_i x_j' \\varepsilon_j' \\mathbb{I} \\lbrace i,j \\text{ in the same cluster} \\rbrace $$\n On clusters:\n If the number of observations near a boundary is small relative to the sample size, ignoring the dependence should not affect inference too adversely. The higher the dimension of the data, the easier it is to have observations near boundaries (curse of dimensionality). We would like to have few clusters in order to make less independence assumptions. However, few clusters means bigger blocks and hence a larger number of cross-products to estimate. If the number of cross-products is too large (relative to the sample size), $\\hat{\\Omega}^{CL}$ does not converge   Theorem: Under regularity conditions: $$ \\hat{t} \\overset{d}{\\to} \\sqrt{\\frac{G}{G-1}} t _ {G-1} $$\nCode - DGP This code draws 100 observations from the model $y = 2 x_1 - x_2 + \\varepsilon$ where $x_1, x_2 \\sim U[0,1]$ and $\\varepsilon \\sim N(0,1)$.\n# Set seed Random.seed!(123); # Set the number of observations n = 100; # Set the dimension of X k = 2; # Draw a sample of explanatory variables X = rand(Uniform(0,1), n, k); # Draw the error term σ = 1; ε = rand(Normal(0,1), n, 1) * sqrt(σ); # Set the parameters β = [2; -1]; # Calculate the dependent variable y = X*β + ε;  Ideal Estimate # OLS estimator β_hat = (X'*X)\\(X'*y); # Residuals ε_hat = y - X*β_hat; # Homoskedastic standard errors std_h = var(ε_hat) * inv(X'*X); # Projection matrix P = X * inv(X'*X) * X'; # Leverage h = diag(P);  HC Estimates # HC0 variance and standard errors Ω_hc0 = X' * (I(n) .* ε_hat.^2) * X; std_hc0 = sqrt.(diag(inv(X'*X) * Ω_hc0 * inv(X'*X)))  ## 2-element Array{Float64,1}: ## 0.24691300271914793 ## 0.28044707935951835  # HC1 variance and standard errors Ω_hc1 = n/(n-k) * X' * (I(n) .* ε_hat.^2) * X; std_hc1 = sqrt.(diag(inv(X'*X) * Ω_hc1 * inv(X'*X)))  ## 2-element Array{Float64,1}: ## 0.24941979797977423 ## 0.2832943308272532  # HC2 variance and standard errors Ω_hc2 = X' * (I(n) .* ε_hat.^2 ./ (1 .- h)) * X; std_hc2 = sqrt.(diag(inv(X'*X) * Ω_hc2 * inv(X'*X)))  ## 2-element Array{Float64,1}: ## 0.2506509902982869 ## 0.2850878737103963  # HC3 variance and standard errors Ω_hc3 = X' * (I(n) .* ε_hat.^2 ./ (1 .- h).^2) * X; std_hc3 = sqrt.(diag(inv(X'*X) * Ω_hc3 * inv(X'*X)))  ## 2-element Array{Float64,1}: ## 0.25446321015850176 ## 0.2898264779289438  # Note what happens if you allow for full autocorrelation omega_full = X'*ε_hat*ε_hat'*X;  Inference Hypothesis Testing In order to do inference on $\\hat \\beta$ we need to know its distribution. We have two options: (i) assume gaussian error term (extended GM) or (ii) rely on asymptotic approximations (CLT).\nA statistical hypothesis is a subset of a statistical model, $\\mathcal K \\subset \\mathcal F$. A hypothesis test is a map $\\mathcal D \\rightarrow \\lbrace 0,1 \\rbrace$, $D \\mapsto T$. If $\\mathcal F$ is the statistical model and $\\mathcal K$ is the statistical hypothesis, we use the notation $H_0: \\Pr \\in \\mathcal K$.\n Generally, we are interested in understanding whether it is likely that data $D$ are drawn from $\\mathcal K$ or not.\n A hypothesis test, $T$ is our tool for deciding whether the hypothesis is consistent with the data. $T(D)= 0$ implies fail to reject $H_0$ and test inconclusive $T(D)=1$ $\\implies$ reject $H_0$ and $D$ is inconsistent with any $\\Pr \\in \\mathcal K$.\nLet $\\mathcal K \\subseteq \\mathcal F$ be a statistical hypothesis and $T$ a hypothesis test.\n Suppose $\\Pr \\in \\mathcal K$. A Type I error (relative to $\\Pr$) is an event $T(D)=1$ under $\\Pr$. Suppose $\\Pr \\in \\mathcal K^c$. A Type II error (relative to $\\Pr$) is an event $T(D)=0$ under $\\Pr$.  The corresponding probability of a type I error is called size. The corresponding probability of a type II error is called power (against the alternative $\\Pr$).\nIn this section, we are interested in testing three hypotheses, under the assumptions of linearity, strict exogeneity, no multicollinearity, normality on the error term. They are:\n $H_0: \\beta _ {0k} = \\bar \\beta _ {0k}$ (single coefficient, $\\bar \\beta _ {0k} \\in \\mathbb R$, $k \\leq K$) $a' \\beta_0 = c$ (linear combination, $a \\in \\mathbb R^K, c \\in \\mathbb R$) $R \\beta_0 = r$ (linear restrictions, $R \\in \\mathbb R^{p \\times K}$, full rank, $r \\in \\mathbb R^p$)  Testing Problem Consider the testing problem $H_0: \\beta _ {0k} = \\bar \\beta _ {0k}$ where $\\bar \\beta _ {0k}$ is a pre-specified value under the null. The t-statistic for this problem is defined by $$ t_k:= \\frac{b_k - \\bar \\beta _ {0k}}{SE(b_k)}, \\ \\ SE(b_k):= \\sqrt{s^2 [(X\u0026rsquo;X)^{-1}] _ {kk}} $$\nTheorem: In the testing procedure above, the sampling distribution under the null $H_0$ is given by $$ t_k|X \\sim t _ {n-k} \\ \\ \\text{and so} \\ \\ t_k \\sim t _ {n-k} $$\n$t _ {(n-K)}$ denotes the t-distribution with $(n-k)$ degress of freedom. The test can be one sided or two sided. The above sampling distribution can be used to construct a confidence interval.\nExample We want to asses whether or not the ``true” coefficient $\\beta_0$ equals a specific value $\\hat \\beta$. Specifically, we are interested in testing $H_0$ against $H_1$, where:\n Null Hypothesis: $H_0: \\beta_0 = \\hat \\beta$ Alternative Hypothesis: $H_1: \\beta_0 \\ne \\hat \\beta$.  Hence, we are interested in a statistic informative about $H_1$, which is the Wald test statistic $$ |T^*| = \\bigg| \\frac{\\hat \\beta - \\beta_0}{\\sigma(\\hat \\beta)}\\bigg| \\sim N(0,1) $$\nHowever, the true variance $\\sigma^2(\\hat \\beta )$ is not known and has to be estimated. Therefore we plug in the sample variance $\\hat \\sigma^2(\\hat \\beta) = \\frac{n}{n-1} \\mathbb E_n[\\hat e_i^2]$ and we use $$ |T| = \\bigg| \\frac{\\hat \\beta - \\beta_0}{\\hat \\sigma (\\hat \\beta)}\\bigg| \\sim t _ {(n-k)} $$\nComments Hypothesis testing is like proof by contradiction. Imagine the sampling distribution was generated by $\\beta$. If it is highly improbable to observe $\\hat \\beta$ given $\\beta_0 = \\beta$ then we reject the hypothesis that the sampling distribution was generated by $\\beta$.\nThen, given a realized value of the statistic $|T|$, we take the following decision:\n Do not reject $H_0$: it is consistent with random variation under true $H_0$—i.e., $|T|$ small as it has an exact student t distribution with $(n-k)$ degree of freedom in the normal regression model. Reject $H_0$ in favor of $H_1$: $|T| \u0026gt; c$, with $c$ being the critical values selected to control for false rejections: $\\Pr(|t _ {n-k}| \\geq c) = \\alpha$. Moreover, you can also reject $H_0$ if the p-value $p$ is such that: $p \u0026lt; \\alpha$.  Comments (2) The probability of false rejection is decreasing in $c$, i.e. the critical value for a given significant level. $$ \\begin{aligned} \\Pr (\\text{Reject } H_0 | H_0) \u0026amp; = \\Pr (|T|\u0026gt; c | H_0 ) = \\newline \u0026amp; = \\Pr (T \u0026gt; c | H_0 ) + \\Pr (T \u0026lt; -c | H_0 ) = \\newline \u0026amp; = 1 - F(c) + F(-c) = 2(1-F(c)) \\end{aligned} $$\nExample: Consider the testing problem $H_0: a'\\beta_0=c$ where $a$ is a pre-specified linear combination under study. The t-statistic for this problem is defined by: $$ t_k:= \\frac{a\u0026rsquo;b - c}{SE(a\u0026rsquo;b)}, \\ \\ SE(a\u0026rsquo;b):= \\sqrt{s^2 a'(X\u0026rsquo;X)^{-1}a} $$\nt Stat Theorem\nIn the testing procedure above, the sampling distribution under the null $H_0$ is given by $$ t_a|X \\sim t _ {n-K} \\quad\\text{and so} \\quad t_a \\sim t _ {n-K} $$\nLike in the previous test, $t _ {(n-K)}$ denotes the t-distribution with $(n-K)$ degress of freedom. The test can again be one sided or two sided. The above sampling distribution can be used to construct a confidence interval\nF Stat Example\nConsider the testing problem $$ H_0: R \\beta_0 = r $$ where $R \\in \\mathbb R^{p \\times k}$ is a presepecified set of linear combinations and $r \\in \\mathbb R^p$ is a restriction vector.\nThe F-statistic for this problem is given by $$ F:= \\frac{(Rb-r)'[R(X\u0026rsquo;X)R']^{-1}(Rb-r)/p }{s^2} $$\nTheorem\nFor the problem, the sampling distribution of the F-statistic under the null $H_0:$ $$ F|X \\sim F _ {p,n-K} \\ \\ \\text{and so} \\ \\ F \\sim F _ {p,n-K} $$\nThe test is intrinsically two-sided. The above sampling distribution can be used to construct a confidence interval.\nEquivalence Theorem\nConsider the testing problem $H_0: R \\beta_0 = r$ where $R \\in \\mathbb R^{p\\times K}$ is a presepecified set of linear combinations and $r \\in \\mathbb R^p$ is a restriction vector.\nConsider the restricted least squares estimator, denoted $\\hat \\beta_R$: $\\hat \\beta_R: = \\text{arg} \\min _ { \\beta: R \\beta = r } Q( \\beta)$. Let $SSR_U = Q(b), \\ \\ SSR_R=Q(\\hat \\beta_R)$. Then the $F$ statistic is numerically equivalent to the following expression: $F = \\frac{(SSR_R - SSR_U)/p}{SSR_U/(n-K)}$.\nConfidence Intervals A confidence interval at $(1-\\alpha)$ is a random set $C$ such that $$ \\Pr(\\beta_0 \\in C) \\geq 1- \\alpha $$ i.e. the probability that $C$ covers the true value $\\beta$ is fixed at $(1-\\alpha)$.\nSince $C$ is not known, it has to be estimated ($\\hat{C}$). We construct confidence intervals such that:\n they are symmetric around $\\hat \\beta$; their length is proportional to $\\sigma(\\hat \\beta) = \\sqrt{Var(\\hat \\beta)}$.  A CI is equivalent to the set of parameter values such that the t-statistic is less than $c$, i.e., $$ \\hat{C} = \\bigg\\lbrace \\beta: |T(\\beta) | \\leq c \\bigg\\rbrace = \\bigg\\lbrace \\beta: - c\\leq \\frac{\\beta - \\hat \\beta}{\\sigma(\\hat \\beta)} \\leq c \\bigg\\rbrace $$\nIn practice, to construct a 95% confidence interval for a single coefficient estimate $\\hat \\beta_j$, we use the fact that $$ \\Pr \\left( \\frac{| \\hat \\beta_j - \\beta _ {0,j} |}{ \\sqrt{\\sigma^2 [(X\u0026rsquo;X)^{-1}] _ {jj} }} \u0026gt; 1.96 \\right) = 0.05 $$\nCode # t-test for beta=0 t = abs.(β_hat ./ (std_hc1)); # p-value p_val = 1 .- cdf.(Normal(0,1), t); # F statistic of joint significance SSR_u = ε_hat'*ε_hat; SSR_r = y'*y; F = (SSR_r - SSR_u)/k / (SSR_u/(n-k)); # 95# confidente intervals conf_int = [β_hat - 1.96*std_hc1, β_hat + 1.96*std_hc1];  ","date":1635465600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1635465600,"objectID":"3a3dba8681825325823978a98a7af4ac","permalink":"https://matteocourthoud.github.io/course/metrics/06_ols_inference/","publishdate":"2021-10-29T00:00:00Z","relpermalink":"/course/metrics/06_ols_inference/","section":"course","summary":"Asymptotic Theory of the OLS Estimator OLS Consistency Theorem: Assume that $(x_i, y_i) _ {i=1}^n$ i.i.d. , $\\mathbb E[x_i x_i'] = Q$ positive definite, $\\mathbb E[x_i x_i'] \u0026lt; \\infty$ and $\\mathbb E [y_i^2] \u0026lt; \\infty$, then $\\hat \\beta _ {OLS}$ is a consistent estimator of $\\beta_0$, i.","tags":null,"title":"OLS Inference","type":"book"},{"authors":null,"categories":null,"content":"import os import re import time import requests import pandas as pd from bs4 import BeautifulSoup from pprint import pprint from selenium import webdriver  There is no silver bullet to getting info from the internet. The coding requirements in these notes start easy and will gradually become more demanding. We will cover the following web scraping techniques:\n Pandas APIs Scraping static webpages with BeautifulSoup Scraping dynamic wepages with Selenium  Pandas The Pandas library has a very useful webscraping command: read_html. The read_html command works for webpages that contain tables that are particularly well behaved. Let\u0026rsquo;s see an example: https://en.wikipedia.org/wiki/List_of_countries_by_GDP_(nominal)\nAt first glance, it seems that there are three tables in this Wikipedia page:\n data from the IMF data from the World Bank data from the UN  Let\u0026rsquo;s see which tables pandas recognizes.\n# Scrape all tables from Wikipedia page url = 'https://en.wikipedia.org/wiki/List_of_countries_by_GDP_(nominal)' df_list = pd.read_html(url) # Check number of tables on the page print(len(df_list))  7  Apparently Pandas has found 10 tables in this webpage. Let\u0026rsquo;s see what is their content.\n# Check headers of each table for df in df_list: print(df.shape)  (1, 1) (1, 3) (216, 9) (9, 2) (7, 2) (13, 2) (2, 2)  It seems that pandas has found many more tables that we could see. The ones that are of interest to us are probably the 3rd, 4th and 5th. But that are the others? Let\u0026rsquo;s look at the them.\n# Check first df_list[0].head()   .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }  \n  0     0 Largest economies by nominal GDP in 2021[1]     # Check second df_list[1].head()   .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }  \n  0 1 2     0 .mw-parser-output .legend{page-break-inside:av... $750 billion – $1 trillion $500–50 billion $25... $50–100 billion $25–50 billion $5–25 billion \u0026lt;...     Apparently, the first two are simply picture captions.\n# Check third df_list[2].head()   .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead tr th { text-align: left; }  \n   Country/Territory Subregion Region IMF[1] United Nations[12] World Bank[13][14]    Country/Territory Subregion Region Estimate Year Estimate Year Estimate Year     0 United States Northern America Americas 22939580.0 2021 20893746.0 2020 20936600.0 2020   1 China Eastern Asia Asia 16862979.0 [n 2]2021 14722801.0 [n 3]2020 14722731.0 2020   2 Japan Eastern Asia Asia 5103110.0 2021 5057759.0 2020 4975415.0 2020   3 Germany Western Europe Europe 4230172.0 2021 3846414.0 2020 3806060.0 2020   4 United Kingdom Western Europe Europe 3108416.0 2021 2764198.0 2020 2707744.0 2020     This is clearly what we were looking for. A part from the footnotes, the table is already clean and organized.\nIf we knew the name of the table, we could directly retrieve it. However, we will see more about it in the next lecture.\nSpecific Libraries Sometimes, there are libraries that are already written down to do the scraping for you. Each one is tailored for a specific website and they are usually userwritten and prone to bugs and errors. However, they are often efficient and save you the time to worry about getting around some website-specific issues.\nOne example is the pytrends library for scraping Google Trends. Let\u0026rsquo;s first install it\npip3 install pytrends  Let\u0026rsquo;s see how it works. Imagine we want to do the following search:\n words \u0026ldquo;python\u0026rdquo;, \u0026ldquo;matlab\u0026rdquo;, \u0026ldquo;stata\u0026rdquo; the the second half of in 2019 daily in the US  We can get more details on how pytrends works here. The important thing to know is that if you query a time period of more than 200 days, Google will give you weekly results, instead of daily.\n# Pytrends search from pytrends.request import TrendReq # Set parameters words = ['python', 'matlab', 'stata'] timeframe = '2019-07-01 2019-12-31' country = 'US' # Get data pytrend = TrendReq() pytrend.build_payload(kw_list=words, timeframe=timeframe, geo=country) df_trends = pytrend.interest_over_time() # Plot trends_plot = df_trends.plot.line()  Apparently people don\u0026rsquo;t code during the weekend\u0026hellip;.\nAPIs From Wikipedia\n An application programming interface (API) is an interface or communication protocol between different parts of a computer program intended to simplify the implementation and maintenance of software.\n In practice, it means that the are some webpages that are structured not to be user-readable but to be computer-readable. Let\u0026rsquo;s see one example.\nGoogle provides many APIs for its services. However, they now all need identification, which means that you have to log in into your Google account and request an API key from there. This allows Google to monitor your behavior since the number of API requests is limited and beyond a certain treshold, one need to pay (a lot).\nThere are however some free APIs. One\nLet\u0026rsquo;s have a look at one of these: zippopotam. Zippopotam lets you retrieve location information from a zip code in the US. Other countries are supported as well.\n# Let's search the department locatiton import requests zipcode = '90210' url = 'https://api.zippopotam.us/us/'+zipcode response = requests.get(url) data = response.json() data  {'post code': '90210', 'country': 'United States', 'country abbreviation': 'US', 'places': [{'place name': 'Beverly Hills', 'longitude': '-118.4065', 'state': 'California', 'state abbreviation': 'CA', 'latitude': '34.0901'}]}  Data is in JSON (JavaScript Object Notation) format which is basically a nested dictionary-list format. Indeed, we see that in our case, data is a dictionary where the last elements is a list with one element - another dictionary.\n# Check type of value for d in data.values(): print(type(d)) # Check list length print(len(data['places'])) # Check type of content of list print(type(data['places'][0]))  \u0026lt;class 'str'\u0026gt; \u0026lt;class 'str'\u0026gt; \u0026lt;class 'str'\u0026gt; \u0026lt;class 'list'\u0026gt; 1 \u0026lt;class 'dict'\u0026gt;  The part that could be interesting to us is contained in the places category. We can easily extract it and transform it into a dataframe.\n# Add zipcode to data data['places'][0]['zipcode'] = zipcode # Export data df = pd.DataFrame(data['places']) df   .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }  \n  place name longitude state state abbreviation latitude zipcode     0 Beverly Hills -118.4065 California CA 34.0901 90210     Static Webscraping We have so far used pre-made tools in order to do web-scraping. When the website contains the data in a nice table or an API is available, we do not need to worry much and we can directly retrieve the data. However, most of web scraping is much more complicated. Data is often the product of webscraping and is not readily available. Moreover, sometimes webscraping knowledge can supplement the need to pay for an API.\nHTTP What happens when you open a page on the internet? In short, your web browser is sending a request to the website that, in turn, sends back a reply/response. The exchange of messages is complex but its core involves a HyperText Transfer Protocol (HTTP) request message to a web server, followed by a HTTP response (or reply). All static webscraping is build on HTTP so let\u0026rsquo;s have a closer look.\nAn HTTP message essentially has 4 components:\n A request line A number of request headers An empty line An optional message  Example\nA request message could be\nGET /hello.htm HTTP/1.1  The response would be\nHTTP/1.1 200 OK Date: Sun, 10 Oct 2010 23:26:07 GMT Server: Apache/2.2.8 (Ubuntu) mod_ssl/2.2.8 OpenSSL/0.9.8g Last-Modified: Sun, 26 Sep 2010 22:04:35 GMT ETag: \u0026quot;45b6-834-49130cc1182c0\u0026quot; Accept-Ranges: bytes Content-Length: 12 Connection: close Content-Type: text/html \u0026lt;html\u0026gt; \u0026lt;body\u0026gt; \u0026lt;h1\u0026gt;Hello, World!\u0026lt;/h1\u0026gt; \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt;  So, in this case the parts are:\n The request line  HTTP/1.1 200 OK  The request headers  Date: Sun, 10 Oct 2010 23:26:07 GMT Server: Apache/2.2.8 (Ubuntu) mod_ssl/2.2.8 OpenSSL/0.9.8g Last-Modified: Sun, 26 Sep 2010 22:04:35 GMT ETag: \u0026quot;45b6-834-49130cc1182c0\u0026quot; Accept-Ranges: bytes Content-Length: 12 Connection: close Content-Type: text/html  The empty line The optional message  \u0026lt;html\u0026gt; \u0026lt;body\u0026gt; \u0026lt;h1\u0026gt;Hello, World!\u0026lt;/h1\u0026gt; \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt;  We are interested in the optional message, which is essentially the content of the page we want to scrape. The content is usually written in HTML which is not a proper programming language but rather a typesetting language since it is the language underlying web pages and is usually generated from other programming languages.\nRequests There are many different packages in python to send requests to a web page and read its response. The most user-friendly is the requests package. You can find plenty of useful information on the requests library on its website: https://requests.readthedocs.io/en/master/.\nWe are now going to have a look at a simple example: http://pythonscraping.com/pages/page1.html.\n# Request a simple web page url1 = 'http://pythonscraping.com/pages/page1.html' response = requests.get(url1) print(response)  \u0026lt;Response [200]\u0026gt;  We are (hopefully) getting a \u0026lt;Response [200]\u0026gt; message. In short, what we got is the status code of the request we sent to the website. The status code is a 3-digit code and essentially there are two broad categories of status codes:\n 2XX: success 4XX, 5XX: failure  It can be useful to know this codes as they are a fast way to check whether your request has failed or not. When webscraping the most common reasons you get an error are\n The link does not exist: wither the link is old/expired or you misspelled it and hence there is no page to request You have been \u0026ldquo;caught\u0026rdquo;. This is pretty common when webscraping and happens every time you are too aggressive with your scraping. How much \u0026ldquo;aggressive\u0026rdquo; is \u0026ldquo;too agrressive\u0026rdquo; depends on the website. Usually big tech websites are particularly hard to scrape and anything that is \u0026ldquo;faster than human\u0026rdquo; gets blocked. Sometimes also slow but persistent requests get blocked as well.  We have now analyzed the response status but, what is actually the response content? Let\u0026rsquo;s inspect the response object more in detail.\n# Print response attributes dir(response)  ['__attrs__', '__bool__', '__class__', '__delattr__', '__dict__', '__dir__', '__doc__', '__enter__', '__eq__', '__exit__', '__format__', '__ge__', '__getattribute__', '__getstate__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__iter__', '__le__', '__lt__', '__module__', '__ne__', '__new__', '__nonzero__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__setstate__', '__sizeof__', '__str__', '__subclasshook__', '__weakref__', '_content', '_content_consumed', '_next', 'apparent_encoding', 'close', 'connection', 'content', 'cookies', 'elapsed', 'encoding', 'headers', 'history', 'is_permanent_redirect', 'is_redirect', 'iter_content', 'iter_lines', 'json', 'links', 'next', 'ok', 'raise_for_status', 'raw', 'reason', 'request', 'status_code', 'text', 'url']  We are actually interested in the text of the response.\n# Print response content response.text  '\u0026lt;html\u0026gt;\\n\u0026lt;head\u0026gt;\\n\u0026lt;title\u0026gt;A Useful Page\u0026lt;/title\u0026gt;\\n\u0026lt;/head\u0026gt;\\n\u0026lt;body\u0026gt;\\n\u0026lt;h1\u0026gt;An Interesting Title\u0026lt;/h1\u0026gt;\\n\u0026lt;div\u0026gt;\\nLorem ipsum dolor sit amet, consectetur adipisicing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollit anim id est laborum.\\n\u0026lt;/div\u0026gt;\\n\u0026lt;/body\u0026gt;\\n\u0026lt;/html\u0026gt;\\n'  This is the whole content of the table. There is a large chunk of text and other parts which look more obscure. In order to understand the structure of the page, we need to have a closer look at the language in which the webpage is written: HTML. We will do it in the next section.\nHowever, let\u0026rsquo;s first analyze the other relevant components of the response. We have already had a look at the status. Let\u0026rsquo;s inspect the headers.\n# Print response headers response.headers  {'Server': 'nginx', 'Date': 'Thu, 10 Feb 2022 11:11:41 GMT', 'Content-Type': 'text/html', 'Content-Length': '361', 'Connection': 'keep-alive', 'X-Accel-Version': '0.01', 'Last-Modified': 'Sat, 09 Jun 2018 19:15:58 GMT', 'ETag': '\u0026quot;234-56e3a58a63780-gzip\u0026quot;', 'Accept-Ranges': 'bytes', 'Vary': 'Accept-Encoding', 'Content-Encoding': 'gzip', 'X-Powered-By': 'PleskLin'}  From the headers we can see\n the present date the name of the server hosting the page the last time the page was modified other stuff  Let\u0026rsquo;s now look at the headers of our request.\n# Request headers def check_headers(r): test_headers = dict(zip(r.request.headers.keys(), r.request.headers.values())) pprint(test_headers) check_headers(response)  {'Accept': '*/*', 'Accept-Encoding': 'gzip, deflate, br', 'Connection': 'keep-alive', 'User-Agent': 'python-requests/2.27.1'}  The headers of our request are pretty minimal. In order to see what normal headers look like, go to https://www.whatismybrowser.com/developers/what-http-headers-is-my-browser-sending\nNormal headers look something like:\n{'Accept': 'text/html,application/xhtml+xml,application/xml;q = 0.9, image / ' 'webp, * / *;q = 0.8', 'Accept-Encoding': 'gzip, deflate, br', 'Accept-Language': 'en-US,en;q=0.9,it-IT;q=0.8,it;q=0.7,de-DE;q=0.6,de;q=0.5', 'Connection': 'keep-alive', 'Host': 'www.whatismybrowser.com', 'Referer': 'http://localhost:8888/', 'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_14_6) ' 'AppleWebKit/537.36 (KHTML, like Gecko) Chrome/79.0.3945.88 ' 'Safari/537.36'}  The most important difference is that the requests model default User-Agent is python-requests/2.22.0 which means that we are walking around the web with a big WARNING: web scrapers sign. This is the simplest way to get caught and blocked by a website. Luckily, we can easily change our headers in order to be more subtle.\n# Change headers headers = {\u0026quot;User-Agent\u0026quot;: \u0026quot;Mozilla/5.0\u0026quot;, \u0026quot;Accept\u0026quot;: \u0026quot;webp, * / *;q = 0.8\u0026quot;, \u0026quot;Accept-Language\u0026quot;: \u0026quot;en-US,en;q=0.9\u0026quot;, \u0026quot;Accept-Encoding\u0026quot;: \u0026quot;br, gzip, deflate\u0026quot;, \u0026quot;Referer\u0026quot;: \u0026quot;https://www.google.ch/\u0026quot;} # Test if change worked response = requests.get(url1, headers=headers) check_headers(response)  {'Accept': 'webp, * / *;q = 0.8', 'Accept-Encoding': 'br, gzip, deflate', 'Accept-Language': 'en-US,en;q=0.9', 'Connection': 'keep-alive', 'Referer': 'https://www.google.ch/', 'User-Agent': 'Mozilla/5.0'}  Nice! Now we are a little more stealthy.\nYou might now be asking yourself what are the ethical limits of webscraping. Information on the internet is public but scraping a website imposes a workload on the website\u0026rsquo;s server. If the website is not protected against aggressive scrapers (most websites are), your activity could significantly slower the website or even crash it.\nUsually websites include their policies for scraping in a text file named robots.txt.\nLet\u0026rsquo;s have a look at the robots.txt file of http://pythonscraping.com/.\n# Read robots.txt response = requests.get('http://pythonscraping.com/robots.txt') print(response.text)  # # robots.txt # # This file is to prevent the crawling and indexing of certain parts # of your site by web crawlers and spiders run by sites like Yahoo! # and Google. By telling these \u0026quot;robots\u0026quot; where not to go on your site, # you save bandwidth and server resources. # # This file will be ignored unless it is at the root of your host: # Used: http://example.com/robots.txt # Ignored: http://example.com/site/robots.txt # # For more information about the robots.txt standard, see: # http://www.robotstxt.org/robotstxt.html # # For syntax checking, see: # http://www.frobee.com/robots-txt-check User-agent: * Crawl-delay: 10 # Directories Disallow: /includes/ Disallow: /misc/ Disallow: /modules/ Disallow: /profiles/ Disallow: /scripts/ Disallow: /themes/ # Files Disallow: /CHANGELOG.txt Disallow: /cron.php Disallow: /INSTALL.mysql.txt Disallow: /INSTALL.pgsql.txt Disallow: /INSTALL.sqlite.txt Disallow: /install.php Disallow: /INSTALL.txt Disallow: /LICENSE.txt Disallow: /MAINTAINERS.txt Disallow: /update.php Disallow: /UPGRADE.txt Disallow: /xmlrpc.php # Paths (clean URLs) Disallow: /admin/ Disallow: /comment/reply/ Disallow: /filter/tips/ Disallow: /node/add/ Disallow: /search/ Disallow: /user/register/ Disallow: /user/password/ Disallow: /user/login/ Disallow: /user/logout/ # Paths (no clean URLs) Disallow: /?q=admin/ Disallow: /?q=comment/reply/ Disallow: /?q=filter/tips/ Disallow: /?q=node/add/ Disallow: /?q=search/ Disallow: /?q=user/password/ Disallow: /?q=user/register/ Disallow: /?q=user/login/ Disallow: /?q=user/logout/  As we can see, this robots.txt file mostly deals with crawlers, i.e. scripts that are designed to recover the structure of a website by exploring it. Crawlers are mostly used by browsers that want to index websites.\nNow we have explored most of the issues around HTTP requests. We can now proceed to what we are interested in: the content of the web page. In order to do that, we need to know the language in which wabpages are written: HTML.\nHTML Hypertext Markup Language (HTML) is the standard markup language for documents designed to be displayed in a web browser. Web browsers receive HTML documents from a web server or from local storage and render the documents into multimedia web pages. HTML describes the structure of a web page semantically and originally included cues for the appearance of the document.\nHTML elements are delineated by tags, written using angle brackets.\nTags Tags are the cues that HTML uses to surround content and provide information about its nature. There is a very large amount of tags but some of the most common are:\n \u0026lt;head\u0026gt; and \u0026lt;body\u0026gt; for head and body of the page \u0026lt;p\u0026gt; for paragraphs \u0026lt;br\u0026gt; for line breaks \u0026lt;table\u0026gt; for tables. These are the ones that pandas reads. However, we have seen that not all elements that look like tables are actually \u0026lt;table\u0026gt; and viceversa. Table elements are tagged as \u0026lt;th\u0026gt; (table header), \u0026lt;tr\u0026gt; (table row) and \u0026lt;td\u0026gt; (table data: a cell) \u0026lt;img\u0026gt; for images \u0026lt;h1\u0026gt; to \u0026lt;h6\u0026gt; for headers (titles and subtitles) \u0026lt;div\u0026gt; dor divisions, i.e. for grouping elements \u0026lt;a\u0026gt; for hyperlinks \u0026lt;ul\u0026gt; and \u0026lt;ol\u0026gt; for unordered and ordered lists where list elements are tagged as \u0026lt;li\u0026gt;  Let\u0026rsquo;s have a look at the previous page\n# Inspect HTML response.text  '#\\n# robots.txt\\n#\\n# This file is to prevent the crawling and indexing of certain parts\\n# of your site by web crawlers and spiders run by sites like Yahoo!\\n# and Google. By telling these \u0026quot;robots\u0026quot; where not to go on your site,\\n# you save bandwidth and server resources.\\n#\\n# This file will be ignored unless it is at the root of your host:\\n# Used: http://example.com/robots.txt\\n# Ignored: http://example.com/site/robots.txt\\n#\\n# For more information about the robots.txt standard, see:\\n# http://www.robotstxt.org/robotstxt.html\\n#\\n# For syntax checking, see:\\n# http://www.frobee.com/robots-txt-check\\n\\nUser-agent: *\\nCrawl-delay: 10\\n# Directories\\nDisallow: /includes/\\nDisallow: /misc/\\nDisallow: /modules/\\nDisallow: /profiles/\\nDisallow: /scripts/\\nDisallow: /themes/\\n# Files\\nDisallow: /CHANGELOG.txt\\nDisallow: /cron.php\\nDisallow: /INSTALL.mysql.txt\\nDisallow: /INSTALL.pgsql.txt\\nDisallow: /INSTALL.sqlite.txt\\nDisallow: /install.php\\nDisallow: /INSTALL.txt\\nDisallow: /LICENSE.txt\\nDisallow: /MAINTAINERS.txt\\nDisallow: /update.php\\nDisallow: /UPGRADE.txt\\nDisallow: /xmlrpc.php\\n# Paths (clean URLs)\\nDisallow: /admin/\\nDisallow: /comment/reply/\\nDisallow: /filter/tips/\\nDisallow: /node/add/\\nDisallow: /search/\\nDisallow: /user/register/\\nDisallow: /user/password/\\nDisallow: /user/login/\\nDisallow: /user/logout/\\n# Paths (no clean URLs)\\nDisallow: /?q=admin/\\nDisallow: /?q=comment/reply/\\nDisallow: /?q=filter/tips/\\nDisallow: /?q=node/add/\\nDisallow: /?q=search/\\nDisallow: /?q=user/password/\\nDisallow: /?q=user/register/\\nDisallow: /?q=user/login/\\nDisallow: /?q=user/logout/\\n'  The response looks a little bit messy and not really readable.\nBeautifulSoup is a python library that renders http responses in a user friendly format and helps recovering elements from tags and attributes.\npip3 install bs4  Let\u0026rsquo;s have a look.\n# Make response readable soup = BeautifulSoup(response.text, 'lxml') print(soup)  \u0026lt;html\u0026gt;\u0026lt;body\u0026gt;\u0026lt;p\u0026gt;# # robots.txt # # This file is to prevent the crawling and indexing of certain parts # of your site by web crawlers and spiders run by sites like Yahoo! # and Google. By telling these \u0026quot;robots\u0026quot; where not to go on your site, # you save bandwidth and server resources. # # This file will be ignored unless it is at the root of your host: # Used: http://example.com/robots.txt # Ignored: http://example.com/site/robots.txt # # For more information about the robots.txt standard, see: # http://www.robotstxt.org/robotstxt.html # # For syntax checking, see: # http://www.frobee.com/robots-txt-check User-agent: * Crawl-delay: 10 # Directories Disallow: /includes/ Disallow: /misc/ Disallow: /modules/ Disallow: /profiles/ Disallow: /scripts/ Disallow: /themes/ # Files Disallow: /CHANGELOG.txt Disallow: /cron.php Disallow: /INSTALL.mysql.txt Disallow: /INSTALL.pgsql.txt Disallow: /INSTALL.sqlite.txt Disallow: /install.php Disallow: /INSTALL.txt Disallow: /LICENSE.txt Disallow: /MAINTAINERS.txt Disallow: /update.php Disallow: /UPGRADE.txt Disallow: /xmlrpc.php # Paths (clean URLs) Disallow: /admin/ Disallow: /comment/reply/ Disallow: /filter/tips/ Disallow: /node/add/ Disallow: /search/ Disallow: /user/register/ Disallow: /user/password/ Disallow: /user/login/ Disallow: /user/logout/ # Paths (no clean URLs) Disallow: /?q=admin/ Disallow: /?q=comment/reply/ Disallow: /?q=filter/tips/ Disallow: /?q=node/add/ Disallow: /?q=search/ Disallow: /?q=user/password/ Disallow: /?q=user/register/ Disallow: /?q=user/login/ Disallow: /?q=user/logout/ \u0026lt;/p\u0026gt;\u0026lt;/body\u0026gt;\u0026lt;/html\u0026gt;  First of all, what is the html5lib option? It\u0026rsquo;s the parser. In short, there are often small mistakes/variations in HTML and each parser interprets it differently. In principles, the latest HTML standard is HTML5, therefore the html5lib parser should be the most \u0026ldquo;correct\u0026rdquo; parser. It might happen that the same code does not work for another person if you use a different parser.\nThis is much better but it can be improved.\n# Prettify response print(soup.prettify())  \u0026lt;html\u0026gt; \u0026lt;body\u0026gt; \u0026lt;p\u0026gt; # # robots.txt # # This file is to prevent the crawling and indexing of certain parts # of your site by web crawlers and spiders run by sites like Yahoo! # and Google. By telling these \u0026quot;robots\u0026quot; where not to go on your site, # you save bandwidth and server resources. # # This file will be ignored unless it is at the root of your host: # Used: http://example.com/robots.txt # Ignored: http://example.com/site/robots.txt # # For more information about the robots.txt standard, see: # http://www.robotstxt.org/robotstxt.html # # For syntax checking, see: # http://www.frobee.com/robots-txt-check User-agent: * Crawl-delay: 10 # Directories Disallow: /includes/ Disallow: /misc/ Disallow: /modules/ Disallow: /profiles/ Disallow: /scripts/ Disallow: /themes/ # Files Disallow: /CHANGELOG.txt Disallow: /cron.php Disallow: /INSTALL.mysql.txt Disallow: /INSTALL.pgsql.txt Disallow: /INSTALL.sqlite.txt Disallow: /install.php Disallow: /INSTALL.txt Disallow: /LICENSE.txt Disallow: /MAINTAINERS.txt Disallow: /update.php Disallow: /UPGRADE.txt Disallow: /xmlrpc.php # Paths (clean URLs) Disallow: /admin/ Disallow: /comment/reply/ Disallow: /filter/tips/ Disallow: /node/add/ Disallow: /search/ Disallow: /user/register/ Disallow: /user/password/ Disallow: /user/login/ Disallow: /user/logout/ # Paths (no clean URLs) Disallow: /?q=admin/ Disallow: /?q=comment/reply/ Disallow: /?q=filter/tips/ Disallow: /?q=node/add/ Disallow: /?q=search/ Disallow: /?q=user/password/ Disallow: /?q=user/register/ Disallow: /?q=user/login/ Disallow: /?q=user/logout/ \u0026lt;/p\u0026gt; \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt;  This is much better. Now the tree structure of the HTML page is clearly visible and we can visually separate the different elements.\nIn particular, the structure of the page is:\n page head  with ttle: \u0026ldquo;A Useful Page\u0026rdquo;   page body  with level 1 header \u0026ldquo;An Interesting Title\u0026rdquo; a division with text \u0026ldquo;Lorem ipsum\u0026hellip;\u0026rdquo;    How do we work with these elements? Suppose we want to recover the title and the text. The requests library has some useful functions.\n# Find the title url = 'http://pythonscraping.com/pages/page1.html' response = requests.get(url) soup = BeautifulSoup(response.text, 'lxml') soup.find('title')  \u0026lt;title\u0026gt;A Useful Page\u0026lt;/title\u0026gt;  # Extract text soup.find('title').text  'A Useful Page'  # Find all h1 elements soup.find_all('h1')  [\u0026lt;h1\u0026gt;An Interesting Title\u0026lt;/h1\u0026gt;]  # Find all title or h1 elements soup.find_all(['title','h1'])  [\u0026lt;title\u0026gt;A Useful Page\u0026lt;/title\u0026gt;, \u0026lt;h1\u0026gt;An Interesting Title\u0026lt;/h1\u0026gt;]  Regular Expressions Note that there is always a more direct alternative: using regular expressions directly on the response!\n# Find the title re.findall('\u0026lt;title\u0026gt;(.*)\u0026lt;/title\u0026gt;', response.text)[0]  'A Useful Page'  # Find all h1 elements re.findall('\u0026lt;h1\u0026gt;(.*)\u0026lt;/h1\u0026gt;', response.text)  ['An Interesting Title']  # Find all title or h1 elements [x[1] for x in re.findall('\u0026lt;(title|h1)\u0026gt;(.*)\u0026lt;', response.text)]  ['A Useful Page', 'An Interesting Title']  This was a very simple page and there was not so much to look for. Let\u0026rsquo;s now look at a more realistic example.\nAttributes Let\u0026rsquo;s inspect a slightly more complicated page: http://pythonscraping.com/pages/page3.html.\nIn this page, there is much more content than in the previous one. There seems to be a table, there are images, hyperlinks, etc\u0026hellip; It\u0026rsquo;s the perfect playground. Let\u0026rsquo;s have a look at what does the HTML code look like.\n# Inspect HTML code url2 = 'http://pythonscraping.com/pages/page3.html' response = requests.get(url2) soup = BeautifulSoup(response.text,'lxml') print(soup.prettify())  \u0026lt;html\u0026gt; \u0026lt;head\u0026gt; \u0026lt;style\u0026gt; img{ width:75px; } table{ width:50%; } td{ margin:10px; padding:10px; } .wrapper{ width:800px; } .excitingNote{ font-style:italic; font-weight:bold; } \u0026lt;/style\u0026gt; \u0026lt;/head\u0026gt; \u0026lt;body\u0026gt; \u0026lt;div id=\u0026quot;wrapper\u0026quot;\u0026gt; \u0026lt;img src=\u0026quot;../img/gifts/logo.jpg\u0026quot; style=\u0026quot;float:left;\u0026quot;/\u0026gt; \u0026lt;h1\u0026gt; Totally Normal Gifts \u0026lt;/h1\u0026gt; \u0026lt;div id=\u0026quot;content\u0026quot;\u0026gt; Here is a collection of totally normal, totally reasonable gifts that your friends are sure to love! Our collection is hand-curated by well-paid, free-range Tibetan monks. \u0026lt;p\u0026gt; We haven't figured out how to make online shopping carts yet, but you can send us a check to: \u0026lt;br/\u0026gt; 123 Main St. \u0026lt;br/\u0026gt; Abuja, Nigeria We will then send your totally amazing gift, pronto! Please include an extra $5.00 for gift wrapping. \u0026lt;/p\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;table id=\u0026quot;giftList\u0026quot;\u0026gt; \u0026lt;tr\u0026gt; \u0026lt;th\u0026gt; Item Title \u0026lt;/th\u0026gt; \u0026lt;th\u0026gt; Description \u0026lt;/th\u0026gt; \u0026lt;th\u0026gt; Cost \u0026lt;/th\u0026gt; \u0026lt;th\u0026gt; Image \u0026lt;/th\u0026gt; \u0026lt;/tr\u0026gt; \u0026lt;tr class=\u0026quot;gift\u0026quot; id=\u0026quot;gift1\u0026quot;\u0026gt; \u0026lt;td\u0026gt; Vegetable Basket \u0026lt;/td\u0026gt; \u0026lt;td\u0026gt; This vegetable basket is the perfect gift for your health conscious (or overweight) friends! \u0026lt;span class=\u0026quot;excitingNote\u0026quot;\u0026gt; Now with super-colorful bell peppers! \u0026lt;/span\u0026gt; \u0026lt;/td\u0026gt; \u0026lt;td\u0026gt; $15.00 \u0026lt;/td\u0026gt; \u0026lt;td\u0026gt; \u0026lt;img src=\u0026quot;../img/gifts/img1.jpg\u0026quot;/\u0026gt; \u0026lt;/td\u0026gt; \u0026lt;/tr\u0026gt; \u0026lt;tr class=\u0026quot;gift\u0026quot; id=\u0026quot;gift2\u0026quot;\u0026gt; \u0026lt;td\u0026gt; Russian Nesting Dolls \u0026lt;/td\u0026gt; \u0026lt;td\u0026gt; Hand-painted by trained monkeys, these exquisite dolls are priceless! And by \u0026quot;priceless,\u0026quot; we mean \u0026quot;extremely expensive\u0026quot;! \u0026lt;span class=\u0026quot;excitingNote\u0026quot;\u0026gt; 8 entire dolls per set! Octuple the presents! \u0026lt;/span\u0026gt; \u0026lt;/td\u0026gt; \u0026lt;td\u0026gt; $10,000.52 \u0026lt;/td\u0026gt; \u0026lt;td\u0026gt; \u0026lt;img src=\u0026quot;../img/gifts/img2.jpg\u0026quot;/\u0026gt; \u0026lt;/td\u0026gt; \u0026lt;/tr\u0026gt; \u0026lt;tr class=\u0026quot;gift\u0026quot; id=\u0026quot;gift3\u0026quot;\u0026gt; \u0026lt;td\u0026gt; Fish Painting \u0026lt;/td\u0026gt; \u0026lt;td\u0026gt; If something seems fishy about this painting, it's because it's a fish! \u0026lt;span class=\u0026quot;excitingNote\u0026quot;\u0026gt; Also hand-painted by trained monkeys! \u0026lt;/span\u0026gt; \u0026lt;/td\u0026gt; \u0026lt;td\u0026gt; $10,005.00 \u0026lt;/td\u0026gt; \u0026lt;td\u0026gt; \u0026lt;img src=\u0026quot;../img/gifts/img3.jpg\u0026quot;/\u0026gt; \u0026lt;/td\u0026gt; \u0026lt;/tr\u0026gt; \u0026lt;tr class=\u0026quot;gift\u0026quot; id=\u0026quot;gift4\u0026quot;\u0026gt; \u0026lt;td\u0026gt; Dead Parrot \u0026lt;/td\u0026gt; \u0026lt;td\u0026gt; This is an ex-parrot! \u0026lt;span class=\u0026quot;excitingNote\u0026quot;\u0026gt; Or maybe he's only resting? \u0026lt;/span\u0026gt; \u0026lt;/td\u0026gt; \u0026lt;td\u0026gt; $0.50 \u0026lt;/td\u0026gt; \u0026lt;td\u0026gt; \u0026lt;img src=\u0026quot;../img/gifts/img4.jpg\u0026quot;/\u0026gt; \u0026lt;/td\u0026gt; \u0026lt;/tr\u0026gt; \u0026lt;tr class=\u0026quot;gift\u0026quot; id=\u0026quot;gift5\u0026quot;\u0026gt; \u0026lt;td\u0026gt; Mystery Box \u0026lt;/td\u0026gt; \u0026lt;td\u0026gt; If you love suprises, this mystery box is for you! Do not place on light-colored surfaces. May cause oil staining. \u0026lt;span class=\u0026quot;excitingNote\u0026quot;\u0026gt; Keep your friends guessing! \u0026lt;/span\u0026gt; \u0026lt;/td\u0026gt; \u0026lt;td\u0026gt; $1.50 \u0026lt;/td\u0026gt; \u0026lt;td\u0026gt; \u0026lt;img src=\u0026quot;../img/gifts/img6.jpg\u0026quot;/\u0026gt; \u0026lt;/td\u0026gt; \u0026lt;/tr\u0026gt; \u0026lt;/table\u0026gt; \u0026lt;div id=\u0026quot;footer\u0026quot;\u0026gt; © Totally Normal Gifts, Inc. \u0026lt;br/\u0026gt; +234 (617) 863-0736 \u0026lt;/div\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt;  As we can see, now the page is much more complicated than before. An important distintion is that now some tags have classes. For example, the first \u0026lt;img\u0026gt; tag now has a class src and a class style.\n\u0026lt;img src=\u0026quot;../img/gifts/logo.jpg\u0026quot; style=\u0026quot;float:left;\u0026quot;\u0026gt;  Moreover, even though BeautifulSoup is formatting the page in a nicer way, it\u0026rsquo;s still pretty hard to go through it. How can one locate one specific element? And, most importantly, if you know the element only graphically, how do you recover the equivalent in the HTML code?\nThe best way is to use the inspect function from Chrome. Firefox has an equivalent function. Let\u0026rsquo;s inspect the original page.\n Suppose now you want to recover all item names. Let\u0026rsquo;s inspect the first. The corresponding line looks like this:\n \u0026lt;tr class=\u0026quot;gift\u0026quot; id=\u0026quot;gift1\u0026quot;\u0026gt; \u0026lt;td\u0026gt; Vegetable Basket \u0026lt;/td\u0026gt; \u0026lt;td\u0026gt; This vegetable basket is the perfect gift for your health conscious (or overweight) friends! \u0026lt;span class=\u0026quot;excitingNote\u0026quot;\u0026gt; Now with super-colorful bell peppers! \u0026lt;/span\u0026gt; \u0026lt;/td\u0026gt; \u0026lt;td\u0026gt; $15.00 \u0026lt;/td\u0026gt; \u0026lt;td\u0026gt;  Let\u0026rsquo;s see some alternative ways.\n# Select the first td element soup.find('td')  \u0026lt;td\u0026gt; Vegetable Basket \u0026lt;/td\u0026gt;  # Select the first td element of the second tr element (row) second_row = soup.find_all('tr')[1] second_row.find('td')  \u0026lt;td\u0026gt; Vegetable Basket \u0026lt;/td\u0026gt;  # Select the first element of the table with id=\u0026quot;giftList\u0026quot; table = soup.find('table', {\u0026quot;id\u0026quot;:\u0026quot;giftList\u0026quot;}) table.find('td')  \u0026lt;td\u0026gt; Vegetable Basket \u0026lt;/td\u0026gt;  The last is the most robust way to scrape. In fact, the first two methods are likely to fail if the page gets modified. If another td element gets added on top of the table, the code will recover something else entirely. In general it\u0026rsquo;s a good practice, to look if the element we want to scrape can be identified by some attribute that is likely to be invariant to changes to other parts of the web page. In this case, the table with id=\u0026quot;giftList\u0026quot; is likely to be our object of interest even if another table id added, for example.\nLet\u0026rsquo;s say no we want to recover the whole table. What would you do?\nimport pandas as pd # Shortcut df = pd.read_html(url2)[0] df   .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }  \n  Item Title Description Cost Image     0 Vegetable Basket This vegetable basket is the perfect gift for ... $15.00 NaN   1 Russian Nesting Dolls Hand-painted by trained monkeys, these exquisi... $10,000.52 NaN   2 Fish Painting If something seems fishy about this painting, ... $10,005.00 NaN   3 Dead Parrot This is an ex-parrot! Or maybe he's only resting? $0.50 NaN   4 Mystery Box If you love suprises, this mystery box is for ... $1.50 NaN     # Scraping with response table = soup.find('table', {\u0026quot;id\u0026quot;:\u0026quot;giftList\u0026quot;}) # Create empty dataframe col_names = [x.text.strip() for x in table.find_all('th')] df = pd.DataFrame(columns=col_names) # Loop over rows and append them to dataframe for row in table.find_all('tr')[1:]: columns = [x.text.strip() for x in row.find_all('td')] df_row = dict(zip(col_names, columns)) df = df.append(df_row, ignore_index=True) df  /var/folders/xy/szm3b7211cj16ldwcywbvfzdqv7yhd/T/ipykernel_92159/3999490009.py:12: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead. df = df.append(df_row, ignore_index=True) /var/folders/xy/szm3b7211cj16ldwcywbvfzdqv7yhd/T/ipykernel_92159/3999490009.py:12: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead. df = df.append(df_row, ignore_index=True) /var/folders/xy/szm3b7211cj16ldwcywbvfzdqv7yhd/T/ipykernel_92159/3999490009.py:12: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead. df = df.append(df_row, ignore_index=True) /var/folders/xy/szm3b7211cj16ldwcywbvfzdqv7yhd/T/ipykernel_92159/3999490009.py:12: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead. df = df.append(df_row, ignore_index=True) /var/folders/xy/szm3b7211cj16ldwcywbvfzdqv7yhd/T/ipykernel_92159/3999490009.py:12: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead. df = df.append(df_row, ignore_index=True)   .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }  \n  Item Title Description Cost Image     0 Vegetable Basket This vegetable basket is the perfect gift for ... $15.00    1 Russian Nesting Dolls Hand-painted by trained monkeys, these exquisi... $10,000.52    2 Fish Painting If something seems fishy about this painting, ... $10,005.00    3 Dead Parrot This is an ex-parrot! Or maybe he's only resting? $0.50    4 Mystery Box If you love suprises, this mystery box is for ... $1.50      # Compact alternative table = soup.find('table', {\u0026quot;id\u0026quot;:\u0026quot;giftList\u0026quot;}) content = [[x.text.strip() for x in row.find_all(['th','td'])] for row in table.find_all('tr')] df = pd.DataFrame(content[1:], columns=content[0]) df   .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }  \n  Item Title Description Cost Image     0 Vegetable Basket This vegetable basket is the perfect gift for ... $15.00    1 Russian Nesting Dolls Hand-painted by trained monkeys, these exquisi... $10,000.52    2 Fish Painting If something seems fishy about this painting, ... $10,005.00    3 Dead Parrot This is an ex-parrot! Or maybe he's only resting? $0.50    4 Mystery Box If you love suprises, this mystery box is for ... $1.50      We have now seen how to scrape a simple but realistic webpage. Let\u0026rsquo;s proceed with a practical example.\nCSS Selectors One alternative way of doing exactly the same thing is to use select. The select function is very similar to find_all but has a different syntax. In particular, to search an element with a certain tag and  attribute, we have to pass the following input:\nsoup.select(tag[attribute=\u0026quot;attribute_name\u0026quot;])  # Select the first element of the table whose id contains \u0026quot;List\u0026quot; table = soup.select('table[id*=\u0026quot;List\u0026quot;]')[0] table.find('td')  \u0026lt;td\u0026gt; Vegetable Basket \u0026lt;/td\u0026gt;  Forms and post requests When you are scraping, you sometimes have to fill-in forms, either to log-in into an account, or to input the arguments for a search query. Often forms are dynamic objects, but not always. Sometimes we can fill in forms also using the requests library. In whis section we see a simple example.\nShortcut Often we can bypass forms, if the form redirects us to another page whose URL contains the parameters of the form. These are \u0026ldquo;well-behaved\u0026rdquo; forms and are actually quite frequent.\nWe can find a simple example at: http://www.webscrapingfordatascience.com/basicform/. This form takes as input a bunch of information and when we click on \u0026ldquo;Submit my information\u0026rdquo;, we get exactly the same page but with a different URL that contains the information we have inserted.\nSuppose I insert the following information:\n Your gender: \u0026ldquo;Male\u0026rdquo; Food you like: \u0026ldquo;Pizza!\u0026rdquo; and \u0026ldquo;Fries please\u0026rdquo;  We should get the following url: http://www.webscrapingfordatascience.com/basicform/?name=\u0026gender=M\u0026pizza=like\u0026fries=like\u0026haircolor=black\u0026comments=\nWe can decompose the url in various components, separated by one \u0026ldquo;?\u0026rdquo; and multiple \u0026ldquo;\u0026amp;\u0026quot;:\n http://www.webscrapingfordatascience.com/basicform/ name= gender=M pizza=like fries=like haircolor=black comments=  We can clearly see a pattern: the first component is the cose of the url and the other components are the form options. The ones we didn\u0026rsquo;t fill have the form option= while the ones we did fill are option=value. Knowing the syntax of a particular form we could fill it ourselves.\nFor example, we could remove the fries and change the hair color to brown: http://www.webscrapingfordatascience.com/basicform/?name=\u0026gender=M\u0026pizza=like\u0026fries=\u0026haircolor=brown\u0026comments=\nMoreover, most forms work even if you remove the empty options. For example, the url above is equivalent to:http://www.webscrapingfordatascience.com/basicform/?gender=M\u0026amp;pizza=like\u0026amp;haircolor=brown\u0026amp;comments=\nOne way to scrape websites with such forms is to create a string with the url with all the empty options and fill them using string formatting functions.\n# Building form url url_core = 'http://www.webscrapingfordatascience.com/basicform/?' url_options = 'name=%s\u0026amp;gender=%s\u0026amp;pizza=%s\u0026amp;fries=%s\u0026amp;haircolor=%s\u0026amp;comments=%s' options = ('','M','like','','brown','') url = url_core + url_options % options print(url)  http://www.webscrapingfordatascience.com/basicform/?name=\u0026amp;gender=M\u0026amp;pizza=like\u0026amp;fries=\u0026amp;haircolor=brown\u0026amp;comments=  An alternative way is to name the options. This alternative is more verbose but more precise and does not require you to provide always all the options, even if empty.\n# Alternative 1 url_core = 'http://www.webscrapingfordatascience.com/basicform/?' url_options = 'name={name}\u0026amp;gender={gender}\u0026amp;pizza={pizza}\u0026amp;fries={fries}\u0026amp;haircolor={haircolor}\u0026amp;comments={comments}' options = { 'name': '', 'gender': 'M', 'pizza': 'like', 'fries': '', 'haircolor': 'brown', 'comments': '' } url = url_core + url_options.format(**options) print(url)  http://www.webscrapingfordatascience.com/basicform/?name=\u0026amp;gender=M\u0026amp;pizza=like\u0026amp;fries=\u0026amp;haircolor=brown\u0026amp;comments=  Lastly, one could build the url on the go.\n# Alternative 2 url = 'http://www.webscrapingfordatascience.com/basicform/?' options = { 'gender': 'M', 'pizza': 'like', 'haircolor': 'brown', } for key, value in options.items(): url += key + '=' + value + '\u0026amp;' print(url)  http://www.webscrapingfordatascience.com/basicform/?gender=M\u0026amp;pizza=like\u0026amp;haircolor=brown\u0026amp;  Post forms Sometimes however, forms do not provide nice URLs as output. This is particularly true for login forms. There is however still a method, for some of them, to deal with them.\nFor this section, we will use the same form example as before: http://www.webscrapingfordatascience.com/postform2/.\nThis looks like the same form but now when the user clicks on \u0026ldquo;Submit my information\u0026rdquo;, we get a page with a summary of the information. The biggest difference however, is that the output URL is exactly the same. Hence, we cannot rely on the same URL-bulding strategy as before.\nIf we inspect the page, we observe the following line at the very beginning\n\u0026lt;form method=\u0026quot;POST\u0026quot;\u0026gt; [...] \u0026lt;/form\u0026gt;  And inside there are various input fields:\n \u0026lt;input type=\u0026quot;text\u0026quot;\u0026gt; for name \u0026lt;input type=\u0026quot;radio\u0026quot;\u0026gt; for gender \u0026lt;input type=\u0026quot;checkbox\u0026quot;\u0026gt; for food \u0026lt;select\u0026gt;...\u0026lt;/select\u0026gt; for the hair color \u0026lt;textarea\u0026gt;...\u0026lt;/textarea\u0026gt; for comments  These are all fields with which we can interact using the response package. The main difference is that we won\u0026rsquo;t use the get method to get the response from the URL but we will use the post method to post our form parameters and get a response.\nIf we input the following options:\n gender: male pizza: yes hair color: brown hair  and we click \u0026ldquo;Submit my information\u0026rdquo; we get to a page with the following text:\nThanks for submitting your information Here's a dump of the form data that was submitted: array(5) { [\u0026quot;name\u0026quot;]=\u0026gt; string(0) \u0026quot;\u0026quot; [\u0026quot;gender\u0026quot;]=\u0026gt; string(1) \u0026quot;M\u0026quot; [\u0026quot;pizza\u0026quot;]=\u0026gt; string(4) \u0026quot;like\u0026quot; [\u0026quot;haircolor\u0026quot;]=\u0026gt; string(5) \u0026quot;brown\u0026quot; [\u0026quot;comments\u0026quot;]=\u0026gt; string(0) \u0026quot;\u0026quot; }  We will not try to get to the same page using the requests package.\n# URL url = 'http://www.webscrapingfordatascience.com/postform2/' # Options options = { 'gender': 'M', 'pizza': 'like', 'haircolor': 'brown', } # Post request response = requests.post(url, data=options) print(response.text)  \u0026lt;html\u0026gt; \u0026lt;body\u0026gt; \u0026lt;h2\u0026gt;Thanks for submitting your information\u0026lt;/h2\u0026gt; \u0026lt;p\u0026gt;Here's a dump of the form data that was submitted:\u0026lt;/p\u0026gt; \u0026lt;pre\u0026gt;array(3) { [\u0026quot;gender\u0026quot;]=\u0026gt; string(1) \u0026quot;M\u0026quot; [\u0026quot;pizza\u0026quot;]=\u0026gt; string(4) \u0026quot;like\u0026quot; [\u0026quot;haircolor\u0026quot;]=\u0026gt; string(5) \u0026quot;brown\u0026quot; } \u0026lt;/pre\u0026gt; \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt;  We have obtained exactly what we wanted! However, sometimes, websites block direct post requests.\nOne simple example is: http://www.webscrapingfordatascience.com/postform3/.\n# Post request url = 'http://www.webscrapingfordatascience.com/postform3/' response = requests.post(url, data=options) print(response.text)  \u0026lt;html\u0026gt; \u0026lt;body\u0026gt; Are you trying to submit information from somewhere else? \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt;  What happened? If we inspect the page, we can see that there is a new line at the beginning:\n\u0026lt;input type=\u0026quot;hidden\u0026quot; name=\u0026quot;protection\u0026quot; value=\u0026quot;2c17abf5d5b4e326bea802600ff88405\u0026quot;\u0026gt;  Now the form contains one more value - protection which is conventiently hidden. In order to bypass the protection, we need to provide the correct protection value to the form.\n# Post request url = 'http://www.webscrapingfordatascience.com/postform3/' response = requests.get(url) # Get out the value for protection soup = BeautifulSoup(response.text, 'lxml') options['protection'] = soup.find('input', attrs={'name': 'protection'}).get('value') # Post request response = requests.post(url, data=options) print(response.text)  \u0026lt;html\u0026gt; \u0026lt;body\u0026gt; \u0026lt;h2\u0026gt;Thanks for submitting your information\u0026lt;/h2\u0026gt; \u0026lt;p\u0026gt;Here's a dump of the form data that was submitted:\u0026lt;/p\u0026gt; \u0026lt;pre\u0026gt;array(4) { [\u0026quot;gender\u0026quot;]=\u0026gt; string(1) \u0026quot;M\u0026quot; [\u0026quot;pizza\u0026quot;]=\u0026gt; string(4) \u0026quot;like\u0026quot; [\u0026quot;haircolor\u0026quot;]=\u0026gt; string(5) \u0026quot;brown\u0026quot; [\u0026quot;protection\u0026quot;]=\u0026gt; string(32) \u0026quot;16c87fc858e4d9fcb8d9c920b699388d\u0026quot; } \u0026lt;/pre\u0026gt; \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt;  Indeed, now the post request was successful.\nProxies We have discussed at the beginning how to be more subtle while scraping, by changing headers. In this section we will explore one step forward in anonimity: proxies.\nWhen we send an HTTP request, first the request is sent to a proxy server. The important thing is that the destination web server will which is the origin proxy server. Therefore, when one destination web server sees too many requests coming from one machine, it will block the proxy server.\nHow can we change proxy? There are many websites that offer proxies for money but there are also some that offer proxies for free. The problem with free proxies (but often also with premium ones) is that there are many users using the same proxy, hence they are\n slow blocked fast by many websites  Nevertheless, it might be still useful to know how to change proxies.\nGet proxy list One website where we can get some free proxies to use for scraping is https://free-proxy-list.net/.\nIf we open the page, we see that there is a long list of proxies, from different countries and with different characteristics. Importantly, we are mostly interested in https proxies. We are now going to retrieve a list of them. Note that the proxy list of this website is updated quite often. However, free proxies usually \u0026ldquo;expire\u0026rdquo; even faster.\n# Retrieve proxy list def get_proxies(): response = requests.get('https://free-proxy-list.net/') soup = BeautifulSoup(response.text, 'lxml') table = soup.find('table', {'class':'table'}) proxies = [] rows = table.find_all('tr') for row in rows: cols = row.find_all('td') if len(cols)\u0026gt;0: line = [col.text for col in cols] if line[6]=='yes': proxies += [line[0]+':'+line[1]] return proxies len(get_proxies())  176  We have found many proxies. How do we use them? We have to provide them as an argment to a requests session.\n# Test proxies url = 'https://www.google.com' proxies = get_proxies() for proxy in proxies[:10]: try: response = session.get(url, proxies={\u0026quot;https\u0026quot;: proxy}, timeout=5) print(response) except Exception as e: print(type(e))  \u0026lt;class 'NameError'\u0026gt; \u0026lt;class 'NameError'\u0026gt; \u0026lt;class 'NameError'\u0026gt; \u0026lt;class 'NameError'\u0026gt; \u0026lt;class 'NameError'\u0026gt; \u0026lt;class 'NameError'\u0026gt; \u0026lt;class 'NameError'\u0026gt; \u0026lt;class 'NameError'\u0026gt; \u0026lt;class 'NameError'\u0026gt; \u0026lt;class 'NameError'\u0026gt;  Yes, most proxies were extremely slow (and consider we are opening Google\u0026hellip;) and we got a ConnetTimeout error. Other proxies worked and for one or two of the others we might have got a ProxyError.\nDynamic Webscraping Let\u0026rsquo;s try to scrape the quotes from this link: http://www.webscrapingfordatascience.com/simplejavascript/. It seems like a straightforward job.\n# Scrape javascript page url = 'http://www.webscrapingfordatascience.com/simplejavascript/' response = requests.get(url) print(response.text)  \u0026lt;html\u0026gt; \u0026lt;head\u0026gt; \u0026lt;script src=\u0026quot;https://code.jquery.com/jquery-3.2.1.min.js\u0026quot;\u0026gt;\u0026lt;/script\u0026gt; \u0026lt;script\u0026gt; $(function() { document.cookie = \u0026quot;jsenabled=1\u0026quot;; $.getJSON(\u0026quot;quotes.php\u0026quot;, function(data) { var items = []; $.each(data, function(key, val) { items.push(\u0026quot;\u0026lt;li id='\u0026quot; + key + \u0026quot;'\u0026gt;\u0026quot; + val + \u0026quot;\u0026lt;/li\u0026gt;\u0026quot;); }); $(\u0026quot;\u0026lt;ul/\u0026gt;\u0026quot;, { html: items.join(\u0026quot;\u0026quot;) }).appendTo(\u0026quot;body\u0026quot;); }); }); \u0026lt;/script\u0026gt; \u0026lt;/head\u0026gt; \u0026lt;body\u0026gt; \u0026lt;h1\u0026gt;Here are some quotes\u0026lt;/h1\u0026gt; \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt;  Weird. Our response does not contain the quotes on the page, even though they are clearly visible when we open it in our browser.\nSelenium Selenium is a python library that emulates a browser and lets us see pages exactly as with a normal browser. This is the most user-friendly way to do web scraping, however it has a huge cost: speed. This is by far the slowest way to do web scraping.\nAfter installing selenium, we need to download a browser to simulate. We will use Google\u0026rsquo;s chromedriver. You can download it from here: https://sites.google.com/a/chromium.org/chromedriver/. Make sure to select \u0026ldquo;latest stable release\u0026rdquo; and not \u0026ldquo;latest beta release\u0026rdquo;.\nMove the downloaded chromedriver in the current directory (\u0026quot;/11-python-webscraping\u0026rdquo; for me). We will now try open the url above with selenium and see if we can scrape the quotes in it.\n# Set your chromedriver name chromedriver_name = '/chromedriver_mac'  # Open url path = os.getcwd() print(path) driver = webdriver.Chrome(path+chromedriver_name)  /Users/mcourt/Dropbox/Projects/Data-Science-Python/notebooks /var/folders/xy/szm3b7211cj16ldwcywbvfzdqv7yhd/T/ipykernel_92159/2846782857.py:4: DeprecationWarning: executable_path has been deprecated, please pass in a Service object driver = webdriver.Chrome(path+chromedriver_name)  Awesome! Now, if everything went smooth, you should have a new Chrome window with a banner that says \u0026ldquo;Chrome is being controlled by automated test software\u0026rdquo;. We can now open the web page and check that the list appears.\n# Open url url = 'http://www.webscrapingfordatascience.com/simplejavascript/' driver.get(url)  Again, if averything went well, we are now abl to see our page with all the quotes in it. How do we scrape them?\nIf we inspect the elements of the list with the right-click inspect option, we should see something like:\n\u0026lt;html\u0026gt;\u0026lt;head\u0026gt; \u0026lt;script src=\u0026quot;https://code.jquery.com/jquery-3.2.1.min.js\u0026quot;\u0026gt;\u0026lt;/script\u0026gt; \u0026lt;script\u0026gt; $(function() { document.cookie = \u0026quot;jsenabled=1\u0026quot;; $.getJSON(\u0026quot;quotes.php\u0026quot;, function(data) { var items = []; $.each(data, function(key, val) { items.push(\u0026quot;\u0026lt;li id='\u0026quot; + key + \u0026quot;'\u0026gt;\u0026quot; + val + \u0026quot;\u0026lt;/li\u0026gt;\u0026quot;); }); $(\u0026quot;\u0026lt;ul/\u0026gt;\u0026quot;, { html: items.join(\u0026quot;\u0026quot;) }).appendTo(\u0026quot;body\u0026quot;); }); }); \u0026lt;/script\u0026gt; \u0026lt;/head\u0026gt; \u0026lt;body\u0026gt; \u0026lt;h1\u0026gt;Here are some quotes\u0026lt;/h1\u0026gt; \u0026lt;ul\u0026gt;\u0026lt;li id=\u0026quot;0\u0026quot;\u0026gt;Every strike brings me closer to the next home run. –Babe Ruth\u0026lt;/li\u0026gt;\u0026lt;li id=\u0026quot;1\u0026quot;\u0026gt;The two most important days in your life are the day you are born and the day you find out why. –Mark Twain\u0026lt;/li\u0026gt;\u0026lt;li id=\u0026quot;2\u0026quot;\u0026gt;Whatever you can do, or dream you can, begin it. Boldness has genius, power and magic in it. –Johann Wolfgang von Goethe\u0026lt;/li\u0026gt;\u0026lt;/ul\u0026gt;\u0026lt;/body\u0026gt;\u0026lt;/html\u0026gt;  Now we can see the content! Can we actually retrieve it? Let\u0026rsquo;s try.\nThe most common selenium functions to get elements of a page, have a very intuitive syntax and are: find_element_by_id\n find_element_by_name find_element_by_xpath find_element_by_link_text find_element_by_partial_link_text find_element_by_tag_name find_element_by_class_name find_element_by_css_selector  We will not try to recover all elements with tag \u0026lt;li\u0026gt; (element of list \u0026lt;ul\u0026gt;).\n# Scrape content quotes = [li.text for li in driver.find_elements_by_tag_name('li')] quotes  /var/folders/xy/szm3b7211cj16ldwcywbvfzdqv7yhd/T/ipykernel_92159/157107938.py:2: DeprecationWarning: find_elements_by_* commands are deprecated. Please use find_elements() instead quotes = [li.text for li in driver.find_elements_by_tag_name('li')] []  Yes! It worked! But why?\n# Headless option headless_option = webdriver.ChromeOptions() headless_option.add_argument('--headless') # Scraping driver = webdriver.Chrome(path+chromedriver_name, options=headless_option) driver.get(url) quotes = [li.text for li in driver.find_elements_by_tag_name('li')] quotes  /var/folders/xy/szm3b7211cj16ldwcywbvfzdqv7yhd/T/ipykernel_92159/2173692453.py:6: DeprecationWarning: executable_path has been deprecated, please pass in a Service object driver = webdriver.Chrome(path+chromedriver_name, options=headless_option) /var/folders/xy/szm3b7211cj16ldwcywbvfzdqv7yhd/T/ipykernel_92159/2173692453.py:8: DeprecationWarning: find_elements_by_* commands are deprecated. Please use find_elements() instead quotes = [li.text for li in driver.find_elements_by_tag_name('li')] []  Mmm, it (probably) didn\u0026rsquo;t work. Why?\nThe problem is that we are trying to retrieve the content of the page too fast. The page hasn\u0026rsquo;t loaded yet. This is a common issue with selenium. Where are two ways to solve it:\n waiting waiting for the element to load  The second way is the best way but we will first try the first and simpler one: we will just ask the browser to wait for 1 second before searching for \u0026lt;li\u0026gt; tags\n# Scraping driver = webdriver.Chrome(path+chromedriver_name, options=headless_option) driver.get(url) time.sleep(1) quotes = [li.text for li in driver.find_elements_by_tag_name('li')] quotes  /var/folders/xy/szm3b7211cj16ldwcywbvfzdqv7yhd/T/ipykernel_92159/2964398594.py:2: DeprecationWarning: executable_path has been deprecated, please pass in a Service object driver = webdriver.Chrome(path+chromedriver_name, options=headless_option) /var/folders/xy/szm3b7211cj16ldwcywbvfzdqv7yhd/T/ipykernel_92159/2964398594.py:5: DeprecationWarning: find_elements_by_* commands are deprecated. Please use find_elements() instead quotes = [li.text for li in driver.find_elements_by_tag_name('li')] ['The best time to plant a tree was 20 years ago. The second best time is now. –Chinese Proverb', 'The most common way people give up their power is by thinking they don’t have any. –Alice Walker', 'I am not a product of my circumstances. I am a product of my decisions. –Stephen Covey']  Nice! Now you should have obtained the list that we could not scrape with requests. If it didn\u0026rsquo;t work, just increase the waiting time and it should work.\nWe can now have a look at the \u0026ldquo;better\u0026rdquo; way to use a series of built-in functions:\n WebDriverWait: the waiting function. We will call the until method expected_conditions: the condition function. We will call the visibility_of_all_elements_located method By: the selector function. Some of the options are:  By.ID By.XPATH By.NAME By.TAG_NAME By.CLASS_NAME By.CSS_SELECTOR By.LINK_TEXT By.PARTIAL_LINK_TEXT    from selenium.webdriver.common.by import By from selenium.webdriver.support.ui import WebDriverWait from selenium.webdriver.support import expected_conditions as EC # Scraping driver = webdriver.Chrome(path+chromedriver_name, options=headless_option) driver.get(url) quotes = WebDriverWait(driver, 10).until(EC.visibility_of_all_elements_located((By.TAG_NAME, 'li'))) quotes = [quote.text for quote in quotes] quotes  /var/folders/xy/szm3b7211cj16ldwcywbvfzdqv7yhd/T/ipykernel_92159/152412441.py:6: DeprecationWarning: executable_path has been deprecated, please pass in a Service object driver = webdriver.Chrome(path+chromedriver_name, options=headless_option) ['The most common way people give up their power is by thinking they don’t have any. –Alice Walker', 'The best time to plant a tree was 20 years ago. The second best time is now. –Chinese Proverb', 'An unexamined life is not worth living. –Socrates']  In this case, we have told the browser to wait until either all elements with tag \u0026lt;li\u0026gt; are visible or 10 seconds have passed. After one condition is realized, the WebDriverWait function also automatically retrieves all the elements which the expected_condition function is conditioning on. There are many different conditions we can use. A list can be found here: https://selenium-python.readthedocs.io/waits.html.\nWe can easily generalize the function above as follows.\n# Find element function def find_elements(driver, function, identifier): element = WebDriverWait(driver, 10).until(EC.visibility_of_all_elements_located((function, identifier))) return element quotes = [quote.text for quote in find_elements(driver, By.TAG_NAME, 'li')] quotes  ['The most common way people give up their power is by thinking they don’t have any. –Alice Walker', 'The best time to plant a tree was 20 years ago. The second best time is now. –Chinese Proverb', 'An unexamined life is not worth living. –Socrates']  ","date":1651363200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1651363200,"objectID":"bfe25f3c8fd85b78b79bafee570b4721","permalink":"https://matteocourthoud.github.io/course/data-science/07_web_scraping/","publishdate":"2022-05-01T00:00:00Z","relpermalink":"/course/data-science/07_web_scraping/","section":"course","summary":"import os import re import time import requests import pandas as pd from bs4 import BeautifulSoup from pprint import pprint from selenium import webdriver  There is no silver bullet to getting info from the internet.","tags":null,"title":"Web Scraping","type":"book"},{"authors":null,"categories":null,"content":"%matplotlib inline from utils.lecture07 import *  Decision Trees Decision trees involve segmenting the predictor space into a number of simple regions. In order to make a prediction for a given observation, we typically use the mean or the mode of the training observations in the region to which it belongs. Since the set of splitting rules used to segment the predictor space can be summarized in a tree, these types of approaches are known as decision tree methods.\nRegression Trees For this session we will consider the Hitters dataset. It consists in individual level data of baseball players. In our applications, we are interested in predicting the players Salary.\n# Load the data hitters = pd.read_csv('data/Hitters.csv').dropna() hitters.head()   .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }  \n  Unnamed: 0 AtBat Hits HmRun Runs RBI Walks Years CAtBat CHits ... CRuns CRBI CWalks League Division PutOuts Assists Errors Salary NewLeague     1 -Alan Ashby 315 81 7 24 38 39 14 3449 835 ... 321 414 375 N W 632 43 10 475.0 N   2 -Alvin Davis 479 130 18 66 72 76 3 1624 457 ... 224 266 263 A W 880 82 14 480.0 A   3 -Andre Dawson 496 141 20 65 78 37 11 5628 1575 ... 828 838 354 N E 200 11 3 500.0 N   4 -Andres Galarraga 321 87 10 39 42 30 2 396 101 ... 48 46 33 N E 805 40 4 91.5 N   5 -Alfredo Griffin 594 169 4 74 51 35 11 4408 1133 ... 501 336 194 A W 282 421 25 750.0 A    5 rows × 21 columns\n In particular, we are interested in looking how the number of Hits and the Years of experience predict the Salary.\n# Get Features features = ['Years', 'Hits'] X = hitters[features].values y = np.log(hitters.Salary.values)  We are actually going to use log(salary) since it has a more gaussian distribution.\nfig, (ax1, ax2) = plt.subplots(1,2, figsize=(11,4)) # Plot salary distribution ax1.hist(hitters.Salary.values) ax1.set_xlabel('Salary') ax2.hist(y) ax2.set_xlabel('Log(Salary)');  In order to understand what is a tree, let\u0026rsquo;s first have a look at one. We fit a regression three with 3 leaves or, equivalently put, 2 nodes.\n# Fit regression tree tree = DecisionTreeRegressor(max_leaf_nodes=3) tree.fit(X, y)  DecisionTreeRegressor(max_leaf_nodes=3)  We are now going to plot the results visually. The biggest avantage of trees is interpretability.\n# Figure 8.1 fig, ax = plt.subplots(1,1) ax.set_title('Figure 8.1'); # Plot tree plot_tree(tree, filled=True, feature_names=features, fontsize=14, ax=ax);  The tree consists of a series of splitting rules, starting at the top of the tree. The top split assigns observations having Years\u0026lt;4.5 to the left branch.1 The predicted salary for these players is given by the mean response value for the players in the data set with Years\u0026lt;4.5. For such players, the mean log salary is 5.107, and so we make a prediction of 5.107 thousands of dollars, i.e. $165,174, for these players. Players with Years\u0026gt;=4.5 are assigned to the right branch, and then that group is further subdivided by Hits.\nOverall, the tree stratifies or segments the players into three regions of predictor space:\n players who have played for four or fewer years players who have played for five or more years and who made fewer than 118 hits last year, and players who have played for five or more years and who made at least 118 hits last year.  These three regions can be written as\n R1 = {X | Years\u0026lt;4.5} R2 = {X | Years\u0026gt;=4.5, Hits\u0026lt;117.5}, and R3 = {X | Years\u0026gt;=4.5, Hits\u0026gt;=117.5}.  Since the dimension of $X$ is 2, we can visualize the space and the regions in a 2-dimensional graph.\n# Figure 8.2 def make_figure_8_2(): # Init hitters.plot('Years', 'Hits', kind='scatter', color='orange', figsize=(7,6)) plt.title('Figure 8.2') plt.xlim(0,25); plt.ylim(ymin=-5); plt.xticks([1, 4.5, 24]); plt.yticks([1, 117.5, 238]); # Split lines plt.vlines(4.5, ymin=-5, ymax=250, color='g') plt.hlines(117.5, xmin=4.5, xmax=25, color='g') # Regions plt.annotate('R1', xy=(2,117.5), fontsize='xx-large') plt.annotate('R2', xy=(11,60), fontsize='xx-large') plt.annotate('R3', xy=(11,170), fontsize='xx-large');  make_figure_8_2()  We might interpret the above regression tree as follows: Years is the most important factor in determining Salary, and players with less experience earn lower salaries than more experienced players. Given that a player is less experienced, the number of hits that he made in the previous year seems to play little role in his salary. But among players who have been in the major leagues for five or more years, the number of hits made in the previous year does affect salary, and players who made more hits last year tend to have higher salaries.\nBuilding a Tree There are two main steps in the construction of a tree:\n We divide the predictor space—that is, the set of possible values for $X_1, X_2, \u0026hellip; , X_p$ into $J$ distinct and non-overlapping regions, $R_1,R_2,\u0026hellip;,R_J$. For every observation that falls into the region $R_j$ , we make the same prediction, which is simply the mean of the response values for the training observations in $R_j$.  The second step is easy. But how does one construct the regions? Our purpose is to minimize the Sum of Squared Residuals, across the different regions:\n$$ \\sum_{j=1}^{J} \\sum_{i \\in R_{j}}\\left(y_{i}-\\hat{y}{R{j}}\\right)^{2} $$\nUnfortunately, it is computationally infeasible to consider every possible partition of the feature space into $J$ boxes.\nFor this reason, we take a top-down, greedy approach that is known as recursive binary splitting. The approach is top-down because it begins at the top of the tree (at which point all observations belong to a single region) and then successively splits the predictor space; each split is indicated via two new branches further down on the tree. It is greedy because at each step of the tree-building process, the best split is made at that particular step, rather than looking ahead and picking a split that will lead to a better tree in some future step.\nIn practice, the method is the following:\n we select the predictor $X_j$ we select the cutpoint $s$ such that splitting the predictor space into the regions ${X|X_j \u0026lt; s}$ and ${X|X_j \\geq s}$ leads to the greatest possible reduction in RSS we repeat (1)-(2) for all predictors $X_1, \u0026hellip; , X_p$, i.e. we solve  $$ \\arg \\min_{j,s} \\ \\sum_{i: x_{i} \\in {X|X_j \u0026lt; s}}\\left(y_{i}-\\hat{y}i\\right)^{2}+\\sum{i: x_{i} \\in {X|X_j \\geq s}}\\left(y_{i}-\\hat{y}_i\\right)^{2} $$\nwe choose the predictor and cutpoint such that the resulting tree has the lowest RSS we keep repeating (1)-(4) until a certain condition is met. However, after the first iteration we also have to pick which region to split which adds a further dimension to optimize over.  Let\u0026rsquo;s build our own Node class to play around with trees.\nclass Node: \u0026quot;\u0026quot;\u0026quot; Class used to represent nodes in a Regression Tree Attributes ---------- x : np.array independent variables y : np.array dependent variables idxs : np.array indexes fo x and y for current node depth : int depth of the sub-tree (default 5) Methods ------- find_next_nodes(self) Keep growing the tree find_best_split(self) Find the best split split(self) Split the tree \u0026quot;\u0026quot;\u0026quot; def __init__(self, x, y, idxs, depth=5): \u0026quot;\u0026quot;\u0026quot;Initialize node\u0026quot;\u0026quot;\u0026quot; self.x = x self.y = y self.idxs = idxs self.depth = depth self.get_next_nodes() def get_next_nodes(self): \u0026quot;\u0026quot;\u0026quot;If the node is not terminal, get further splits\u0026quot;\u0026quot;\u0026quot; if self.is_last_leaf: return self.find_best_split() self.split() def find_best_split(self): \u0026quot;\u0026quot;\u0026quot;Loop over variables and their values to find the best split\u0026quot;\u0026quot;\u0026quot; best_score = float('inf') # Loop over variables for col in range(self.x.shape[1]): x = self.x[self.idxs, col] # Loop over all splits for s in np.unique(x): lhs = x \u0026lt;= s rhs = x \u0026gt; s curr_score = self.get_score(lhs, rhs) # If best score, save it if curr_score \u0026lt; best_score: best_score = curr_score self.split_col = col self.split_val = s return self def get_score(self, lhs, rhs): \u0026quot;\u0026quot;\u0026quot;Get score of a given split\u0026quot;\u0026quot;\u0026quot; y = self.y[self.idxs] lhs_mse = self.get_mse(y[lhs]) rhs_mse = self.get_mse(y[rhs]) return lhs_mse * lhs.sum() + rhs_mse * rhs.sum() def get_mse(self, y): return np.mean((y-np.mean(y))**2) def split(self): \u0026quot;\u0026quot;\u0026quot;Split a node into 2 sub-nodes (recursive)\u0026quot;\u0026quot;\u0026quot; x = self.x[self.idxs, self.split_col] lhs = x \u0026lt;= self.split_val rhs = x \u0026gt; self.split_val self.lhs = Node(self.x, self.y, self.idxs[lhs], self.depth-1) self.rhs = Node(self.x, self.y, self.idxs[rhs], self.depth-1) to_print = (self.depth, self.split_col, self.split_val, sum(lhs), sum(rhs)) print('Split on layer %.0f: var%1.0f = %.4f (%.0f/%.0f)' % to_print) return self @property def is_last_leaf(self): return self.depth\u0026lt;=1  What does a Node look like?\n# Init first node tree1 = Node(X, y, np.arange(len(y)), 1) # Documentation (always comment and document your code!) print(tree1.__doc__)   Class used to represent nodes in a Regression Tree Attributes ---------- x : np.array independent variables y : np.array dependent variables idxs : np.array indexes fo x and y for current node depth : int depth of the sub-tree (default 5) Methods ------- find_next_nodes(self) Keep growing the tree find_best_split(self) Find the best split split(self) Split the tree  Which properties does it have?\n# Inspect the class dir(tree1)  ['__class__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__le__', '__lt__', '__module__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__sizeof__', '__str__', '__subclasshook__', '__weakref__', 'depth', 'find_best_split', 'get_mse', 'get_next_nodes', 'get_score', 'idxs', 'is_last_leaf', 'split', 'x', 'y']  What is the depth? How many observations are there?\n# Get info print('Tree of depth %.0f with %.0f observations' % (tree1.depth, len(tree1.idxs)))  Tree of depth 1 with 263 observations  Fair enough, the tree is just a single leaf.\n# Check if terminal tree1.is_last_leaf  True  Let\u0026rsquo;s find the first split.\n# Find best split tree1.find_best_split() print('Split at var%1.0f = %.4f' % (tree1.split_col, tree1.split_val))  Split at var0 = 4.0000  If has selected the first variable, at the value $4$.\nIf we call the split function, it will also tell us how many observations per leaf the split generates.\n# Split tree tree1.split();  Split on layer 1: var0 = 4.0000 (90/173)  Now we are ready to compute even deeper trees\n# Check depth-3 tree tree3 = Node(X, y, np.arange(len(y)), 3)  Split on layer 2: var1 = 4.0000 (2/88) Split on layer 2: var1 = 117.0000 (90/83) Split on layer 3: var0 = 4.0000 (90/173)  Pruning The process described above may produce good predictions on the training set, but is likely to overfit the data, leading to poor test set performance. This is because the resulting tree might be too complex. A smaller tree with fewer splits might lead to lower variance and better interpretation at the cost of a little bias.\nWe can see it happening if we build the same tree as above, but with 5 leaves.\n# Compute tree overfit_tree = DecisionTreeRegressor(max_leaf_nodes=5).fit(X, y)  We plot the 5-leaf tree.\n# Plot tree fig, ax = plt.subplots(1,1) plot_tree(overfit_tree, filled=True, feature_names=features, fontsize=14, ax=ax);  The split on the far left is predicting a very high Salary (7.243) for players with few Years of experience and few Hits. Indeed this prediction is based on an extremely tiny subsample (2). They are probably outliers and our tree is most likely overfitting.\nOne possible alternative is to insert a minimum number of observation per leaf.\n# Compute tree no_overfit_tree = DecisionTreeRegressor(max_leaf_nodes=5, min_samples_leaf=10).fit(X, y) # Plot tree fig, ax = plt.subplots(1,1) plot_tree(no_overfit_tree, filled=True, feature_names=features, fontsize=14, ax=ax);  Now the tree makes much more sense: the lower the Years and the Hits, the lower the predicted Salary as we can see from the shades getting darker and darker as we move left to right\nAnother possible alternative to the process described above is to build the tree only so long as the decrease in the RSS due to each split exceeds some (high) threshold.\nThis strategy will result in smaller trees, but is too short-sighted since a seemingly worthless split early on in the tree might be followed by a very good split—that is, a split that leads to a large reduction in RSS later on.\nWe can use cross-validation to pick the optimal tree length.\n# Import original split features = ['Years', 'Hits', 'RBI', 'PutOuts', 'Walks', 'Runs', 'AtBat', 'HmRun'] X_train = pd.read_csv('data/Hitters_X_train.csv').dropna()[features] X_test = pd.read_csv('data/Hitters_X_test.csv').dropna()[features] y_train = pd.read_csv('data/Hitters_y_train.csv').dropna() y_test = pd.read_csv('data/Hitters_y_test.csv').dropna()  # Init params = range(2,11) reg_scores = np.zeros((len(params),3)) best_score = 10**6 # Loop over all parameters for i,k in enumerate(params): # Model tree = DecisionTreeRegressor(max_leaf_nodes=k) # Loop over splits tree.fit(X_train, y_train) reg_scores[i,0] = mean_squared_error(tree.predict(X_train), y_train) reg_scores[i,1] = mean_squared_error(tree.predict(X_test), y_test) # Get CV score kf6 = KFold(n_splits=6) reg_scores[i,2] = -cross_val_score(tree, X_train, y_train, cv=kf6, scoring='neg_mean_squared_error').mean() # Save best model if reg_scores[i,2]\u0026lt;best_score: best_model = tree best_score = reg_scores[i,2]  Let\u0026rsquo;s plot the optimal tree depth, using 6-fold cv.\n# Figure 8.5 def make_figure_8_5(): # Init fig, (ax1,ax2) = plt.subplots(1,2,figsize=(16,6)) fig.suptitle('Figure 8.5') # Plot scores ax1.plot(params, reg_scores); ax1.axvline(params[np.argmin(reg_scores[:,2])], c='k', ls='--') ax1.legend(['Train','Test','6-fold CV']); ax1.set_title('Cross-Validation Scores'); # Plot best tree plot_tree(best_model, filled=True, impurity=False, feature_names=features, fontsize=12, ax=ax2); ax2.set_title('Best Model');  make_figure_8_5()  The optimal tree has 4 leaves.\nClassification Trees A classification tree is very similar to a regression tree, except that it is used to predict a qualitative response rather than a quantitative one.\nFor a classification tree, we predict that each observation belongs to the most commonly occurring class of training observations in the region to which it belongs.\nBuilding a Classification Tree The task of growing a classification tree is similar to the task of growing a regression tree. However, in the classification setting, RSS cannot be used as a criterion for making the binary splits.\nWe define $\\hat p_{mk}$ as the proportion of training observations in the $m^{th}$ region that are from the $k^{th}$ class. Possible loss functions to decide the splits are:\n Classification error rate  $$ E = 1 - \\max {k}\\left(\\hat{p}{m k}\\right) $$\n Gini index  $$ G=\\sum_{k=1}^{K} \\hat{p}{m k}\\left(1-\\hat{p}{m k}\\right) $$\n Entropy  $$ D=-\\sum_{k=1}^{K} \\hat{p}{m k} \\log \\hat{p}{m k} $$\nIn 2-class classification problems, this is what the different scores look like, for different proportions of class 2 ($p$), when the true proportion is $p_0 =0.5$.\nWhen building a classification tree, either the Gini index or the entropy are typically used to evaluate the quality of a particular split, since these two approaches are more sensitive to node purity than is the classification error rate.\nFor this section we will work with the Heart dataset on individual heart failures. We will try to use individual characteristics in order to predict heart deseases (HD). The varaible is binary: Yes, No.\n# Load heart dataset heart = pd.read_csv('data/Heart.csv').drop('Unnamed: 0', axis=1).dropna() heart.head()   .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }  \n  Age Sex ChestPain RestBP Chol Fbs RestECG MaxHR ExAng Oldpeak Slope Ca Thal AHD     0 63 1 typical 145 233 1 2 150 0 2.3 3 0.0 fixed No   1 67 1 asymptomatic 160 286 0 2 108 1 1.5 2 3.0 normal Yes   2 67 1 asymptomatic 120 229 0 2 129 1 2.6 2 2.0 reversable Yes   3 37 1 nonanginal 130 250 0 0 187 0 3.5 3 0.0 normal No   4 41 0 nontypical 130 204 0 2 172 0 1.4 1 0.0 normal No     # Fastorize variables heart.ChestPain = pd.factorize(heart.ChestPain)[0] heart.Thal = pd.factorize(heart.Thal)[0]  # Set features features = [col for col in heart.columns if col!='AHD'] X2 = heart[features] y2 = pd.factorize(heart.AHD)[0]  We now fit our classifier.\n# Fit classification tree clf = DecisionTreeClassifier(max_depth=None, max_leaf_nodes=11) clf.fit(X2,y2)  DecisionTreeClassifier(max_leaf_nodes=11)  What is the score?\n# Final score clf.score(X2,y2)  0.8686868686868687  Let\u0026rsquo;s have a look at the whole tree.\n# Figure 8.6 a def make_fig_8_6a(): # Init fig, ax = plt.subplots(1,1, figsize=(16,12)) ax.set_title('Figure 8.6'); # Plot tree plot_tree(clf, filled=True, feature_names=features, class_names=['No','Yes'], fontsize=12, ax=ax);  make_fig_8_6a()  This figure has a surprising characteristic: some of the splits yield two terminal nodes that have the same predicted value.\nFor instance, consider the split Age\u0026lt;=57.5 near the bottom left of the unpruned tree. Regardless of the value of Age, a response value of No is predicted for those observations. Why, then, is the split performed at all?\nThe split is performed because it leads to increased node purity. That is, 2/81 of the observations corresponding to the left-hand leaf have a response value of Yes, whereas 9/36 of those corresponding to the right-hand leaf have a response value of Yes. Why is node purity important? Suppose that we have a test observation that belongs to the region given by that left-hand leaf. Then we can be pretty certain that its response value is No. In contrast, if a test observation belongs to the region given by the right-hand leaf, then its response value is probably No, but we are much less certain. Even though the split Age\u0026lt;=57.5 does not reduce the classification error, it improves the Gini index and the entropy, which are more sensitive to node purity.\nPruning for Classification We can repeat the pruning exercise also for the classification task.\n# Figure 8.6 b def make_figure_8_6b(): # Init fig, (ax1, ax2) = plt.subplots(1,2,figsize=(14,6)) fig.suptitle('Figure 8.6') # Plot scores ax1.plot(params, clf_scores); ax1.legend(['Train','Test','6-fold CV']); # Plot best tree plot_tree(best_model, filled=True, impurity=False, feature_names=features, fontsize=12, ax=ax2);  # Init J = 10 params = range(2,11) clf_scores = np.zeros((len(params),3)) best_score = 100 # Loop over all parameters for i,k in enumerate(params): # Model tree = DecisionTreeClassifier(max_leaf_nodes=k) # Loop J times temp_scores = np.zeros((J,3)) for j in range (J): # Loop over splits X2_train, X2_test, y2_train, y2_test = train_test_split(X2, y2, test_size=0.5, random_state=j) m = tree.fit(X2_train, y2_train) temp_scores[j,0] = mean_squared_error(m.predict(X2_train), y2_train) temp_scores[j,1] = mean_squared_error(m.predict(X2_test), y2_test) # Get CV score kf6 = KFold(n_splits=6) temp_scores[j,2] = -cross_val_score(tree, X2_train, y2_train, cv=kf6, scoring='neg_mean_squared_error').mean() # Save best model if temp_scores[j,2]\u0026lt;best_score: best_model = m best_score = temp_scores[j,2] # Average clf_scores[i,:] = np.mean(temp_scores, axis=0)  make_figure_8_6b()  Other Issues Missing Predictor Values There are usually 2 main ways to deal with missing values:\n discard the observations fill the missing values with predictions using the other observations (e.g. mean)  With trees we can do better:\n code them as a separate class (e.g. \u0026lsquo;missing\u0026rsquo;) generate splits using non-missing data and use non-missing variables on missing data to mimic the splits with missing data  Categorical Predictors When splitting a predictor having q possible unordered values, there are $2^{q−1} − 1$ possible partitions of the q values into two groups, and the computations become prohibitive for large $q$. However, with a $0 − 1$ outcome, this computation simplifies.\nLinear Combination Splits Rather than restricting splits to be of the form $X_j \\leq s$, one can allow splits along linear combinations of the form $a_j X_j \\leq s$. The weights $a_j$ become part of the optimization procedure.\nOther Tree-Building Procedures The procedure we have seen for building trees is called CART (Classification and Regression Tree). There are other procedures.\nThe Loss Matrix With respect to other methods, the choice of the loss functions plays a much more important role.\nBinary Splits You can do non-binary splits but in the end they are just weaker versions of binary splits.\nInstability Trees have very high variance.\nDifficulty in Capturing Additive Structure Trees are quite bad at modeling additive structures.\nLack of Smoothness Trees are not smooth.\nTrees vs Regression Advantages\n Trees are very easy to explain to people. In fact, they are even easier to explain than linear regression! Some people believe that decision trees more closely mirror human decision-making than do the regression and classification approaches seen in previous chapters. Trees can be displayed graphically, and are easily interpreted even by a non-expert (especially if they are small). Trees can easily handle qualitative predictors without the need to create dummy variables.  Disadvantages\n trees generally do not have the same level of predictive accuracy as some of the other regression and classification approaches seen in this book. trees can be very non-robust. In other words, a small change in the data can cause a large change in the final estimated tree.  7.2 Bagging, Random Forests, Boosting Bagging, random forests, and boosting use trees as building blocks to construct more powerful prediction models.\nBagging The main problem of decision trees is that they suffer from high variance. Bootstrap aggregation, or bagging, is a general-purpose procedure for reducing the variance of a statistical learning method.\nThe main idea behind bagging is that, given a set of n independent observations $Z_1,\u0026hellip;,Z_n$, each with variance $\\sigma^2$, the variance of the mean $\\bar Z$ of the observations is given by $\\sigma^2/n$. In other words, averaging a set of observations reduces variance.\nIndeed bagging consists in taking many training sets from the population, build a separate prediction model using each training set, and average the resulting predictions. Since we do not have access to many training sets, we resort to bootstrapping.\nOut-of-Bag Error Estimation It turns out that there is a very straightforward way to estimate the test error of a bagged model, without the need to perform cross-validation or the validation set approach. Recall that the key to bagging is that trees are repeatedly fit to bootstrapped subsets of the observations. One can show that on average, each bagged tree makes use of around two-thirds of the observations. The remaining one-third of the observations not used to fit a given bagged tree are referred to as the out-of-bag (OOB) observations. We can predict the response for the ith observation using each of the trees in which that observation was OOB.\nWe are now going to compute the Gini index for the Heart dataset using different numbers of trees.\n# Init (takes a lot of time with J=30) params = range(2,50) bagging_scores = np.zeros((len(params),2)) J = 30; # Loop over parameters for i, k in enumerate(params): print(\u0026quot;Computing k=%1.0f\u0026quot; % k, end =\u0026quot;\u0026quot;) # Repeat J temp_scores = np.zeros((J,2)) for j in range(J): X2_train, X2_test, y2_train, y2_test = train_test_split(X2, y2, test_size=0.5, random_state=j) bagging = BaggingClassifier(DecisionTreeClassifier(), max_samples=k, oob_score=True) bagging.fit(X2_train,y2_train) temp_scores[j,0] = bagging.score(X2_test, y2_test) temp_scores[j,1] = bagging.oob_score_ # Average bagging_scores[i,:] = np.mean(temp_scores, axis=0) print(\u0026quot;\u0026quot;, end=\u0026quot;\\r\u0026quot;)  Computing k=49  Let\u0026rsquo;s plot the Out-of-Bag error computed while generating the bagged estimator.\n# Make new figure 1 def make_new_figure_1(): # Init fig, ax = plt.subplots(1,1,figsize=(10,6)) fig.suptitle(\u0026quot;Estimated $R^2$\u0026quot;) # Plot scores ax.plot(params, bagging_scores); ax.legend(['Test','OOB']); ax.set_xlabel('Number of Trees'); ax.set_ylabel('R^2');  make_new_figure_1()  It can be shown that with B sufficiently large, OOB error is virtually equivalent to leave-one-out cross-validation error. The OOB approach for estimating the test error is particularly convenient when performing bagging on large data sets for which cross-validation would be computationally onerous.\nVariable Importance Measures As we have discussed, the main advantage of bagging is to reduce prediction variance. However, with bagging it can be difficult to interpret the resulting model. In fact we cannot draw trees anymore given we have too many of them.\nHowever, one can obtain an overall summary of the importance of each predictor using the RSS (for bagging regression trees) or the Gini index (for bagging classification trees). In the case of bagging regression trees, we can record the total amount that the RSS is decreased due to splits over a given predictor, averaged over all trees. A large value indicates an important predictor. Similarly, in the context of bagging classification trees, we can add up the total amount that the Gini index is decreased by splits over a given predictor, averaged over all trees.\n# Compute feature importance feature_importances = np.mean([tree.feature_importances_ for tree in bagging.estimators_], axis=0)  We can have a look at the importance of each feature.\n# Figure 8.9 def make_figure_8_9(): # Init fig, ax = plt.subplots(1,1,figsize=(8,8)) ax.set_title('Figure 8.9: Feature Importance'); # Plot feature importance h1 = pd.DataFrame({'Importance':feature_importances*100}, index=features) h1 = h1.sort_values(by='Importance', axis=0, ascending=False) h1.plot(kind='barh', color='r', ax=ax) ax.set_xlabel('Variable Importance'); plt.yticks(fontsize=14); plt.gca().legend_ = None;  make_figure_8_9()  Random Forests Random forests provide an improvement over bagged trees by way of a small tweak that decorrelates the trees. As in bagging, we build a number of decision trees on bootstrapped training samples. But when building these decision trees, each time a split in a tree is considered, a random sample of $m$ predictors is chosen as split candidates from the full set of $p$ predictors. The split is allowed to use only one of those m predictors. A fresh sample of $m$ predictors is taken at each split, and typically we choose $m \\sim \\sqrt{p}$ — that is, the number of predictors considered at each split is approximately equal to the square root of the total number of predictors\nIn other words, in building a random forest, at each split in the tree, the algorithm is not even allowed to consider a majority of the available predictors. This may sound crazy, but it has a clever rationale. Suppose that there is one very strong predictor in the data set, along with a number of other moderately strong predictors. Then in the collection of bagged trees, most or all of the trees will use this strong predictor in the top split. Consequently, all of the bagged trees will look quite similar to each other. Hence the predictions from the bagged trees will be highly correlated. Unfortunately, averaging many highly correlated quantities does not lead to as large of a reduction in variance as averaging many uncorrelated quantities. In particular, this means that bagging will not lead to a substantial reduction in variance over a single tree in this setting.\nRandom forests overcome this problem by forcing each split to consider only a subset of the predictors.\nLet\u0026rsquo;s split the data in 2 and compute test and estimated $R^2$, for both forest and trees.\nimport warnings warnings.simplefilter('ignore') # Init (takes a lot of time with J=30) params = range(2,50) forest_scores = np.zeros((len(params),2)) J = 30 # Loop over parameters for i, k in enumerate(params): print(\u0026quot;Computing k=%1.0f\u0026quot; % k, end =\u0026quot;\u0026quot;) # Repeat J temp_scores = np.zeros((J,2)) for j in range(J): X2_train, X2_test, y2_train, y2_test = train_test_split(X2, y2, test_size=0.5, random_state=j) forest = RandomForestClassifier(n_estimators=k, oob_score=True, max_features=\u0026quot;sqrt\u0026quot;) forest.fit(X2_train,y2_train) temp_scores[j,0] = forest.score(X2_test, y2_test) temp_scores[j,1] = forest.oob_score_ # Average forest_scores[i,:] = np.mean(temp_scores, axis=0) print(\u0026quot;\u0026quot;, end=\u0026quot;\\r\u0026quot;)  Computing k=49  # Figure 8.8 def make_figure_8_8(): # Init fig, ax = plt.subplots(1,1,figsize=(10,6)) ax.set_title('Figure 8.8'); # Plot scores ax.plot(params, bagging_scores); ax.plot(params, forest_scores); ax.legend(['Test - Bagging','OOB - Bagging', 'Test - Forest','OOB - Forest']); ax.set_xlabel('Number of Trees'); ax.set_ylabel('R^2');  make_figure_8_8()  As for bagging, we can plot feature importance.\n# Make new figure 2 def make_new_figure_2(): # Init fig, (ax1,ax2) = plt.subplots(1,2,figsize=(15,7)) # Plot feature importance - Bagging h1 = pd.DataFrame({'Importance':feature_importances*100}, index=features) h1 = h1.sort_values(by='Importance', axis=0, ascending=False) h1.plot(kind='barh', color='r', ax=ax1) ax1.set_xlabel('Variable Importance'); ax1.set_title('Tree Bagging') # Plot feature importance h2 = pd.DataFrame({'Importance':forest.feature_importances_*100}, index=features) h2 = h2.sort_values(by='Importance', axis=0, ascending=False) h2.plot(kind='barh', color='r', ax=ax2) ax2.set_title('Random Forest') # All plots for ax in fig.axes: ax.set_xlabel('Variable Importance'); ax.legend([])  make_new_figure_2()  From the figure we observe that varaible importance ranking is similar with bagging and random forests, but there are significant differences.\nWe are now going to look at the importance of random forests using the Khan gene dataset. This dataset has the peculiarity of having a large number of features and very few observations.\n# Load data gene = pd.read_csv('data/Khan.csv') print(len(gene)) gene.head()  83   .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }  \n  x V1 V2 V3 V4 V5 V6 V7 V8 V9 ... V2299 V2300 V2301 V2302 V2303 V2304 V2305 V2306 V2307 V2308     0 2 0.773344 -2.438405 -0.482562 -2.721135 -1.217058 0.827809 1.342604 0.057042 0.133569 ... -0.238511 -0.027474 -1.660205 0.588231 -0.463624 -3.952845 -5.496768 -1.414282 -0.647600 -1.763172   1 2 -0.078178 -2.415754 0.412772 -2.825146 -0.626236 0.054488 1.429498 -0.120249 0.456792 ... -0.657394 -0.246284 -0.836325 -0.571284 0.034788 -2.478130 -3.661264 -1.093923 -1.209320 -0.824395   2 2 -0.084469 -1.649739 -0.241308 -2.875286 -0.889405 -0.027474 1.159300 0.015676 0.191942 ... -0.696352 0.024985 -1.059872 -0.403767 -0.678653 -2.939352 -2.736450 -1.965399 -0.805868 -1.139434   3 2 0.965614 -2.380547 0.625297 -1.741256 -0.845366 0.949687 1.093801 0.819736 -0.284620 ... 0.259746 0.357115 -1.893128 0.255107 0.163309 -1.021929 -2.077843 -1.127629 0.331531 -2.179483   4 2 0.075664 -1.728785 0.852626 0.272695 -1.841370 0.327936 1.251219 0.771450 0.030917 ... -0.200404 0.061753 -2.273998 -0.039365 0.368801 -2.566551 -1.675044 -1.082050 -0.965218 -1.836966    5 rows × 2309 columns\n The dataset has 83 rows and 2309 columns.\nSince it\u0026rsquo;s a very wide dataset, selecting the right features is crucial.\nAlso note that we cannot run linear regression on this dataset.\n# Reduce dataset size gene_small = gene.iloc[:,0:202] X = gene_small.iloc[:,1:] y = gene_small.iloc[:,0]  Let\u0026rsquo;s now cross-validate over number of trees and maximum number of features considered.\n# Init (takes a lot of time with J=30) params = range(50,150,10) m_scores = np.zeros((len(params),3)) p = np.shape(X)[1] J = 30; # Loop over parameters for i, k in enumerate(params): # Array of features ms = [round(p/2), round(np.sqrt(p)), round(np.log(p))] # Repeat L times temp_scores = np.zeros((J,3)) for j in range(J): print(\u0026quot;Computing k=%1.0f (iter=%1.0f)\u0026quot; % (k,j+1), end =\u0026quot;\u0026quot;) # Loop over values of m for index, m in enumerate(ms): forest = RandomForestClassifier(n_estimators=k, max_features=m, oob_score=True) forest.fit(X, y) temp_scores[j,index] = forest.oob_score_ print(\u0026quot;\u0026quot;, end=\u0026quot;\\r\u0026quot;) # Average m_scores[i,:] = np.mean(temp_scores, axis=0)  Computing k=140 (iter=30)  # Figure 8.10 def make_figure_8_10(): # Init fig, ax = plt.subplots(1,1,figsize=(10,6)) ax.set_title('Figure 8.10'); # Plot scores ax.plot(params, m_scores); ax.legend(['m=p/2','m=sqrt(p)','m=log(p)']); ax.set_xlabel('Number of Trees'); ax.set_ylabel('Test Classification Accuracy');  make_figure_8_10()  It seems that the best scores are achieved with few features and many trees.\nBoosting Like bagging, boosting is a general approach that can be applied to many statistical learning methods for regression or classification. Here we restrict our discussion of boosting to the context of decision trees.\nBoosting works similarly to bagging, except that the trees are grown sequentially: each tree is grown using information from previously grown trees. Boosting does not involve bootstrap sampling; instead each tree is fit on a modified version of the original data set.\nWhat is the idea behind this procedure? Given the current model, we fit a decision tree to the residuals from the model. That is, we fit a tree using the current residuals, rather than the outcome $y$, as the response. We then add this new decision tree into the fitted function in order to update the residuals. Each of these trees can be rather small, with just a few terminal nodes, determined by the parameter $d$ in the algorithm. By fitting small trees to the residuals, we slowly improve $\\hat f$ in areas where it does not perform well. The shrinkage parameter λ slows the process down even further, allowing more and different shaped trees to attack the resid- uals. In general, statistical learning approaches that learn slowly tend to perform well.\nAlgorithm The boosting algorithm works as follows:\n  Set $\\hat f(x)=0$ and $r_i=y_i$ for all $i$ in the training set.\n  For $b=1,2,\u0026hellip;,B$ repeat:\na. Fit a tree $\\hat f^b $ with $d$ splits ($d+1$ terminal nodes) to the training data $(X,r)$.\nb. Update $\\hat f$ by adding in a shrunken version of the new tree: $$ \\hat f(x) \\leftarrow \\hat f(x) + \\lambda \\hat f^b(x) $$\nc. Update the residuals $$ r_i = r_i - \\lambda \\hat f^b(x_i) $$\n  Output the boosted model $$ \\hat{f}(x)=\\sum_{b=1}^{B} \\lambda \\hat{f}^{b}(x) $$\n  Boosting has three tuning parameters:\n The number of trees $B$ The shrinkage parameter $\\lambda$. This controls the rate at which boosting learns. The number of splits in each tree $d$ , which controls the complexity of the boosted ensemble. Often d = 1 works well, in which case each tree is a stump, consisting of a single split.  # Init , oob_score=True params = range(50,150,10) boost_scores = np.zeros((len(params),3)) p = np.shape(X)[1] J = 30 # Loop over parameters for i, k in enumerate(params): # Repeat L times temp_scores = np.zeros((J,3)) for j in range(J): print(\u0026quot;Computing k=%1.0f (iter=%1.0f)\u0026quot; % (k,j+1), end =\u0026quot;\u0026quot;) X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=20, random_state=j) # First score: random forest forest = RandomForestClassifier(n_estimators=k, max_features=\u0026quot;sqrt\u0026quot;) forest.fit(X_train, y_train) temp_scores[j,0] = forest.score(X_test, y_test) # Second score: boosting with 1-split trees boost1 = GradientBoostingClassifier(learning_rate=0.01, max_depth=1, n_estimators=k, max_features=\u0026quot;sqrt\u0026quot;) boost1.fit(X_train, y_train) temp_scores[j,1] = boost1.score(X_test, y_test) # Third score: boosting with 1-split trees boost2 = GradientBoostingClassifier(learning_rate=0.01, max_depth=2, n_estimators=k, max_features=\u0026quot;sqrt\u0026quot;) boost2.fit(X_train, y_train) temp_scores[j,2] = boost2.score(X_test, y_test) print(\u0026quot;\u0026quot;, end=\u0026quot;\\r\u0026quot;) # Average boost_scores[i,:] = np.mean(temp_scores, axis=0)  Computing k=140 (iter=30)  Let\u0026rsquo;s compare boosting and forest.\n# Figure 8.11 def make_figure_8_11(): # Init fig, ax = plt.subplots(1,1,figsize=(10,6)) ax.set_title('Figure 8.11'); # Plot scores ax.plot(params, m_scores); ax.legend(['forest','boosting with d=1','boosting with d=2']); ax.set_xlabel('Number of Trees'); ax.set_ylabel('Test Classification Accuracy');  make_figure_8_11()  ","date":1646784000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1646784000,"objectID":"54aa48269b53b49cab7798a7b9e67a1b","permalink":"https://matteocourthoud.github.io/course/ml-econ/07_trees/","publishdate":"2022-03-09T00:00:00Z","relpermalink":"/course/ml-econ/07_trees/","section":"course","summary":"%matplotlib inline from utils.lecture07 import *  Decision Trees Decision trees involve segmenting the predictor space into a number of simple regions. In order to make a prediction for a given observation, we typically use the mean or the mode of the training observations in the region to which it belongs.","tags":null,"title":"Tree-based Methods","type":"book"},{"authors":null,"categories":null,"content":"Instrumental Variables Endogeneity We say that there is endogeneity in the linear regression model if $\\mathbb E[x_i \\varepsilon_i] \\neq 0$.\nThe random vector $z_i$ is an instrumental variable in the linear regression model if the following conditions are met.\n Exclusion restriction: the instruments are uncorrelated with the regression error $$ \\mathbb E_n[z_i \\varepsilon_i] = 0 $$ almost surely, i.e. with probability $p \\to 1$. Rank condition: no linearly redundant instruments $$ \\mathbb E_n[z_i z_i'] \\neq 0 $$ almost surely, i.e. with probability $p \\to 1$. Relevance condition (need $L \u0026gt; K$): $$ rank \\ (\\mathbb E_n[z_i x_i']) = K $$ almost surely, i.e. with probability $p \\to 1$.  IV and 2SLS Let $K = dim(x_i)$ and $L = dim(z_i)$. We say that the model is just-identified if $L = K$ (method: IV) and over-identified if $L \u0026gt; K$ (method: 2SLS).\nAssume $z_i$ satisfies the instrumental variable assumptions above and $dim(z_i) = dim(x_i)$, then the instrumental variables (IV) estimator $\\hat{\\beta} _ {IV}$ is given by $$ \\begin{aligned} \\hat{\\beta} _ {IV} \u0026amp;= \\mathbb E_n[z_i x_i']^{-1} \\mathbb E_n[z_i y_i] = \\newline \u0026amp;= \\left( \\frac{1}{n} \\sum _ {i=1}^n z_i x_i\\right)^{-1} \\left( \\frac{1}{n} \\sum _ {i=1}^n z_i y_i\\right) = \\newline \u0026amp;= (Z\u0026rsquo;X)^{-1} (Z\u0026rsquo;y) \\end{aligned} $$\nAssume $z_i$ satisfies the instrumental variable assumptions above and $dim(z_i) \u0026gt; dim(x_i)$, then the two-stage-least squares (2SLS) estimator $\\hat{\\beta} _ {2SLS}$ is given by $$ \\hat{\\beta} _ {2SLS} = \\Big( X\u0026rsquo;Z (Z\u0026rsquo;Z)^{-1} Z\u0026rsquo;X \\Big)^{-1} \\Big( X\u0026rsquo;Z (Z\u0026rsquo;Z)^{-1} Z\u0026rsquo;y \\Big) $$ Where $\\hat{x}_i$ is the predicted $x_i$ from the first stage regression of $x_i$ on $z_i$. This is equivalent to the IV estimator using $\\hat{x}_i$ as an instrument for $x_i$.\n2SLS Algebra   The estimator is called two-stage-least squares since it can be rewritten as an IV estimator that uses $\\hat{X}$ as instrument: $$ \\begin{aligned} \\hat{\\beta} _ {\\text{2SLS}} \u0026amp;= \\Big( X\u0026rsquo;Z (Z\u0026rsquo;Z)^{-1} Z\u0026rsquo;X \\Big)^{-1} \\Big( X\u0026rsquo;Z (Z\u0026rsquo;Z)^{-1} Z\u0026rsquo;y \\Big) = \\newline \u0026amp;= (\\hat{X}' X)^{-1} \\hat{X}' y = \\newline \u0026amp;= \\mathbb E_n[\\hat{x}_i x_i']^{-1} \\mathbb E_n[\\hat{x}_i y_i] \\end{aligned} $$\n  Moreover it can be rewritten as $$ \\begin{aligned} \\hat{\\beta} _ {\\text{2SLS}} \u0026amp;= (\\hat{X}' X)^{-1} \\hat{X}' y = \\newline \u0026amp;= (X' P_Z X)^{-1} X' P_Z y = \\newline \u0026amp;= (X' P_Z P_Z X)^{-1} X' P_Z y = \\newline \u0026amp;= (\\hat{X}' \\hat{X})^{-1} \\hat{X}' y = \\newline \u0026amp;= \\mathbb E_n [\\hat{x}_i \\hat{x}_i]^{-1} \\mathbb E_n[\\hat{x}_i y_i] \\end{aligned} $$\n  Rule of Thumb How to the test the relevance condition? Rule of thumb: $F$-test in the first stage $\u0026gt;10$ (joint test on $z_i$).\n Problem: as $n \\to \\infty$, with finite $L$, $F \\to \\infty$ (bad rule of thumb).\n Equivalence Theorem\nIf $K=L$, $\\hat{\\beta} _ {\\text{2SLS}} = \\hat{\\beta} _ {\\text{IV}}$.\nProof\nIf $K=L$, $X\u0026rsquo;Z$ and $Z\u0026rsquo;X$ are squared matrices and, by the relevance condition, non-singular (invertible). $$ \\begin{aligned} \\hat{\\beta} _ {\\text{2SLS}} \u0026amp;= \\Big( X\u0026rsquo;Z (Z\u0026rsquo;Z)^{-1} Z\u0026rsquo;X \\Big)^{-1} \\Big( X\u0026rsquo;Z (Z\u0026rsquo;Z)^{-1} Z\u0026rsquo;y \\Big) = \\newline \u0026amp;= (Z\u0026rsquo;X)^{-1} (Z\u0026rsquo;Z) (X\u0026rsquo;Z)^{-1} X\u0026rsquo;Z (Z\u0026rsquo;Z)^{-1} Z\u0026rsquo;y = \\newline \u0026amp;= (Z\u0026rsquo;X)^{-1} (Z\u0026rsquo;Z) (Z\u0026rsquo;Z)^{-1} Z\u0026rsquo;y = \\newline \u0026amp;= (Z\u0026rsquo;X)^{-1} (Z\u0026rsquo;y) = \\newline \u0026amp;= \\hat{\\beta} _ {\\text{IV}} \\end{aligned} $$ $$\\tag*{$\\blacksquare$}$$\nDemand Example Example from Hayiashi (2000) page 187: demand and supply simultaneous equations. $$ \\begin{aligned} \u0026amp; q_i^D(p_i) = \\alpha_0 + \\alpha_1 p_i + u_i \\newline \u0026amp; q_i^S(p_i) = \\beta_0 + \\beta_1 p_i + v_i \\end{aligned} $$\nWe have an endogeneity problem. To see why, we solve the system of equations for $(p_i, q_i)$: $$ \\begin{aligned} \u0026amp; p_i = \\frac{\\beta_0 - \\alpha_0}{\\alpha_1 - \\beta_1} + \\frac{v_i - u_i}{\\alpha_1 - \\beta_1 } \\newline \u0026amp; q_i = \\frac{\\alpha_1\\beta_0 - \\alpha_0 \\beta_1}{\\alpha_1 - \\beta_1} + \\frac{\\alpha_1 v_i - \\beta_1 u_i}{\\alpha_1 - \\beta_1 } \\end{aligned} $$\nDemand Example (2) Then the price variable is not independent from the error term in neither equation: $$ \\begin{aligned} \u0026amp; Cov(p_i, u_i) = - \\frac{Var(u_i)}{\\alpha_1 - \\beta_1 } \\newline \u0026amp; Cov(p_i, v_i) = \\frac{Var(v_i)}{\\alpha_1 - \\beta_1 } \\end{aligned} $$\nAs a consequence, the OLS estimators are not consistent: $$ \\begin{aligned} \u0026amp; \\hat{\\alpha} _ {1, OLS} \\overset{p}{\\to} \\alpha_1 + \\frac{Cov(p_i, u_i)}{Var(p_i)} \\newline \u0026amp; \\hat{\\beta} _ {1, OLS} \\overset{p}{\\to} \\beta_1 + \\frac{Cov(p_i, v_i)}{Var(p_i)} \\end{aligned} $$\nDemand Example (3) In general, running regressing $q$ on $p$ you estimate $$ \\begin{aligned} \\hat{\\gamma} _ {OLS} \u0026amp;\\overset{p}{\\to} \\frac{Cov(p_i, q_i)}{Var(p_i)} = \\newline \u0026amp;= \\frac{\\alpha_1 Var(v_i) + \\beta_1 Var(u_i)}{(\\alpha_1 - \\beta_1)^2} \\left( \\frac{Var(v_i) + Var(u_i)}{(\\alpha_1 - \\beta_1)^2} \\right)^{-1} = \\newline \u0026amp;= \\frac{\\alpha_1 Var(v_i) + \\beta_1 Var(u_i)}{Var(v_i) + Var(u_i)} \\end{aligned} $$ Which is neither $\\alpha_1$ nor $\\beta_1$ but a variance weighted average of the two.\nDemand Example (4) Suppose we have a supply shifter $z_i$ such that\n $\\mathbb E[z_i v_i] \\neq 0$ $\\mathbb E[z_i u_i] = 0$.  We combine the second condition and $\\mathbb E[u_i] = 0$ to get a system of 2 equations in 2 unknowns: $\\alpha_0$ and $\\alpha_1$. $$ \\begin{aligned} \u0026amp; \\mathbb E[z_i u_i] = \\mathbb E[ z_i (q_i^D(p_i) - \\alpha_0 - \\alpha_1 p_i) ] = 0 \\newline \u0026amp; \\mathbb E[u_i] = \\mathbb E[q_i^D(p_i) - \\alpha_0 - \\alpha_1 p_i] = 0\n\\end{aligned} $$\nWe could try to solve for the vector $\\alpha$ that solves $$ \\begin{aligned} \u0026amp; \\mathbb E_n[z_i (q_i^D - x_i\\alpha)] = 0 \\newline \u0026amp; \\mathbb E_n[z_i q_i^D] - \\mathbb E_n[z_ix_i\\alpha] = 0 \\end{aligned} $$\nIf $\\mathbb E_n[z_ix_i]$ is invertible, we get $\\hat{\\alpha} = \\mathbb E_n[z_ix_i]^{-1} \\mathbb E_n[z_i q^D_i]$ which is indeed the IV estimator of $\\alpha$ using $z_i$ as an instrument for the endogenous variable $p_i$.\nCode - DGP This code draws 100 observations from the model $y = 2 x_1 - x_2 + \\varepsilon$ where $x_1, x_2 \\sim U[0,1]$ and $\\varepsilon \\sim N(0,1)$.\n# Set seed Random.seed!(123); # Set the number of observations n = 100; # Set the dimension of Z l = 3; # Draw instruments Z = rand(Uniform(0,1), n, l); # Correlation matrix for error terms S = [1 0.8; 0.8 1]; # Endogenous X γ = [2 0; 0 -1; -1 3]; ε = rand(Normal(0,1), n, 2) * cholesky(S).U; X = Z*γ .+ ε[:,1]; # Calculate y y = X*β .+ ε[:,2];  Code - IV # Estimate beta OLS β_OLS = (X'*X)\\(X'*y)  ## 2-element Array{Float64,1}: ## 2.335699233358403 ## -0.8576266209987325  # IV: l=k=2 instruments Z_IV = Z[:,1:k]; β_IV = (Z_IV'*X)\\(Z_IV'*y)  ## 2-element Array{Float64,1}: ## 1.6133344277861439 ## -0.6678537395714547  # Calculate standard errors ε_hat = y - X*β_IV; V_NHC_IV = var(ε_hat) * inv(Z_IV'*X)*Z_IV'*Z_IV*inv(Z_IV'*X); V_HC0_IV = inv(Z_IV'*X)*Z_IV' * (I(n) .* ε_hat.^2) * Z_IV*inv(Z_IV'*X);  Code - 2SLS # 2SLS: l=3 instruments Pz = Z*inv(Z'*Z)*Z'; β_2SLS = (X'*Pz*X)\\(X'*Pz*y)  ## 2-element Array{Float64,1}: ## 1.904553638377971 ## -0.8810907510370429  # Calculate standard errors ε_hat = y - X*β_2SLS; V_NCH_2SLS = var(ε_hat) * inv(X'*Pz*X); V_HC0_2SLS = inv(X'*Pz*X)*X'*Pz * (I(n) .* ε_hat.^2) *Pz*X*inv(X'*Pz*X);  GMM Setting We have a system of $L$ moment conditions $$ \\begin{aligned} \u0026amp; \\mathbb E[g_1(\\omega_i, \\delta_0)] = 0 \\newline \u0026amp; \\vdots \\newline \u0026amp; \\mathbb E[g_L(\\omega_i, \\delta_0)] = 0 \\end{aligned} $$\nIf $L = \\dim (\\delta_0)$, no problem. If $L \u0026gt; \\dim (\\delta_0)$, there may be no solution to the system of equations.\nOptions There are two possibilities.\n First Solution: add moment conditions until the system is identified $$ \\mathbb E[ a' g(\\omega_i, \\delta_0)] = 0 $$ Solve $\\mathbb E[Ag(\\omega_i, \\delta)] = 0$ for $\\hat{\\delta}$. How to choose $A$? Such that it minimizes $Var(\\hat{\\delta})$. Second Solution: generalized method of moments (GMM) $$ \\begin{aligned} \\hat{\\delta} _ {GMM} \u0026amp;= \\arg \\min _ \\delta \\quad \\Big| \\Big| \\mathbb E_n [ g(\\omega_i, \\delta) ] \\Big| \\Big| = \\newline \u0026amp;= \\arg \\min _ \\delta \\quad n \\mathbb E_n[g(\\omega_i, \\delta)]' W \\mathbb E_n [g(\\omega_i, \\delta)] \\end{aligned} $$   The choice of $A$ and $W$ are closely related to each other.\n 1-step GMM Since $J(\\delta,W)$ is a quadratic form, a closed form solution exists: $$ \\hat{\\delta}(W) = \\Big(\\mathbb E_n[z_i x_i'] W \\mathbb E_n[z_i x_i'] \\Big)^{-1}\\mathbb E_n[z_i x_i'] W \\mathbb E_n[z_i y_i] $$\nAssumptions for consistency of the GMM estimator given data $\\mathcal D = \\lbrace y_i, x_i, z_i \\rbrace _ {i=1}^n$:\n Linearity: $y_i = x_i\\gamma_0 + \\varepsilon_i$ IID: $(y_i, x_i, z_i)$ iid Orthogonality: $\\mathbb E [z_i(y_i - x_i\\gamma_0)] = \\mathbb E[z_i \\varepsilon_i] = 0$ Rank identification: $\\Sigma_{xz} = \\mathbb E[z_i x_i']$ has full rank  Convergence Theorem\nUnder linearity, independence, orthogonality and rank conditions, if $\\hat{W} \\overset{p}{\\to} W$ positive definite, then $$ \\hat{\\delta}(\\hat{W}) \\to \\delta(W) $$ If in addition to the above assumption, $\\sqrt{n} \\mathbb E_n [g(\\omega_i, \\delta_0)] \\overset{d}{\\to} N(0,S)$ for a fixed positive definite $S$, then $$ \\sqrt{n} (\\hat{\\delta} (\\hat{W}) - \\delta(W)) \\overset{d}{\\to} N(0,V) $$ where $V = (\\Sigma' _ {xz} W \\Sigma _ {xz})^{-1} \\Sigma _ {xz} W S W \\Sigma _ {xz}(\\Sigma' _ {xz} W \\Sigma _ {xz})^{-1}$.\nFinally, if a consistent estimator $\\hat{S}$ of $S$ is available, then using sample analogues $\\hat{\\Sigma}_{xz}$ it follows that $$ \\hat{V} \\overset{p}{\\to} V $$\n If $W = S^{-1}$ then $V$ reduces to $V = (\\Sigma' _ {xz} W \\Sigma _ {xz})^{-1}$. Moreover, $(\\Sigma' _ {xz} W \\Sigma _ {xz})^{-1}$ is the smallest possible form of $V$, in a positive definite sense.\n Therefore, to have an efficient estimator, you want to construct $\\hat{W}$ such that $\\hat{W} \\overset{p}{\\to} S^{-1}$.\n2-step GMM Estimation steps:\n Choose an arbitrary weighting matrix $\\hat{W}_{init}$ (usually the identity matrix $I_K$) Estimate $\\hat{\\delta} _ {init}(\\hat{W} _ {init})$ Estimate $\\hat{S}$ (asymptotic variance of the moment condition) Estimate $\\hat{\\delta}(\\hat{S}^{-1})$   On the procedure:\n This estimator achieves the semiparametric efficiency bound. This strategy works only if $\\hat{S} \\overset{p}{\\to} S$ exists. For iid cases: we can use $\\hat{\\delta} = \\mathbb E_n[(\\hat{\\varepsilon}_i z_i)(\\hat{\\varepsilon}_i z_i) ' ]$ where $\\hat{\\varepsilon}_i = y_i - x_i \\hat{\\delta}(\\hat{W} _ {init})$.   Code - 1-step GMM # GMM 1-step: inefficient weighting matrix W_1 = I(l); # Objective function gmm_1(b) = ( y - X*b )' * Z * W_1 * Z' * ( y - X*b ); # Estimate GMM β_gmm_1 = optimize(gmm_1, β_OLS).minimizer  ## 2-element Array{Float64,1}: ## 1.91556882526808 ## -0.8769689391885799  # Standard errors GMM ε_hat = y - X*β_gmm_1; S_hat = Z' * (I(n) .* ε_hat.^2) * Z; d_hat = -X'*Z; V_gmm_1 = inv(d_hat * inv(S_hat) * d_hat')  ## 2×2 Array{Float64,2}: ## 0.0158497 -0.00346601 ## -0.00346601 0.00616531  Code - 2-step GMM # GMM 2-step: efficient weighting matrix W_2 = inv(S_hat); # Objective function gmm_2(b) = ( y - X*b )' * Z * W_2 * Z' * ( y - X*b ); # Estimate GMM β_gmm_2 = optimize(gmm_2, β_OLS).minimizer  ## 2-element Array{Float64,1}: ## 1.905326742963115 ## -0.881808949213345  # Standard errors GMM ε_hat = y - X*β_gmm_2; S_hat = Z' * (I(n) .* ε_hat.^2) * Z; d_hat = -X'*Z; V_gmm_2 = inv(d_hat * inv(S_hat) * d_hat')  ## 2×2 Array{Float64,2}: ## 0.0162603 -0.00357632 ## -0.00357632 0.00631259  Testing Overidentifying Restrictions If the equations are exactly identified, then it is possible to choose $\\delta$ so that all the elements of the sample moments $\\mathbb E_n[g(\\omega_i; \\delta)]$ are zero and thus that the distance $$ J(\\delta, \\hat{W}) = n \\mathbb E_n[g(\\omega_i, \\delta)]' \\hat{W} \\mathbb E_n[g(\\omega_i, \\delta)] $$ is zero. (The $\\delta$ that does it is the IV estimator.)\nIf the equations are overidentified, i.e. $L$ (number of instruments) $\u0026gt; K$ (number of equations), then the distance cannot be zero exactly in general, but we would expect the minimized distance to be close to zero.\nNaive Test Suppose your model is overidentified ($L \u0026gt; K$) and you use the following naive testing procedure:\n Estimate $\\hat{\\delta}$ using a subset of dimension $K$ of instruments $\\lbrace z_1 , .. , z_K\\rbrace$ for $\\lbrace x_1 , \u0026hellip; , x_K\\rbrace$ Set $\\hat{\\varepsilon}_i = y_i - x_i \\hat{\\delta} _ {\\text{GMM}}$ Infer the size of the remaining $L-K$ moment conditions $\\mathbb E[z _{i, K+1} \\varepsilon_i], \u0026hellip;, \\mathbb E[z _{i, L} \\varepsilon_i]$ looking at their empirical counterparts $\\mathbb E_n[z _{i, K+1} \\hat{\\varepsilon}_i], \u0026hellip;, \\mathbb E_n[z _{i, L} \\hat{\\varepsilon}_i]$ Reject exogeneity if the empirical expectations are high. How high? Calculate p-values.  Example If you have two invalid instruments and you use one to test the validity of the other, it might happen by chance that you don’t reject it.\n  Model: $y_i = x_i + \\varepsilon_i$ and $x_i = \\frac{1}{2} z _{i1} - \\frac{1}{2} z _{i2} + u_i$\n  Have $$ Cov (z _{i1}, z _{i2}, \\varepsilon_i, u_i) = \\begin{bmatrix} 1 \u0026amp; 0 \u0026amp; 0 \u0026amp; 0 \\newline 0 \u0026amp; 1 \u0026amp; 0 \u0026amp; 0 \\newline 0 \u0026amp; 0 \u0026amp; 1 \u0026amp; 0.5 \\newline 0 \u0026amp; 0 \u0026amp; 0.5 \u0026amp; 1 \\end{bmatrix} $$\n  You want to test whether the second instrument is valid (is not since $\\mathbb E[z_2 \\varepsilon] \\neq 0$). You use $z_1$ and estimate $\\hat{\\beta} \\to$ the estimator is consistent.\n  You obtain $\\mathbb E_n[z _{i2} \\hat{\\varepsilon}_i] \\simeq 0$ even if $z_2$ is invalid\n  Problem: you are using an invalid instrument in the first place.\n  Hansen’s Test Theorem: We are interested in testing $H_0: \\mathbb E[z_i \\varepsilon_i] = 0$ against $H_1: \\mathbb E[z_i \\varepsilon_i] \\neq 0$. Suppose $\\hat{S} \\overset{p}{\\to} S$. Then $$ J(\\hat{\\delta}(\\hat{S}^{-1}) , \\hat{S}^{-1}) \\overset{d}{\\to} \\chi^2 _ {L-K} $$ For $c$ satisfying $\\alpha = 1- G_{L - K} ( c )$, $\\Pr(J\u0026gt;c | H_0) \\to \\alpha$ so the test reject $H_0$ if $J \u0026gt; c$ has asymptotic size $\\alpha$.\nComments  The degrees of freedom of the asymptotic distribution are the number of overidentifying restrictions. This is a specification test, testing whether all model assumptions are true jointly. Only when we are confident that about the other assumptions, can we interpret a large $J$ statistic as evidence for the endogeneity of some of the $L$ instruments included in $x$. Unlike the tests we have encountered so far, the test is not consistent against some failures of the orthogonality conditions (that is, it is not consistent against some fixed elements of the alternative). Several papers in the July 1996 issue of JBES report that the finite-sample null rejection probability of the test can far exceed the nominal significance level $\\alpha$.  Special Case: Conditional Homoskedasticity The main implication of conditional homoskedasticity is that efficient GMM becomes 2SLS. With efficient GMM estimation, the weighting matrix is $\\hat{S}^{-1} = \\mathbb En [z_i z_i' \\varepsilon_i^2]^{-1}$. With conditional homoskedasticity, the efficient weighting matrix is $\\mathbb E_n[z_iz_i']^{-1} \\sigma^{-2}$, or equivalently $\\mathbb E_n[z_iz_i']^{-1}$. Then, the GMM estimator becomes $$ \\hat{\\delta}(\\hat{S}^{-1}) = \\Big(\\mathbb E_n[z_i x_i']' \\underbrace{\\mathbb E_n[z_iz_i']^{-1} \\mathbb E[z_i x_i']} _ {\\text{ols of } x_i \\text{ on }z_i} \\Big)^{-1}\\mathbb E_n[z_i x_i']' \\underbrace{\\mathbb E_n[z_iz_i']^{-1} \\mathbb E[z_i y_i']} _ {\\text{ols of } y_i \\text{ on }z_i}= \\hat{\\delta} _ {2SLS} $$\nProof: Consider the matrix notation. $$ \\begin{aligned} \\hat{\\delta} \\left( \\frac{Z\u0026rsquo;Z}{n}\\right) \u0026amp;= \\left( \\frac{X\u0026rsquo;Z}{n} \\left( \\frac{Z\u0026rsquo;Z}{n}\\right)^{-1} \\frac{Z\u0026rsquo;X}{n} \\right)^{-1} \\frac{X\u0026rsquo;Z}{n} \\left( \\frac{Z\u0026rsquo;Z}{n}\\right)^{-1} \\frac{Z\u0026rsquo;Y}{n} = \\newline \u0026amp;= \\left( X\u0026rsquo;Z(Z\u0026rsquo;Z)^{-1} Z\u0026rsquo;X \\right)^{-1} X\u0026rsquo;Z(Z\u0026rsquo;Z)^{-1} Z\u0026rsquo;Y = \\newline \u0026amp;= \\left(X\u0026rsquo;P_ZX\\right)^{-1} X\u0026rsquo;P_ZY = \\newline \u0026amp;= \\left(X\u0026rsquo;P_ZP_ZX\\right)^{-1} X\u0026rsquo;P_ZY = \\newline \u0026amp;= \\left(\\hat{X}'_Z \\hat{X}_Z\\right)^{-1} \\hat{X}'_ZY = \\newline \u0026amp;= \\hat{\\delta} _ {2SLS} \\end{aligned} $$ $$\\tag*{$\\blacksquare$}$$\nSmall-Sample Properties of 2SLS Theorem: When the number of instruments is equal to the sample size ($L = n$), then $\\hat{\\delta} _ {2SLS} = \\hat{\\delta} _ {OLS}$\nProof: We have a perfect prediction problem. The first stage estimated coefficient $\\hat{\\gamma}$ is such that it solves the normal equations: $\\hat{\\gamma} = z_i^{-1} x_i$. Then $$ \\begin{aligned} \\hat{\\delta} _ {2SLS} \u0026amp;= \\mathbb E_n[\\hat{x}_i x'_i]^{-1} \\mathbb E_n[\\hat{x}_i y_i] = \\newline \u0026amp;= \\mathbb E_n[z_i z_i^{-1} x_i x'_i]^{-1} \\mathbb E_n[z_i z_i^{-1} x_i y_i] = \\newline \u0026amp;= \\mathbb E_n[x_i x'_i]^{-1} \\mathbb E_n[x_i y_i] = \\newline \u0026amp;= \\hat{\\delta} _ {OLS} \\end{aligned} $$ $$\\tag*{$\\blacksquare$}$$\n You have this overfitting problem in general when the number of instruments is large relative to the sample size. This problem arises even if the instruments are valid.\n Example from Angrist (1992)  They regress wages on years of schooling. Problem: endogeneity: both variables are correlated with skills which are unobserved. Solution: instrument years of schooling with the quarter of birth.  Idea: if born in the first three quarters, can attend school from the year of your sixth birthday. Otherwise, you have to wait one more year.   Problem: quarters of birth are three dummies.  In order to ``improve the first stage fit” they interact them with year of birth (180 effective instruments) and also with the state (1527 effective instruments). This mechanically increases the $R^2$ but also increases the bias of the 2SLS estimator.   Solutions: LIML, JIVE, RJIVE (Hansen et al., 2014), Post-Lasso (Belloni et al., 2012).  Example from Angrist (1992) Many Instrument Robust Estimation Issue Why having too many instruments is problematic? As the number of instruments increases, the estimated coefficient gets closer to OLS which is biased. As seen in the theorem above, for $L=n$, the two estimators coincide.\nLIML An alternative method to estimate the parameters of the structural equation is by maximum likelihood. Anderson and Rubin (1949) derived the maximum likelihood estimator for the joint distribution of $(y_i, x_i)$. The estimator is known as limited information maximum likelihood, or LIML.\nThis estimator is called “limited information” because it is based on the structural equation for $(y_i, x_i)$ combined with the reduced form equation for $x_i$. If maximum likelihood is derived based on a structural equation for $x_i$ as well, then this leads to what is known as full information maximum likelihood (FIML). The advantage of the LIML approach relative to FIML is that the former does not require a structural model for $x_i$, and thus allows the researcher to focus on the structural equation of interest - that for $y_i$.\nK-class Estimators The k-class estimators have the form $$ \\hat{\\delta}(\\alpha) = (X' P_Z X - \\alpha X' X)^{-1} (X' P_Z Y - \\alpha X' Y) $$\nThe limited information maximum likelihood estimator LIML is the k-class estimator $\\hat{\\delta}(\\alpha)$ where $$ \\alpha = \\lambda_{min} \\Big( ([X' , Y]^{-1} [X' , Y])^{-1} [X' , Y]^{-1} P_Z [X' , Y] \\Big) $$\nIf $\\alpha = 0$ then $\\hat{\\delta} _ {\\text{LIML}} = \\hat{\\delta} _ {\\text{2SLS}}$ while for $\\alpha \\to \\infty$, $\\hat{\\delta} _ {\\text{LIML}} \\to \\hat{\\delta} _ {\\text{OLS}}$.\nComments on LIML  The particular choice of $\\alpha$ gives a many instruments robust estimate The LIML estimator has no finite sample moments. $\\mathbb E[\\delta(\\alpha_{LIML})]$ does not exist in general In simulation studies performs well Has good asymptotic properties  Asymptotically the LIML estimator has the same distribution as 2SLS. However, they can have quite different behaviors in finite samples. There is considerable evidence that the LIML estimator has superior finite sample performance to 2SLS when there are many instruments or the reduced form is weak. However, on the other hand there is worry that since the LIML estimator is derived under normality it may not be robust in non-normal settings.\nJIVE The Jacknife IV procedure is the following\n Regress $\\lbrace x_j \\rbrace _ {j \\neq i}$ on $\\lbrace z_j \\rbrace _ {j \\neq i}$ and estimate $\\pi_{-i}$ (leave the $i^{th}$ observation out). Form $\\hat{x}_i = \\hat{\\pi} _ {-i} z_i$. Run IV using $\\hat{x}_i$ as instruments. $$ \\hat{\\delta} _ {JIVE} = \\mathbb E_n[\\hat{x}_i x_i']^{-1} \\mathbb E_n[\\hat{x}_i y_i'] $$  Comments on JIVE:  Prevents overfitting. With many instruments you get bad out of sample prediction which implies low correlation between $\\hat{x}_i$ and $x_i$: $\\mathbb E_n[\\hat{x}_i x_i'] \\simeq 0$. Use lasso/ridge regression in the first stage in case of too many instruments.  Hausman Test Here we consider testing the validity of OLS. OLS is generally preferred to IV in terms of precision. Many researchers only doubt the (joint) validity of the regressor $z_i$ instead of being certain that it is invalid (in the sense of not being predetermined). So then they wish to choose between OLS and 2SLS, assuming that they have an instrument vector $x_i$ whose validity is not in question. Further, assume for simplicity that $L = K$ so that the efficient GMM estimator is the IV estimator.\nThe Hausman test statistic $$ H \\equiv n (\\hat{\\delta} _ {IV} - \\hat{\\delta} _ {OLS})' [\\hat{Avar} (\\hat{\\delta} _ {IV} - \\hat{\\delta} _ {OLS})]^{-1} (\\hat{\\delta} _ {IV} - \\hat{\\delta} _ {OLS}) $$ is asymptotically distributed as a $\\chi^2_{L-s}$ under the null where $s = | z_i \\cup x_i |$: the number of regressors that are retained as instruments in $x_i$.\nComments In general, the idea of the Hausman test is the following. If you have two estimators, one which is efficient under $H_0$ but inconsistent under $H_1$ (in this case, OLS), and another which is consistent under $H_1$ (in this case, IV), then construct a test as a quadratic form in the differences of the estimators. Another classic example arises in panel data with the hypothesis $H_0$ of unconditional strict exogeneity. In that case, under $H_0$ Random Effects estimators are efficient but under $H_1$ they are inconsistent. Fixed Effects estimators instead are consistent under $H_1$.\nThe Hausman test statistic can be used as a pretest procedure: select either OLS or IV according to the outcome of the test. Although widely used, this pretest procedure is not advisable. When the null is false, it is still possible that the test accepts the null (committing a Type 2 error). In particular, this can happen with a high probability when the sample size is small and/or when the regressor $z_i$ is almost valid. In such an instance, estimation and also inference will be based on incorrect methods. Therefore, the overall properties of the Hausman pretest procedure are undesirable.\nThe Hausman test is an example of a specification test. There are many other specification tests. One could for example test for conditional homoskedasticity. Unlike for the OLS case, there does not exist a convenient test for conditional homoskedasticity for the GMM case. A test statistic that is asymptotically chi-squared under the null is available but is extremely cumbersome; see White (1982, note 2). If in doubt, it is better to use the more generally valid inference methods that allow for conditional heteroskedasticity. Similarly, there does not exist a convenient test for serial correlation for the GMM case. If in doubt, it is better to use the more generally valid inference methods that allow for serial correlation; for example, when data are collected over time (that is, time-series data).\n","date":1635465600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1635465600,"objectID":"ce8d70a7023a1cf2ff6e68938b4ca4c9","permalink":"https://matteocourthoud.github.io/course/metrics/07_endogeneity/","publishdate":"2021-10-29T00:00:00Z","relpermalink":"/course/metrics/07_endogeneity/","section":"course","summary":"Instrumental Variables Endogeneity We say that there is endogeneity in the linear regression model if $\\mathbb E[x_i \\varepsilon_i] \\neq 0$.\nThe random vector $z_i$ is an instrumental variable in the linear regression model if the following conditions are met.","tags":null,"title":"Endogeneity","type":"book"},{"authors":null,"categories":null,"content":"Introduction Motivation IO: role of market structure on equilibrium outcomes.\nDynamics: study the endogenous evolution of market structure.\n Supply side dynamics  Irreversible investment Entry sunk costs Product repositioning costs Price adjustment costs Learning by doing   Demand side dynamics  Switching costs Durable or storable products    Bonus motivation: AI literature studies essentially the same set of problems with similar tools (Igami 2020)\n Irony: niche topic in IO (super niche in econ), but at the core of the frontier in computer science  Why? Computation is hard, estimation harder, but extremely powerful prediction tool The world is intrinsecally dynamic    Examples (1) Some examples in empirical IO\n Investment  Rust (1987): bus engine replacement decision   Durable goods  Gowrisankaran and Rysman (2012): consumer demand in the digital camcorder industry   Stockpiling  Erdem, Imai, and Keane (2003): promotions and stockpiling of ketchup Hendel and Nevo (2006): stockpiling of laundry detergents   Learning  Erdem and Keane (1996): brand learning in the laundry detergent industry Crawford and Shum (2005): demand learning of anti‐ulcer drug prescriptions   Switching costs  Handel (2013): inertia in demand for health insurance    Examples (2) But also in other applied micro fields:\n Labor economics  Should you go to college? (Keane and Wolpin 1997)   Health economics   Which health insurance to pick given there are switching costs? (Handel 2013)\n  Addiction (Becker and Murphy 1988)\n   Public finance  How should you set optimal taxes in a dynamic environment? (Golosov et al. 2006)    Do we really need dynamics? In some cases, we can reduce a dynamic problem to a:\n Static problem Reduced-form problem  E.g., Investment decision\n  Dynamic problem, as gains are realized after costs\n  “Static” solution: invest if $\\mathbb E (NPV ) \u0026gt; TC$\n  Action today ($a_t=0$ or $1$) does not affect the amount of future payoffs (NPV)\n  But many cases where it’s hard to evaluate dynamic questions in a static/reduced-form setting.\n Typically, cases where decision today would affect payoffs tomorrow And you care about those payoffs ($\\neq$ myopia)   “A dynamic model can do anything a static model can.”\n New Empirical IO So-called New Empirical IO (summary in Bresnahan (1989))\n Some decisions today might affect payoffs tomorrow But the decision today depends on the state today And the state today might have been the result of a decision yesterday Etc… Need dynamics to study these questions Where does it all start?  Pakes (1986) Berry (1992)    Pros and Cons Advantages\n  We can adress intertemporal trade-offs\n Flow vs stock stocks and benefits    We can examine transitions and not only steady states\n  We are able to address policy questions that cannot be addressed with reduced-form methods\n Standard advantage of structural estimation But in a context with relevant intertemporal trade-offs / decisions    Disadvantages\n  We typically need more assumptions\n Robustness testing will therefore be important    Identification in dynamic models is less transparent\n Thus time should be spent articulating what variation in the data identifies our parameters of interest)    It is often computationally intensive (i.e., slow / unfeasible)\n  From Statics to Dynamics Typical steps\n Specify the primitives of the model  Static: single period agents’ payoff functions (utility or profit) Dynamic: static payoffs + evolution of state variables  Can be exogenous … or endogenous: decision today has an effect on the state tomorrow     Solve for optimal behavior  Static: tipically agents maximize current utility or profit Dynamic: agents maximize present discounted value of future utilities or profits   Search for parameter values that result in the “best match” between our model predictions and observed behavior  1st year Macro Recap Markov Decision Processes Formally, a discrete-time MDP consists of the following objects\n  A discrete time index $t \\in \\lbrace 0,1,2,\u0026hellip;,T \\rbrace$, for $T \\leq \\infty$\n  A state space $\\mathcal S$\n  An action space $\\mathcal A$\n and a family of constraint sets $\\lbrace \\mathcal a_t(s_t) \\subseteq \\mathcal A \\rbrace$    A family of transition probabilities $\\lbrace \\Pr_{t}(s_{t+1}|s_t,a_t) \\rbrace$\n  A discount factor, $\\beta$\n  A family of single-period reward functions $\\lbrace (u_t(s_t,a_t) \\rbrace$\n so that the utility functional $U$ has an additively separable decomposition $$ U(\\boldsymbol s, \\boldsymbol a) = \\sum_{t=0}^{T} \\beta^{t} u_{t}\\left(s_t, a_{t}\\right) $$    MDP (2) In words\n  The state space $\\mathcal S$ contains all the information needed to\n compute static utilities $u_t (s_t, a_t)$ compute transition probabilities $\\lbrace \\Pr_{t} (s_{t+1}|s_t,a_t) \\rbrace$    The (conditional) action space $\\mathcal A (s_t)$ contains all the actions available in state $s_t$\n How can it be different by state? E.g. entry/exit decision if you’re in/out of the market    The transition probabilities $\\lbrace \\Pr_{t+1}(s_{t+1}|s_t,a_t) \\rbrace$ define the probabilities of future states $s_{t+1}$ conditional on\n Present state $s_t$ Present decision $a_t$    The discount factor $\\beta$ together with the static reward functions $\\lbrace (u_t(s_t,a_t) \\rbrace$ determines the objective function $$ \\mathbb E_{\\boldsymbol s'} \\Bigg[ \\sum_{t=0}^{T} \\beta^{t} u_{t}\\left(s_t, a_{t}\\right) \\Bigg] $$\n  Notation Brief parenthesis on notation\n  I have seen states denoted as\n $s$ (for state) $x$ $\\omega$ others, depending on the specific context, e.g. $e$ for experience  I will try to stick to $s$ all the time\n  I have seen decisions denoted as\n $a$ (for action) $d$ (for decision) $x$ others, depending on the specific context, e.g. $i$ for investment  I will try to stick to $a$ all the time\n  Maximization Problem The objective is to pick the decision rule (or policy function) $P = \\boldsymbol a^* = \\lbrace a_1^*, \u0026hellip;, a_t ^ * \\rbrace$ that solves $$ \\max_{\\boldsymbol a} \\ \\mathbb E_{\\boldsymbol s'} \\Bigg[ \\sum_{t=0}^{T} \\beta^{t} u_{t} \\left(s_{t}, a_{t} \\right) \\Bigg] $$ Where the expectation is taken over transition probabilities generated by the decision rule $\\boldsymbol a$.\nStationarity In many applications, we assume stationarity\n  The transition probabilities and utility functions do not directly depend on $t$\n i.e., are the same for all $t$  $\\Pr_{{\\color{red}{t}}} (s_{t+1}|s_t,a_t) \\ \\to \\ \\Pr(s_{t+1}|s_t,a_t)$ $u_{{\\color{red}{t}}} (s_t,a_t) \\ \\to \\ u(s_t,a_t)$      Uncomfortable assumption?\n  You think there is some reason (variable) why today’s probabilities should be different from tomorrow’s?\n If observable, include that variable in the state space If unobservable, integrate it out    Stationarity (2)   In the finite horizon case ($T \\leq \\infty$), stationarity does not help much\n $\\sum_{t=0}^{T} \\beta^{t} u(s_t, a_{t})$ still depends on $t$, conditional on $s_t$ Why? Difference between $t$ and $T$ matters in the sum    In infinite-horizon problems, stationarity helps a lot\n  Now the difference between $t$ and $T$ is always the same, i.e. $\\infty$\n  $\\sum_{t=0}^{\\infty} \\beta^{t} u(s_t, a_{t})$ does not depend on $t$, conditional on $s_t$\n  The future looks the same whether the agent is in state $s_t$ at time $t$ or in state $s_{t+\\tau} = s_t$ at time $t + \\tau$\n    Value Function Consider a stationary infinite-horizon problem\n  The only variable which affects the agent’s view about the future is the current value of the state, $s_t$\n  We can rewrite the agent’s problem as $$ V_0(s_0) = \\max_{\\boldsymbol a} \\ \\mathbb E_{\\boldsymbol s'} \\Bigg[ \\sum_{t=0}^{\\infty} \\beta^{t} u\\left(s_t, a_{t}\\right) \\Bigg] $$ where\n $a_t \\in \\mathcal A(s_t) \\ \\forall t$ The expectation is taken over future states $\\boldsymbol s'$  that evolve according to $\\lbrace \\Pr(s_{t+1}|s_t,a_t) \\rbrace$   $V(\\cdot)$ is called the value function    How to solve?  One could try to solve it by brute force  i.e. try to solve for the structure of all of the optimal decisions, $\\boldsymbol a^*$ Indeed, for finite-horizon problems, that might be necessary   For stationary infinite-horizon problems, the value and policy function should be time invariant  $V_{\\color{red}{t}} (s_t) = V(s_t)$ $P_{\\color{red}{t}} (s_t) = P(s_t)$   What do we gain?  Bellman Equation $$ \\begin{align} V(s_0) \u0026amp;= \\max_{\\boldsymbol a} \\ \\mathbb E_{\\boldsymbol s'} \\Bigg[ \\sum_{t=0}^{\\infty} \\beta^{t} u(s_t, a_{t}) \\Bigg] = \\newline \u0026amp;= \\max_{\\boldsymbol a} \\ \\mathbb E_{\\boldsymbol s'} \\Bigg[ {\\color{red}{u(s_{0}, a_{0})}} + \\sum_{{\\color{red}{t=1}}}^{\\infty} \\beta^{t} u(s_t, a_{t}) \\Bigg] = \\newline \u0026amp;= \\max_{\\boldsymbol a} \\ \\Bigg\\lbrace u(s_{0}, a_{0}) + {\\color{red}{\\mathbb E_{\\boldsymbol s'}}} \\Bigg[ \\sum_{t=1}^{\\infty} \\beta^{t} u(s_t, a_{t}) \\Bigg] \\Bigg\\rbrace = \\newline \u0026amp;= \\max_{\\boldsymbol a} \\ \\Bigg\\lbrace u(s_{0}, a_{0}) + {\\color{red}{\\beta}} \\ \\mathbb E_{\\boldsymbol s'} \\Bigg[ \\sum_{t=1}^{\\infty} \\beta^{{\\color{red}{t-1}}} u(s_t, a_{t}) \\Bigg] \\Bigg\\rbrace = \\newline \u0026amp;= \\max_{{\\color{red}{a_0}}} \\ \\Bigg\\lbrace u(s_{0}, a_{0}) + \\beta \\ {\\color{red}{\\max_{\\boldsymbol a}}}\\ \\mathbb E_{\\boldsymbol s'} \\Bigg[ \\sum_{t=1}^{\\infty} \\beta^{t-1} u(s_t, a_{t}) \\Bigg] \\Bigg\\rbrace = \\newline \u0026amp;= \\max_{a_0} \\ \\Bigg\\lbrace u(s_{0}, a_{0}) + \\beta \\ {\\color{red}{\\int V(s_1) \\Pr(s_1 | s_0, a_0)}} \\Bigg\\rbrace \\end{align} $$\nBellman Equation (2) We have now a recursive formulation of the value function: the Bellman Equation $$ {\\color{red}{V(s_0)}} = \\max_{a_0} \\ \\Bigg\\lbrace u(s_{0}, a_{0}) + \\beta \\ \\int {\\color{red}{V(s_1)}} \\Pr(s_1 | s_0, a_0) \\Bigg\\rbrace $$ Intuition\n The Bellman Equation is a functional equation  Has to be satisfied in every state Can be written as ${\\color{red}{V}} = T({\\color{red}{V}})$ We are actually looking for a fixed point of $T$    The decision rule that satisfies the Bellman Equation is called the policy function $$ a(s_0) = \\arg \\max_{a_0} \\ \\Bigg\\lbrace u(s_{0}, a_{0}) + \\beta \\ \\int V(s_1) \\Pr(s_1 | s_0, a_0) \\Bigg\\rbrace $$\nContractions Under regularity conditions\n $u(s, a)$ is jointly continuous and bounded in $(s, a)$ $\\mathcal A (s)$ is a continuous correspondence  It is possible to show that $$ T(W)(s) = \\max_{a \\in \\mathcal A(s)} \\ \\Bigg\\lbrace u(s, a) + \\beta \\ \\int W(s') \\Pr(s' | s, a) \\Bigg\\rbrace $$ is a contraction mapping of modulus $\\beta$.\n Contraction Mapping Theorem: then $T$ has a unique fixed point!  Solving for the Value Function How do we actually do it in practice?\n For finite horizon MDPs: backward induction  Start from the last period: static maximization problem Move backwards taking the future value as given   For infinite horizon MDPs: different options  value function iteration  most common   policy function iteration successive approximations    Difference with 1st year Macro So what’s going to be new here?\n Estimation: retrieve model primitives from observed behavior  And related: uncertainty   Strategic interaction: multiple agents taking dynamic decisions  Next lecture    Rust (1987) Setting Rust (1987): An Empirical Model of Harold Zurcher\n  Harold Zurcher (HZ) is the city bus superintendant in Madison, WI\n  As bus engines get older, the probability of malfunctions increases\n  HZ decides when to replace old bus engines with new ones\n Optimal stopping / investment problem    Tradeoff\n Cost of a new engine (fixed, stock) Repair costs, because of engine failures (continuous, flow)    Do we care about Harold Zurcher?\n Obviously not (and neither did Rust), it’s a method paper But referee asked for an application    Data Units of observation\n Rust observes 162 buses over time  Observables: for each bus, he sees\n monthly mileage (RHS, state variable) and whether the engine was replaced (LHS, choice variable), in a given month  Variation\n on average, bus engines were replaced every 5 years with over 200,000 elapsed miles considerable variation in the time and mileage at which replacement occurs  Idea  Construct a (parametric) model which predicts the time and mileage at which engine replacement occurs Use the model predictions (conditional on parameter values) to estimate parameters that “fit” the data  predicted replacements, given mileage VS observed replacements, given mileage   Ideally use the estimates to learn something new  e.g. the correct dynamic demand curve for bus engine replacement    Static Alternative What would you do otherwise?\n You observe replacement decisions … and replacement costs $\\to$ Regress replacement decision on replacement costs  Problem\n Replacement benefits are a flow (lower maintenance costs) … while the cost is a stock  Outcome\n We expect the overestimate demand elasticity. Why? Overpredict substitutions at low costs and underpredict substitution at high cost  Model Assumptions of the structural model\n State: $s_t \\in \\lbrace 0, \u0026hellip; , s_{max} \\rbrace$  engine accumulated mileage at time $t$ Note: “continuous” in the data but has to be discretized into bins   Action: $a_t \\in \\lbrace 0, 1 \\rbrace$  replace engine at time $t$   State transitions: $\\Pr ( s_{t+1} | s_{0}, \u0026hellip; , s_t ; \\theta)= \\Pr (s_{t+1} | s_t ; \\theta )$  mileage $s_t$ evolves exogenously according to a 1st-order Markov process The transition function is the same for every bus. If HZ replaces in period $t$ ($a_t = 1$), then $s_t = 0$    Model (2) HZ static utility function (for a single bus) $$ u\\left(s_t, a_{t} ; \\theta\\right)= \\begin{cases}-c\\left(s_t ; \\theta\\right) \u0026amp; \\text { if } a_{t}=0 \\text { (not replace) } \\newline -R-c(0 ; \\theta) \u0026amp; \\text { if } a_{t}=1 \\text { (replace) }\\end{cases} $$ where\n $c(s_t ; \\theta)$: expected costs of operating a bus with mileage $s_t$  ​ including maintenance costs \u0026amp; social costs of breakdown We would expect $\\frac{\\partial c}{\\partial s}\u0026gt;0$   $R$ is the cost of replacement (i.e., a new engine)  Note that replacement occurs immediately   $u(s_t , a_t ; \\theta)$: expected current utility from operating a bus with mileage $s_t$ and making replacement decision $a_t$  Model (3) HZ objective function is to maximize the expected present discounted sum of future utilities $$ V(s_t ; \\theta) = \\max_{\\boldsymbol a} \\mathbb E_{s_{t+1}} \\left[\\sum_{\\tau=t}^{\\infty} \\beta^{\\tau-t} u\\left(s_{\\tau}, a_{\\tau} ; \\theta\\right) \\ \\Bigg| \\ s_t, \\boldsymbol a ; \\theta\\right] $$ where\n The expectation $\\mathbb E$ is over future $x$, which evolve according to Markov process $\\max$ is over future choices $a_{t+1}, \u0026hellip; ,a_{\\infty}$,  because HZ will observe future states $s_{\\tau}$ before choosing future actions $a_\\tau$, this is a functional    Notes\n This is for one bus (but multiple engines). HZ has an infinite horizon for his decision making $s_t$ summarizes state at time $t$, i.e., the expected value of future utilities only depends on $s_t$  Bellman Equation This (sequential) representation of HZ’s problem is very cumbersome to work with.\nWe can rewrite $V (s_t; \\theta)$ with the following Bellman equation $$ V\\left(s_t ; \\theta\\right) = \\max_{a_{t}} \\Bigg\\lbrace u\\left(s_t, a_{t} ; \\theta\\right)+\\beta \\mathbb E_{s_{t+1}} \\Big[V\\left(s_{t+1} ; \\theta\\right) \\Big| s_t, a_{t} ; \\theta\\Big] \\Bigg\\rbrace $$ Basically we are dividing the infinite sum (in the sequential form) into a present component and a future component.\nNotes:\n Same $V$ on both sides of equation because of infinite horizon - the future looks the same as the present for a given $s$ (i.e., it doesn’t matter where you are in time). The expectation $\\mathbb E$ is over the state-transition probabilities, $\\Pr (s_{t+1} | s_t, a_t ; \\theta)$  Order of Markow Process Suppose for a moment that $s_t$ follows a second-order markov process $$ s_{t+1}=f\\left(s_t, {\\color{red}{s_{t-1}}}, \\varepsilon ; \\theta\\right) $$ Now $s_t$ is not sufficient to describe current $V$\n We need both $s_t$ and $s_{t-1}$ in the state space (i.e., $V (s_t , {\\color{red}{s_{t-1}}}; \\theta)$ contains $s_{t-1}$, too), and the expectation is over the transition probability $\\Pr (s_{t+1} | s_t, {\\color{red}{s_{t-1}}}, a_t ; \\theta)$  Parenthesis: State Variables Which variables should be state variables? I.e. should be included in the state space?\nGeneral rule for 1st order markow processes: variables need to\n define expected current payoff, and define expectations over next period state (i.e., distribution of $s_{t+1}$)  What do you do otherwise? Integrate them out! Examples\n Weather affects static utitilies but not transition probabilities  More annoying to replace the engine if it rains Integration means: “compute expected utility of Harold Zurcher before he opens the window”   Month of the year affects transition probabilities but not utilities  Buses are used more in the winter Integration means: “compute average transition probabilities over months”     Note: you can always get the non-expected value function if you know the probability of raining or the transition probabilities by month\n Policy Function Along with this value function comes a corresponding policy (or choice) function mapping the state $s_t$ into HZ’s optimal replacement choice $a_t$ $$ P \\left(s_t ; \\theta\\right) = \\max_{a_{t}} \\Bigg\\lbrace u\\left(s_t, a_{t} ; \\theta\\right) + \\beta \\mathbb E_{s_{t+1}} \\Big[ V \\left(s_{t+1} ; \\theta\\right) \\Big| s_t, a_{t} ; \\theta\\Big] \\Bigg\\rbrace $$ Given $\\frac{\\partial c}{\\partial s}\u0026gt;0$, the policy function has the form $$ P \\left(s_t ; \\theta\\right) = \\begin{cases}1 \u0026amp; \\text { if } s_t \\geq \\gamma(\\theta) \\newline 0 \u0026amp; \\text { if } s_t\u0026lt;\\gamma(\\theta)\\end{cases} $$ where $\\gamma$ is the replacement mileage.\nHow would this compare with the optimal replacement mileage if HZ was myopic?\n Answer: HZ would wait until $R \\leq c(s)$ for the replacement action  Solving the Model Why do we want to solve for the value and policy functions?\n We want to know the agent’s optimal behavior and the equilibrium outcomes and be able to conduct comparative statics/dynamics (a.k.a. counterfactual simulations)  We have the Bellman Equation $$ V\\left(s_t ; \\theta\\right) = \\max_{a_{t}} \\Bigg\\lbrace u\\left(s_t, a_{t} ; \\theta\\right)+\\beta \\mathbb E_{s_{t+1}} \\Big[V\\left(s_{t+1} ; \\theta\\right) \\ \\Big| \\ s_t, a_{t} ; \\theta\\Big] \\Bigg\\rbrace $$ Which we can compactly write as $$ V\\left(s_t ; \\theta\\right) = T \\Big( V\\left(s_{t+1} ; \\theta\\right) \\Big) $$ Blackwell’s Theorem: under regularity conditions, $T$ is a contraction mapping with modulus $\\beta$.\nContraction Mapping Theorem: $T$ has a fixed point and we can find it by iterating $T$ from any starting value $V^{(0)}$.\nValue Function Iteration What does Blackwell’s Theorem allow us to do?\n Start with any arbitrary function $V^{(0)}(\u0001\\cdot)$ Apply the mapping $T$ to get $V^{(1)}(\u0001\\cdot) = T (V^{(0)}(\u0001\\cdot))$ Apply again $V^{(2)}(\\cdot\u0001) = T (V^{(1)}(\\cdot\u0001))$ Continue applying $T$ , and $V^{(k)}$ will converge to the unique fixed point of $T$  i.e., the true value function $V(s_t; \\theta)$   Once we have $V(s_t; \\theta)$, it’s fairly trivial to compute the policy function $P(s_t; \\theta)$  Static optimization problem (given $V$)    This process is called value function iteration\nHow to Reconcile Model and Data? Ideal Estimation Routine\n Pick a parameter value $\\theta$ Solve value and policy function (inner loop) Match predicted choices with observed choices Find the parameter value $\\hat \\theta$ that best fits the data (outer loop)  Makes the observed choices “closest” to the predicted choices (or maximizes the likelihood of the observed choices)    Issue: model easily rejected by the data\n  The policy function takes the the form: replace iff $s_t \\geq \\gamma(\\theta)$\n  Can’t explain the coexistence of e.g. “a bus without replacement at 22K miles” and “another bus being replaced at 17K miles” in the data\n  We need some unobservables in the model to explain why observed choices do not exactly match predicted choices\n  Rust (1987) - Estimation Uncertainty How can we explain different replacement actions at different mileages in the data?\n Add other observables Add some stochastic element  But where? Two options\n Randomness in decisions  I.e. “Harold Zurcher sometimes would like to replace the bus engine but he forgets” Probably still falsifiable Also need “Harold Zurcher sometimes would like not to replace but replacement happens” 🤔🤔🤔   Randomness in the state  Harold Zurcher knows something that we don’t He always makes the optimal decision but based on somethig we don’t observe    Unobservables Rust uses the following utility specification: $$ u\\left(s_t, a_{t}, {\\color{red}{\\epsilon_{t}}} ; \\theta\\right) = u\\left(s_t, a_{t} ; \\theta\\right) + {\\color{red}{\\epsilon_{a_{t} t}}} = \\begin{cases} - c\\left(s_t ; \\theta\\right) + {\\color{red}{\\epsilon_{0 t}}} \u0026amp; \\text { if } \\ a_{t}=0 \\newline \\newline -R-c(0 ; \\theta) + {\\color{red}{\\epsilon_{1 t}}} \u0026amp; \\text { if } \\ a_{t}=1 \\end{cases} $$\n The $\\epsilon_{it}$ are components of utility of alternative $a$ that are observed by HZ but not by us, the econometrician.  E.g., the fact that an engine is running unusually smoothly given its mileage, or the fact that HZ is sick and doesn’t feel like replacing the engine this month   Note: we have assumed addictive separability of $\\epsilon$ The $\\epsilon_a$s also affect HZ’s replacement decision $\\epsilon_{it}$ are both observed and relevant $\\to$ part of the state space   Can we still solve the model? Can we estimate it?\n Unobservables (2) The Bellman Equation becomes $$ V \\Big( {\\color{red}{ \\lbrace s_\\tau \\rbrace_{\\tau=1}^t , \\lbrace \\epsilon_\\tau \\rbrace_{\\tau=1}^t }} ; \\theta \\Big) = \\max_{a_{t}} \\Bigg\\lbrace u\\left(s_t, a_{t} ; \\theta\\right) + {\\color{red}{\\epsilon_{it}}} + \\beta \\mathbb E_{s_{t+1}, {\\color{red}{\\epsilon_{t+1}}}} \\Big[V\\left(s_{t+1}, {\\color{red}{\\epsilon_{it+1}}} ; \\theta\\right) \\ \\Big| \\ {\\color{red}{ \\lbrace s_\\tau \\rbrace_{\\tau=1}^t , \\lbrace \\epsilon_\\tau \\rbrace_{\\tau=1}^t }}, a_{t} ; \\theta\\Big] \\Bigg\\rbrace $$ Issues\n The problem is not Markow anymore  Is $\\epsilon_t$ correlated with $\\epsilon_{t-\\tau}$? How? Is $\\epsilon_t$ correlated with $s_t$? And $s_{t-\\tau}$? How?   Dimension of the state space has increased  From $k = (k \\text{ points})^{1 \\text{ variable} \\times 1 \\text{ period}}$ points, to $\\infty = (k \\text{ points})^{3 \\text{ variables} \\times \\infty \\text{ periods}}$ 🤯🤯 Assuming all variables assume $k$ values   Number of variables to integrate over to compute expectation $\\mathbb E$ has increased  From one variable, $s$, to three, $(s, \\epsilon_{0}, \\epsilon_{1})$    Assumptions Rust makes 4 assumptions to make the problem tractable:\n First order Markow process of $\\epsilon$ Conditional independence of $\\epsilon_t | s_t$ from $\\epsilon_{t-1}$ and $s_{t-1}$ Independence of $\\epsilon_t$ from $s_t$ Logit distribution of $\\epsilon$  Assumption 1 A1: first-order markov process of $\\epsilon$ $$ \\Pr \\Big(s_{t+1}, \\epsilon_{t+1} \\Big| s_{1}, \u0026hellip;, s_t, \\epsilon_{1}, \u0026hellip;, \\epsilon_{t}, a_{t} ; \\theta\\Big) = \\Pr \\Big(s_{t+1}, \\epsilon_{t+1} \\Big| s_t, \\epsilon_{t}, a_{t} ; \\theta \\Big) $$\n  What it buys\n $s$ and $\\epsilon$ prior to current period are irrelevant    What it still allows:\n allows $s_t$ to be correlated with $\\epsilon_t$    What are we assuming away\n Any sort of longer run dependence Does it matter? If yes, just re-consider what is one time period Or make the state space larger (as usual in Markow processes)    Assumption 1 - Implications The Bellman Equation becomes $$ V\\left(s_t, {\\color{red}{\\epsilon_{t}}} ; \\theta\\right) = \\max_{a_{t}} \\Bigg\\lbrace u\\left(s_t, a_{t} ; \\theta\\right) + {\\color{red}{\\epsilon_{a_{t} t}}} + \\beta \\mathbb E_{s_{t+1}, {\\color{red}{\\epsilon_{t+1}}}} \\Big[V(s_{t+1}, {\\color{red}{\\epsilon_{t+1}}} ; \\theta) \\ \\Big| \\ s_t, a_{t}, {\\color{red}{\\epsilon_{t}}} ; \\theta \\Big] \\Bigg\\rbrace $$\n Now the state is $(s_t, \\epsilon_t)$  sufficient, because defines both current utility and (the expectation of) next-period state, under the first-order Markov assumption $\\epsilon_t$ is now analogous to $s_t$ State space now is $k^3 = (k \\text{ points})^{3 \\text{ variables} \\times 1 \\text{ period}}$  From $\\infty = (k \\text{ points})^{3 \\text{ variables} \\times \\infty \\text{ periods}}$     Now we could use value function iteration to solve the problem  If $\\epsilon_t$ is continuous, it has to be discretised    Assumption 1 - Issues Open issues\n  Curse of dimensionality in the state space: ($s_t, \\epsilon_{0t}, \\epsilon_{1t}$)\n Before, there were $k$ points in state space (discrete values of $x$) Now there are $k^3$ : $k$ each for $s$, $\\epsilon_0$, $\\epsilon_1$  (Assuming we discretize all state variables into $k$ values)   Generally, number of points in state space (and thus computational time) increases exponentially in the number of variables    Curse of dimensionality in the expected value: $\\mathbb E_{s_{t+1}, \\epsilon_{0,t+1}, \\epsilon_{1,t+1}}$\n For each point in state space (at each iteration of the contraction mapping), need to compute  $$ \\mathbb E_{s_{t+1}, \\epsilon_{t+1}} \\Big[V (s_{t+1}, \\epsilon_{t+1} ; \\theta) \\ \\Big| \\ s_t, a_{t}, \\epsilon_{t} ; \\theta \\Big] $$\n Before, this was a 1-dimensional integral (or sum), now it’s 3-dimensional    Initial conditions\n  Assumption 2 A2: conditional independence of $\\epsilon_t | s_t$ from $\\epsilon_{t-1}$ and $s_{t-1}$ $$ \\Pr \\Big(s_{t+1}, \\epsilon_{t+1} \\Big| s_t, \\epsilon_{t}, a_{t} ; \\theta \\Big) = \\Pr \\Big( \\epsilon_{t+1} \\Big| s_{t+1} ; \\theta \\Big) \\Pr \\Big( s_{t+1} \\Big| s_t, a_{t} ; \\theta \\Big) $$\n  What it buys\n $s_{t+1}$ is independent of $\\epsilon_t$ $\\epsilon_{t+1}$ is independent of $\\epsilon_t$ and $s_t$, conditional on $s_{t+1}$    What it still allows:\n $\\epsilon$ can be correlated across time, but only through the $s$ process    What are we assuming away\n  Any time of persistent heterogeneity\n  Does it matter? Easily yes\n  There are tons of applications where the unobservables are either fixed or correlated over time\n If fixed, there are methods to handle unobserved heterogeneity (i.e. bus “types”)      Assumption 2 - Implications The Bellman Equation is $$ V\\left(s_t, {\\color{red}{\\epsilon_{t}}} ; \\theta\\right) = \\max_{a_{t}} \\Bigg\\lbrace u\\left(s_t, a_{t} ; \\theta\\right) + {\\color{red}{\\epsilon_{a_{t} t}}} + \\beta \\mathbb E_{s_{t+1}, {\\color{red}{\\epsilon_{t+1}}}} \\Big[V (s_{t+1}, {\\color{red}{\\epsilon_{t+1}}} ; \\theta) \\Big| s_t, a_{t} ; \\theta \\Big] \\Bigg\\rbrace $$\n Now $\\epsilon_{t}$ is noise that doesn’t affect the future  That is, conditional on $s_{t+1}$, $\\epsilon_{t+1}$ is uncorrelated with $\\epsilon_{t}$     Remeber: if $\\epsilon$ does not affect the future, it should’t be in the state space!\nHow? Integrate it out.\n Rust Shortcut: ASV Rust: define the alternative-specific value function $$ \\begin{align} \u0026amp;\\bar V_0 \\left(s_t ; \\theta\\right) = u\\left(s_t, 0 ; \\theta\\right) + \\beta \\mathbb E_{s_{t+1}, {\\color{red}{\\epsilon_{t+1}}}} \\Big[V\\left(s_{t+1}, {\\color{red}{\\epsilon_{t+1}} }; \\theta\\right) | s_t, a_{t}=0 ; \\theta\\Big] \\newline \u0026amp;\\bar V_1 \\left(s_t ; \\theta\\right) = u\\left(s_t, 1 ; \\theta\\right) + \\beta \\mathbb E_{s_{t+1}, {\\color{red}{\\epsilon_{t+1}}}} \\Big[V\\left(s_{t+1}, {\\color{red}{\\epsilon_{t+1}}} ; \\theta\\right) | s_t, a_{t}=1 ; \\theta\\Big] \\end{align} $$\n  $\\bar V_0 (s_t)$ is the present discounted value of not replacing, net of $\\epsilon_{0t}$\n  The state does not depend on $\\epsilon_{t}$!\n  What is the relationship with the value function? $$ V\\left(s_t, \\epsilon_{t} ; \\theta\\right) = \\max_{a_{t}} \\Bigg\\lbrace \\begin{array}{l} \\bar V_0 \\left(s_t ; \\theta\\right)+\\epsilon_{0 t} \\ ; \\newline \\bar V_1 \\left(s_t ; \\theta\\right)+\\epsilon_{1 t} \\end{array} \\Bigg\\rbrace $$\n  We have a 1-to-1 mapping between $V\\left(s_t, \\epsilon_{t} ; \\theta\\right)$ and $\\bar V_a \\left(s_t ; \\theta\\right)$ !\n If we have one, we can get the other    Rust Shortcut Can we solve for $\\bar V$?\nYes! They have a recursive formulation $$ \\begin{aligned} \u0026amp; \\bar V_0 \\left(s_t ; \\theta\\right) = u\\left(s_t, 0 ; \\theta\\right) + \\beta \\mathbb E_{s_{t+1}, {\\color{red}{\\epsilon_{t+1}}}} \\Bigg[ \\max_{a_{t+1}} \\Bigg\\lbrace \\begin{array}{l} \\bar V_0 \\left(s_{t+1} ; \\theta\\right) + {\\color{red}{\\epsilon_{0 t+1}}} \\ ; \\newline \\bar V_1 \\left(s_{t+1} ; \\theta\\right) + {\\color{red}{\\epsilon_{1 t+1}}} \\end{array} \\Bigg\\rbrace \\ \\Bigg| \\ s_t, a_{t}=0 ; \\theta \\Bigg] \\newline \u0026amp; \\bar V_1 \\left(s_t ; \\theta\\right) = u\\left(s_t, 1 ; \\theta\\right) + \\beta \\mathbb E_{s_{t+1}, {\\color{red}{\\epsilon_{t+1}}}} \\Bigg[ \\max_{a_{t+1}} \\Bigg\\lbrace \\begin{array}{l} \\bar V_0 \\left(s_{t+1} ; \\theta\\right) + {\\color{red}{\\epsilon_{0 t+1}}} \\ ; \\newline \\bar V_1 \\left(s_{t+1} ; \\theta\\right) + {\\color{red}{\\epsilon_{1 t+1}}} \\end{array} \\Bigg\\rbrace \\ \\Bigg| \\ s_t, a_{t}=1 ; \\theta \\Bigg] \\newline \\end{aligned} $$\n Rust (1988) shows that it’s a joint contraction mapping Memo: the state space now is $2k = (2 \\text{ actions}) \\times (k \\text{ points})^{1 \\text{ variables} \\times 1 \\text{ period}}$  instead of $3^k = (k \\text{ points})^{3 \\text{ variables} \\times 1 \\text{ period}}$ Much smaller!   Lesson: any state variable that does not affect continuation values (the future) does not have to be in the “actual” state space  Assumption 2 - Implications We can also split the expectation in the alternative-specific value function $$ \\begin{aligned} \u0026amp; \\bar V_0 \\left(s_t ; \\theta\\right) = u\\left(s_t, 0 ; \\theta\\right) + \\beta \\mathbb E_{s_{t+1}} \\Bigg[ \\mathbb E_{{\\color{red}{\\epsilon_{t+1}}}} \\Bigg[ \\max_{a_{t+1}} \\Bigg\\lbrace \\begin{array}{l} \\bar V_0 \\left(s_{t+1} ; \\theta\\right) + {\\color{red}{\\epsilon_{0 t+1}}} \\ ; \\newline \\bar V_1 \\left(s_{t+1} ; \\theta\\right) + {\\color{red}{\\epsilon_{1 t+1}}} \\end{array} \\Bigg\\rbrace \\ \\Bigg| \\ s_t \\Bigg] \\ \\Bigg| \\ s_t, a_{t}=0 ; \\theta \\Bigg] \\newline \u0026amp; \\bar V_1 \\left(s_t ; \\theta\\right) = u\\left(s_t, 1 ; \\theta\\right) + \\beta \\mathbb E_{s_{t+1}} \\Bigg[ \\mathbb E_{{\\color{red}{\\epsilon_{t+1}}}} \\Bigg[ \\max_{a_{t+1}} \\Bigg\\lbrace \\begin{array}{l} \\bar V_0 \\left(s_{t+1} ; \\theta\\right) + {\\color{red}{\\epsilon_{0 t+1}}} \\ ; \\newline \\bar V_1 \\left(s_{t+1} ; \\theta\\right) + {\\color{red}{\\epsilon_{1 t+1}}} \\end{array} \\Bigg\\rbrace \\ \\Bigg| \\ s_t \\Bigg] \\ \\Bigg| \\ s_t, a_{t}=1 ; \\theta \\Bigg] \\newline \\end{aligned} $$ This allows us to concentrate on one single term $$ \\mathbb E_{{\\color{red}{\\epsilon_{t+1}}}} \\Bigg[ \\max_{a_{t+1}} \\Bigg\\lbrace \\begin{array}{l} \\bar V_0 \\left(s_{t+1} ; \\theta\\right) + {\\color{red}{\\epsilon_{0 t+1}}} \\ ; \\newline \\bar V_1 \\left(s_{t+1} ; \\theta\\right) + {\\color{red}{\\epsilon_{1 t+1}}} \\end{array} \\Bigg\\rbrace \\ \\Bigg| \\ s_t \\Bigg] $$ Open issues\n Distribution of $\\epsilon_{t+1}$ has to be simulated Distribution of $\\epsilon_{t+1}$ depends on $s_t$  Assumption 3 A3: independence of $\\epsilon_t$ from $s_t$ $$ \\Pr \\Big( \\epsilon_{t+1} \\Big| s_{t+1} ; \\theta \\Big) \\Pr \\Big( s_{t+1} \\Big| s_t, a_{t} ; \\theta \\Big) = \\Pr \\big( \\epsilon_{t+1} \\big| \\theta \\big) \\Pr \\Big( s_{t+1} \\Big| s_t, a_{t} ; \\theta \\Big) $$\n  What it buys\n $\\epsilon$ not correlated with anything $$ \\mathbb E_{{\\color{red}{\\epsilon_{t+1}}}} \\Bigg[ \\max_{a_{t+1} \\in \\lbrace 0, 1 \\rbrace } \\Bigg\\lbrace \\begin{array}{l} \\bar V_0 \\left(s_{t+1} ; \\theta\\right) + {\\color{red}{\\epsilon_{0 t+1}}} \\ ; \\newline \\bar V_1 \\left(s_{t+1} ; \\theta\\right) + {\\color{red}{\\epsilon_{1 t+1}}} \\end{array} \\Bigg\\rbrace \\ \\Bigg] $$    What are we assuming away\n Some state-specific noise… probably irrelevant    Open Issues\n Distribution of $\\epsilon_{t+1}$ has to be simulated    Assumption 4 A4: $\\epsilon$ is type 1 extreme value distributed (logit)\n  What it buys\n Closed form solution for $\\mathbb E_{\\epsilon_{t+1}}$    What are we assuming away\n  Different substitution patterns\n  Relevant? Maybe, if there are at least three options (here binary choice)\n As logit assumption in demand estimation      Logit magic 🧙🪄 $$ \\mathbb E_{\\epsilon} \\Bigg[ \\max_n \\bigg( \\Big\\lbrace \\delta_n + \\epsilon_n \\Big\\rbrace_{n=1}^N \\bigg) \\Bigg] = 0.5772 + \\ln \\bigg( \\sum_{n=1}^N e^{\\delta_n} \\bigg) $$\nwhere $0.5772$ is Euler’s constant\nAssumption 4 - Implications The Bellman equation becomes $$ \\begin{aligned} \u0026amp; \\bar V_0 \\left(s_t ; \\theta\\right) = u\\left(s_t, 0 ; \\theta\\right) + \\beta \\mathbb E_{s_{t+1}} \\Bigg[ 0.5772 + \\ln \\Bigg( \\sum_{a' \\in \\lbrace 0, 1 \\rbrace} e^{\\bar V_{a'} (s_{t+1} ; \\theta)} \\Bigg) \\ \\Bigg| \\ s_t, a_{t}=0 ; \\theta \\Bigg] \\newline \u0026amp; \\bar V_1 \\left(s_t ; \\theta\\right) = u\\left(s_t, 1 ; \\theta\\right) + \\beta \\mathbb E_{s_{t+1}} \\Bigg[ 0.5772 + \\ln \\Bigg( \\sum_{a' \\in \\lbrace 0, 1 \\rbrace} e^{\\bar V_{a'} (s_{t+1} ; \\theta)} \\Bigg) \\ \\Bigg| \\ s_t, a_{t}=1 ; \\theta \\Bigg] \\newline \\end{aligned} $$\n We got fully rid of $\\epsilon$!  How? With a lot of assumptions    Estimation So far we have analysized how the 4 assumptions help solving the model.\n What about estimation?  Maximum Likelihood\n For a single bus, the likelihood function is  $$ \\mathcal L = \\Pr \\Big(s_{1}, \u0026hellip; , s_T, a_{0}, \u0026hellip; , a_{T} \\ \\Big| \\ s_{0} ; \\theta\\Big) $$\n i.e. probability of observed decisions $\\lbrace a_{0}, \u0026hellip; , a_{T} \\rbrace$ and sequence of states $\\lbrace s_{1}, \u0026hellip; , s_T \\rbrace$ conditional on the initial state $s_0$ and the parameter values $\\theta$   What is the impact of the 4 assumptions on the likelihood function?\n Likelihood Function (A1) A1: First order Markow process of $\\epsilon$\n We gain independence across time We can decompose the joint distribution in marginals across time  $$ \\begin{align} \\mathcal L(\\theta) \u0026amp;= \\Pr \\Big(s_{1}, \u0026hellip; , s_T, a_{0}, \u0026hellip; , a_{T} \\Big| s_{0} ; \\theta\\Big)\\newline \u0026amp;= \\prod_{t=1}^T \\Pr \\Big(a_{t+1} , s_{t+1} \\Big| s_t, a_t ; \\theta\\Big) \\end{align} $$\nLikelihood Function (A2) A2: independence of $\\epsilon_t$ from $\\epsilon_{t-1}$ and $s_{t-1}$ on $s_t$\n  We can decompose the joint distribution of $a_t$ and $s_{t+1}$ into marginals $$ \\begin{align} \\mathcal L(\\theta) \u0026amp;= \\prod_{t=1}^T \\Pr \\Big(a_{t+1} , s_{t+1} \\Big| s_t, a_t ; \\theta\\Big) = \\newline \u0026amp;= \\prod_{t=1}^T \\Pr \\big(a_t \\big| s_t ; \\theta\\big) \\Pr \\Big(s_{t+1} \\Big| s_t, a_t ; \\theta\\Big) \\end{align} $$\n  $\\Pr \\big(s_{t+1} \\big| s_t, a_t ; \\theta\\big)$ can be estimated from the data\n we’ll come back to it    for $\\Pr \\big(a_t \\big| s_t ; \\theta\\big)$ we need the two remaining assumptions\n  Likelihood Function (A3) A3: Independence of $\\epsilon_t$ from $s_t$\n No need to condition on $s_t$ E.g. probability of replacement  $$ \\begin{align} \\Pr \\big(a_t=1 \\big| s_t ; \\theta \\big) \u0026amp;= \\Pr \\Big( \\bar V_1 (s_{t+1} ; \\theta) + \\epsilon_{1 t+1} \\geq \\bar V_0 (s_{t+1} ; \\theta) + \\epsilon_{0 t+1} \\ \\Big| \\ s_t ; \\theta \\Big) = \\newline \u0026amp;= \\Pr \\Big( \\bar V_1 (s_{t+1} ; \\theta) + \\epsilon_{1 t+1} \\geq \\bar V_0 (s_{t+1} ; \\theta) + \\epsilon_{0 t+1} \\ \\Big| \\ \\theta \\Big) \\end{align} $$\n In words: same distribution of shocks in every state  Likelihood Function (A4) A4: Logit distribution of $\\epsilon$\n E.g. probability of replacement becomes  $$ \\begin{align} \\Pr \\big(a_t=1 \\big| s_t ; \\theta \\big) \u0026amp;= \\Pr \\Big( \\bar V_1 (s_{t+1} ; \\theta) + \\epsilon_{1 t+1} \\geq \\bar V_0 (s_{t+1} ; \\theta) + \\epsilon_{0 t+1} \\ \\Big| \\ \\theta \\Big) = \\newline \u0026amp;= \\frac{e^{\\bar V_1 (s_{t+1} ; \\theta)}}{e^{\\bar V_0 (s_{t+1} ; \\theta)} + e^{\\bar V_1 (s_{t+1} ; \\theta)}} \\end{align} $$\n We have a closed form expression!  Likelihood Function The final form of the likelihood function for one bus is $$ \\mathcal L(\\theta) = \\prod_{t=1}^T \\Pr\\big(a_t \\big| s_t ; \\theta \\big) \\Pr \\Big(s_{t+1} \\ \\Big| \\ s_t, a_t ; \\theta\\Big) $$ where\n $\\Pr \\Big(s_{t+1} \\ \\Big| \\ s_t, a_t ; \\theta\\Big)$ can be estimated from the data  given mileage $x$ and investment decision $a$, what are the observed frequencies of future states $x'$? does not have to depend on $\\theta$   $\\Pr\\big(a_t \\big| s_t ; \\theta \\big)$ depends on $\\bar V_a (s ; \\theta)$  $\\bar V_a (s ; \\theta)$ we know how to compute given a value of $\\theta$ solve by value function iteration    Likelihood Function (2) Since we have may buses, $j$, the likelihood of the data is $$ \\mathcal L(\\theta) = \\prod_{j} \\mathcal L_j (\\theta) = \\prod_{j} \\prod_{t=1}^T \\Pr\\big(a_{jt} \\big| s_{jt} ; \\theta \\big) \\Pr \\Big(s_{j,t+1} \\ \\Big| \\ s_{jt}, a_{jt} ; \\theta\\Big) $$ And, as usual, we prefer to work with log-likelihoods $$ \\log \\mathcal L(\\theta) = \\sum_{j} \\sum_{t=1}^T \\Bigg( \\log \\Pr\\big(a_{jt} \\big| s_{jt} ; \\theta \\big) + \\log\\Pr \\Big(s_{j,t+1} \\ \\Big| \\ s_{jt}, a_{jt} ; \\theta\\Big) \\Bigg) $$\nEstimation Now we have all the pieces to estimate $\\theta$!\nProcedure\n Estimate the state transition probabilities $\\Pr \\big(s_{t+1} \\big| s_t, a_t ; \\theta\\big)$ Select a value of $\\theta$ Init a choice-specific value function $\\bar V_a^{(0)} (s_{t+1} ; \\theta)$  Apply the Bellman operator to compute $\\bar V_a^{(1)} (s_{t+1} ; \\theta)$ Iterate until convergence to $\\bar V_d^{(k \\to \\infty)} (s_{t+1} ; \\theta)$ (inner loop)   Compute the choice probabilities $\\Pr \\big(a_t\\big| s_t ; \\theta \\big)$ Compute the likelihood $\\mathcal L = \\prod_j \\prod_{t=1}^T \\Pr \\big(a_t \\big| s_t ; \\theta\\big) \\Pr \\Big(s_{t+1} \\Big| s_t, a_t ; \\theta\\Big)$ Iterate (2-5) until you are have found a (possibly global) minimum (outer loop)  Results What do dynamics add?\n Static demand curve ($\\beta =0$) is much more sensitive to the price of engine replacement. Why?  Compares present price with present savings   If you compare present price with flow of future benefits, you are less price sensitive  More realistic    Extensions Main limitation of Rust (1987): value function iteration\n Costly: has to be done for each parameter explored during optimization Particularly costly if the state space is large  Solutions\n Solve the model without solving a fixed point problem  Hotz and Miller (1993)   Solve the model and estimate the parameters at the same time  Inner and outer loop in parallel Imai, Jain, and Ching (2009)   Treat the estimation as a constrained optimization problem  MPEC, as for demand Use off-the-shelf optimization algorithms Su and Judd (2012)    We’ll cover Hotz and Miller (1993) since at the core of the estimation of dynamic games.\nHotz \u0026amp; Miller (1993) Motivation Setting: Harold Zurcher problem\n same model same assumptions same notation same objective  Problem: computationally intense to do value function iteration\n Can we solve the model without solving a fixed point problem?\n Estimation in Rust How did we estimate the model in Rust? Two main equations\n  Solve the Bellman equation of the alternative-specific value function $$ {\\color{green}{\\bar V(s; \\theta)}} = \\tilde f( {\\color{green}{\\bar V(s; \\theta)}}) $$\n  Compute the expected policy function $$ {\\color{blue}{P( \\cdot | s; \\theta)}} = \\tilde g( {\\color{green}{\\bar V(s; \\theta)}} ; \\theta) $$\n  Maximize the likelihood function\n  $$ \\mathcal L(\\theta) = \\prod_{j} \\prod_{t=1}^T {\\color{blue}{ \\Pr\\big(a_{jt} \\big| s_{jt} ; \\theta \\big)}} \\Pr \\Big(s_{j,t+1} \\ \\Big| \\ s_{jt}, a_{jt} ; \\theta\\Big) $$\n Can we remove step 1?\n Hotz \u0026amp; Miller Idea(s) Idea 1: it would be great if we could start from something like $$ {\\color{blue}{P(\\cdot|s; \\theta)}} = T( {\\color{blue}{P(\\cdot|s; \\theta)}} ; \\theta) $$\n No need to solve for the value function But we would still need a to solve a fixed point problem Back from the start? No  Idea 2: could replace the RHS element with a consistent estimate $$ {\\color{blue}{P(\\cdot|s; \\theta)}} = T( {\\color{red}{\\hat P(\\cdot|s; \\theta)}} ; \\theta) $$ And this could give us an estimating equation!\n Unclear? No problem, let’s go slowly step by step\n Two Main Equations   Bellman equation $$ {\\color{green}{\\bar V_a \\left(s_t ; \\theta\\right)}} = u\\left(s_t, a ; \\theta\\right) + \\beta \\mathbb E_{s_{t+1}, \\epsilon_{t+1}} \\Bigg[ \\max_{a'} \\Big\\lbrace {\\color{green}{\\bar V_{a'}}} \\left(s_{t+1}; \\theta\\right) + \\epsilon_{a',t+1} \\Big\\rbrace \\ \\Big| \\ s_t, a_t=a ; \\theta \\Bigg] $$\n  Expected policy function\n  $$ {\\color{blue}{\\Pr \\big(a_t=a \\big| s_t ; \\theta \\big)}} = \\Pr \\Big( {\\color{green}{\\bar V_a (s_{t+1} ; \\theta)}} + \\epsilon_{a, t+1} \\geq {\\color{green}{\\bar V_{a'} (s_{t+1} ; \\theta)}} + \\epsilon_{a', t+1} , \\ \\forall a' \\ \\Big| \\ \\theta \\Big) $$\nExpected decision before the shocks $\\epsilon_t$ are realized\n Not the policy function  The policy function maps $s_t \\times \\epsilon \\to \\lbrace 0 , 1 \\rbrace$ The expected policy function maps $s_t \\to [ 0 , 1 ]$   Easier to work with: does not depend on the shocks  Not a deterministic policy, but a stochastic one    Hotz \u0026amp; Miller - Idea 1 How do we get from the two equations $$ \\begin{aligned} {\\color{green}{\\bar V(s; \\theta)}} \u0026amp;= \\tilde f( {\\color{green}{\\bar V(s; \\theta)}}) \\newline {\\color{blue}{P(\\cdot|s; \\theta)}} \u0026amp;= \\tilde g( {\\color{green}{\\bar V(s; \\theta)}} ; \\theta) \\end{aligned} $$ To one? $$ {\\color{blue}{P(\\cdot|s; \\theta)}} = T ({\\color{blue}{P(\\cdot|s; \\theta)}}; \\theta) $$ If we could express $\\bar V$ in terms of $P$, … $$ \\begin{aligned} {\\color{green}{\\bar V(s; \\theta)}} \u0026amp; = \\tilde h( {\\color{blue}{P(\\cdot|s; \\theta)}}) \\newline {\\color{blue}{P(\\cdot|s; \\theta)}} \u0026amp;= \\tilde g( {\\color{green}{\\bar V(s; \\theta)}} ; \\theta) \\end{aligned} $$\n…. we could then substitute the first equation into the second …\nBut, easier to work with a different representation of the value function.\nExpected Value Function Recall Rust value function (not the alternative-specific $\\bar V$) $$ V\\left(s_t, \\epsilon_t ; \\theta\\right) = \\max_{a_{t}} \\Bigg\\lbrace u \\left( s_t, a_{t} ; \\theta \\right) + \\epsilon_{a_{t} t} + \\beta \\mathbb E_{s_{t+1}, \\epsilon_{t+1}} \\Big[V\\left(s_{t+1}, \\epsilon_{t+1} ; \\theta\\right) \\Big| s_t, a_{t} ; \\theta\\Big] \\Bigg\\rbrace $$ We can express it in terms of expected value function\n$$ V\\left(s_t ; \\theta\\right) = \\mathbb E_{\\epsilon_t} \\Bigg[ \\max_{a_{t}} \\Bigg\\lbrace u\\left(s_t, a_t ; \\theta\\right) + \\epsilon_{a_{t} t}+ \\beta \\mathbb E_{s_{t+1}} \\Big[V\\left(s_{t+1}; \\theta\\right) \\Big| s_t, a_{t} ; \\theta\\Big] \\Bigg\\rbrace \\Bigg] $$\n  Value of being in state $s_t$ without knowing the realization of the shock $\\epsilon_t$\n “Value of Harold Zurcher before opening the window and seeing if it’s raining or not”    Analogous to the relationship between policy funciton and expected policy function\n  Note\n expectation of future value now is only over $s_{t+1}$ $V\\left(s_t ; \\theta\\right)$ can be solved via value function iteration as the operator on the RHS is a contraction    Representation Equivalence Recall the alternative-specific value function of Rust\n$$ \\begin{align} {\\color{green}{\\bar V_a \\left( s_t ; \\theta\\right)}} \u0026amp;= u\\left(s_t, d ; \\theta\\right) + \\beta \\mathbb E_{s_{t+1}, \\epsilon_{t+1}} \\Bigg[ \\max_{a'} \\Big\\lbrace {\\color{green}{\\bar V_{a'} \\left(s_{t+1}; \\theta\\right)}} + \\epsilon_{a',t+1} \\Big\\rbrace \\ \\Big| \\ s_t, a_t=a ; \\theta \\Bigg] \\newline \u0026amp;=u\\left(s_t, a ; \\theta\\right)+\\beta \\mathbb E_{s_{t+1}, \\epsilon_{t+1}} \\Big[ {\\color{orange}{V \\left( s_{t+1}, \\epsilon_{t+1} ; \\theta \\right)}} \\Big| s_t, a_t=a ; \\theta \\Big] \\newline \u0026amp;= u \\left( s_t, a ; \\theta \\right) + \\beta \\mathbb E_{s_{t+1}} \\Big[ {\\color{red}{V \\left( s_{t+1} ; \\theta \\right)}} \\Big| s_t, a_t=a; \\theta \\Big] \\end{align} $$\nRelationship with the value function\n$$ {\\color{orange}{V \\left(s_t, \\epsilon_{t} ; \\theta \\right)}} = \\max_{a_{t}} \\Big\\lbrace {\\color{green}{ \\bar V_0 \\left( s_t ; \\theta \\right)}} + \\epsilon_{0t}, {\\color{green}{\\bar V_1 \\left( s_t ; \\theta \\right)}} + \\epsilon_{1t} \\Big\\rbrace $$\nRelationship with the expected value function $$ {\\color{red}{V\\left(s_t ; \\theta\\right)}} = \\mathbb E_{\\epsilon_t} \\Big[ {\\color{orange}{V\\left(s_t, \\epsilon_{t} ; \\theta\\right)}} \\ \\Big| \\ s_t \\Big] $$\nGoal We switched from alternative-specific value function ${\\color{green}{\\bar V (s_t ; \\theta)}}$ to expected value function ${\\color{red}{V(s_t ; \\theta)}}$\n But the goal is the same  Go from this representation $$ \\begin{align} {\\color{red}{V(s ; \\theta)}} \u0026amp; = f( {\\color{red}{V(s ; \\theta)}}) \\newline {\\color{blue}{P(\\cdot | s ; \\theta)}} \u0026amp; = g( {\\color{red}{V(s ; \\theta)}}; \\theta) \\end{align} $$ To this $$ \\begin{align} {\\color{red}{V(s ; \\theta)}} \u0026amp; = h( {\\color{blue}{P(\\cdot|s ; \\theta)}} ; \\theta) \\newline {\\color{blue}{P(\\cdot|s ; \\theta)}} \u0026amp; = g({\\color{red}{V(s ; \\theta)}}; \\theta) \\end{align} $$ I.e. we want to express the expected value function (EV) in terms of the expected policy function (EP).\n **Note **: the $f$, $g$ and $h$ functions are different functions now.\n Express EV in terms of EP (1) First, let’s ged rid of one operator: the max operator $$ V\\left(s_t ; \\theta\\right) = \\sum_a \\Pr \\Big(a_t=a | s_t ; \\theta \\Big) * \\left[\\begin{array}{c} u\\left(s_t, a ; \\theta\\right) + \\mathbb E_{\\epsilon_t} \\Big[\\epsilon_{at}\\Big| a_t=a, s_t\\Big] \\newline \\qquad + \\beta \\mathbb E_{s_{t+1}} \\Big[V\\left(s_{t+1} ; \\theta\\right) \\Big| s_t, a_t=a ; \\theta\\Big] \\end{array}\\right] $$\n  We are just substituting the $\\max$ with the policy $\\Pr\\left(a_t=a| s_t ; \\theta\\right)$\n  Important: we got rid of the $\\max$ operator\n  But we are still taking the expectation over\n Future states $s_{t+1}$ Shocks $\\epsilon_t$    Express EV in terms of EP (2) Now we get rid of another operator: the expectation over $s_{t+1}$ $$ \\mathbb E_{s_{t+1}} \\Big[V\\left(s_{t+1} ; \\theta\\right) \\Big| s_t, a_t=a ; \\theta\\Big] \\qquad \\to \\qquad \\sum_{s_{t+1}} V\\left(s_{t+1} ; \\theta\\right) \\Pr \\Big(s_{t+1} \\Big| s_t, a_t=a ; \\theta \\Big) $$ where\n $\\sum_{s_{t+1}}$ is the summation over the next states $\\Pr (s_{t+1} | s_t, a_t=a ; \\theta )$ is the transition probability (conditional on a particular choice)  so that the expected value function becomes $$ V\\left(s_t ; \\theta\\right) = \\sum_a \\Pr \\Big(a_t=a | s_t ; \\theta \\Big) * \\left[\\begin{array}{c} u\\left(s_t, a ; \\theta\\right) + \\mathbb E_{\\epsilon_t} \\Big[\\epsilon_{at}\\Big| a_t=a, s_t\\Big] \\newline + \\beta \\sum_{s_{t+1}} V\\left(s_{t+1} ; \\theta\\right) \\Pr \\Big(s_{t+1} \\Big| s_t, a_t=a ; \\theta \\Big) \\end{array}\\right] $$\nExpress EV in terms of EP (3) The previous equation, was defined at the state level $s_t$\n system of $k$ equations, 1 for each state (value of $x$)  If we stack them, we can write them as $$ V\\left(s ; \\theta\\right) = \\sum_a \\Pr \\Big(a \\ \\Big| \\ s ; \\theta \\Big) .* \\Bigg[ u\\left(s, a ; \\theta\\right) + \\mathbb E_{\\epsilon} \\Big[\\epsilon_{a} \\ \\Big| \\ a, s \\Big] + \\beta \\ T(a ; \\theta) \\ V(s ; \\theta) \\Bigg] $$ where\n $T(a)$: $k \\times k$ matrix of transition probabilities from state $s_t$ to $s_{t+1}$, given decision $a$ $.*$ is the dot product operator (or element-wise matrix multiplication)  Express EV in terms of EP (4) Now we have a system of $k$ equations in $k$ unknowns that we can solve.\nTearing down notation to the bare minimum, we have $$ V = \\sum_a P_a .* \\bigg[ u_a + \\mathbb E [\\epsilon_a ] + \\beta \\ T_a \\ V \\bigg] $$ which we can rewrite as $$ V - \\beta \\ \\left( \\sum_a P_a .* T_a \\right) V = \\sum_a P_a .* \\bigg[ u_a + \\mathbb E [\\epsilon_a ] \\bigg] $$\nand finally we can solve for $V$ through the famous Hotz and Miller inversion $$ V = \\left[I - \\beta \\ \\sum_a P_a .* T_a \\right]^{-1} \\ * \\ \\left( \\sum_a P_a \\ .* \\ \\bigg[ u_a + \\mathbb E [\\epsilon_a] \\bigg] \\right) $$ Solved? No. We still need to do something about $\\mathbb E [\\epsilon_a]$.\nExpress EV in terms of EP (5) What is $\\mathbb E [\\epsilon_a]$?\nLet’s consider for example the expected value of the shock, conditional on investment $$ \\begin{aligned} \\mathbb E \\Big[ \\epsilon_{1 t} \\ \\Big| \\ a_t = 1, \\cdot \\Big] \u0026amp;= \\mathbb E \\Big[ \\epsilon_{t} \\ \\Big| \\ \\bar V_1 \\left( s_t ; \\theta \\right) + \\epsilon_{1 t} \u0026gt; \\bar V_0 \\left( s_t ; \\theta \\right) + \\epsilon_{0 t} \\Big] \\newline \u0026amp; = \\mathbb E \\Big[ \\epsilon_{1 t} \\ \\Big| \\ \\bar V_1 \\left( s_t ; \\theta \\right) - \\bar V_0 \\left( s_t ; \\theta \\right) \u0026gt; \\epsilon_{0 t} - \\epsilon_{1 t} \\Big] \\end{aligned} $$ with logit magic 🧙🪄 is $$ \\mathbb E\\left[\\epsilon_{1 t} | a_{t}=1, s_t\\right] = 0.5772 - \\ln \\left(P\\left(s_t ; \\theta\\right)\\right) $$\n where $0.5772$ is Euler’s constant.  We again got rid of another $\\max$ operator!\nExpress EV in terms of EP (6) Now we can substitute it back and we have an equation which is just a function of primitives $$ \\begin{aligned} V(\\cdot ; \\theta) =\u0026amp; \\Big[I-(1-P(\\cdot ; \\theta)) \\beta T(0 ; \\theta)-P(\\cdot ; \\theta) \\beta T(1 ; \\theta)\\Big]^{-1} \\newline * \u0026amp; \\left[ \\begin{array}{c} (1-P(\\cdot ; \\theta))\\Big[u(\\cdot, 0 ; \\theta)+0.5772-\\ln (1-P(\\cdot ; \\theta))\\Big] \\newline + P(\\cdot ; \\theta)\\Big[u(\\cdot, 1 ; \\theta) + 0.5772 - \\ln (P(\\cdot ; \\theta))\\Big] \\end{array} \\right] \\end{aligned} $$\nOr more compactly $$ V = \\left[I - \\beta \\ \\sum_a P_a .* T_a \\right]^{-1} \\ * \\ \\left( \\sum_a P_a \\ .* \\ \\bigg[ u_a + 0.5772 - \\ln(P_d) \\bigg] \\right) $$\nFirst Equation What is the first equation? $$ V = \\left[I - \\beta \\ \\sum_a P_a .* T_a \\right]^{-1} \\ * \\ \\left( \\sum_a P_a \\ .* \\ \\bigg[ u_a + 0.5772 - \\ln(P_a) \\bigg] \\right) $$ Expected static payoff: $\\sum_a P_a \\ .* \\ \\bigg[ u_a + 0.5772 + \\ln(P_a) \\bigg]$\n Is the expected static payoff of choice $a$ in each state, $u_a + 0.5772 + \\ln(P_a)$ … integrated over the choice probabilities, $P_a$ It’s a $k \\times 1$ vector  Unconditional transition probabilities: $\\sum_a P_a .* T_a$\n Are the transition probabilities conditional on a choice $a$ for every present and future state, $T_a$ … integrated over the choice probabilities, $P_a$ It’s a $k \\times k$ matrix  Recap We got our first equation $$ {\\color{red}{V}} = \\left[I - \\beta \\ \\sum_a {\\color{blue}{P_a}} .* T_a \\right]^{-1} \\ * \\ \\left( \\sum_a {\\color{blue}{P_a}} \\ .* \\ \\bigg[ u_a + 0.5772 - \\ln({\\color{blue}{P_a}}) \\bigg] \\right) $$\nI.e. $$ \\begin{align} {\\color{red}{V(s ; \\theta)}} \u0026amp; = h( {\\color{blue}{P(s ; \\theta)}} ; \\theta) \\newline \\end{align} $$\n What about the second equation ${\\color{blue}{P(\\cdot|s ; \\theta)}} = g({\\color{red}{V(s ; \\theta)}}; \\theta)$?\n From V to P In general, the expected probability of investment is $$ P(a=1; \\theta)= \\Pr \\left[\\begin{array}{c} u(\\cdot, 1 ; \\theta)+\\epsilon_{1 t}+\\beta \\mathbb E \\Big[V(\\cdot ; \\theta) \\Big| \\cdot, a_{t}=1 ; \\theta \\Big]\u0026gt; \\newline \\qquad u(\\cdot, 0 ; \\theta) + \\epsilon_{0 t}+\\beta \\mathbb E \\Big[V(\\cdot ; \\theta) \\Big| \\cdot, a_{t}=0 ; \\theta \\Big] \\end{array}\\right] $$\nWith the logit assumption, simplifies to $$ {\\color{blue}{P(a=1 ; \\theta)}} = \\frac{\\exp \\Big(u(\\cdot, 1 ; \\theta)+\\beta T(1 ; \\theta) V(\\cdot ; \\theta) \\Big)}{\\sum_{a'} \\exp \\Big(u(\\cdot, a' ; \\theta)+\\beta T(a' ; \\theta) V(\\cdot ; \\theta) \\Big)} = \\frac{\\exp (u_1 +\\beta T_1 {\\color{red}{V}} )}{\\sum_{a'} \\exp (u_{a'} +\\beta T_{a'} {\\color{red}{V}} )} $$\nNow we have also the second equation! $$ \\begin{align} {\\color{blue}{P(s ; \\theta)}} \u0026amp; = g({\\color{red}{V(s ; \\theta)}}; \\theta) \\end{align} $$\nHotz \u0026amp; Miller - Idea 2 Idea 2: Replace ${\\color{blue}{P} (\\cdot)}$ on the RHS with a consistent estimator ${\\color{Turquoise}{\\hat P (\\cdot)}}$ $$ {\\color{cyan}{\\bar P(\\cdot ; \\theta)}} = g(h({\\color{Turquoise}{\\hat P(\\cdot)}} ; \\theta); \\theta) $$\n  ${\\color{cyan}{\\bar P(\\cdot ; \\theta_0)}}$ will converge to the true ${\\color{blue}{P(\\cdot ; \\theta_0)}}$, because ${\\color{Turquoise}{\\hat P (\\cdot)}}$ is converging to ${\\color{blue}{P(\\cdot ; \\theta_0)}}$ asymptotically.\n Note: pay attention to $\\theta_0$ vs $\\theta$ here: ${\\color{cyan}{\\bar P(\\cdot ; \\theta)}}$ does not generally converge to ${\\color{blue}{P(\\cdot ; \\theta)}}$for arbitrary $\\theta$, because ${\\color{Turquoise}{\\hat P(\\cdot)}}$ is converging to ${\\color{blue}{P(\\cdot ; \\theta_0)}}$ but not ${\\color{blue}{P(\\cdot ; \\theta)}}$ with any $\\theta$.    How to compute ${\\color{Turquoise}{\\hat P(\\cdot)}}$?\n  From the data, you observe states and decisions\n  You can compute frequency of decisions given states\n In Rust: frequency of engine replacement, given a mileage (discretized)    Assumption: you have enough data\n What if a state is not realised? Use frequencies in observed states to extrapolate frequencies in unobserved states    Recap Steps so far\n  Estimate the conditional choice probabilities ${\\color{Turquoise}{\\hat P}}$ from the data\n Nonparametrically: frequency of each decision in each state    Solve for the expected value function with the inverstion step $$ {\\color{orange}{\\hat V}} = \\left[I - \\beta \\ \\sum_a {\\color{Turquoise}{\\hat P_a}} .* T_a \\right]^{-1} \\ * \\ \\left( \\sum_a {\\color{Turquoise}{\\hat P_a}} \\ .* \\ \\bigg[ u_a + 0.5772 - \\ln({\\color{Turquoise}{\\hat P_a}}) \\bigg] \\right) $$\n  Compute the predicted CCP, given $V$ $$ {\\color{cyan}{\\bar P(a=1 ; \\theta)}} = \\frac{\\exp (u_1 +\\beta T_1 {\\color{orange}{\\hat V}} )}{\\sum_{a'} \\exp (u_{a'} +\\beta T_{a'} {\\color{orange}{\\hat V}} )} $$\n   What now? Use the estimated CCP to build an objective function.\n Objective Function We have (at least) 2 options\n Hotz and Miller (1993) use GMM  $$ \\mathbb E \\Big[a_t - \\bar P(s_t, \\theta) \\ \\Big| \\ s_t \\Big] = 0 \\quad \\text{ at } \\quad \\theta = \\theta_0 $$\n Aguirregabiria and Mira (2002) use MLE  by putting $\\bar P(s_t, \\theta)$ in the likelihood function instead of $P(s_t, \\theta)$    We will follow the second approach\nPseudo-Likelihood The likelihood function for one bus is $$ \\mathcal{L}(\\theta) = \\prod_{t=1}^{T}\\left(\\hat{\\operatorname{Pr}}\\left(a=1 \\mid s_{t}; \\theta\\right) \\mathbb{1}\\left(a_{t}=1\\right)+\\left(1-\\hat{\\operatorname{Pr}}\\left(a=0 \\mid s_{t}; \\theta\\right)\\right) \\mathbb{1}\\left(a_{t}=0\\right)\\right) $$ where $\\hat \\Pr\\big(a_{t} \\big| s_{t} ; \\theta \\big)$ is a function of\n CCPs $\\hat P$: estimated from data transition matrix $T$: estimated from the data, given $\\theta$ static payoffs $u$: known, given $\\theta$ discount factor $\\beta$ : assumed   Why pseudo-likelihood? We have inputed something that is not a primitive but a consistent estimate of an equilibrium object, $\\hat P$\n Comments Now a few comments on Hotz and Miller (1993)\n Computational bottleneck Aguirregabiria and Mira (2002) Importance of the T1EV assumption Data requirements Unobserved heterogeneity Identification  Computational Bottleneck There is still 1 computational bottleneck in HM: the inversion step $$ V = \\left[I - \\beta \\ \\sum_a P_a .* T_a \\right]^{-1} \\ * \\ \\left( \\sum_a P_a \\ .* \\ \\bigg[ u_a + 0.5772 - \\ln(P_a) \\bigg] \\right) $$ The $\\left[I - \\beta \\ \\sum_a P_a .* T_a \\right]$ matrix has dimension $k \\times k$\n With large state space, hard to invert Even with modern computational power Hotz et al. (1994): forward simulation of the value function  You have the policy, the transitions and the utilities Just compute discounted flow of payoffs Core idea behind the estimation of dynamic games    Aguirregabiria, Mira (2002) Hotz and Miller (1993) inversion gets us a recursive equation in probability space\n instead of the Bellman Equation in the value space  $$ \\bar P(\\cdot ; \\theta) = g(h(\\hat P(\\cdot) ; \\theta); \\theta) $$\nIdea\n Do you gain something by iterating $K$ imes?  $K=1$: Hotz and Miller (1993) $K \\to \\infty$: Rust (1987)   Monte Carlo simulations: finite sample properties of K−stage estimators improve monotonically with K  But especially for $K=2$! Really worth iterating once    Type 1 EV errors Crucial assumption\n Without logit errors, we need to simulate their distribution True also for Rust But it’s generally accepted  doesn’t imply it’s innocuous    Data Requirements For both Hotz et al. (1994) and Rust (1987), we need to discretize the state space\n Can be complicated with continuous variables Problem also in Rust But particularly problematic in Hotz et al. (1994)  Relies crucially on consistency of CCP estimates Need sufficient variation in actions for each state    Unobserved Heterogeneity Hotz et al. (1994) cannot handle unobserved heterogeneity or “unobserved state variables” that are persistent over time.\nExample\n  Suppose there are 2 bus types $\\tau$: high and low quality\n  We don’t know the share of types in the data\n  With Rust\n  Parametrize the effect of the difference in qualities\n E.g. high quality engines break less often    Parametrize the proportion of high quality buses\n  Solve the value function by type $V(s_t, \\tau ; \\theta)$\n  Integrate over types when computing choice probabilities $$ P(a|s) = \\int P(a|s,\\tau) P(\\tau) = \\Pr(a|s, \\tau=0) * \\Pr(\\tau=0) + \\Pr(a|s, \\tau=1) * \\Pr(\\tau=1) $$\n    Unobserved Heterogeneity (2) What is the problem with Hotz et al. (1994)?\n  The unobserved heterogeneity generates persistency in choices\n I don’t replace today because it’s high quality, but I also probably don’t replace tomorrow either Decisions independent across time only conditional on type    Likelihood of decisions must be integrated over types $$ \\mathcal L (\\theta) = \\sum_{\\tau_a} \\prod_{t=1}^{T} \\Pr (a_{jt}| s_{jt}, \\tau_a) \\Pr(\\tau_a) $$\n  Hotz \u0026amp; Miller needs consistent estimates of $P(a, s, \\tau)$\n  Difficult when $\\tau$ is not observed!\n  Identification Work on identification\n Rust (1994) and Magnac and Thesmar (2002)  Rust (1987) is non-paramentrically underidentified $\\to$ parametric assumptions are essential   Aguirregabiria and Suzuki (2014) Kalouptsidi, Scott, and Souza-Rodrigues (2017) Abbring and Daljord (2020)  Can identify discount factor with some “instrument” that shifts future utilities but not current payoff   Kalouptsidi et al. (2020)  Appendix References [references] Abbring, Jaap H, and Øystein Daljord. 2020. “Identifying the Discount Factor in Dynamic Discrete Choice Models.” Quantitative Economics 11 (2): 471–501.\n Aguirregabiria, Victor, and Pedro Mira. 2002. “Swapping the Nested Fixed Point Algorithm: A Class of Estimators for Discrete Markov Decision Models.” Econometrica 70 (4): 1519–43.\n Aguirregabiria, Victor, and Junichi Suzuki. 2014. “Identification and Counterfactuals in Dynamic Models of Market Entry and Exit.” Quantitative Marketing and Economics 12 (3): 267–304.\n Becker, Gary S, and Kevin M Murphy. 1988. “A Theory of Rational Addiction.” Journal of Political Economy 96 (4): 675–700.\n Berry, Steven T. 1992. “Estimation of a Model of Entry in the Airline Industry.” Econometrica: Journal of the Econometric Society, 889–917.\n Bresnahan, Timothy F. 1989. “Empirical Studies of Industries with Market Power.” Handbook of Industrial Organization 2: 1011–57.\n Crawford, Gregory S, and Matthew Shum. 2005. “Uncertainty and Learning in Pharmaceutical Demand.” Econometrica 73 (4): 1137–73.\n Erdem, Tülin, Susumu Imai, and Michael P Keane. 2003. “Brand and Quantity Choice Dynamics Under Price Uncertainty.” Quantitative Marketing and Economics 1 (1): 5–64.\n Erdem, Tülin, and Michael P Keane. 1996. “Decision-Making Under Uncertainty: Capturing Dynamic Brand Choice Processes in Turbulent Consumer Goods Markets.” Marketing Science 15 (1): 1–20.\n Golosov, Mikhail, Aleh Tsyvinski, Ivan Werning, Peter Diamond, and Kenneth L Judd. 2006. “New Dynamic Public Finance: A User’s Guide [with Comments and Discussion].” NBER Macroeconomics Annual 21: 317–87.\n Gowrisankaran, Gautam, and Marc Rysman. 2012. “Dynamics of Consumer Demand for New Durable Goods.” Journal of Political Economy 120 (6): 1173–1219.\n Handel, Benjamin R. 2013. “Adverse Selection and Inertia in Health Insurance Markets: When Nudging Hurts.” American Economic Review 103 (7): 2643–82.\n Hendel, Igal, and Aviv Nevo. 2006. “Measuring the Implications of Sales and Consumer Inventory Behavior.” Econometrica 74 (6): 1637–73.\n Hotz, V Joseph, and Robert A Miller. 1993. “Conditional Choice Probabilities and the Estimation of Dynamic Models.” The Review of Economic Studies 60 (3): 497–529.\n Hotz, V Joseph, Robert A Miller, Seth Sanders, and Jeffrey Smith. 1994. “A Simulation Estimator for Dynamic Models of Discrete Choice.” The Review of Economic Studies 61 (2): 265–89.\n Igami, Mitsuru. 2020. “Artificial Intelligence as Structural Estimation: Deep Blue, Bonanza, and AlphaGo.” The Econometrics Journal 23 (3): S1–24.\n Imai, Susumu, Neelam Jain, and Andrew Ching. 2009. “Bayesian Estimation of Dynamic Discrete Choice Models.” Econometrica 77 (6): 1865–99.\n Kalouptsidi, Myrto, Yuichi Kitamura, Lucas Lima, and Eduardo A Souza-Rodrigues. 2020. “Partial Identification and Inference for Dynamic Models and Counterfactuals.” National Bureau of Economic Research.\n Kalouptsidi, Myrto, Paul T Scott, and Eduardo Souza-Rodrigues. 2017. “On the Non-Identification of Counterfactuals in Dynamic Discrete Games.” International Journal of Industrial Organization 50: 362–71.\n Keane, Michael P, and Kenneth I Wolpin. 1997. “The Career Decisions of Young Men.” Journal of Political Economy 105 (3): 473–522.\n Magnac, Thierry, and David Thesmar. 2002. “Identifying Dynamic Discrete Decision Processes.” Econometrica 70 (2): 801–16.\n Pakes, Ariel. 1986. “Patents as Options: Some Estimates of the Value of Holding European Patent Stocks.” Econometrica 54 (4): 755–84.\n Rust, John. 1987. “Optimal Replacement of GMC Bus Engines: An Empirical Model of Harold Zurcher.” Econometrica: Journal of the Econometric Society, 999–1033.\n ———. 1988. “Maximum Likelihood Estimation of Discrete Control Processes.” SIAM Journal on Control and Optimization 26 (5): 1006–24.\n ———. 1994. “Structural Estimation of Markov Decision Processes.” Handbook of Econometrics 4: 3081–3143.\n Su, Che-Lin, and Kenneth L Judd. 2012. “Constrained Optimization Approaches to Estimation of Structural Models.” Econometrica 80 (5): 2213–30.\n  ","date":1635465600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1635465600,"objectID":"33374ffd438bd550113e66846a8b147d","permalink":"https://matteocourthoud.github.io/course/empirical-io/07_dynamics_singleagent/","publishdate":"2021-10-29T00:00:00Z","relpermalink":"/course/empirical-io/07_dynamics_singleagent/","section":"course","summary":"Introduction Motivation IO: role of market structure on equilibrium outcomes.\nDynamics: study the endogenous evolution of market structure.\n Supply side dynamics  Irreversible investment Entry sunk costs Product repositioning costs Price adjustment costs Learning by doing   Demand side dynamics  Switching costs Durable or storable products    Bonus motivation: AI literature studies essentially the same set of problems with similar tools (Igami 2020)","tags":null,"title":"Single Agent Dynamics","type":"book"},{"authors":null,"categories":null,"content":"# Remove warnings import warnings warnings.filterwarnings('ignore')  # Import everything import pandas as pd import numpy as np import seaborn as sns import statsmodels.api as sm import copy import torch import torch.nn as nn import torch.utils.data as Data from torch.autograd import Variable from sklearn.linear_model import LinearRegression from torchviz import make_dot  # Import matplotlib for graphs import matplotlib.pyplot as plt from mpl_toolkits.mplot3d import axes3d from IPython.display import clear_output # Set global parameters %matplotlib inline plt.style.use('seaborn-white') plt.rcParams['lines.linewidth'] = 3 plt.rcParams['figure.figsize'] = (10,6) plt.rcParams['figure.titlesize'] = 20 plt.rcParams['axes.titlesize'] = 18 plt.rcParams['axes.labelsize'] = 14 plt.rcParams['legend.fontsize'] = 14  While sklearn has a library for neural networks, it is very basic and not the standard in the industry. The most commonly used libraries as of 2020 are Tensorflow and Pytorch.\n  TensorFlow is developed by Google Brain and actively used at Google both for research and production needs. Its closed-source predecessor is called DistBelief.\n  PyTorch is a cousin of lua-based Torch framework which was developed and used at Facebook. However, PyTorch is not a simple set of wrappers to support popular language, it was rewritten and tailored to be fast and feel native.\n  Here is an article that explains very well the difference between the two libraries: pytorch-vs-tensorflow. In short, pytorch is much more intuitive for a python programmer and more user friendly. It also has a superior development and debugging experience. However, if you want more control on the fundamentals, a better community support and you need to train large models, Tensorflow is better.\n8.1 Introduction The term neural network has evolved to encompass a large class of models and learning methods. Here I describe the most widely used “vanilla” neural net, sometimes called the single hidden layer back-propagation network, or single layer perceptron.\nRegression Imagine a setting with two inputs available (let’s denote these inputs $i_1$ and $i_2$), and no special knowledge about the relationship between these inputs and the output that we want to predict (denoted by $o$) except that this relationship is, a priori, pretty complex and non-linear.\nSo we want to learn the function $f$ such that f($i_1$, $i_2$) is a good estimator of $o$. We could then suggest the following first model:\n$$ o = w_{11} i_1 + w_{12} i_2 $$\nwhere $w_{11}$ and $w_{12}$ are just weights/coefficients (do not take care about the indices for now). Before going any further, we should notice that, here, there is no constant term in the model. However, we could have introduced such term by setting $f(i_1, i_2) = w_{11} i_1 + w_{12} i_2 + c$. The constant is often called bias.\nWe can represent the setting as follows.\nIn this case, the model is easy to understand and to fit but has a big drawback : there is no non-linearity! This obviously do not respect our non-linear assumption.\nActivation Functions In order to introduce a non-linearity, let us make a little modification in the previous model and suggest the following one.\n$$ o = a ( w_{11} i_1 + w_{12} i_2) $$\nwhere $a$ is a function called activation function which is non-linear.\nOne activation function that is well known in economics (and other disciplines) is the sigmoid function or logit function\n$$ a (w_{11} i_1 + w_{12} i_2) = \\frac{1}{1 + e^{w_{11} i_1 + w_{12} i_2}} $$\nLayers However, even if better than multilinear model, this model is still too simple and can’t handle the assumed underlying complexity of the relationship between inputs and output. We can make a step further and enrich the model the following way.\n First we could consider that the quantity $a ( w_{11} i_1 + w_{12} i_2)$ is no longer the final output but instead a new intermediate feature of our function, called $l_1$, which stands for layer.  $$ l_1 = a ( w_{11} i_1 + w_{12} i_2) $$\nSecond we could consider that we build several (3 in our example) such features in the same way, but possibly with different weights and different activation functions  $$ l_1 = a ( w_{11} i_1 + w_{12} i_2) \\ l_2 = a ( w_{21} i_1 + w_{22} i_2) \\ l_3 = a ( w_{31} i_1 + w_{32} i_2) $$\nwhere the $a$’s are just activation functions and the $w$’s are weights.\nFinally, we can consider that our final output is build based on these intermediate features with the same “template”  $$ a_2 ( v_1 l_1 + v_2 l_2 + v_3 * l_3 ) $$\nIf we aggregate all the pieces, we then get our prediction $p$\n$$ \\begin{aligned} p = f_{3}\\left(i_{1}, i_{2}\\right) \u0026amp;=a_{2}\\left(v_{1} l_{1}+v_{2} l_{2}+v_{3} l_{3}\\right) \\ \u0026amp;=a_{2}\\left(v_{1} \\times a_{11}\\left(w_{11} i_{1}+w_{12} i_{2}\\right)+v_{2} \\times a_{12}\\left(w_{21} i_{1}+w_{22} i_{2}\\right)+v_{3} \\times a_{13}\\left(w_{31} i_{1}+w_{32} i_{2}\\right)\\right) \\end{aligned} $$\nwhere we should mainly keep in mind that $a$’s are non-linear activation functions and $w$’s and $v$’s are weights.\nGraphically:\nThis last model is a basic feedforward neural network with:\n 2 entries ($i_1$ and $i_2$) 1 hidden layer with 3 hidden neurones (whose outputs are $l_1$, $l_2$ and $l_3$) 1 final output ($p$)  Pytorch Tensors We can express the data as a numpy array.\nx_np = np.arange(6).reshape((3, 2)) x_np  array([[0, 1], [2, 3], [4, 5]])  Or equivalently as a pytorch tensor.\nx_tensor = torch.from_numpy(x_np) x_tensor  tensor([[0, 1], [2, 3], [4, 5]])  We can also translate tensors back to arrays.\ntensor2array = x_tensor.numpy() tensor2array  array([[0, 1], [2, 3], [4, 5]])  We can make operations over this data. For example we can take the mean\ntry: torch.mean(x_tensor) except Exception as e: print(e)  mean(): input dtype should be either floating point or complex dtypes. Got Long instead.  We first have to convert the data in float\nx_tensor = torch.FloatTensor(x_np) x_tensor  tensor([[0., 1.], [2., 3.], [4., 5.]])  print(np.mean(x_np), '\\n\\n', torch.mean(x_tensor))  2.5 tensor(2.5000)  We can also apply compontent-wise functions\nprint(np.sin(x_np), '\\n\\n', torch.sin(x_tensor))  [[ 0. 0.84147098] [ 0.90929743 0.14112001] [-0.7568025 -0.95892427]] tensor([[ 0.0000, 0.8415], [ 0.9093, 0.1411], [-0.7568, -0.9589]])  We can multiply tensors as we multiply matrices\nprint(np.matmul(x_np.T, x_np), '\\n\\n', torch.mm(x_tensor.T, x_tensor))  [[20 26] [26 35]] tensor([[20., 26.], [26., 35.]])  But the element-wise multiplication does not work\ntry: x_tensor.dot(x_tensor) except Exception as e: print(e)  1D tensors expected, but got 2D and 2D tensors  Variables Variable in torch is to build a computational graph, but this graph is dynamic compared with a static graph in Tensorflow or Theano. So torch does not have placeholder, torch can just pass variable to the computational graph.\n# build a variable, usually for compute gradients x_variable = Variable(x_tensor, requires_grad=True) x_variable  tensor([[0., 1.], [2., 3.], [4., 5.]], requires_grad=True)  Until now the tensor and variable seem the same. However, the variable is a part of the graph, it\u0026rsquo;s a part of the auto-gradient.\nSuppose we are interested in:\n$$ y = \\text{mean} (x_1^2) = \\frac{1}{6} x^2 $$\ny = torch.mean(x_variable*x_variable) print(y)  tensor(9.1667, grad_fn=\u0026lt;MeanBackward0\u0026gt;)  We can compute the gradient by backpropagation\n$$ \\nabla y(x) = \\frac{2}{3} x $$\ni.e. if we call the backward method on our outcome y, we see that the gradient of our variable x gets updated.\nprint(x_variable.grad)  None  y.backward()  print(x_variable.grad)  tensor([[0.0000, 0.3333], [0.6667, 1.0000], [1.3333, 1.6667]])  However, its value has not changed.\nprint(x_variable)  tensor([[0., 1.], [2., 3.], [4., 5.]], requires_grad=True)  We can also access the tensor part of the variable alone by calling the data method.\nprint(x_variable.data)  tensor([[0., 1.], [2., 3.], [4., 5.]])  Activation Function The main advantage of neural networks is that they introduce non-linearities among the layers. The standard non-linear function\n ReLu Sigmoid TanH Softmax  # X grid x_grid = torch.linspace(-5, 5, 200) # x data (tensor), shape=(100, 1) x_grid = Variable(x_grid) x_grid_np = x_grid.data.numpy() # numpy array for plotting # Activation functions y_relu = torch.relu(x_grid).data.numpy() y_sigmoid = torch.sigmoid(x_grid).data.numpy() y_tanh = torch.tanh(x_grid).data.numpy() y_softmax = torch.softmax(x_grid, dim=0).data.numpy()  # New figure 1 def make_new_figure_1(): # Init figure fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2,2, figsize=(8,6)) fig.suptitle('Activation Functions') # Relu ax1.plot(x_grid_np, y_relu, c='red', label='relu') ax1.set_ylim((-1, 6)); ax1.legend() # Sigmoid ax2.plot(x_grid_np, y_sigmoid, c='red', label='sigmoid') ax2.set_ylim((-0.2, 1.2)); ax2.legend() # Tanh ax3.plot(x_grid_np, y_tanh, c='red', label='tanh') ax3.set_ylim((-1.2, 1.2)); ax3.legend() # Softmax ax4.plot(x_grid_np, y_softmax, c='red', label='softmax') ax4.set_ylim((-0.01, 0.06)); ax4.legend();  Let\u0026rsquo;s compare the different activation functions.\nmake_new_figure_1()  ReLu is very popular since it\u0026rsquo;s non-linear.\n8.3 Optimization and Gradient Descent Gradient Descent Gradient descent works as follows:\nInitialize the parameters Compute the Loss Compute the Gradients Update the Parameters Repeat (1)-(3) until convergence  Gradient Descent in Linear Regression In order to understand how are NN optimized, we start with a linear regression example. Remember that linear regression can be interpreted as the simplest possible NN.\nWe generate the following data:\n$$ y = 1 + 2 x - 3 x^2 + \\varepsilon $$\nwith $x \\sim N(0,1)$ and $\\varepsilon \\sim N(0,0.1)$\n# Data Generation np.random.seed(42) N = 100 x = np.sort(np.random.rand(N, 1), axis=0) e = .1*np.random.randn(N, 1) y_true = 1 + 2*x - 3*x**2 y = y_true + e  Let\u0026rsquo;s plot the data.\n# New figure 2 def make_new_figure_2(): # Init fig, ax = plt.subplots(figsize=(8,6)) fig.suptitle('Activation Functions') # Scatter ax.scatter(x,y); ax.plot(x,y_true,color='orange'); ax.set_xlabel('X'); ax.set_ylabel('Y'); ax.legend(['y true','y']);  make_new_figure_2()  Suppose we try to fit the data with a linear model\n$$ y = a + b x $$\nWe proceed iteratively by gradient descent. Our objective function is the Mean Squared Error.\nAlgorithm\n Take an initial guess of the parameters $$ a = a_0 \\ b = b_0 $$\n  Compute the Mean Squared Error $$ \\begin{array} \\text{MSE} \u0026amp;= \\frac{1}{N} \\sum_{i=1}^{N}\\left(y_{i}-\\hat{y}{i}\\right)^{2} \\ \u0026amp;= \\frac{1}{N} \\sum{i=1}^{N}\\left(y_{i}-a-b x_{i}\\right)^{2} \\end{array} $$\n  Compute its derivative $$ \\begin{array}{l} \\frac{\\partial M S E}{\\partial a}=\\frac{\\partial M S E}{\\partial \\hat{y}{i}} \\cdot \\frac{\\partial \\hat{y}{i}}{\\partial a}=\\frac{1}{N} \\sum_{i=1}^{N} 2\\left(y_{i}-a-b x_{i}\\right) \\cdot(-1)=-2 \\frac{1}{N} \\sum_{i=1}^{N}\\left(y_{i}-\\hat{y}{i}\\right) \\ \\frac{\\partial M S E}{\\partial b}=\\frac{\\partial M S E}{\\partial \\hat{y}{i}} \\cdot \\frac{\\partial \\hat{y}{i}}{\\partial b}=\\frac{1}{N} \\sum{i=1}^{N} 2\\left(y_{i}-a-b x_{i}\\right) \\cdot\\left(-x_{i}\\right)=-2 \\frac{1}{N} \\sum_{i=1}^{N} x_{i}\\left(y_{i}-\\hat{y}_{i}\\right) \\end{array} $$\n  Update the parameters $$ \\begin{array}{l} a=a-\\eta \\frac{\\partial M S E}{\\partial a} \\ b=b-\\eta \\frac{\\partial M S E}{\\partial b} \\end{array} $$\nWhere $\\eta$ is the learning rate. A lower learning rate makes learning more stable but slower.\n  Repeat (1)-(3) $T$ times, where the number of total iterations $T$ is called epochs.\n  We start by taking a random guess of $\\alpha$ and $\\beta$.\n# Initializes parameters \u0026quot;a\u0026quot; and \u0026quot;b\u0026quot; randomly np.random.seed(42) a = np.random.randn(1) b = np.random.randn(1) print(a, b)  [0.49671415] [-0.1382643]  # Plot gradient def gradient_plot(x, y, y_hat, y_true, EPOCHS, losses): clear_output(wait=True) fig, (ax1, ax2) = plt.subplots(1,2, figsize=(12,5)) # First figure ax1.clear() ax1.scatter(x, y) ax1.plot(x, y_true, 'orange') ax1.plot(x, y_hat, 'r-') ax1.set_title('Data and Fit') ax1.legend(['True', 'Predicted']) # Second figure ax2.clear() ax2.plot(range(len(losses)), losses, color='g') ax2.set_xlim(0,EPOCHS); ax2.set_ylim(0,1.1*np.max(losses)) ax2.set_title('True MSE = %.4f' % losses[-1]) # Plot plt.show();  We set the learning rate $\\eta = 0.1$ and the number of epochs $T=200$\n# parameters LR = 0.1 # learning rate EPOCHS = 200 # number of epochs  We can now plot the training and the result.\n# New figure 3 def make_new_figure_3(a, b): # Init losses = [] # train for t in range(EPOCHS): # compute loss y_hat = a + b * x error = (y - y_hat) loss = (error**2).mean() # compute gradient a_grad = -2 * error.mean() b_grad = -2 * (x * error).mean() # update parameters a -= LR * a_grad b -= LR * b_grad # plot losses += [loss] if (t+1) % (EPOCHS/25) == 0: # print 25 times gradient_plot(x, y, y_hat, y_true, EPOCHS, losses) print(a, b) return a, b  a_fit, b_fit = make_new_figure_3(a, b)  [1.40589939] [-0.83739496]  Sanity Check: do we get the same results as our gradient descent?\n# OLS estimates ols = LinearRegression() ols.fit(x, y) print(ols.intercept_, ols.coef_[0])  [1.4345303] [-0.89397853]  Close enough!\nLet\u0026rsquo;s plot both lines in the graph.\n# New figure 4 def make_new_figure_4(): # Init fig, ax = plt.subplots(figsize=(8,6)) # Scatter ax.plot(x,y_true,color='orange'); ax.plot(x,a_fit + b_fit*x,color='red'); ax.plot(x,ols.predict(x),color='green'); ax.legend(['y true','y gd', 'y ols']) ax.scatter(x,y); ax.set_xlabel('X'); ax.set_ylabel('Y'); ax.set_title(\u0026quot;Data\u0026quot;);  make_new_figure_4()  Now we are going to do exactly the same but with pytorch.\nAutograd Autograd is PyTorch’s automatic differentiation package. Thanks to it, we don’t need to worry about partial derivatives, chain rule or anything like it.\nSo, how do we tell PyTorch to do its thing and compute all gradients? That’s what backward() is good for. § Do you remember the starting point for computing the gradients? It was the loss, as we computed its partial derivatives w.r.t. our parameters. Hence, we need to invoke the backward() method from the corresponding Python variable, like, loss.backward().\nWhat about the actual values of the gradients? We can inspect them by looking at the grad attribute of a tensor.\nIf you check the method’s documentation, it clearly states that gradients are accumulated. So, every time we use the gradients to update the parameters, we need to zero the gradients afterwards. And that’s what zero_() is good for.\nWhat does the underscore (_) at the end of the method name mean? Do you remember? If not, scroll back to the previous section and find out.\nSo, let’s ditch the manual computation of gradients and use both backward() and zero_() methods instead.\nFirst, we convert our variables to tensors.\n# Convert data to tensors x_tensor = torch.from_numpy(x).float().to('cpu') y_tensor = torch.from_numpy(y).float().to('cpu') print(type(x), type(x_tensor))  \u0026lt;class 'numpy.ndarray'\u0026gt; \u0026lt;class 'torch.Tensor'\u0026gt;  We take the initial parameters guess\n# initial parameter guess torch.manual_seed(42) a = torch.randn(1, requires_grad=True, dtype=torch.float, device='cpu') b = torch.randn(1, requires_grad=True, dtype=torch.float, device='cpu')  Now we are ready to fit the model.\n# New figure 5 def make_new_figure_5(a, b): # Init losses = [] # parameters LR = 0.1 EPOCHS = 200 # train for t in range(EPOCHS): # compute loss y_hat = a + b * x_tensor error = y_tensor - y_hat loss = (error ** 2).mean() # compute gradient loss.backward() # update parameters with torch.no_grad(): a -= LR * a.grad b -= LR * b.grad # clear gradients a.grad.zero_() b.grad.zero_() # Plot losses += [((y_true - y_hat.detach().numpy())**2).mean()] if (t+1) % (EPOCHS/25) == 0: # print 25 times gradient_plot(x, y, y_hat.data.numpy(), y_true, EPOCHS, losses) print(a, b)  make_new_figure_5(a, b)  tensor([1.3978], requires_grad=True) tensor([-0.8214], requires_grad=True)  Optimizer So far, we’ve been manually updating the parameters using the computed gradients. That’s probably fine for two parameters… but what if we had a whole lot of them?! We use one of PyTorch’s optimizers.\nAn optimizer takes the following arguments:\n the parameters we want to update he learning rate we want to use (possibly many other hyper-parameters)  Moreover, we can now call the function zero_grad() to automatically update the parameters. In particular, we will need to perform the following steps at each iteration:\n Clear the parameters: optimizer.zero_grad() Compute the gradient: loss.backward() Update the parameters: optimizer.step()  In the code below, we create a Stochastic Gradient Descent (SGD) optimizer to update our parameters $a$ and $b$.\n# Init parameters torch.manual_seed(42) a = torch.randn(1, requires_grad=True, dtype=torch.float, device='cpu') b = torch.randn(1, requires_grad=True, dtype=torch.float, device='cpu') # Defines a SGD optimizer to update the parameters optimizer = torch.optim.SGD([a, b], lr=LR) print(optimizer)  SGD ( Parameter Group 0 dampening: 0 lr: 0.1 momentum: 0 nesterov: False weight_decay: 0 )  We can also define a default loss function so that we don\u0026rsquo;t have to compute it by hand. We are going to use the MSE loss function.\n# Define a loss function loss_func = torch.nn.MSELoss() print(loss_func)  MSELoss()  Let\u0026rsquo;s plot the estimator and the MSE.\n# New figure 6 def make_new_figure_6(a, b): # parameters EPOCHS = 200 # init losses = [] # train for t in range(EPOCHS): # compute loss y_hat = a + b * x_tensor error = y_tensor - y_hat loss = (error ** 2).mean() # update parameters optimizer.zero_grad() # clear gradients for next train loss.backward() # backpropagation, compute gradients optimizer.step() # apply gradients, update parameters # Plot losses += [((y_true - y_hat.detach().numpy())**2).mean()] if (t+1) % (EPOCHS/25) == 0: gradient_plot(x, y, y_hat.data.numpy(), y_true, EPOCHS, losses) print(a, b)  make_new_figure_6(a, b)  tensor([1.3978], requires_grad=True) tensor([-0.8214], requires_grad=True)  Building a NN In our model, we manually created two parameters to perform a linear regression. Let’s use PyTorch’s Sequential module to create our neural network.\nWe first want to build the linear regression framework\n$$ y = a + b x $$\nWhich essentially is a network with\n 1 input no hidden layer no activation function 1 output  Let\u0026rsquo;s build the simplest possible neural network with PyTorch.\n# Simplest possible neural network linear_net = torch.nn.Sequential( torch.nn.Linear(1, 1) ) print(linear_net)  Sequential( (0): Linear(in_features=1, out_features=1, bias=True) )  Now, if we call the parameters() method of this model, PyTorch will figure the parameters of its attributes in a recursive way.\n[*linear_net.parameters()]  [Parameter containing: tensor([[-0.2191]], requires_grad=True), Parameter containing: tensor([0.2018], requires_grad=True)]  We can now define the definitive training function.\ndef train_NN(x, y, y_true, net, optimizer, loss_func, EPOCHS): # transform variables x_tensor = torch.from_numpy(x).float().to('cpu') y_tensor = torch.from_numpy(y).float().to('cpu') # init losses = [] # train for t in range(EPOCHS): # compute loss y_hat = net(x_tensor) loss = loss_func(y_hat, y_tensor) # update parameters optimizer.zero_grad() # clear gradients for next train loss.backward() # backpropagation, compute gradients optimizer.step() # apply gradients, update parameters # plot losses += [((y_true - y_hat.detach().numpy())**2).mean()] if (t+1) % (EPOCHS/25) == 0: # print 25 times gradient_plot(x, y, y_hat.data.numpy(), y_true, EPOCHS, losses)  Now we are ready to train our neural network.\noptimizer = torch.optim.SGD(linear_net.parameters(), lr=LR) # train train_NN(x, y, y_true, linear_net, optimizer, loss_func, EPOCHS)  We now define a more complicated NN. In particular we, build a neural network with\n 1 input 1 hidden layer with 10 neurons and Relu activation function 1 output layer  # Relu Net relu_net = torch.nn.Sequential( torch.nn.Linear(1, 10), torch.nn.ReLU(), torch.nn.Linear(10, 1) ) print(relu_net)  Sequential( (0): Linear(in_features=1, out_features=10, bias=True) (1): ReLU() (2): Linear(in_features=10, out_features=1, bias=True) )  This network has much more parameters.\n[*relu_net.parameters()]  [Parameter containing: tensor([[-0.4869], [ 0.5873], [ 0.8815], [-0.7336], [ 0.8692], [ 0.1872], [ 0.7388], [ 0.1354], [ 0.4822], [-0.1412]], requires_grad=True), Parameter containing: tensor([ 0.7709, 0.1478, -0.4668, 0.2549, -0.4607, -0.1173, -0.4062, 0.6634, -0.7894, -0.4610], requires_grad=True), Parameter containing: tensor([[-0.0893, -0.1901, 0.0298, -0.3123, 0.2856, -0.2686, 0.2441, 0.0526, -0.1027, 0.1954]], requires_grad=True), Parameter containing: tensor([0.0493], requires_grad=True)]  We are again using Stochastic Gradient Descent (SGD) as optimization algorithm and Mean Squared Error (MSELoss) as objective function.\n# parameters LR = 0.1 EPOCHS = 1000 # optimizer and loss function optimizer = torch.optim.SGD(relu_net.parameters(), lr=LR) loss_func = torch.nn.MSELoss() # Train train_NN(x, y, y_true, relu_net, optimizer, loss_func, EPOCHS)  It seems that we can use fewer nodes to get the same result.\nLet\u0026rsquo;s make a smallet network.\n# Relu Net relu_net2 = torch.nn.Sequential( torch.nn.Linear(1, 4), torch.nn.ReLU(), torch.nn.Linear(4, 1) )  And train it.\n# parameters LR = 0.1 EPOCHS = 1000 # optimizer and loss function optimizer = torch.optim.SGD(relu_net2.parameters(), lr=LR) loss_func = torch.nn.MSELoss() # Train train_NN(x, y, y_true, relu_net2, optimizer, loss_func, EPOCHS)  We can try different activation functions.\nFor example the tangent.\n# TanH Net tanh_net = torch.nn.Sequential( torch.nn.Linear(1, 10), torch.nn.Tanh(), torch.nn.Linear(10, 1) )  # parameters LR = 0.2 EPOCHS = 1000 # optimizer and loss function optimizer = torch.optim.SGD(tanh_net.parameters(), lr=LR) loss_func = torch.nn.MSELoss() # train train_NN(x, y, y_true, tanh_net, optimizer, loss_func, EPOCHS)  Loss functions So far we have used the Stochastic as loss function.\nNotice that nn.MSELoss actually creates a loss function for us — it is NOT the loss function itself. Moreover, you can specify a reduction method to be applied, that is, how do you want to aggregate the results for individual points — you can average them (reduction=mean) or simply sum them up (reduction=sum).\nWe are now going to use different ones.\n# parameters LR = 0.1 EPOCHS = 25 # nets n = torch.nn.Sequential(torch.nn.Linear(1, 10), torch.nn.ReLU(), torch.nn.Linear(10, 1)) nets = [n,n,n,n] # optimizers optimizers = [torch.optim.SGD(n.parameters(), lr=LR) for n in nets] # different loss functions loss_MSE = torch.nn.MSELoss() loss_L1 = torch.nn.L1Loss() loss_NLL = torch.nn.NLLLoss() loss_KLD = torch.nn.KLDivLoss() loss_funcs = [loss_MSE, loss_L1, loss_NLL, loss_KLD]  This is the description of the loss functions:\n  MSELoss: Creates a criterion that measures the mean squared error (squared L2 norm) between each element in the input $x$ and target $y$.\n  L1Loss: Creates a criterion that measures the mean absolute error (MAE) between each element in the input $x$ and target $y$.\n  NLLLoss: The negative log likelihood loss.\n  KLDivLoss: The Kullback-Leibler divergence loss measure\n  # Train multiple nets def train_nets(x, y, y_true, nets, optimizers, loss_funcs, labels, EPOCHS): # Put dateset into torch dataset x_tensor = torch.from_numpy(x).float().to('cpu') y_tensor = torch.from_numpy(y).float().to('cpu') torch_dataset = Data.TensorDataset(x_tensor, y_tensor) # Init losses = np.zeros((0,4)) # Train for epoch in range(EPOCHS): # for each epoch losses = np.vstack((losses, np.zeros((1,4)))) for k, net, opt, lf in zip(range(4), nets, optimizers, loss_funcs): y_hat = net(x_tensor) # get output for every net loss = loss_func(y_hat, y_tensor) # compute loss for every net opt.zero_grad() # clear gradients for next train loss.backward() # backpropagation, compute gradients opt.step() # apply gradients losses[-1,k] = ((y_true - y_hat.detach().numpy())**2).mean() plot_losses(losses, labels, EPOCHS)  # Plot losses def plot_losses(losses, labels, EPOCHS): clear_output(wait=True) fig, ax = plt.subplots(1,1, figsize=(10,6)) # Plot ax.clear() ax.plot(range(len(losses)), losses) ax.set_xlim(0,EPOCHS-1); ax.set_ylim(0,1.1*np.max(losses)) ax.set_title('Compare Losses'); ax.set_ylabel('True MSE') legend_txt = ['%s=%.4f' % (label, loss) for label,loss in zip(labels, losses[-1,:])] ax.legend(legend_txt) # Shot plt.show();  Let\u0026rsquo;s compare them.\n# Train labels = ['MSE', 'L1', 'LogL', 'KLdiv'] train_nets(x, y, y_true, nets, optimizers, loss_funcs, labels, EPOCHS)  In this very simple case, all loss functions are very similar.\nOptimizers So far we have used the Stochastic Gradient Descent to fit the neural network. We are now going to use different ones.\nThis is the description of the optimizers:\n  SGD: Implements stochastic gradient descent (optionally with momentum).\n  Momentum: Nesterov momentum is based on the formula from On the importance of initialization and momentum in deep learning.\n  RMSprop: Proposed by G. Hinton in his course. The centered version first appears in Generating Sequences With Recurrent Neural Networks. The implementation here takes the square root of the gradient average before adding epsilon (note that TensorFlow interchanges these two operations). The effective learning rate is thus $\\frac{\\alpha}{\\sqrt{v} + \\epsilon}$ where $\\alpha$ is the scheduled learning rate and $v$ is the weighted moving average of the squared gradient.\n  Adam: Proposed in Adam: A Method for Stochastic Optimization. The implementation of the L2 penalty follows changes proposed in Decoupled Weight Decay Regularization.\n  # parameters LR = 0.1 EPOCHS = 25 # nets n = torch.nn.Sequential(torch.nn.Linear(1, 10), torch.nn.ReLU(), torch.nn.Linear(10, 1)) nets = [n,n,n,n] # different optimizers opt_SGD = torch.optim.SGD(nets[0].parameters(), lr=LR) opt_Momentum = torch.optim.SGD(nets[1].parameters(), lr=LR, momentum=0.8) opt_RMSprop = torch.optim.RMSprop(nets[2].parameters(), lr=LR, alpha=0.9) opt_Adam = torch.optim.Adam(nets[3].parameters(), lr=LR, betas=(0.9, 0.99)) optimizers = [opt_SGD, opt_Momentum, opt_RMSprop, opt_Adam] # loss functions l = torch.nn.MSELoss() loss_funcs = [l,l,l,l]  Let\u0026rsquo;s prot the loss functions over training, for different optimizers.\n# train labels = ['SGD', 'Momentum', 'RMSprop', 'Adam'] train_nets(x, y, y_true, nets, optimizers, loss_funcs, labels, EPOCHS)  Training on batch Until now, we have used the whole training data at every training step. It has been batch gradient descent all along.\nThis is fine for our ridiculously small dataset, sure, but if we want to go serious about all this, we must use mini-batch gradient descent. Thus, we need mini-batches. Thus, we need to slice our dataset accordingly. Do you want to do it manually?! Me neither!\nSo we use PyTorch’s DataLoader class for this job. We tell it which dataset to use (the one we just built in the previous section), the desired mini-batch size and if we’d like to shuffle it or not. That’s it!\nOur loader will behave like an iterator, so we can loop over it and fetch a different mini-batch every time.\n# Init data x_tensor = torch.from_numpy(x).float().to('cpu') y_tensor = torch.from_numpy(y).float().to('cpu') torch_dataset = Data.TensorDataset(x_tensor, y_tensor) # Build DataLoader BATCH_SIZE = 25 loader = Data.DataLoader( dataset=torch_dataset, # torch TensorDataset format batch_size=BATCH_SIZE, # mini batch size shuffle=True, # random shuffle for training )  Let\u0026rsquo;s try using sub-samples of dimension BATCH_SIZE = 25.\ndef train_NN_batch(loader, y_true, net, optimizer, loss_func, EPOCHS): # init losses = [] # train for t in range(EPOCHS): # train entire dataset 3 times for step, (batch_x, batch_y) in enumerate(loader): # compute loss y_hat = net(batch_x) loss = loss_func(y_hat, batch_y) # update parameters optimizer.zero_grad() # clear gradients for next train loss.backward() # backpropagation, compute gradients optimizer.step() # apply gradients # plt every epoch y_hat = net(x_tensor) losses += [((y_true - y_hat.detach().numpy())**2).mean()] if (t+1) % (EPOCHS/25) == 0: gradient_plot(x, y, y_hat.data.numpy(), y_true, EPOCHS, losses)  # parameters LR = 0.1 EPOCHS = 1000 net = torch.nn.Sequential(torch.nn.Linear(1, 10), torch.nn.ReLU(), torch.nn.Linear(10, 1)) optimizer = torch.optim.SGD(net.parameters(), lr=LR) loss_func = torch.nn.MSELoss() # Train train_NN_batch(loader, y_true, net, optimizer, loss_func, EPOCHS)  Two things are different now: not only we have an inner loop to load each and every mini-batch from our DataLoader but, more importantly, we are now sending only one mini-batch to the device.\nFor bigger datasets, loading data sample by sample (into a CPU tensor) using Dataset’s __get_item__ and then sending all samples that belong to the same mini-batch at once to your GPU (device) is the way to go in order to make the best use of your graphics card’s RAM.\nMoreover, if you have many GPUs to train your model on, it is best to keep your dataset “agnostic” and assign the batches to different GPUs during training.\n8.4 Advanced Topics Issues Starting Values Usually starting values for weights are chosen to be random values near zero. Hence the model starts out nearly linear, and becomes nonlinear as the weights increase.\nOverfitting In early developments of neural networks, either by design or by accident, an early stopping rule was used to avoid overfitting.\nA more explicit method for regularization is weight decay.\nScaling of the Inputs Since the scaling of the inputs determines the effective scaling of the weights in the bottom layer, it can have a large effect on the quality of the final solution. At the outset it is best to standardize all inputs to have mean zero and standard deviation one.\nNumber of Hidden Units and Layers Generally speaking it is better to have too many hidden units than too few. With too few hidden units, the model might not have enough flexibility to capture the nonlinearities in the data; with too many hidden units, the extra weights can be shrunk toward zero if appropriate regularization is used.\nChoice of the number of hidden layers is guided by background knowledge and experimentation. Each layer extracts features of the input for regression or classification. Use of multiple hidden layers allows construction of hierarchical features at different levels of resolution.\nYou can get an intuition on the role of hidden layers here: https://playground.tensorflow.org/\nMultiple Minima The error function R(θ) is nonconvex, possessing many local minima. One approach is to use the average predictions over the collection of networks as the final prediction. Another approach is via bagging.\nDeep Neural Networks and Deep Learning Deep Neural Networks are just Neural Networks with more than one hidden layer.\nConvolutional Neural Nets Convolutional Neural Nets are often applied when dealing with image/video data. They are usually coded with each feature being a pixel and its value is the pixel color (3 dimensional RGB array).\nVideos and images have 2 main characteristics:\n have lots of features \u0026ldquo;close\u0026rdquo; features are often similar  Convolutional Neural Nets exploit the second characteristic to alleviate the computational problems arising from the first. They do it by constructing a first layer that does not build on evey feature but only on adjacent ones.\nIn this way, most of the information is preserved, on a lower dimensional representation.\nRecurrent Neural Nets Recurrent Neural Networks are often applied in contexts in which the data generating process is dynamic. The most important example is Natural Language Processing. The idea is that you want to make predictions \u0026ldquo;live\u0026rdquo; as data comes in. Moreover, the order of the data is relevant, so that you also what to keep track of what the model has learned so far.\nWhile RNNs learn similarly while training, in addition, they remember things learnt from prior input(s) while generating output(s). It’s part of the network. RNNs can take one or more input vectors and produce one or more output vectors and the output(s) are influenced not just by weights applied on inputs like a regular NN, but also by a “hidden” state vector representing the context based on prior input(s)/output(s). So, the same input could produce a different output depending on previous inputs in the series.\nGrafically:\nIn summary, in a vanilla neural network, a fixed size input vector is transformed into a fixed size output vector. Such a network becomes “recurrent” when you repeatedly apply the transformations to a series of given input and produce a series of output vectors.\nBidirectional RNN Sometimes it’s not just about learning from the past to predict the future, but we also need to look into the future to fix the past. In speech recognition and handwriting recognition tasks, where there could be considerable ambiguity given just one part of the input, we often need to know what’s coming next to better understand the context and detect the present.\nThis does introduce the obvious challenge of how much into the future we need to look into, because if we have to wait to see all inputs then the entire operation will become costly.\nRecursive Neural Netw A recurrent neural network parses the inputs in a sequential fashion. A recursive neural network is similar to the extent that the transitions are repeatedly applied to inputs, but not necessarily in a sequential fashion. Recursive Neural Networks are a more general form of Recurrent Neural Networks. It can operate on any hierarchical tree structure. Parsing through input nodes, combining child nodes into parent nodes and combining them with other child/parent nodes to create a tree like structure. Recurrent Neural Networks do the same, but the structure there is strictly linear. i.e. weights are applied on the first input node, then the second, third and so on.\nBut this raises questions pertaining to the structure. How do we decide that? If the structure is fixed like in Recurrent Neural Networks then the process of training, backprop etc makes sense in that they are similar to a regular neural network. But if the structure isn’t fixed, is that learnt as well?\n","date":1646784000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1646784000,"objectID":"9d0e7e97b8f20fca1686acda2d5705cf","permalink":"https://matteocourthoud.github.io/course/ml-econ/08_neuralnets/","publishdate":"2022-03-09T00:00:00Z","relpermalink":"/course/ml-econ/08_neuralnets/","section":"course","summary":"# Remove warnings import warnings warnings.filterwarnings('ignore')  # Import everything import pandas as pd import numpy as np import seaborn as sns import statsmodels.api as sm import copy import torch import torch.","tags":null,"title":"Neural Networks","type":"book"},{"authors":null,"categories":null,"content":"Introduction Intro Setting: agents making strategic decisions (new) in dynamic environments.\n Entry and exit: Collard-Wexler (2013) Sunk costs: Ryan (2012) Innovation: Goettler and Gordon (2011)  (or whatever changes in response to investment)   Exploitation of natural resources: Huang and Smith (2014) Durable goods: Esteban and Shum (2007)   Lit review: forthcoming IO Handbook chapter Aguirregabiria, Collard-Wexler, and Ryan (2021)\n Single- vs Multi-Agent Typically in IO we study agents in strategic environments. Complicated in dynamic environments.\n Curse of dimensionality  Single agent: need to track what the agent sees ($k$ states) Multi-agent: need to keep track what every agent sees ($k^J$states) Difference exponential in the number of agents   Expectations  Need not only to keep track of how the environment evolves … but also of how other players act   Equilibrium  Because of the strategic interaction, the Bellman equation is not a contraction anymore  Equilibrium existence? Equilibrium uniqueness?      Plan We will cover first the estimation and then the computation of dynamic games\n Weird… Standard estimation method: Bajari, Benkard, and Levin (2007) Does not require to solve the model Indeed, that’s the advantage of the method Disadvantages: still need to solve the model for counterfactuals So we’ll cover computation afterwards  Last: bridge between Structural IO and Artificial Intelligence\n Different objectives but similar methods Dynamic tools niche in IO but at the core of AI  Bajari, Benkard, Levin (2008) Model Stylized version of Ericson and Pakes (1995) (no entry/exit)\n  $J$ firms (products) indexed by $j \\in \\lbrace 1, \u0026hellip;, J \\rbrace$\n  Time $t$ is dicrete, horizon is infinite\n  States $s_{jt} \\in \\lbrace 1, \u0026hellip; \\bar s \\rbrace$: quality of product $j$ in period $t$\n  Actions $a_{jt} \\in \\mathbb R^+$: investment decision of firm $j$ in period $t$\n  Static payoffs $$ \\pi_j (s_{jt}, \\boldsymbol s_{-jt}, a_{jt}; \\theta^\\pi) $$ where\n $\\boldsymbol s_{-it}$: state vector of all other firms in period $t$ $\\theta^\\pi$: parameters that govern static profits     Note: if we micro-fund $\\pi(\\cdot)$ , e.g. with some demand and supply model, we have 2 strategic decisions: prices (static) and investment (dynamic).\n Model (2)   State transitions $$ \\boldsymbol s_{t+1} = f(\\boldsymbol s_t, \\boldsymbol a_t, \\boldsymbol \\epsilon_t; \\theta^f) $$ where\n $\\boldsymbol a_t$: vector of actions of all firm $\\boldsymbol \\epsilon_t$: vector of idiosyncratic shocks $\\theta^f$: parameters that govern state transitions    Objective function: firms maximize expected discounted future profits $$ \\max_{\\boldsymbol a} \\ \\mathbb E_t \\left[ \\sum_{\\tau=0}^\\infty \\beta^{\\tau} \\pi_{j, t+\\tau} (\\theta^\\pi) \\right] $$\n  Value Function The value function of firm $j$ at time $t$ in state $\\boldsymbol s_{t}$, under a set of strategy functions $\\boldsymbol P$ (one for each firm) is $$ V^{\\boldsymbol P_{-j}}{j} (\\mathbf{s}{t}) = \\max_{a_{jt} \\in \\mathcal{A}j \\left(\\mathbf{s}{t}\\right)} \\Bigg\\lbrace \\pi_{j}^{\\boldsymbol P_{-j}} (a_{jt}, \\mathbf{s}{t} ; \\theta^\\pi ) + \\beta \\mathbb E{\\boldsymbol s_{t+1}} \\Big[ V_{j}^{\\boldsymbol P_{-j}} \\left(\\mathbf{s}{t+1}\\right) \\ \\Big| \\ a{jt}, \\boldsymbol s_{t} ; \\theta^f \\Big] \\Bigg\\rbrace $$ where\n  $\\pi_{j}^{\\boldsymbol P_{-j}} (a_{jt}, \\mathbf{s}{t} ; \\theta^\\pi )$ are the static profits of firm $j$ given action $a{jt}$ and policy functions $\\boldsymbol P_{-j}$ for all firms a part from $j$\n  The expecation $\\mathbb E$ is taken with respect to the conditional transition probabilities $f^{\\boldsymbol P_{-j}} (\\mathbf{s}{t+1} | \\mathbf{s}{t}, a_{jt} ; \\theta^f)$\n  Equilibrium Equillibrium notion: Markow Perfect Equilibrium (Maskin and Tirole 1988)\n Assumption: players’ strategies at period $t$ are functions only of payoff-relevant state variables at the same period Definition: a set of $J$ value and policy functions, $\\boldsymbol V$ and $\\boldsymbol P$ such that each firm  maximizes its value function $V_j$ given the policy function of every other firm $\\boldsymbol P_{-j}$    What is it basically?\n Nash Equilibrium in the policy functions What are we ruling out?  Strategies that depend on longer histories E.g. “has anyone ever cheated in a cartel?”    Estimation We want to estimate 2 sets of parameters:\n $\\theta^\\pi$: parameterizes period profit function $\\pi(\\cdot)$ $\\theta^f$: parameterizes state transition function $f(\\cdot)$  Generally 2 approaches\n Full solution  Impractical (we’ll see more details later)   Rely on some sort of Hotz and Miller (1993) CCP inversion  Aguirregabiria and Mira (2007) Bajari, Benkard, and Levin (2007) Pakes, Ostrovsky, and Berry (2007) Pesendorfer and Schmidt-Dengler (2008)    BBL Overview Bajari, Benkard, and Levin (2007) plan\n Estimate transition probabilities and conditional choice probabilities from the data Use them to simulate the expected value function, given a set of parameters Use optimality of estimated choices to pin down static profit parameters  I.e. repeat (2) for alternative strategies  By definition suboptimal   Estimating equation: values implied by observed strategies should be higher than values implied by alternative strategies    BBL: First Stage  Estimate the transition probabilities $f ( \\cdot | a_{jt}, \\boldsymbol s_t; \\hat \\theta^f )$  I.e. what is the observed frequency of any state-to-state transition? For any given action of firm $j$   … and conditional choice probabilities $\\hat P_j(\\cdot | \\boldsymbol s_t)$  I.e. what is the probability of each action, for each firm $j$ in each state $\\boldsymbol s$   Can be done non-parametrically  i.e. just observe frequencies Conditional on having enough data Note: need to estimate transitions, conditional on each state and action Problem with many states and actions, but especially with many players  Curse of dimensionality Number of states increases exponentially in number of players       Important: parametric assumptions would contradict the model for the estimation of value/policy functions\n BBL: Second Stage First step: from transitions $f(\\hat \\theta^f)$ and CCPs $\\boldsymbol{\\hat P}$ to values\n  We can use transitions and CCPs to simulate histories (of length $\\tilde T$)\n of states $\\lbrace \\boldsymbol{\\tilde{s}{\\tau}} \\rbrace{\\tau = 1}^{\\tilde T}$ and actions $\\lbrace \\boldsymbol{\\tilde{a}{\\tau}} \\rbrace{\\tau = 1}^{\\tilde T}$    Given a parameter value $\\tilde \\theta^\\pi$, we can compute static payoffs: $\\pi_{j}^{\\boldsymbol {\\hat{P}{-j}}} \\left( \\tilde a{j\\tau}, \\boldsymbol{\\tilde s}_{\\tau} ; \\tilde \\theta^\\pi \\right)$\n  Simulated history + static payoffs = simulated value function $$ {V}{j}^{\\boldsymbol {\\hat{P}}} \\left(\\boldsymbol{s}{t} ; \\tilde \\theta^\\pi \\right) = \\sum_{\\tau=0}^{\\tilde T} \\beta^{\\tau} \\pi_{j}^{\\boldsymbol {\\hat{P}{-j}}} \\left( \\tilde a{j\\tau}, \\boldsymbol{\\tilde s}_{\\tau} ; \\tilde \\theta^\\pi \\right) $$\n  We can average over many, e.g. $R$, simulated value functions to get an expected value function $$ {V}{j}^{\\boldsymbol {\\hat{P}}, R} \\left( \\boldsymbol{s}{t} ; \\tilde \\theta^\\pi \\right) = \\frac{1}{R} \\sum_{r=0}^{R}\\Bigg( \\sum_{\\tau=0}^{\\tilde T} \\beta^{\\tau} \\pi_{j}^{\\boldsymbol {\\hat{P}{-j}}} \\left(\\tilde a^{(r)}{j\\tau}, \\boldsymbol{\\tilde s}^{(r)}_{\\tau} ; \\tilde \\theta^\\pi \\right) \\Bigg) $$\n  In practice, for a parameter value $\\tilde \\theta^\\pi$ For $r = 1, \u0026hellip;, R$ simulations do:\n Initialize firms value to zero Fot $\\tau=0, \u0026hellip;, \\tilde T$ do  For each state in $\\boldsymbol{\\tilde s}^{(r)}_{\\tau}$ do:  Use $\\boldsymbol{\\hat P}$ to draw a vector of firm actions $\\boldsymbol{\\tilde a}^{(r)}_{\\tau}$ For each firm $j = 1, \u0026hellip;, J$ do:  Compute static profits $\\pi_{j}^{\\boldsymbol {\\hat{P}{-j}}} \\left(\\tilde a^{(r)}{j\\tau}, \\boldsymbol{\\tilde s}^{(r)}_{\\tau} ; \\tilde \\theta^\\pi \\right)$ Add discounted profits $\\beta^{\\tau} \\pi_{j}^{\\boldsymbol {\\hat{P}{-j}}} \\left(\\tilde a^{(r)}{j\\tau}, \\boldsymbol{\\tilde s}^{(r)}_{\\tau} ; \\tilde \\theta^\\pi \\right)$ to the value function   Use $f ( \\cdot | \\boldsymbol {a_{t}}, \\boldsymbol s_t; \\hat \\theta^f )$ to draw the next state $\\boldsymbol{\\tilde s}^{(r)}_{\\tau + 1}$ Use the next state, $\\boldsymbol{\\tilde s}^{(r)}_{\\tau + 1}$ as current state for the next iteration      Then average all the value functions together to obtain an expected value function $V_{j}^{\\boldsymbol {\\hat{P}}, R} \\left(\\boldsymbol{s}_{t} ; \\tilde \\theta^\\pi \\right)$\n Note: advantage of simulations: can be parallelized\n Objective Function What have we done so far?\n Given some parameters $\\theta^\\pi$, we computed the expected value function  How do we pick the $\\theta^\\pi$ that best rationalizes the data?\n I.e. what is the objective function? Potentially many options  BBL idea\n the expected value function has to be optimal, given the CCPs I.e. any other policy function should give a lower expected value “Best” $\\theta^\\pi$: those for which the implied expected value function under the estimated CCPs is greater than the one implied by any other CCP   Note: it’s an inequality statement\n Objective Function (2) Idea\n  If the observed policy ${\\color{green}{\\boldsymbol{\\hat P}}}$ is optimal,\n  All other policies ${\\color{red}{\\boldsymbol{\\tilde P}}}$\n  … at the true parameters $\\theta^f$\n  … should give a lower expected value $$ V_{j}^{{\\color{red}{\\boldsymbol{\\tilde P}}}, R} \\left( \\boldsymbol{s}{t} ; \\tilde \\theta^\\pi \\right) \\leq V{j}^{{\\color{green}{\\boldsymbol{\\hat P}}}, R} \\left( \\boldsymbol{s}_{t} ; \\tilde \\theta^\\pi \\right) $$\n    So which are the true parameters?\n  Those for which any deviation from the observed policy ${\\color{green}{\\boldsymbol{\\hat P}}}$ yields a lower value\n  Objective function to minimize: violations under alternative policies ${\\color{red}{\\boldsymbol{\\tilde P}}}$ $$ \\min_{\\tilde \\theta^\\pi} \\sum_{\\boldsymbol s_{t}} \\sum_{{\\color{red}{\\boldsymbol{\\tilde P}}}} \\Bigg[\\min \\bigg\\lbrace V_{j}^{{\\color{green}{\\boldsymbol{\\hat P}}}, R} \\left( \\boldsymbol{s}{t} ; \\tilde \\theta^\\pi \\right) - V{j}^{{\\color{red}{\\boldsymbol{\\tilde P}}}, R} \\left( \\boldsymbol{s}_{t} ; \\tilde \\theta^\\pi \\right) \\ , \\ 0 \\bigg\\rbrace \\Bigg]^{2} $$\n    Estimator Estimator: $\\theta^\\pi$ that minimizes the average (squared) magnitude of violations for any alternative policy ${\\color{red}{\\boldsymbol{\\tilde P}}}$ $$ \\hat{\\theta}^\\pi= \\arg \\min_{\\tilde \\theta^\\pi} \\sum_{\\boldsymbol s_{t}} \\sum_{{\\color{red}{\\boldsymbol{\\tilde P}}}} \\Bigg[\\min \\bigg\\lbrace V_{j}^{{\\color{green}{\\boldsymbol{\\hat P}}}, R} \\left( \\boldsymbol{s}{t} ; \\tilde \\theta^\\pi \\right) - V{j}^{{\\color{red}{\\boldsymbol{\\tilde P}}}, R} \\left( \\boldsymbol{s}_{t} ; \\tilde \\theta^\\pi \\right) \\ , \\ 0 \\bigg\\rbrace \\Bigg]^{2} $$\n $\\min \\Big\\lbrace V_{j}^{{\\color{green}{\\boldsymbol{\\hat P}}}, R} - V_j^{{\\color{red}{\\boldsymbol{\\tilde P}}}, R} \\ , \\ 0 \\Big\\rbrace$ to pick only the violations  If ${\\color{green}{\\boldsymbol{\\hat P}}}$ implies higher value, we can ignore Doesn’t matter by how much you respect the inequality   Which alternative policies ${\\color{red}{\\boldsymbol{\\tilde P}}}$ should we use?  In principle, any perturbation is ok But in practice, if we perturbe it too much, we can go too far off Tip 1: start with very small perturbations Tip 2: use perturbation that sensibly affect the dynamics  E.g. exiting in a state in which a firm is not a competitive threat   Tip 3: use perturbations on dimensions that are relevant for the research question  E.g. they affect dimensions where you want to make counterfactual predictions      Advantages We have seen that there are competing methods.\nWhat are the advantages of Bajari, Benkard, and Levin (2007) over those?\n Continuous actions  BBL does not require actions to be discretised You can just sample actions from the data!   Choice of alternative CCPs  The researcher is free to choose the alternative CCPs ${\\color{red}{\\boldsymbol{\\tilde P}}}$ Pros: can make source of variation more transparent  allows the researcher to focus on those predictions of the model that are key for the specific research questions   Cons: it’s a very high dimensional space  There are very very many alternative policy functions      Problems  Computational curse of dimensionality is gone (in the state space)  But we have a curse of dimensionality in data Need a lot of markets because now 1 market is 1 observation   Multiple equilibria??  We are basically assuming it away Estimating the CCPs in the first stage we assume that is the equilibrium that is played in all markets at all times To run counterfactuals, we still need to solve the model   Unobserved heterogeneity  Kasahara and Shimotsu (2009): how to identify the (minimum) number of unobserved types Arcidiacono and Miller (2011): how to use an EM algorithm for the 1st stage estimation with unobserved types, conditional on the number of types Berry and Compiani (2021): instrumental variables approach, relying on observed states in the distant past   Non-stationarity  If we have a long time period, something fundamentally might have changed    Ericson Pakes (1995) Introduction Ericson and Pakes (1995) and companion paper Pakes and McGuire (1994) for the computation\n  $J$ firms indexed by $j \\in \\lbrace 1, \u0026hellip;, J \\rbrace$\n  Time $t$ is dicrete $t$, horizon is infinite\n  State $s_{jt}$: quality of firm $j$ in period $t$\n  Per period profits $$ \\pi (s_{jt}, \\boldsymbol s_{-jt}, ; \\theta^\\pi) $$ where\n $\\boldsymbol s_{-it}$: state vector of all other firms in period $t$ $\\theta^\\pi$: parameters that govern static profits    We can micro-fund profits with some demand and supply functions\n There can be some underlying static strategic interaction E.g. logit demand and bertrand competition in prices $p_{it}$    State Transitions Investment: firms can invest an dollar amount $x$ to increase their future quality\n  Continuous decision variable ($\\neq$ Rust)\n  Probability that investment is successful $$ \\Pr \\big(i_{jt} \\ \\big| \\ a_{it} = x \\big) = \\frac{\\alpha x}{1 + \\alpha x} $$\n  Higher investment, higher success probability\n  $\\alpha$ parametrizes the returns on investment\n  Quality depreciation\n With probability $\\delta$, quality decreases by one level  Law of motion $$ s_{j,t+1} = s_{jt} + i_{jt} - \\delta $$\nDecision Variables Note that in Ericson and Pakes (1995) we have two separate decision variables\n Static decision variable: price $p_{jt}$ Dynamic decision variable: investment $i_{jt}$  Does not have to be the case!\nExample: Besanko et al. (2010)\n Model of learning-by-doing: firms decrease their marginal cost through sales State variable: firm stock of know how $e$  The higher the stock of know-how, the lower the marginal cost Increases when a firm manages to make a sale  $q \\in [0,1]$ now is both static quantity and transition probability     Single decision variable: price $p$  Usual static effects on profits $\\pi_{jt} = (p_{jt} - c(e_{jt})) \\cdot q_j(\\boldsymbol p_t)$ But also dynamic effect through transition probabilities  Probability of increasing $e_t$: $q_j(\\boldsymbol p_t)$      Equilibrium Firms maximize the expected flow of discounted profits $$ \\max_{\\boldsymbol a} \\ \\mathbb E_t \\left[ \\sum_{\\tau=0}^\\infty \\beta^{\\tau} \\pi_{j, t+\\tau} (\\theta^\\pi) \\right] $$ Markow Perfect Equilibrium\nEquillibrium notion: Markow Perfect Equilibrium (Maskin and Tirole 1988)\n A set of $J$ value and policy functions, $\\boldsymbol V$ and $\\boldsymbol P$ such that each firm  maximizes its value function $V_j$ given the policy function of every other firm $\\boldsymbol P_{-j}$    Exit One important extension is exit.\n In each time period, incuments decide whether to stay … or exit and get a scrap value $\\phi^{exit}$  The Belman Equation of incumbent $j$ at time $t$ is $$ V^{\\boldsymbol P_{-j}}{j} (\\mathbf{s}{t}) = \\max_{d^{exit}{jt} \\in \\lbrace 0, 1 \\rbrace} \\Bigg\\lbrace \\begin{array}{c} \\beta \\phi^{exit} \\ , \\newline \\max{a_{jt} \\in \\mathcal{A}j \\left(\\mathbf{s}{t}\\right)} \\Big\\lbrace \\pi_{j}^{\\boldsymbol P_{-j}} (a_{jt}, \\mathbf{s}{t} ; \\theta^\\pi ) + \\beta \\mathbb E{\\boldsymbol s_{t+1}} \\Big[ V_{j}^{\\boldsymbol P_{-j}} \\left(\\mathbf{s}{t+1}\\right) \\ \\Big| \\ a{jt}, \\boldsymbol s_{t} ; \\theta^f \\Big] \\Big\\rbrace \\end{array} \\Bigg\\rbrace $$ where\n $\\phi^{exit}$: exit scrap value $d^{exit}_{jt} \\in \\lbrace 0,1 \\rbrace$: exit decision  Entry We can also incorporate endogenous entry.\n One or more potential entrants exist outside the market They can pay an entry cost $\\phi^{entry}$ and enter the market at a quality state $\\bar s$ … or remain outside at no cost  Value function $$ V_{j}^{\\boldsymbol P_{-j}} (e, \\boldsymbol x_{-jt} ; \\theta) = \\max_{d^{entry} \\in \\lbrace 0,1 \\rbrace } \\Bigg\\lbrace \\begin{array}{c} 0 \\ ; \\newline\n  \\phi^{entry} + \\beta \\mathbb E_{\\boldsymbol s_{t+1}} \\Big[ V_{j}^{\\boldsymbol P_{-j}} (\\bar s, \\boldsymbol s_{-j, t+1} ; \\theta) \\ \\Big| \\ \\boldsymbol s_{t} ; \\theta^f \\Big] \\end{array} \\Bigg\\rbrace $$ where\n  $d^{entry} \\in \\lbrace 0,1 \\rbrace$: entry decision\n  $\\phi^{entry}$: entry cost\n  $\\bar s$: state in which entrants enters (could be random)\n  Do we observe potential entrants?\n Igami (2017): tech industry announce their entry Critique: not really potential entrants, they are half-way inside  Equilibrium Existence Doraszelski and Satterthwaite (2010): a MPE might not exist in Ericson and Pakes (1995) model.\nSolution\n Replace fixed entry costs $\\phi^{entry}$ and exit scrap values $\\phi^{exit}$ with random ones It becomes a game of incomplete information  First explored in Rust (1994)   New equilibrium concept  Markov Perfect Bayesian Nash Equilibrium (MPBNE)\n Basically the same, with rational beliefs on random variables  Solving the Model Solving the model is very similar to Rust\n Given parameter values $\\theta$ Start with a guess for the value and policy functions Until convergence, do:  For each firm $j = 1, \u0026hellip;, J$, do:  Take the policy functions of all other firms Compute the implied transition probabilities Use them to compute the new policy function for firm $j$ Compute the implied value function       Where do things get complicated / tricky? Policy function update\n Policy Update Example: exit game Imagine a stylized exit game with 2 firms\n Easy to get an update rule of the form: “exit if opponent stays, stay if opponent exits”  Computationally\n Initialize policy functions to $(exit, exit)$ Iteration 1:  Each firm takes opponent policy as given: $exit$ Update own optimal policy: $stay$ New policy: $(stay, stay)$   Iteration 2: $(stay, stay) \\to (exit, exit)$ Iteration 2: $(exit, exit) \\to (stay, stay)$ Etc…   Issues: value function iteration might not converge and equilibrium multeplicity.\n Convergence Tips  Try different starting values  Often it’s what makes the biggest difference Ideally, start as close as possible to true values Approximation methods can help (we’ll see more later)  I.e. get a fast approximation to use as starting vlaue for solution algorithm     Partial/stochastic value function update rule  Instead of $V' = T(V)$, use $V' = \\alpha T(V) + (1-\\alpha)V$ Very good to break loops, especially if $\\alpha$ is stochastic, e.g. $\\alpha \\sim U(0,1)$   How large is the support of the entry/exit costs?  If support is too small, you end up back in the entry/exit loop   Try alternative non-parallel updating schemes  E.g. update value one state at the time (in random order?)   Last but not least: change the model  In particular, from simultaneous to alternating moves or continuous time    Multiple Equilibria How to find them?\n Besanko et al. (2010) and Borkovsky, Doraszelski, and Kryukov (2010): homotopy method  can find some equilibria, but not all complicated to implement: need to compute first order conditions $H(\\boldsymbol V, \\theta) = 0$ and their Jacobian $\\Delta H(\\boldsymbol V, \\theta)$ Idea: trace the equilibrium correspondence $H^{-1} = \\lbrace (\\boldsymbol V, \\theta) : H(\\boldsymbol V, \\theta) = 0 \\rbrace$ in the value-parameter space   Eibelshäuser and Poensgen (2019)  Markov Quantal Response Equilibrium approact dynamic games from a evolutionary game theory perspective  actions played at random and those bringing highest payoffs survive   $\\to$ homothopy method guaranteed to find one equilibrium   Pesendorfer and Schmidt-Dengler (2010): some equilibria are not Lyapunov-stable  BR iteration cannot find them unless you start exactly at the solution   Su and Judd (2012) and Egesdal, Lai, and Su (2015): same point, but numerically  using MPEC approach    Multiple Equilibria (2) Can we assume them away?\n Igami (2017)  Finite horizon Homogenous firms (in profit functions and state transitions) One dynamic move per period (overall, not per-firm)   Abbring and Campbell (2010)  Entry/exit game Homogeneous firms Entry and exit decisions are follow a last-in first-out (LIFO) structure  “An entrant expects to produce no longer than any incumbent”     Iskhakov, Rust, and Schjerning (2016)  can find all equilibria, but for very specific class of dynamic games must always proceed “forward”  e.g. either entry or exit but not both   Idea: can solve by backward induction even if horizon is infinite    Curse of Dimensionality What are the computational bottlenecks? $$ V^{\\boldsymbol P_{-j}}{j} ({\\color{red}{\\mathbf{s}{t}}}) = \\max_{a_{jt} \\in \\mathcal{A}j \\left(\\mathbf{s}{t}\\right)} \\Bigg\\lbrace \\pi_{j}^{\\boldsymbol P_{-j}} (a_{jt}, \\mathbf{s}{t} ; \\theta^\\pi ) + \\beta \\mathbb E{{\\color{red}{\\mathbf{s}{t+1}}}} \\Big[ V{j}^{\\boldsymbol P_{-j}} \\left(\\mathbf{s}{t+1}\\right) \\ \\Big| \\ a{jt}, \\boldsymbol s_{t} ; \\theta^f \\Big] \\Bigg\\rbrace $$\n Dimension of the state space  In single agent problems, we have as many states as many values of $s_{jt}$ ($k$) In dynamics games, the state space goes from $k$ to $k^J$ symmetry helps: state $[1,2,3]$ and $[1,3,2]$ become the same for firm 1 How much do we gain? From $k^J$ to $k \\cdot {k + J - 2 \\choose k - 1}$   Dimension of the integrand  If in single agent problems, we have to integrate over $\\kappa$ outcomes,  4 in Rust: engine replaced (yes|no) $\\times$ mileage increases (yes|no)   … in dynamic games, we have to consider $\\kappa^J$ outcomes     Note: bottlenecks are not addittive but multiplicative: have to solve the expectation for each point in the state space. Improving on any of the two helps a lot.\n Curse of Dimensionality (2) Two and a half classes of solutions:\n Computational: approximate the equilibrium  Doraszelski (2003): use Chebyshev polynomials for a basis function Farias, Saure, and Weintraub (2012): combine approximations with a MPEC-like approach   Conceptual: define another game  Weintraub, Benkard, and Van Roy (2008): oblivious equilibrium Ifrach and Weintraub (2017): moment based equilibrium Doraszelski and Judd (2012): games in continuous time Doraszelski and Judd (2019): games with random moves   Kind of both: Pakes and McGuire (2001)  experience-based equilibrium (Fershtman and Pakes 2012)     Note: useful also to get good starting values for a full solution method!\n Oblivious Equilibrium Weintraub, Benkard, and Van Roy (2008): what if firms had no idea about the state of other firms?\n or atomistic firms  The value function becomes $$ V_{j} ({\\color{red}{s_{t}}}) = \\max_{a_{jt} \\in \\mathcal{A}j \\left({\\color{red}{s{t}}}\\right)} \\Bigg\\lbrace {\\color{red}{\\mathbb E_{\\boldsymbol s_t}}} \\Big[ \\pi_{j} (a_{jt}, \\mathbf{s}{t} ; \\theta^\\pi ) \\Big| P \\Big] + \\beta \\mathbb E{{\\color{red}{s_{t+1}}}} \\Big[ V_{j} \\left({\\color{red}{s_{t+1}}}\\right) \\ \\Big| \\ a_{jt}, {\\color{red}{s_{t}}} ; \\theta^f \\Big] \\Bigg\\rbrace $$\n Now the state is just $s_t$ instead of $\\boldsymbol s_t$  Huge computational gain: from $k^J$ points to $k$ Also the expectation of future states is taken over $3$ instead of $3^J$ points  (3 because quality can go up, down or stay the same)     But need to compute static profits as the expected value given the current policy function  Need to keep track of the asymptotic state distribution as you iterate the value    Games with Random Moves Doraszelski and Judd (2019): what if instead of simultaneously, firms would move one at the time at random?\n Important: to have the same frequency of play, a period now is $J$ times shorter  The value function becomes $$ V^{\\boldsymbol P_{-j}}{j} (\\mathbf{s}{t}, {\\color{red}{n=j}}) = \\max_{a_{jt} \\in \\mathcal{A}j \\left(\\mathbf{s}{t}\\right)} \\Bigg\\lbrace {\\color{red}{\\frac{1}{J}}}\\pi_{j}^{\\boldsymbol P_{-j}} (a_{jt}, \\mathbf{s}{t} ; \\theta^\\pi ) + {\\color{red}{\\sqrt[J]{\\beta}}} \\mathbb E{{\\color{red}{n, s_{j, t+1}}}} \\Big[ V_{j}^{\\boldsymbol P_{-j}} \\left(\\mathbf{s}{t+1}, {\\color{red}{n}} \\right) \\ \\Big| \\ a{jt}, \\boldsymbol s_{t} ; \\theta^f \\Big] \\Bigg\\rbrace $$\n $n$: indicates whose turn is to play since a turn is $J$ times shorter, profits are $\\frac{1}{J} \\pi$ and discount factor is $\\sqrt[J]{\\beta}$  Computational gain\n Expectation now taken over $n, s_{j, t+1}$ instead of $\\boldsymbol s_{t+1}$ I.e. $Jk$ points instead of $3^k$ (3 because quality can go up, down or stay the same) Huge computational difference!  Games in Continuous Time Doraszelski and Judd (2012): what’s the advantage of continuous time?\n Probability that two firms take a decision simultaneously is zero  With continuous time, the value function becomes $$ V^{\\boldsymbol P_{-j}}{j} (\\mathbf{s}{t}) = \\max_{a_{jt} \\in \\mathcal{A}j \\left(\\mathbf{s}{t}\\right)} \\Bigg\\lbrace \\frac{1}{\\lambda(a_{jt}) - \\log(\\beta)} \\Bigg( \\pi_{j}^{\\boldsymbol P_{-j}} (a_{jt}, \\mathbf{s}{t} ; \\theta^\\pi ) + \\lambda(a{jt}) \\mathbb E_{\\boldsymbol s_{t+1}} \\Big[ V_{j}^{\\boldsymbol P_{-j}} \\left(\\mathbf{s}{t+1}\\right) \\ \\Big| \\ a{jt}, \\boldsymbol s_{t} ; \\theta^f \\Big] \\Bigg) \\Bigg\\rbrace $$\n $\\lambda(a_{jt}) = \\delta + \\frac{\\alpha a_{jt}}{1 + \\alpha a_{jt}}$ is the hazard rate for firm $j$ that something happens  i.e. either an increase in quality, with probability $\\frac{\\alpha a_{jt}}{1 + \\alpha a_{jt}}$ … or a decrease in quality with probability $\\delta$    Computational gain\n Now the expectation over future states $\\mathbb E_{\\boldsymbol s_{t+1}}$ is over $2J$ points instead of $3^J$  3 because quality can go up, down or stay the same 2 because in continuous time we don’t care if the state does not change (investment fails)    Comparison Which method is best?\nI compare them in Courthoud (2020)\n Fastest: Weintraub, Benkard, and Van Roy (2008)  Effectively transforms the game into single-agent dynamics   Best trade-off: Doraszelski and Judd (2019)  Simple, practical and also helps in terms of equilibrium multeplicity   Also in Courthoud (2020): games with random order  Better approximation than Doraszelski and Judd (2019) And similar similar time    Applications Some applications of these methods include\n Approximation methods  Sweeting (2013): product repositioning among radio stations Barwick and Pathak (2015): entry and exit in the real estate brokerage industry   Oblivious equilibrium  Xu and Chen (2020): R\u0026amp;D investment in the Korean electric motor industry   Moment based equilibrium  Jeon (2020): demand learning in the container shipping industry Caoui (2019): technology adoption with network effects in the movie industry Vreugdenhil (2020): search and matching in the oil drilling industry   Games in continuous time  Arcidiacono et al. (2016): entry, exit and scale decisions in retail competition   Games with random moves  Igami (2017): innovation, entry, exit in the hard drive industry    From IO to AI Bridging two Literatures There is one method to approximate the equilibrium in dynamic games that is a bit different from the others: Pakes and McGuire (2001)\n Idea: approximate the value function by Monte-Carlo simulation Firms start with a guess for the alternative-specific value function Act according to it Observe realized payoffs and state transitions And update the alternative-specific value function according to the realized outcomes  Experience-Based Equilibrium\n Defined in Fershtman and Pakes (2012) Def: policy is optimal given beliefs of state transitions and observed transitions are consistent with the beliefs Note: definition silent on off-equilibrium path beliefs  Pakes and McGuire (2001)   Players start with alternative-specific value function\n yes, the ASV from Rust (1994) $\\bar V_{j,a}^{(0)} (\\boldsymbol s ; \\theta)$: initial value of player $j$ for action $a$ in state $\\boldsymbol s$    Until convergence, do:\n  Compute optimal action, given $\\bar V_{j, a}^{(t)} (\\boldsymbol s ; \\theta)$ $$ a^* = \\arg \\max_a \\bar V_{j, a}^{(t)} (\\boldsymbol s ; \\theta) $$\n  Observe the realized payoff $\\pi_{j, a^}(\\boldsymbol s ; \\theta)$ and the realized next state $\\boldsymbol {s'}(\\boldsymbol s, a^; \\theta)$\n  Update the alternative-specific value function of the chosen action $k^$ $$ \\bar V_{j, a^}^{(t+1)} (\\boldsymbol s ; \\theta) = (1-\\alpha_{\\boldsymbol s, t}) \\bar V_{j, a^}^{(t)} (\\boldsymbol s ; \\theta) + \\alpha_{\\boldsymbol s, t} \\Big[\\pi_{j, a^}(\\boldsymbol s ; \\theta) + \\arg \\max_a \\bar V_{j, a}^{(t)} (\\boldsymbol s' ; \\theta) \\Big] $$ where\n $\\alpha_{\\boldsymbol s, t} = \\frac{1}{\\text{number of times state } \\boldsymbol s \\text{ has been visited}}$      Comments Where is the strategic interaction?\n Firm always take “best action so far” in each state  Start to take a new action only when the previous best has performed badly for many periods   Remindful of literature of evolutionary game theory  Importance of starting values\n Imagine, all payoffs are positive but value initialized to zero First action in each state $\\to$ only action ever taken in that state Loophole.  Why? Firms always take $\\arg \\max_a \\bar V_a$ and never explore the alternatives    Convergence by desing\n As $\\lim_{t \\to \\infty} \\alpha_{\\boldsymbol s, t} = 1$ Firms stop updating the value by design  Q-Learning Computer Science reinforcement learning literature (AI): Q-learning\nDifferences\n $\\bar V_a( \\boldsymbol s)$ called $Q_a(\\boldsymbol s)$, hence the name Firms don’t always take the optimal action  At the beginning of the algorithm: exploration  Firms take actions at random Just to explore what happens taking different actions   Gradually shift towards exploitation  I.e. take the optimal action, given $\\bar V^{(t)}( \\boldsymbol s)$ at iteration $t$ I.e. shift towards Pakes and McGuire (2001)      Applications  Doraszelski, Lewis, and Pakes (2018)  Firm do actually learn by trial and error Setting: demand learning in the UK frequency response market (electricity)   Asker et al. (2020)  Uses Pakes and McGuire (2001) for estimation Setting: dynamic timber auctions with information sharing   Calvano et al. (2020)  Study Q-learning pricing algorithms In repeated price competition with differentiated products (Computational) lab experiment: what do these algorithms converge to? Finding: algorithms learn reward-punishment collusive strategies    Appendix References [references] Abbring, Jaap H, and Jeffrey R Campbell. 2010. “Last-in First-Out Oligopoly Dynamics.” Econometrica 78 (5): 1491–1527.\n Aguirregabiria, Victor, Allan Collard-Wexler, and Stephen P Ryan. 2021. “Dynamic Games in Empirical Industrial Organization.” National Bureau of Economic Research.\n Aguirregabiria, Victor, and Pedro Mira. 2007. “Sequential Estimation of Dynamic Discrete Games.” Econometrica 75 (1): 1–53.\n Arcidiacono, Peter, Patrick Bayer, Jason R Blevins, and Paul B Ellickson. 2016. “Estimation of Dynamic Discrete Choice Models in Continuous Time with an Application to Retail Competition.” The Review of Economic Studies 83 (3): 889–931.\n Arcidiacono, Peter, and Robert A Miller. 2011. “Conditional Choice Probability Estimation of Dynamic Discrete Choice Models with Unobserved Heterogeneity.” Econometrica 79 (6): 1823–67.\n Asker, John, Chaim Fershtman, Jihye Jeon, and Ariel Pakes. 2020. “A Computational Framework for Analyzing Dynamic Auctions: The Market Impact of Information Sharing.” The RAND Journal of Economics 51 (3): 805–39.\n Bajari, Patrick, C Lanier Benkard, and Jonathan Levin. 2007. “Estimating Dynamic Models of Imperfect Competition.” Econometrica 75 (5): 1331–70.\n Barwick, Panle Jia, and Parag A Pathak. 2015. “The Costs of Free Entry: An Empirical Study of Real Estate Agents in Greater Boston.” The RAND Journal of Economics 46 (1): 103–45.\n Berry, Steven T, and Giovanni Compiani. 2021. “Empirical Models of Industry Dynamics with Endogenous Market Structure.” Annual Review of Economics 13.\n Besanko, David, Ulrich Doraszelski, Yaroslav Kryukov, and Mark Satterthwaite. 2010. “Learning-by-Doing, Organizational Forgetting, and Industry Dynamics.” Econometrica 78 (2): 453–508.\n Borkovsky, Ron N, Ulrich Doraszelski, and Yaroslav Kryukov. 2010. “A User’s Guide to Solving Dynamic Stochastic Games Using the Homotopy Method.” Operations Research 58 (4-part-2): 1116–32.\n Calvano, Emilio, Giacomo Calzolari, Vincenzo Denicolo, and Sergio Pastorello. 2020. “Artificial Intelligence, Algorithmic Pricing, and Collusion.” American Economic Review 110 (10): 3267–97.\n Caoui, El Hadi. 2019. “Estimating the Costs of Standardization: Evidence from the Movie Industry.” R\u0026amp;R, Review of Economic Studies.\n Collard-Wexler, Allan. 2013. “Demand Fluctuations in the Ready-Mix Concrete Industry.” Econometrica 81 (3): 1003–37.\n Courthoud, Matteo. 2020. “Approximation Methods for Large Dynamic Stochastic Games.” Working Paper.\n Doraszelski, Ulrich. 2003. “An r\u0026amp;d Race with Knowledge Accumulation.” Rand Journal of Economics, 20–42.\n Doraszelski, Ulrich, and Kenneth L Judd. 2012. “Avoiding the Curse of Dimensionality in Dynamic Stochastic Games.” Quantitative Economics 3 (1): 53–93.\n ———. 2019. “Dynamic Stochastic Games with Random Moves.” Quantitative Marketing and Economics 17 (1): 59–79.\n Doraszelski, Ulrich, Gregory Lewis, and Ariel Pakes. 2018. “Just Starting Out: Learning and Equilibrium in a New Market.” American Economic Review 108 (3): 565–615.\n Doraszelski, Ulrich, and Mark Satterthwaite. 2010. “Computable Markov-Perfect Industry Dynamics.” The RAND Journal of Economics 41 (2): 215–43.\n Egesdal, Michael, Zhenyu Lai, and Che-Lin Su. 2015. “Estimating Dynamic Discrete-Choice Games of Incomplete Information.” Quantitative Economics 6 (3): 567–97.\n Eibelshäuser, Steffen, and David Poensgen. 2019. “Markov Quantal Response Equilibrium and a Homotopy Method for Computing and Selecting Markov Perfect Equilibria of Dynamic Stochastic Games.” Working Paper.\n Ericson, Richard, and Ariel Pakes. 1995. “Markov-Perfect Industry Dynamics: A Framework for Empirical Work.” The Review of Economic Studies 62 (1): 53–82.\n Esteban, Susanna, and Matthew Shum. 2007. “Durable-Goods Oligopoly with Secondary Markets: The Case of Automobiles.” The RAND Journal of Economics 38 (2): 332–54.\n Farias, Vivek, Denis Saure, and Gabriel Y Weintraub. 2012. “An Approximate Dynamic Programming Approach to Solving Dynamic Oligopoly Models.” The RAND Journal of Economics 43 (2): 253–82.\n Fershtman, Chaim, and Ariel Pakes. 2012. “Dynamic Games with Asymmetric Information: A Framework for Empirical Work.” The Quarterly Journal of Economics 127 (4): 1611–61.\n Goettler, Ronald L, and Brett R Gordon. 2011. “Does AMD Spur Intel to Innovate More?” Journal of Political Economy 119 (6): 1141–1200.\n Hotz, V Joseph, and Robert A Miller. 1993. “Conditional Choice Probabilities and the Estimation of Dynamic Models.” The Review of Economic Studies 60 (3): 497–529.\n Huang, Ling, and Martin D Smith. 2014. “The Dynamic Efficiency Costs of Common-Pool Resource Exploitation.” American Economic Review 104 (12): 4071–4103.\n Ifrach, Bar, and Gabriel Y Weintraub. 2017. “A Framework for Dynamic Oligopoly in Concentrated Industries.” The Review of Economic Studies 84 (3): 1106–50.\n Igami, Mitsuru. 2017. “Estimating the Innovator’s Dilemma: Structural Analysis of Creative Destruction in the Hard Disk Drive Industry, 1981–1998.” Journal of Political Economy 125 (3): 798–847.\n Iskhakov, Fedor, John Rust, and Bertel Schjerning. 2016. “Recursive Lexicographical Search: Finding All Markov Perfect Equilibria of Finite State Directional Dynamic Games.” The Review of Economic Studies 83 (2): 658–703.\n Jeon, Jihye. 2020. “Learning and Investment Under Demand Uncertainty in Container Shipping.” The RAND Journal of Economics.\n Kasahara, Hiroyuki, and Katsumi Shimotsu. 2009. “Nonparametric Identification of Finite Mixture Models of Dynamic Discrete Choices.” Econometrica 77 (1): 135–75.\n Maskin, Eric, and Jean Tirole. 1988. “A Theory of Dynamic Oligopoly, II: Price Competition, Kinked Demand Curves, and Edgeworth Cycles.” Econometrica: Journal of the Econometric Society, 571–99.\n Pakes, Ariel, and Paul McGuire. 1994. “Computing Markov-Perfect Nash Equilibria: Numerical Implications of a Dynamic Differentiated Product Model.” RAND Journal of Economics 25 (4): 555–89.\n ———. 2001. “Stochastic Algorithms, Symmetric Markov Perfect Equilibrium, and the ‘Curse’of Dimensionality.” Econometrica 69 (5): 1261–81.\n Pakes, Ariel, Michael Ostrovsky, and Steven Berry. 2007. “Simple Estimators for the Parameters of Discrete Dynamic Games (with Entry/Exit Examples).” The RAND Journal of Economics 38 (2): 373–99.\n Pesendorfer, Martin, and Philipp Schmidt-Dengler. 2008. “Asymptotic Least Squares Estimators for Dynamic Games.” The Review of Economic Studies 75 (3): 901–28.\n ———. 2010. “Sequential Estimation of Dynamic Discrete Games: A Comment.” Econometrica 78 (2): 833–42.\n Rust, John. 1994. “Structural Estimation of Markov Decision Processes.” Handbook of Econometrics 4: 3081–3143.\n Ryan, Stephen P. 2012. “The Costs of Environmental Regulation in a Concentrated Industry.” Econometrica 80 (3): 1019–61.\n Su, Che-Lin, and Kenneth L Judd. 2012. “Constrained Optimization Approaches to Estimation of Structural Models.” Econometrica 80 (5): 2213–30.\n Sweeting, Andrew. 2013. “Dynamic Product Positioning in Differentiated Product Markets: The Effect of Fees for Musical Performance Rights on the Commercial Radio Industry.” Econometrica 81 (5): 1763–803.\n Vreugdenhil, Nicholas. 2020. “Booms, Busts, and Mismatch in Capital Markets: Evidence from the Offshore Oil and Gas Industry.” R\u0026amp;R at Journal of Political Economy.\n Weintraub, Gabriel Y, C Lanier Benkard, and Benjamin Van Roy. 2008. “Markov Perfect Industry Dynamics with Many Firms.” Econometrica 76 (6): 1375–1411.\n Xu, Daniel Yi, and Yanyou Chen. 2020. “A Structural Empirical Model of r\u0026amp;d, Firm Heterogeneity, and Industry Evolution.” Journal of Industrial Economics.\n  ","date":1635465600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1635465600,"objectID":"503282b0c5ba446d2962ab39615c5eae","permalink":"https://matteocourthoud.github.io/course/empirical-io/08_dynamics_games/","publishdate":"2021-10-29T00:00:00Z","relpermalink":"/course/empirical-io/08_dynamics_games/","section":"course","summary":"Introduction Intro Setting: agents making strategic decisions (new) in dynamic environments.\n Entry and exit: Collard-Wexler (2013) Sunk costs: Ryan (2012) Innovation: Goettler and Gordon (2011)  (or whatever changes in response to investment)   Exploitation of natural resources: Huang and Smith (2014) Durable goods: Esteban and Shum (2007)   Lit review: forthcoming IO Handbook chapter Aguirregabiria, Collard-Wexler, and Ryan (2021)","tags":null,"title":"Dynamic Games","type":"book"},{"authors":null,"categories":null,"content":"Introduction Non-parametric regression is a flexible estimation procedure for\n regression functions $\\mathbb E [y|x ] = g (x)$ and density functions $f(x)$.  You want to let your data to tell you how flexible you can afford to be in terms of estimation procedures. Non-parametric regression is naturally introduced in terms of fitting a curve.\nConsider the problem of estimating the Conditional Expectation Function, defined as $\\mathbb E [y_i |x_i ] = g(x_i)$ given data $D = (x_i, y_i)_{i=1}^n$ under minimal assumption of $g(\\cdot)$, e.g. smoothness. There are two main methods:\n Local methods: Kernel-based estimation Global methods: Series-based estimation  Another way of looking at non-parametrics is to do estimation/inference without specifying functional forms. With no assumptions, informative inference is impossible. Non parametrics tries to work with functional restrictions—continuity, differentiability, etc.—rather than pre-specifying functional form.\nDiscrete x - Cell Estimator Suppose that $x$ can take $R$ distinct values, e.g. gender $R=2$, years of schooling $R=20$, gender $\\times$ years of schooling $R = 2 \\times 20$.\nA simple way for estimating $\\mathbb E \\left[ y |x \\right] = g(x)$ is to split the sample to include observations with $x_i = x$ and calculate the sample mean of $\\bar{y}$ for these observations. Note that this requires no assumptions about how $\\mathbb E [y_i |x_i]$ varies with $x$ since we fit a different value for each value $x$. $$ \\hat{g}(x) = \\frac{1}{| i: x_i = x |} \\sum_{i : x_i = x} y_i $$\nIssues:\n Curse of dimensionality: if $R$ is big compared to $n$, there will be only a small number of observations per $x$ values. If $x_i$ is continuous, $R=n$ with probability 1. Solution: we can borrow information about $g_0(x)$ using neighboring observations of $x$. Averaging for each separate $x_r$ value is only feasible in cases where $x_i$ is coarsely discrete.  Local Non-Parametric Estimation Kernels Suppose we believe that $\\mathbb E [y_i |x_i]$ is a smooth function of $x_i$ – e.g. continuous, differentiable, etc. Then it should not change too much across values of $x$ that are close to each other: we can estimate the conditional expectation at $x = \\bar{x}$ by averaging $y$’s over the values of $x$ that are “close”” to $\\bar{x}$. This procedure relies on two (three) arbitrary choices:\n Choice of the kernel function $K (\\cdot)$; it is used to weight “far out”” observations, such that  $K: \\mathbb R \\to \\mathbb R$ $K$ is symmetric: $K(\\bar{x} + x_i) = K(\\bar{x} - x_i)$ $\\lim_{x_i \\to \\infty}K(x_i - \\bar{x}) = 0$   Choice of the bandwidth $h$: it measures the size of a ``small’’ window around $\\bar{x}$, e.g. $(\\bar{x} - h, \\bar{x} + h)$. Choice of the local estimation procedure. Examples are locally constant, a.k.a. Nadaraya-Watson (NW), and locally linear (LL).   Generally, the choice of $h$ is more important than $K(\\cdot)$ in low dimensional settings.\n Optimal h We need to define what is an “optimal” $h$, depending on the smoothness level of $g_0$, typically unknown. The choice of $h$ relates to the bias-variance trade-off:\n large $h$: small variance, higher bias; small $h$: high variance, smaller bias.   Note that $K_h (\\cdot) = K (\\cdot / h)$.\n Locally Constant Estimator  Nadaraya-Watson estimator, or locally constant estimator. It assumes the CEF locally takes the form $g(x) = \\beta_0(x)$. The local parameter is estimated as: $$ \\hat{\\beta}0 (\\bar{x}) = \\arg\\min{\\beta_0} \\quad \\mathbb E_n \\Big[ K_h (x_i - \\bar{x}) \\cdot \\big(y_i - \\beta_0 \\big)^2 \\Big] $$  CEF The Nadaraya-Watson estimate of the CEF takes the form: $$ \\mathbb E_n \\left[ y | x = \\bar{x}\\right] = \\hat{g}(\\bar{x}) = \\frac{\\sum_{i=1}^n y_i K_h (x_i - \\bar{x})}{\\sum_{i=1}^n K_h (x_i - \\bar{x})} $$\nLocally Linear Estimator  Local Linear estimator. It assumes the CEF locally takes the form $g(x) = \\beta_0(x) + \\beta_1(x) x$. The local parameters are estimated as:  $$ \\left( \\hat{\\beta}_0 (\\bar{x}), \\hat{\\beta}1 (\\bar{x}) \\right) = \\arg\\min{\\beta_0, \\beta_1} \\quad \\mathbb E_n \\Big[ K_h (x_i - \\bar{x}) \\cdot \\big(y_i - \\beta_0 - (x_i - \\bar{x}) \\beta_1 \\big)^2 \\Big] $$\nCEF In this case, we do LS estimate with $i$’s contribution of residual weighted by the kernel $K_h (x_i - \\bar{x})$. The final estimate at $\\bar{x}$ is given by: $$ \\hat{g} (\\bar{x}) = \\hat{\\beta}_0 (\\bar{x}) + (\\bar{x} - \\bar{x}) \\hat{\\beta}_1 (\\bar{x}) = \\hat{\\beta}_0 (\\bar{x}) $$ since we have centered the $x_s$ at $\\bar{x}$ in the kernel. - It is possible to add linearly higher order polynomials, e.g. do locally quadratic least squares using loss function:\n$$ \\mathbb E_n \\left[ K_h (x_i - \\bar{x}) \\big(y_i - \\beta_0 - (x_i - \\bar{x}) \\beta_1 - (x_i - \\bar{x})^2 \\beta_2 \\big)^2 \\right] $$\nUniform Kernel LS restricted to sample $i$ such that $x_i$ within $h$ of $\\bar{x}$. $$ \\begin{aligned} \u0026amp; K (\\cdot) = \\mathbb I\\lbrace \\cdot \\in [-1, 1] \\rbrace \\newline \u0026amp; K_h (\\cdot) = \\mathbb I\\lbrace \\cdot/h \\in [-1, 1] \\rbrace = \\mathbb I\\lbrace \\cdot \\in [-h, h] \\rbrace \\newline \u0026amp; K_h (x_i - \\bar{x}) = \\mathbb I\\lbrace x_i - \\bar{x} \\in [-h, h] \\rbrace = \\mathbb I\\lbrace x_i \\in [\\bar{x}-h, \\bar{x} + h] \\rbrace \\end{aligned} $$ Employed together with the locally linear estimator, the estimation procedure reduces to **local least squares}. The loss function is: $$ \\mathbb E_n \\Big[ K_n (x_i - \\bar{x}) \\big(y_i -\\beta_0 - \\beta_1 (x_i - \\bar{x}) \\big)^2 \\Big] = \\frac{1}{n} \\sum_{i: x_i \\in [\\bar{x}-h, \\bar{x} +h ]} \\big(y_i -\\beta_0 - \\beta_1 (x_i - \\bar{x}) \\big)^2 $$\nThe more local is the estimation, the more appropriate the linear regression: if $g_0$ is smooth, $g_0(\\bar{x}) + g_0'(\\bar{x}) (x_i - \\bar{x})$ is a better approximation for $g_0 (x_i)$.\nHowever, the uniform density is not a good kernel choice as it produces discontinuous CEF estimates. The following are two popular alternative choices that produce continuous CEF estimates.\nOther Kernels   Epanechnikov kernel $$ K_h(x_i - \\bar{x}) = \\frac { 3 } { 4 } \\left( 1 - (x_i - \\bar{x}) ^ { 2 } \\right) \\mathbb I\\lbrace x_i \\in [\\bar{x}-h, \\bar{x} + h] \\rbrace $$\n  Normal or Gaussian kernel $$ K_\\phi (x_i - \\bar{x}) = \\frac { 1 } { \\sqrt { 2 \\pi } } \\exp \\left( - \\frac { (x_i - \\bar{x}) ^ { 2 } } { 2 } \\right) $$\n  K-Nearest Neighbors (KNN): choose bandwidth so that there is a fixed number of observations in each kernel. This kernel is different from the others since it takes a nonparamentric form.\n  Example Choice of the optimal bandwidth Practical methods:\n  Eyeball Method. (i) Choose a bandwidth (ii) Estimate the regression function (iii) Look at the result: if it looks more wiggly than you would like, increase the bandwidth: if it looks more smooth than you would like, decrease the bandwidth. Con: It only works for $\\dim(x_i) = 1$ or $2$.\n  Rule of Thumb. For example, Silverman’s rule of thumb: $h = \\left( \\frac{4 \\hat{\\sigma}^5}{3n} \\right)^{\\frac{1}{5}}$. Con: It requires too much knowledge about $g_0$ (i.e. normality) which you don’t have.\n  Cross Validation. Under some assumptions, CV will approximately gives the MSE optimal bandwidth. The basic idea is to evaluate quality of the bandwidth by looking at how well the resulting estimator forecasts in the given sample.\n  Leave-one-out CV. For each $h \u0026gt; 0$ and each $i$, $\\hat{g}{-i} (x_i)$ is the estimate of the conditional expectation at $x_i$ using bandwidth $h$ and all observations expect observation $i$. The CV bandwidth is defined as $$ \\hat{h} = \\arg \\min_h CV(h) = \\arg \\min_h \\sum{i=1}^n \\Big( y_i - \\hat{g}_{-i} (x_i) \\Big)^2 $$\nPractical Tips  Select a value for $h$. For each observation $i$, calculate $$ \\hat{g}{-i} (x_i) = \\frac{\\sum{j \\ne i} y_j K_h (x_j - x_i) }{\\sum_{i=1}^n K_h (x_j - x_i)}, \\qquad e_{i,h}^2 = \\left(y_i - \\hat{g}_{-i} (x_i) \\right)^2 $$ Calculate $\\text{CV}(h) = \\sum_{i=1}^n e^2_{i,h}$. Repeat for each $h$ and choose the one that minimizes $\\text{CV}(h)$.  Inference Theorem: Consider data $\\lbrace y_i, x_i \\rbrace_{i=1}^n$, iid and suppose that $y_i = g(x_i) + \\varepsilon_i$ where $\\mathbb E[\\varepsilon_i|x_i] = 0$. Assume that $x_i \\in Interior(X)$ where $X \\subseteq \\mathbb R$, $g(x)$ and $f(x)$ are three times continuously differentiable, and $f(x) \u0026gt; 0$ on $X$. $f(x)$ is the probability density of $x \\in X$ , and $g(x)$ is the function of interest. Suppose that $K(\\cdot)$ is a kernel function. Suppose $n\\to\\infty$, $h\\to0$ , $nh\\to\\infty$, and $nh^7\\to0$. Then for any fixed $x\\in X$, $$ AMSE = \\sqrt{nh} \\Big( \\hat{g}(x) - g(x) - h^2 B(x)\\Big) \\overset{d}{\\to} N \\left( 0, \\frac{\\kappa \\sigma^2(x)}{f(x)}\\right) $$ for $\\sigma^2(x) = Var(y_i|x_i = x)$, $\\kappa = \\int K^2(v)dv$, and $B(x) = \\frac{\\kappa_2}{2} \\frac{f'(x)g'(x) + f(x) g''(x)}{f(x)}$ where $\\kappa_2 = \\int v^2 K(v)dv$.\nRemarks  If the function is smooth enough and the bandwidth small enough, you can ignore the bias relative to sampling variation. To make this plausible, use a smaller bandwidth than would be the “optimal”. All kernel regression estimators can be written as a weighted average $$ \\hat{g}(x) = \\frac{1}{n} \\sum_{i=1}^n w_i (x) y_i, \\quad \\text{ with } \\quad w_i (x) = \\frac{n K_h (x_i - x)}{\\sum_{i=1}^n K_h (x_i - x)} $$ Do inference as if you were estimating a mean $\\mathbb E[z_i]$ with sample mean $\\frac{1}{n} \\sum_{i=1}^n z_i$ using $z_i = w_i (x) y_i$. If you are doing inference at more than one value of $x$, do inference as in the previous point, treating each value of $x$ as a different sample mean and note that even with independent data, these means will be correlated in general because there will generally be some common observations in to each of the averages. If you have a time series, make sure you account for correlation between the observations going in the different averages even if they don’t overlap.  Issue when doing inference: the estimation of the bandwidth from the data is generally not accounted for in the distributional approximation (when doing inference). In large-samples, this is unlikely to lead to large changes, but uncertainty is understated in small samples.\nBias-Variance Trade-off Theorem\nFor any estimator mean-square error MSE is decomposable into variance and bias-squared: $$ \\text{MSE} (\\bar{x}, \\hat{g}) = \\mathbb E \\left[ \\left( \\hat{g}(\\bar{x}) - g_0 (\\bar{x}) \\right)^2 \\right] = \\mathbb E \\Big[\\underbrace{ \\hat{g}(\\bar{x}) - g_0 (\\bar{x}) }_{\\text{Bias}} \\Big]^2 + Var (\\hat{g} (\\bar{x})). $$\nProof The theorem follows from the following corollary.\nCorollary\nLet $A$ be a random variable and $\\theta_0$ a fixed parameter. Then, $$ \\mathbb E [ (A - \\theta_0)^2] = Var (A) + \\mathbb E [A-\\theta_0]^2 $$\nProof $$ \\begin{aligned} \\mathbb E [ (A - \\theta_0)^2] \u0026amp; = \\mathbb E[A^2] - 2 \\mathbb E [A \\theta_0] + \\mathbb E [\\theta_0] \\newline \u0026amp; = \\mathbb E[A^2] \\underbrace{- \\mathbb E[A]^2 + E[A]^2}_{\\text{add and subtract}} - 2 \\mathbb E [A \\theta_0] + \\mathbb E [\\theta_0] \\newline \u0026amp; = Var(A) + \\mathbb E [A]^2 - 2 \\theta_0 \\mathbb E [A ] + \\mathbb E [\\theta_0] \\newline \u0026amp; = Var(A) + \\mathbb E [A - \\theta_0]^2 \\end{aligned} $$\nNote that $\\mathbb E [ (A - \\theta_0)^2] = \\mathbb E [A - \\theta_0]^2$. $$\\tag*{$\\blacksquare$}$$\nCriteria Which criteria should we use with non-parametric estimators?\n  Mean squared error (MSE): $$ \\text{MSE} (\\bar{x}) (\\hat{g}) = \\mathbb E \\left[ \\left( \\hat{g}(\\bar{x}) - g_0 (\\bar{x}) \\right)^2 \\right] $$ NB! This is the criterium we are going to use.\n  Integrated mean squared error (IMSE): $$ \\text{IMSE} ( \\hat{g} ) = \\mathbb E \\left[ \\int | \\hat{g} (x) - g_0 (x) |^2 \\mathrm{d} F(x) \\right] $$\n  Type I - Type II error.\n  Comments Hansen (2019): the theorem above implies that we can asymptotically approximate the MSE as $$ \\text{AMSE} = \\Big( h^2 \\sigma_k^2 B(x) \\Big)^2 + \\frac{\\kappa \\sigma^2(x)}{nh f(x)} \\approx \\text{const} \\cdot \\left( h^4 + \\frac{1}{n h} \\right) $$\nWhere\n $Var \\propto \\frac{1}{h n}$, where you can think of $n h$ as the effective sample size. Bias $\\propto h^2$, derived if $g_0$ is twice continuously differentiable using Taylor expansion.  Trade-Off The asymptotic MSE is dominated by the larger of $h^4$ and $\\frac{1}{h n}$. Notice that the bias is increasing in $h$ and the variance is decreasing in $h$ (more smoothing means more observations are used for local estimation: this increases the bias but decreases estimation variance). To select $h$ to minimize the asymptotic MSE, these two components should balance each other: $$ \\frac{1}{h n} \\propto h^4 \\quad \\Rightarrow \\quad h \\propto n^{-1/5} $$\nThis result means that the bandwidth should take the form $h = c \\cdot n^{-1/5}$. The optimal constant $c$ depends on the kernel $k$ the bias function $B(x)$ and the marginal density $f_x(x)$. A common misinterpretation is to set $h = n^{-1/5}$ which is equivalent to setting $c = 1$ and is completely arbitrary. Instead, an empirical bandwidth selection rule such as cross-validation should be used in practice.\nGlobal Non-Parametric Estimation Series The goal is to try to globally approximate the CEF with a function $g(x)$. Series methods are based on the Stone-Weierstrass theorem: a real-valued continuous function $g(x)$ defined in a compact set can be approximated with polynomials for any degree of accuracy $$ g_0 (x) = p_1 (x) \\beta _1 + \\dots + p_K (x) \\beta_K + r(x) $$ where $p_1(x), \\dots, p_K(x)$ are called ``a dictionary of approximating series’’ and $r(x)$ is a remainder function. If $p_1(x), \\dots, p_K(x)$ are sufficiently rich, $r(x)$ will be small. If $K \\to \\infty$, then $r \\to 0$.\n Example - Taylor series: if $g(x)$ is infinitely differentiable, then $$ g(x) = \\sum_{k=0}^{\\infty } a_k x^k $$ where $a_k = \\frac{1}{k!} \\frac{\\partial^k g_0}{\\partial x^k}$.\n In Practice The basic idea is to approximate the infinite sum by chopping it off after $K$ terms and then estimate the coefficients by OLS.\nSeries estimation:\n Choose $K$, i.e. the number of series terms, and an approximating dictionary $p_1(x), \\dots, p_K(x)$ Expand data to $D = \\left( y_i, p_1(x_i), \\dots, p_K(x_i) \\right)_{i=1}^n$ Estimate OLS to get $\\hat{\\beta}_1, \\dots, \\hat{\\beta}_K$ Set $\\hat{g}(x) = p_1 (x)\\hat{\\beta}_1 + \\dots + p_K(x) \\hat{\\beta}_K$  Examples   Monomials: $p_1(x) = 1, p_2(x) = x, p_3(x)=x^2, \\dots$\n  Hermite Polynomials: $p_1(x) = 1$, $p_2(x) = x$, $p_3(x)=x^2 -1$, $p_4(x)= x^3 - 3x, \\dots$. Con: edge effects. The estimated function is particularly volatile at the edges of the sample space (Gibbs effect)\n  Trig Polynomials: $p_1(x) = 1$, $p_2(x) = \\cos 2 \\pi x$, $p_3(x)= \\sin 2 \\pi x$, $p_4(x) = \\cos 2 \\pi x \\cdot 2 x \\dots$. Pro: cyclical therefore good for series. Con: edge effects\n  B-splines: recursively constructed using knot points $$ B_{i, 0} = \\begin{cases} 1 \u0026amp; \\text{if } t_i \\leq x \u0026lt; t_{i+1} \\newline 0 \u0026amp; \\text{otherwise} \\end{cases} \\qquad B_{i_k} (x) = \\frac{x - t_i}{ t_{i+k} - t_i} B_{i, k-1} (x) + \\frac{t_{i+k+1}-x}{t_{i+k+1} - t_{i+1}} B_{i+1, k-1} (x) $$ where $t_0, \\dots, t_i, \\dots$ are knot points and $k$ is the order of the spline. Pro: faster rate of convergence and lower asymptotic bias.\n  Hermite Polynomials Estimation Given $K$, inference proceeds exactly as if one had run an OLS of $y$ on $(p_k)_{k=1}^K$. The idea is that you ignore that you are doing non-parametric regression as long as you believe you have put enough terms (high $K$). Then the function is smooth enough so that the bias of the approximation is small relative to the variance (see Newey, 1997). Note that his approximation does not account for data-dependent estimation of the bandwidth.\nConsistency Newey (1997): results about consistency of $\\hat{g}$ and asymptotic normality of $\\hat{g}$.\n OLS: $\\hat{\\beta} \\overset{p}{\\to} \\beta_0$ Non-parametric: you have a sequence $\\lbrace\\beta_k\\rbrace_{k=1}^K$ with $\\hat{\\beta}_k \\overset{p}{\\to} \\beta_k$ as $n \\to \\infty$ (as $k \\to \\infty$). However, this does not make sense because $\\lbrace\\beta_k\\rbrace$ is not constant. Moreover, $\\beta_k$ is not the quantity of interest. We want to make inference on $\\hat{g}(x)$.  Theorem\nUnder regularity conditions, including $| | \\hat{\\beta} - \\beta_0 | | \\overset{p}{\\to} 0$,\n Uniform Consistency: $\\sup_x | \\hat{g}(x) - g_0(x)| \\overset{p}{\\to} 0$ Mean-square Consistency: $\\int | \\hat{g}(x) - g_0(x)|^2 \\mathrm{d} F(x) \\overset{p}{\\to} 0$  IMSE Theorem\nUnder the following assumptions:\n $(x_i, y_i)$ are iid and $Var(y_i|x_i)$ is bounded; For all $K$, there exists a non-singular matrix $B$ such that $A = \\left[ (B p(x)) (B p(x))' \\right]$ where $p(x) = \\left( p_1(x), \\dots, p_K (x) \\right)$ has the properties that $\\lambda_{\\min} (A)^{-1} = O(1)$. In addition, $\\sup_x | | B p(x) | | = o(\\sqrt{K/n})$. There exists $\\alpha$ and $\\beta_K$ for all $K$ such that $$ \\sup_x | g_0 (x) - p(x) \\beta_K | = O_p(K^{-\\alpha}) $$  Then, it holds that\n$$ \\text{IMSE = }\\int \\left( g_0 (x) - \\hat{g} (x) \\right)^2 \\mathrm{d} F(x) = O_p \\left( \\frac{K}{n} + K^{-2\\alpha}\\right) $$\nChoice of the optimal $K$ The bias-variance trade-off for series comes in through the choice of $K$:\n Higher $K$: smaller bias, since we are leaving out less terms form the infinite sum. Smaller $K$: smaller variance, since we are estimating less regression coefficients from the same amount of data.  Cross-validation for series: For each $K \\geq 0$ and for each $i=1, \\dots, n$, consider\n$$ D_{-i} = \\lbrace (x_1, y_1), \\dots, (x_{i-1}, y_{i-1}),(x_{i+1}, y_{i+1}), \\dots (x_n, y_n) \\rbrace $$ and calculate $\\hat{g}^{(K)}_{-i} (x)$ using series estimate with $p_1(x), \\dots, p_K (x)$ in order to get $e^{(K)}i = y_i - \\hat{g}^{(K)}{-i} (x_i)$. Choose $\\hat{K}$ such that\n$$ \\hat{K} = \\arg \\min_K \\mathbb E_n \\left[ {e^{(K)}_i}^2 \\right] $$\nInference Consider the data $D = \\lbrace (x_i, y_i) \\rbrace_{i=1}^n$ such that $y_i = g_0 (x_i) + \\varepsilon_i$. You may want to form confidence intervals for quantities that depends on $g_0$.\n Example: $\\theta_0$ functional forms of interests:\n Point estimate: $\\theta_0 = g_0 (\\bar{x} )$ for fixed $\\bar{x}$ Interval estimate: $\\theta_0 = g_0 (\\bar{x}_2) - g_0 (\\bar{x}_1)$ Point derivative estimate: $\\theta_0 = g_0 ' (\\bar{x})$ at $\\bar{x}$ Average derivative $\\theta_0 = \\mathbb E [g_0 ' (x) ]$ Consumer surplus: $\\theta_0 = \\int_a^b g_0(x)dx \\quad$ when $g_0$ is a demand function.   Those estimates are functionals: maps from a function to a real number. We are doing inference on a function now, not on a point estimate.\nInference In order to form a confidence interval for $\\theta_0$, with series you can\n Undersmooth: in order to apply a \\textit{central limit theorem}{=tex}, you need deviations around the function to be approximately gaussian. Undersmoothing makes the function oscillate much more than the curve you are estimating in order to obtain such guassian deviations. Use the delta method. It would usually require more series terms than a criterion like cross-validation would suggest.  Undersmoothing If on the contrary you oversmooth (e.g. $g_0$ linear), errors are going to constantly be on either one or the other side of the curve $\\to$ not gaussian!\nDelta Method Theorem: Under the assumptions of the consistency theorem $$ \\frac{\\sqrt{n} \\Big(\\hat{\\theta} - \\theta_0 + B(r_K) \\Big)}{\\sqrt{v_K}} \\overset{d}{\\to} N (0,1) $$\nTheorem: Under the assumptions of the consistency theorem and $\\sqrt{n} K^{-\\alpha} = o(1)$ (or equivalently $n K^{-2\\alpha} = O(1)$ in Hansen), $$ \\frac{\\sqrt{n} \\Big(\\hat{\\theta} - \\theta_0 \\Big)}{\\sqrt{v_K}} \\overset{d}{\\to} N (0,1) $$\nRemark  The rate of convergence of splines is faster than for power series (Newey 1997). We have undersmoothing if $\\sqrt{n} K^{\\alpha} = o(1)$ (see comment below) Usually, in order to prove asymptotic normality, we first prove unbiasedness. However here we have a biased estimator but we make the bias converge to zero faster than the variance.  Hansen (2019): The critical condition is the assumption that $\\sqrt{n} K^{\\alpha} = o(1)$ This requires that $K \\to \\infty$ at a rate faster than $n^{\\frac{1}{2\\alpha}}$ This is a troubling condition. The optimal rate for estimation of $g(x)$ is $K = O(n^{\\frac{1}{1+ 2\\alpha}})$. If we set $K = n^{\\frac{1}{1+ 2\\alpha}}$ by this rule then $n K^{-2\\alpha} = n^{\\frac{1}{1+ 2\\alpha}} \\to \\infty$ not zero. Thus this assumption is equivalent to assuming that $K$ is much larger than optimal. The reason why this trick works (that is, why the bias is negligible) is that by increasing $K$ the asymptotic bias decreases and the asymptotic variance increases and thus the variance dominates. Because $K$ is larger than optimal, we typically say that $\\hat{g}(x)$ is undersmoothed relative to the optimal series estimator.\nMore Remarks  Many authors like to focus their asymptotic theory on the assumptions in the theorem, as the distribution of $\\theta$ appears cleaner. However, it is a poor use of asymptotic theory. There are three problems with the assumption $\\sqrt{n} K^{-\\alpha} = o(1)$ and the approximation of the theorem.\n First, it says that if we intentionally pick $K$ to be larger than optimal, we can increase the estimation variance relative to the bias so the variance will dominate the bias. But why would we want to intentionally use an estimator which is sub-optimal? Second, the assumption $\\sqrt{n} K^{-\\alpha} = o(1)$ does not eliminate the asymptotic bias, it only makes it of lower order than the variance. So the approximation of the theorem is technically valid, but the missing asymptotic bias term is just slightly smaller in asymptotic order, and thus still relevant in finite samples. Third, the condition $\\sqrt{n} K^{\\alpha} = o(1)$ is just an assumption, it has nothing to do with actual empirical practice. Thus the difference between the two theorems is in the assumptions, not in the actual reality or in the actual empirical practice. Eliminating a nuisance (the asymptotic bias) through an assumption is a trick, not a substantive use of theory. My strong view is that the result (1) is more informative than (2). It shows that the asymptotic distribution is normal but has a non-trivial finite sample bias.   Kernel vs Series Hansen (2019): in this and the previous chapter we have presented two distinct methods of nonparametric regression based on kernel methods and series methods. Which should be used in practice? Both methods have advantages and disadvantages and there is no clear overall winner.\nFirst, while the asymptotic theory of the two estimators appear quite different, they are actually rather closely related. When the regression function $g(x)$ is twice differentiable $(s = 2)$ then the rate of convergence of both the MSE of the kernel regression estimator with optimal bandwidth $h$ and the series estimator with optimal $K$ is $n^{-\\frac{2}{k+4}}$ (where $k = \\dim(x)$). There is no difference. If the regression function is smoother than twice differentiable ($s \u0026gt; 2$) then the rate of the convergence of the series estimator improves. This may appear to be an advantage for series methods, but kernel regression can also take advantage of the higher smoothness by using so-called higher-order kernels or local polynomial regression, so perhaps this advantage is not too large.\nBoth estimators are asymptotically normal and have straightforward asymptotic standard error formulae. The series estimators are a bit more convenient for this purpose, as classic parametric standard error formula work without amendment.\nAdvantages of Kernels An advantage of kernel methods is that their distributional theory is easier to derive. The theory is all based on local averages which is relatively straightforward. In contrast, series theory is more challenging, dealing with increasing parameter spaces. An important difference in the theory is that for kernel estimators we have explicit representations for the bias while we only have rates for series methods. This means that plug-in methods can be used for bandwidth selection in kernel regression. However, typically we rely on cross-validation, which is equally applicable in both kernel and series regression.\nKernel methods are also relatively easy to implement when the dimension of $x$, $k$, is large. There is not a major change in the methodology as $k$ increases. In contrast, series methods become quite cumbersome as $k$ increases as the number of cross-terms increases exponentially. E.g ($K=2$) with $k=1$ you have only $\\lbrace x_1, x_1^2\\rbrace$; with $k=2$ you have to add $\\lbrace x_2, x_2^2, x_1 x_2 \\rbrace$; with $k=3$ you have to add $\\lbrace x_3, x_3^2, x_1 x_3, x_2 x_3\\rbrace$, etc..\nAdvantages of Series A major advantage of series methods is that it has inherently a high degree of flexibility, and the user is able to implement shape restrictions quite easily. For example, in series estimation it is relatively simple to implement a partial linear CEF, an additively separable CEF, monotonicity, concavity or convexity. These restrictions are harder to implement in kernel regression.\n","date":1635465600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1635465600,"objectID":"486b7a0f83ad926dab18785e2fbd49e6","permalink":"https://matteocourthoud.github.io/course/metrics/08_nonparametric/","publishdate":"2021-10-29T00:00:00Z","relpermalink":"/course/metrics/08_nonparametric/","section":"course","summary":"Introduction Non-parametric regression is a flexible estimation procedure for\n regression functions $\\mathbb E [y|x ] = g (x)$ and density functions $f(x)$.  You want to let your data to tell you how flexible you can afford to be in terms of estimation procedures.","tags":null,"title":"Non-Parametric Estimation","type":"book"},{"authors":null,"categories":null,"content":"# Remove warnings import warnings warnings.filterwarnings('ignore')  # Import everything import pandas as pd import numpy as np import seaborn as sns import statsmodels.api as sm from numpy.linalg import inv from statsmodels.iolib.summary2 import summary_col from sklearn.model_selection import train_test_split from sklearn.linear_model import LinearRegression, Lasso, Ridge from sklearn.tree import DecisionTreeRegressor from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor from sklearn.preprocessing import PolynomialFeatures, StandardScaler  # Import matplotlib for graphs import matplotlib.pyplot as plt from mpl_toolkits.mplot3d import axes3d # Set global parameters %matplotlib inline plt.style.use('seaborn-white') plt.rcParams['lines.linewidth'] = 3 plt.rcParams['figure.figsize'] = (10,6) plt.rcParams['figure.titlesize'] = 20 plt.rcParams['axes.titlesize'] = 18 plt.rcParams['axes.labelsize'] = 14 plt.rcParams['legend.fontsize'] = 14  9.1 Frisch-Waugh theorem Consider the data $D = { x_i, y_i, z_i }_{i=1}^n$ with DGP:\n$$ y_i = x_i' \\alpha_0+ z_i' \\beta_0 + \\varepsilon_i $$\n. The following estimators of $\\alpha$ are numerically equivalent (if $[x, z]$ has full rank):\n OLS: $\\hat{\\alpha}$ from regressing $y$ on $x, z$ Partialling out: $\\tilde{\\alpha}$ from regressing $y$ on $\\tilde{x}$ \u0026ldquo;Double\u0026rdquo; partialling out: $\\bar{\\alpha}$ from regressing $\\tilde{y}$ on $\\tilde{x}$  where the operation of passing to $y, x$ to $\\tilde{y}, \\tilde{x}$ is called projection out $z$, e.g. $\\tilde{x}$ are the residuals from regressing $x$ on $z$.\n$$ \\tilde{x} = x - \\hat \\gamma z = (I - z (z' z)^{-1} z' ) x = (I-P_z) x = M_z x $$\nI.e we have done the following:\n regress $x$ on $z$ compute $\\hat x$ compute the residuals $\\tilde x = x - \\hat x$  We now explore the theorem through simulation. In particular, we generate a sample from the following model:\n$$ y_i = x_i - 0.3 z_i + \\varepsilon_i $$\nwhere $x_i,z_i,\\varepsilon_i \\sim N(0,1)$ and $n=1000$.\nnp.random.seed(1) # Init n = 1000 a = 1 b = -.3 # Generate data x = np.random.uniform(0,1,n).reshape(-1,1) z = np.random.uniform(0,1,n).reshape(-1,1) e = np.random.normal(0,1,n).reshape(-1,1) y = a*x + b*z + e  Let\u0026rsquo;s compute the value of the OLS estimator.\n# Estimate alpha by OLS xz = np.concatenate([x,z], axis=1) ols_coeff = inv(xz.T @ xz) @ xz.T @ y alpha_ols = ols_coeff[0][0] print('alpha OLS: %.4f (true=%1.0f)' % (alpha_ols, a))  alpha OLS: 1.0928 (true=1)  The partialling out estimator.\n# Partialling out x_tilde = (np.eye(n) - z @ inv(z.T @ z) @ z.T ) @ x alpha_po = inv(x_tilde.T @ x_tilde) @ x_tilde.T @ y print('alpha partialling out: %.4f (true=%1.0f)' % (alpha_po, a))  alpha partialling out: 1.0928 (true=1)  And lastly, the double-partialling out estimator.\n# \u0026quot;Double\u0026quot; partialling out y_tilde = (np.eye(n) - z @ inv(z.T @ z) @ z.T ) @ y alpha_po2 = inv(x_tilde.T @ x_tilde) @ x_tilde.T @ y_tilde print('alpha double partialling out: %.4f (true=%1.0f)' % (alpha_po2, a))  alpha double partialling out: 1.0928 (true=1)  9.2 Omitted Variable Bias Consider two separate statistical models. Assume the following long regression of interest:\n$$ y_i = x_i' \\alpha_0+ z_i' \\beta_0 + \\varepsilon_i $$\nDefine the corresponding short regression as\n$$ y_i = x_i' \\alpha_0 + v_i \\quad \\text{ with } \\quad x_i = z_i' \\gamma_0 + u_i $$\nOVB Theorem Suppose that the DGP for the long regression corresponds to $\\alpha_0$, $\\beta_0$. Suppose further that $\\mathbb E[x_i] = 0$, $\\mathbb E[z_i] = 0$, $\\mathbb E[\\varepsilon_i |x_i,z_i] = 0$. Then, unless $\\beta_0 = 0$ or $z_i$ is orthogonal to $x_i$, the (sole) stochastic regressor $x_i$ is correlated with the error term in the short regression which implies that the OLS estimator of the short regression is inconsistent for $\\alpha_0$ due to the omitted variable bias. In particular, one can show that the plim of the OLS estimator of $\\hat{\\alpha}_{SHORT}$ from the short regression is\n$$ \\hat{\\alpha}_{SHORT} \\overset{p}{\\to} \\frac{Cov(y_i, x_i)}{Var(x_i)} = \\alpha_0 + \\beta_0 \\frac{Cov(z_i, x_i)}{Var(x_i)} $$\nConsider data $D= (y_i, x_i, z_i)_{i=1}^n$, where the true model is:\n$$ \\begin{aligned} \u0026amp; y_i = x_i' \\alpha_0 + z_i' \\beta_0 + \\varepsilon_i \\ \u0026amp; x_i = z_i' \\gamma_0 + u_i \\end{aligned} $$\nLet\u0026rsquo;s investigate the Omitted Variable Bias by simulation. In particular, we generate a sample from the following model:\n$$ \\begin{aligned} \u0026amp; y_i = x_i - 0.3 z_i + \\varepsilon_i \\ \u0026amp; x_i = 3 z_i + u_i \\ \\end{aligned} $$\nwhere $z_i,\\varepsilon_i,u_i \\sim N(0,1)$ and $n=1000$.\ndef generate_data(a, b, c, n): # Generate data z = np.random.normal(0,1,n).reshape(-1,1) u = np.random.normal(0,1,n).reshape(-1,1) x = c*z + u e = np.random.normal(0,1,n).reshape(-1,1) y = a*x + b*z + e return x, y, z  First let\u0026rsquo;s compute the value of the OLS estimator.\n# Init n = 1000 a = 1 b = -.3 c = 3 x, y, z = generate_data(a, b, c, n) # Estimate alpha by OLS ols_coeff = inv(x.T @ x) @ x.T @ y alpha_short = ols_coeff[0][0] print('alpha OLS: %.4f (true=%1.0f)' % (alpha_short, a))  alpha OLS: 0.9115 (true=1)  In our case the expected bias is:\n$$ \\begin{aligned} Bias \u0026amp; = \\beta_0 \\frac{Cov(z_i, x_i)}{Var(x_i)} = \\ \u0026amp; = \\beta_0 \\frac{Cov(z_i' \\gamma_0 + u_i, x_i)}{Var(z_i' \\gamma_0 + u_i)} = \\ \u0026amp; = \\beta_0 \\frac{\\gamma_0 Var(z_i)}{\\gamma_0^2 Var(z_i) + Var(u_i)} \\end{aligned} $$\nwhich in our case is $b \\frac{c}{c^2 + 1}$.\n# Expected bias bias = alpha_short - a exp_bias = b * c / (c**2 + 1) print('Empirical bias: %.4f \\nExpected bias: %.4f' % (bias, exp_bias))  Empirical bias: -0.0885 Expected bias: -0.0900  9.3 Pre-Test Bias Consider data $D= (y_i, x_i, z_i)_{i=1}^n$, where the true model is:\n$$ \\begin{aligned} \u0026amp; y_i = x_i' \\alpha_0 + z_i' \\beta_0 + \\varepsilon_i \\ \u0026amp; x_i = z_i' \\gamma_0 + u_i \\end{aligned} $$\nWhere $x_i$ is the variable of interest (we want to make inference on $\\alpha_0$) and $z_i$ is a high dimensional set of control variables.\nFrom now on, we will work under the following assumptions:\n $\\dim(x_i)=1$ for all $n$ $\\beta_0$ uniformely bounded in $n$ Strict exogeneity: $\\mathbb E[\\varepsilon_i | x_i, z_i] = 0$ and $\\mathbb E[u_i | z_i] = 0$ $\\beta_0$ and $\\gamma_0$ have dimension (and hence value) that depend on $n$  Pre-Testing procedure:\n Regress $y_i$ on $x_i$ and $z_i$ For each $j = 1, \u0026hellip;, p = \\dim(z_i)$ calculate a test statistic $t_j$ Let $\\hat{T} = { j: |t_j| \u0026gt; C \u0026gt; 0 }$ for some constant $C$ (set of statistically significant coefficients). Re-run the new \u0026ldquo;model\u0026rdquo; using $(x_i, z_{\\hat{T},i})$ (i.e. using the selected covariates with statistically significant coefficients). Perform statistical inference (i.e. confidence intervals and hypothesis tests) as if no model selection had been done.  Pre-testing leads to incorrect inference. Why? Because of test errors in the first stage.\n# T-test def t_test(y, x, k): beta_hat = inv(x.T @ x) @ x.T @ y residuals = y - x @ beta_hat sigma2_hat = np.var(residuals) beta_std = np.sqrt(np.diag(inv(x.T @ x)) * sigma2_hat ) return beta_hat[k,0]/beta_std[k]  First of all the t-test for $H_0: \\beta_0 = 0$:\n$$ t = \\frac{\\hat \\beta_k}{\\hat \\sigma_{\\beta_k}} $$\nwhere the standard deviation of the ols coefficient is given by\n$$ \\hat \\sigma_{\\beta_k} = \\sqrt{ \\hat \\sigma^2 \\cdot (X\u0026rsquo;X)^{-1}_{[k,k]} } $$\nwhere we estimate the variance of the error term with the variance of the residuals\n$$ \\hat \\sigma^2 = Var \\big( y - \\hat y \\big) = Var \\big( y - X (X\u0026rsquo;X)^{-1}X\u0026rsquo;y \\big) $$\n# Pre-testing def pre_testing(a, b, c, n, simulations=1000): np.random.seed(1) # Init alpha = {'Long': np.zeros((simulations,1)), 'Short': np.zeros((simulations,1)), 'Pre-test': np.zeros((simulations,1))} # Loop over simulations for i in range(simulations): # Generate data x, y, z = generate_data(a, b, c, n) xz = np.concatenate([x,z], axis=1) # Compute coefficients alpha['Long'][i] = (inv(xz.T @ xz) @ xz.T @ y)[0][0] alpha['Short'][i] = inv(x.T @ x) @ x.T @ y # Compute significance of z on y t = t_test(y, xz, 1) # Select specification based on test if np.abs(t)\u0026gt;1.96: alpha['Pre-test'][i] = alpha['Long'][i] else: alpha['Pre-test'][i] = alpha['Short'][i] return alpha  Let\u0026rsquo;s compare the different estimates.\n# Get pre_test alpha alpha = pre_testing(a, b, c, n) for key, value in alpha.items(): print('Mean alpha %s = %.4f' % (key, np.mean(value)))  Mean alpha Long = 0.9994 Mean alpha Short = 0.9095 Mean alpha Pre-test = 0.9925  The pre-testing coefficient is very close to the true coefficient.\nHowever, the main effect of pre-testing is on inference. With pre-testing, the distribution of the estimator is not gaussian anymore.\ndef plot_alpha(alpha, a): fig = plt.figure(figsize=(17,6)) # Plot distributions x_max = np.max([np.max(np.abs(x-a)) for x in alpha.values()]) # All axes for i, key in enumerate(alpha.keys()): # Reshape exisiting subplots k = len(fig.axes) for i in range(k): fig.axes[i].change_geometry(1, k+1, i+1) # Add new plot ax = fig.add_subplot(1, k+1, k+1) ax.hist(alpha[key], bins=30) ax.set_title(key) ax.set_xlim([a-x_max, a+x_max]) ax.axvline(a, c='r', ls='--') legend_text = [r'$\\alpha_0=%.0f$' % a, r'$\\hat \\alpha=%.4f$' % np.mean(alpha[key])] ax.legend(legend_text, prop={'size': 10})  Let\u0026rsquo;s compare the long, short and pre-test estimators.\n# Plot plot_alpha(alpha, a)  As we can see, the main problem of pre-testing is inference.\nBecause of the testing procedure, the distribution of the estimator is a combination of tho different distributions: the one resulting from the long regression and the one resulting from the short regression. Pre-testing is not a problem in 3 cases:\n  when $\\beta_0$ is very large: in this case the test always rejects the null hypothesis $H_0 : \\beta_0=0$ and we always run the correct specification, i.e. the long regression\n  when $\\beta_0$ is very small: in this case the test has very little power. However, as we saw from the Omitted Variable Bias formula, the bias is small.\n  when $\\gamma_0$ is very small: also in this case the test has very little power. However, as we saw from the Omitted Variable Bias formula, the bias is small.\n  Let\u0026rsquo;s compare the pre-test estimates for different values of the true parameter $\\beta$.\n# Case 1: different betas and same sample size b_sequence = b*np.array([0.1,0.3,1,3]) alpha = {} # Get sequence for k, b_ in enumerate(b_sequence): label = 'beta = %.2f' % b_ alpha[label] = pre_testing(a, b_, c, n)['Pre-test'] print('Mean alpha with beta=%.2f: %.4f' % (b_, np.mean(alpha[label])))  Mean alpha with beta=-0.03: 0.9926 Mean alpha with beta=-0.09: 0.9826 Mean alpha with beta=-0.30: 0.9925 Mean alpha with beta=-0.90: 0.9994  The means are similar, but let\u0026rsquo;s look at the distributions.\n# Plot plot_alpha(alpha, a)  When $\\beta_0$ is \u0026ldquo;small\u0026rdquo;, the distribution of the pre-testing estimator for $\\alpha$ is not normal.\nHowever, the magnitue of $\\beta_0$ is a relative concept. For an infinite sample size, $\\beta_0$ is always going to be \u0026ldquo;big enough\u0026rdquo;, in the sense that with an infinite sample size the probability fo false positives in testing $H_0: \\beta_0 = 0$ is going to zero. I.e. we always select the correct model specification, the long regression.\nLet\u0026rsquo;s have a look at the distibution of $\\hat \\alpha_{\\text{PRE-TEST}}$ when the sample size increaes.\n# Case 2: same beta and different sample sizes n_sequence = [100,300,1000,3000] alpha = {} # Get sequence for k, n_ in enumerate(n_sequence): label = 'n = %.0f' % n_ alpha[label] = pre_testing(a, b, c, n_)['Pre-test'] print('Mean alpha with n=%.0f: %.4f' % (n_, np.mean(alpha[label])))  Mean alpha with n=100: 0.9442 Mean alpha with n=300: 0.9635 Mean alpha with n=1000: 0.9925 Mean alpha with n=3000: 0.9989  # Plot plot_alpha(alpha, a)  As we can see, for large samples, $\\beta_0$ is never \u0026ldquo;small\u0026rdquo;. In the limit, when $n \\to \\infty$, the probability of false positives while testing $H_0: \\beta_0 = 0$ goes to zero.\nWe face a dilemma:\n pre-testing is clearlly a problem in finite samples all our econometric results are based on the assumption that $n \\to \\infty$  The problem is solved by assuming that the value of $\\beta_0$ depends on the sample size. This might seems like a weird assumption but is just to have an asymptotically meaningful concept of \u0026ldquo;big\u0026rdquo; and \u0026ldquo;small\u0026rdquo;.\nWe now look at what happens in the simulations when $\\beta_0$ is proportional to $\\frac{1}{\\sqrt{n}}$.\n# Case 3: beta proportional to 1/sqrt(n) and different sample sizes beta = b * 30 / np.sqrt(n_sequence) # Get sequence alpha = {} for k, n_ in enumerate(n_sequence): label = 'n = %.0f' % n_ alpha[label] = pre_testing(a, beta[k], c, n_)['Pre-test'] print('Mean alpha with n=%.0f: %.4f' % (n_, np.mean(alpha[label])))  Mean alpha with n=100: 0.9703 Mean alpha with n=300: 0.9838 Mean alpha with n=1000: 0.9914 Mean alpha with n=3000: 0.9947  # Plot plot_alpha(alpha, a)  Now the distribution of $\\hat \\alpha$ does not converge to a normal when the sample size increases.\nPre-Testing and Machine Learning How are machine learning and pre-testing related? The best example is Lasso. Suppose you have a dataset with many variables. This means that you have very few degrees of freedom and your OLS estimates are going to be very imprecise. At the extreme, you have more variables than observations so that your OLS coefficient is undefined since you cannot invert the design matrix $X\u0026rsquo;X$.\nIn this case, you might want to do variable selection. One way of doing variable selection is pre-testing. Another way is Lasso. A third alternative is to use machine learning methods that do not suffer this curse of dimensionality.\nThe purpose and outcome of pre-testing and Lasso are the same:\n you have too many variables you exclude some of them from the regression / set their coefficients to zero  As a consequence, also the problems are the same, i.e. pre-test bias.\n9.4 Post-Double Selection Consider again data $D= (y_i, x_i, z_i)_{i=1}^n$, where the true model is:\n$$ \\begin{aligned} \u0026amp; y_i = x_i' \\alpha_0 + z_i' \\beta_0 + \\varepsilon_i \\ \u0026amp; x_i = z_i' \\gamma_0 + u_i \\end{aligned} $$\nWe would like to guard against pretest bias if possible, in order to handle high dimensional models. A good pathway towards motivating procedures which guard against pretest bias is a discussion of classical partitioned regression.\nConsider a regression $y_i$ on $x_i$ and $z_i$. $x_i$ is the 1-dimensional variable of interest, $z_i$ is a high-dimensional set of control variables. We have the following procedure:\n First Stage selection: regress $x_i$ on $z_i$. Select the statistically significant variables in the set $S_{FS} \\subseteq z_i$ Reduced Form selection: lasso $y_i$ on $z_i$. Select the statistically significant variables in the set $S_{RF} \\subseteq z_i$ Regress $y_i$ on $x_i$ and $S_{FS} \\cup S_{RF}$  Theorem: Let ${P^n}$ be a sequence of data-generating processes for $D_n = (y_i, x_i, z_i)^n_{i=1} \\in (\\mathbb R \\times \\mathbb R \\times \\mathbb R^p) ^n$ where $p$ depends on $n$. For each $n$, the data are iid with $yi = x_i'\\alpha_0^{(n)} + z_i' \\beta_0^{(n)} + \\varepsilon_i$ and $x_i = z_i' \\gamma_0^{(n)} + u_i$ where $\\mathbb E[\\varepsilon_i | x_i,z_i] = 0$ and $\\mathbb E[u_i|z_i] = 0$. The sparsity of the vectors $\\beta_0^{(n)}$, $\\gamma_0^{(n)}$ is controlled by $|| \\beta_0^{(n)} ||_0 \\leq s$ with $s^2 (\\log p)^2/n \\to 0$. Suppose that additional regularity conditions on the model selection procedures and moments of the random variables $y_i$ , $x_i$ , $z_i$ as documented in Belloni et al. (2014). Then the confidence intervals, CI, from the post double selection procedure are uniformly valid. That is, for any confidence level $\\xi \\in (0, 1)$ $$ \\Pr(\\alpha_0 \\in CI) \\to 1- \\xi $$\nIn order to have valid confidence intervals you want their bias to be negligibly. Since $$ CI = \\left[ \\hat{\\alpha} \\pm \\frac{1.96 \\cdot \\hat{\\sigma}}{\\sqrt{n}} \\right] $$\nIf the bias is $o \\left( \\frac{1}{\\sqrt{n}} \\right)$ then there is no problem since it is asymptotically negligible w.r.t. the magnitude of the confidence interval. If however the the bias is $O \\left( \\frac{1}{\\sqrt{n}} \\right)$ then it has the same magnitude of the confidence interval and it does not asymptotically vanish.\nThe idea of the proof is to use partitioned regression. An alternative way to think about the argument is: bound the omitted variables bias. Omitted variable bias comes from the product of 2 quantities related to the omitted variable:\n Its partial correlation with the outcome, and Its partial correlation with the variable of interest.  If both those partial correlations are $O( \\sqrt{\\log p/n})$, then the omitted variables bias is $(s \\times O( \\sqrt{\\log p/n})^2 = o \\left( \\frac{1}{\\sqrt{n}} \\right)$, provided $s^2 (\\log p)^2/n \\to 0$. Relative to the $ \\frac{1}{\\sqrt{n}} $ convergence rate, the omitted variables bias is negligible.\nIn our omitted variable bias case, we want $| \\beta_0 \\gamma_0 | = o \\left( \\frac{1}{\\sqrt{n}} \\right)$. Post-double selection guarantees that\n Reduced form selection (pre-testing): any \u0026ldquo;missing\u0026rdquo; variable has $|\\beta_{0j}| \\leq \\frac{c}{\\sqrt{n}}$ First stage selection (additional): any \u0026ldquo;missing\u0026rdquo; variable has $|\\gamma_{0j}| \\leq \\frac{c}{\\sqrt{n}}$  As a consequence, as long as the number of omitted variables is finite, the omitted variable bias is $$ OVB(\\alpha) = |\\beta_{0j}| \\cdot|\\gamma_{0j}| \\leq \\frac{c}{\\sqrt{n}} \\cdot \\frac{c}{\\sqrt{n}} = \\frac{c^2}{n} = o \\left(\\frac{1}{\\sqrt{n}}\\right) $$\n# Pre-testing code def post_double_selection(a, b, c, n, simulations=1000): np.random.seed(1) # Init alpha = {'Long': np.zeros((simulations,1)), 'Short': np.zeros((simulations,1)), 'Pre-test': np.zeros((simulations,1)), 'Post-double': np.zeros((simulations,1))} # Loop over simulations for i in range(simulations): # Generate data x, y, z = generate_data(a, b, c, n) # Compute coefficients xz = np.concatenate([x,z], axis=1) alpha['Long'][i] = (inv(xz.T @ xz) @ xz.T @ y)[0][0] alpha['Short'][i] = inv(x.T @ x) @ x.T @ y # Compute significance of z on y (beta hat) t1 = t_test(y, xz, 1) # Compute significance of z on x (gamma hat) t2 = t_test(x, z, 0) # Select specification based on first test if np.abs(t1)\u0026gt;1.96: alpha['Pre-test'][i] = alpha['Long'][i] else: alpha['Pre-test'][i] = alpha['Short'][i] # Select specification based on both tests if np.abs(t1)\u0026gt;1.96 or np.abs(t2)\u0026gt;1.96: alpha['Post-double'][i] = alpha['Long'][i] else: alpha['Post-double'][i] = alpha['Short'][i] return alpha  Let\u0026rsquo;s now repeat the same exercise as above, but with also post-double selection\n# Get pre_test alpha alpha = post_double_selection(a, b, c, n) for key, value in alpha.items(): print('Mean alpha %s = %.4f' % (key, np.mean(value)))  Mean alpha Long = 0.9994 Mean alpha Short = 0.9095 Mean alpha Pre-test = 0.9925 Mean alpha Post-double = 0.9994  # Plot plot_alpha(alpha, a)  As we can see, post-double selection has solved the pre-testing problem. Does it work for any magnitude of $\\beta$ (relative to the sample size)?\nWe first have a look at the case in which the sample size is fixed and $\\beta_0$ changes.\n# Case 1: different betas and same sample size b_sequence = b*np.array([0.1,0.3,1,3]) alpha = {} # Get sequence for k, b_ in enumerate(b_sequence): label = 'beta = %.2f' % b_ alpha[label] = post_double_selection(a, b_, c, n)['Post-double'] print('Mean alpha with beta=%.2f: %.4f' % (b_, np.mean(alpha[label])))  Mean alpha with beta=-0.03: 0.9994 Mean alpha with beta=-0.09: 0.9994 Mean alpha with beta=-0.30: 0.9994 Mean alpha with beta=-0.90: 0.9994  # Plot plot_alpha(alpha, a)  Post-double selection always selects the correct specification, the long regression, even when $\\beta$ is very small.\nNow we check the same but for fixed $\\beta_0$ and different sample sizes.\n# Case 2: same beta and different sample sizes n_sequence = [100,300,1000,3000] alpha = {} # Get sequence for k, n_ in enumerate(n_sequence): label = 'N = %.0f' % n_ alpha[label] = post_double_selection(a, b, c, n_)['Post-double'] print('Mean alpha with n=%.0f: %.4f' % (n_, np.mean(alpha[label])))  Mean alpha with n=100: 0.9964 Mean alpha with n=300: 0.9985 Mean alpha with n=1000: 0.9994 Mean alpha with n=3000: 0.9990  # Plot plot_alpha(alpha, a)  Post-double selection always selects the correct specification, the long regression, even when the sample size is very small.\nLast, we check the case of $\\beta_0$ proportional to $\\frac{1}{\\sqrt{n}}$.\n# Case 3: beta proportional to 1/sqrt(n) and different sample sizes beta = b * 30 / np.sqrt(n_sequence) # Get sequence alpha = {} for k, n_ in enumerate(n_sequence): label = 'N = %.0f' % n_ alpha[label] = post_double_selection(a, beta[k], c, n_)['Post-double'] print('Mean alpha with n=%.0f: %.4f' % (n_, np.mean(alpha[label])))  Mean alpha with n=100: 0.9964 Mean alpha with n=300: 0.9985 Mean alpha with n=1000: 0.9994 Mean alpha with n=3000: 0.9990  # Plot plot_alpha(alpha, a)  Once again post-double selection always selects the correct specification, the long regression.\nPost-double Selection and Machine Learning As we have seen at the end of the previous section, Lasso can be used to perform variable selection in high dimensional settings. Therefore, post-double selection solves the pre-test bias problem in those settings. The post-double selection procedure with Lasso is:\n First Stage selection: lasso $x_i$ on $z_i$. Let the selected variables be collected in the set $S_{FS} \\subseteq z_i$ Reduced Form selection: lasso $y_i$ on $z_i$. Let the selected variables be collected in the set $S_{RF} \\subseteq z_i$ Regress $y_i$ on $x_i$ and $S_{FS} \\cup S_{RF}$  9.5 Double/debiased Machine Learning This section is taken from Chernozhukov, V., Chetverikov, D., Demirer, M., Duflo, E., Hansen, C., Newey, W., \u0026amp; Robins, J. (2018). \u0026ldquo;Double/debiased machine learning for treatment and structural parameters\u0026quot;.\nConsider the following partially linear model\n$$ y = \\beta_0 D + g_0(X) + u \\ D = m_0(X) + v $$\nwhere $y$ is the outcome variable, $D$ is the treatment to interest and $X$ is a potentially high-dimensional set of controls.\nNaive approach A naive approach to estimation of $\\beta_0$ using ML methods would be, for example, to construct a sophisticated ML estimator $\\beta_0 D + g_0(X)$ for learning the regression function $\\beta_0 D$ + $g_0(X)$.\n Split the sample in two: main sample and auxiliary sample Use the auxiliary sample to estimate $\\hat g_0(X)$ Use the main sample to compute the orthogonalized component of $Y$ on $X$: $\\hat u = \\left(Y_{i}-\\hat{g}{0}\\left(X{i}\\right)\\right)$ Use the main sample to estimate the residualized OLS estimator  $$ \\hat{\\beta}{0}=\\left(\\frac{1}{n} \\sum{i \\in I} D_{i}^{2}\\right)^{-1} \\frac{1}{n} \\sum_{i \\in I} D_{i} \\hat u_i $$\nThis estimator is going to have two problems:\n Slow rate of convergence, i.e. slower than $\\sqrt(n)$ It will be biased because we are employing highdimensional regularized estimators (e.g. we are doing variable selection)  Orthogonalization Now consider a second construction that employs an orthogonalized formulation obtained by directly partialling out the effect of $X$ from $D$ to obtain the orthogonalized regressor $v = D − m_0(X)$.\n  Split the sample in two: main sample and auxiliary sample\n  Use the auxiliary sample to estimate $\\hat g_0(X)$ from\n$$ y = \\beta_0 D + g_0(X) + u \\ $$\n  Use the auxiliary sample to estimate $\\hat m_0(X)$ from\n$$ D = m_0(X) + v $$\n  Use the main sample to compute the orthogonalized component of $D$ on $X$ as\n$$ \\hat v = D - \\hat m_0(X) $$\n  Use the main sample to estimate the double-residualized OLS estimator as\n$$ \\hat{\\beta}{0}=\\left(\\frac{1}{n} \\sum{i \\in I} \\hat v_i D_{i} \\right)^{-1} \\frac{1}{n} \\sum_{i \\in I} \\hat v_i \\left( Y - \\hat g_0(X) \\right) $$\n  The estimator is unbiased but still has a lower rate of convergence because of sample splitting. The problem is solved by inverting the split sample, re-estimating the coefficient and averaging the two estimates. Note that this procedure is valid since the two estimates are independent by the sample splitting procedure.\nApplication to AJR02 In this section we are going to replicate 6.3 of the \u0026ldquo;Double/debiased machine learning\u0026rdquo; paper based on Acemoglu, Johnson, Robinson (2002), \u0026ldquo;The Colonial Origins of Comparative Development\u0026quot;.\nWe first load the dataset\n# Load Acemoglu Johnson Robinson Dataset df = pd.read_csv('data/AJR02.csv',index_col=0)  df.head()   .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }  \n  GDP Exprop Mort Latitude Neo Africa Asia Namer Samer logMort Latitude2     1 8.39 6.50 78.20 0.3111 0 1 0 0 0 4.359270 0.096783   2 7.77 5.36 280.00 0.1367 0 1 0 0 0 5.634790 0.018687   3 9.13 6.39 68.90 0.3778 0 0 0 0 1 4.232656 0.142733   4 9.90 9.32 8.55 0.3000 1 0 0 0 0 2.145931 0.090000   5 9.29 7.50 85.00 0.2683 0 0 0 1 0 4.442651 0.071985     df.info()  \u0026lt;class 'pandas.core.frame.DataFrame'\u0026gt; Int64Index: 64 entries, 1 to 64 Data columns (total 11 columns): # Column Non-Null Count Dtype --- ------ -------------- ----- 0 GDP 64 non-null float64 1 Exprop 64 non-null float64 2 Mort 64 non-null float64 3 Latitude 64 non-null float64 4 Neo 64 non-null int64 5 Africa 64 non-null int64 6 Asia 64 non-null int64 7 Namer 64 non-null int64 8 Samer 64 non-null int64 9 logMort 64 non-null float64 10 Latitude2 64 non-null float64 dtypes: float64(6), int64(5) memory usage: 6.0 KB  In their paper, AJR note that their IV strategy will be invalidated if other factors are also highly persistent and related to the development of institutions within a country and to the country’s GDP. A leading candidate for such a factor, as they discuss, is geography. AJR address this by assuming that the confounding effect of geography is adequately captured by a linear term in distance from the equator and a set of continent dummy variables.\nThey inclue their results in table 2.\n# Add constant term to dataset df['const'] = 1 # Create lists of variables to be used in each regression X1 = df[['const', 'Exprop']] X2 = df[['const', 'Exprop', 'Latitude', 'Latitude2']] X3 = df[['const', 'Exprop', 'Latitude', 'Latitude2', 'Asia', 'Africa', 'Namer', 'Samer']] y = df['GDP'] # Estimate an OLS regression for each set of variables reg1 = sm.OLS(y, X1, missing='drop').fit() reg2 = sm.OLS(y, X2, missing='drop').fit() reg3 = sm.OLS(y, X3, missing='drop').fit()  Let\u0026rsquo;s replicate Table 2 in AJR.\n# Make table 2 def make_table_2(): info_dict={'No. observations' : lambda x: f\u0026quot;{int(x.nobs):d}\u0026quot;} results_table = summary_col(results=[reg1,reg2,reg3], float_format='%0.2f', stars = True, model_names=['Model 1','Model 2','Model 3'], info_dict=info_dict, regressor_order=['const','Exprop','Latitude','Latitude2']) return results_table  table_2 = make_table_2() table_2    Model 1 Model 2 Model 3   const 4.66*** 4.55*** 5.95***    (0.41) (0.45) (0.68)   Exprop 0.52*** 0.49*** 0.40***    (0.06) (0.07) (0.06)   Latitude  2.16 0.42     (1.68) (1.47)   Latitude2  -2.12 0.44     (2.86) (2.48)   Africa   -1.06**      (0.41)   Asia   -0.74*      (0.42)   Namer   -0.17      (0.40)   Samer   -0.12      (0.42)   No. observations 64 64 64   Using DML allows us to relax this assumption and to replace it by a weaker assumption that geography can be sufficiently controlled by an unknown function of distance from the equator and continent dummies, which can be learned by ML methods.\nIn particular, our framework is\n$$ {GDP} = \\beta_0 \\times {Exprop} + g_0({geography}) + u \\ {Exprop} = m_0({geography}) + u $$\nSo that the double/debiased machine learning procedure is\n  Split the sample in two: main sample and auxiliary sample\n  Use the auxiliary sample to estimate $\\hat g_0({geography})$ from\n$$ {GDP} = \\beta_0 \\times {Exprop} + g_0({geography}) + u $$\n  Use the auxiliary sample to estimate $\\hat m_0({geography})$ from\n$$ {Exprop} = m_0({geography}) + v $$\n  Use the main sample to compute the orthogonalized component of ${Exprop}$ on ${geography}$ as\n$$ \\hat v = {Exprop} - \\hat m_0({geography}) $$\n  Use the main sample to estimate the double-residualized OLS estimator as\n$$ \\hat{\\beta}{0}=\\left(\\frac{1}{n} \\sum{i \\in I} \\hat v_i \\times {Exprop}{i} \\right)^{-1} \\frac{1}{n} \\sum{i \\in I} \\hat v_i \\times \\left( {GDP} - \\hat g_0({geography}) \\right) $$\n  Since we employ an intrumental variable strategy, we replace $m_0({geography})$ with $m_0({geography},{logMort})$ in the first stage.\n# Generate variables D = df['Exprop'].values.reshape(-1,1) X = df[['const', 'Latitude', 'Latitude2', 'Asia', 'Africa', 'Namer', 'Samer']].values y = df['GDP'].values.reshape(-1,1) Z = df[['const', 'Latitude', 'Latitude2', 'Asia', 'Africa', 'Namer', 'Samer','logMort']].values  def estimate_beta(algorithm, alg_name, D, X, y, Z, sample): # Split sample D_main, D_aux = (D[sample==1], D[sample==0]) X_main, X_aux = (X[sample==1], X[sample==0]) y_main, y_aux = (y[sample==1], y[sample==0]) Z_main, Z_aux = (Z[sample==1], Z[sample==0]) # Residualize y on D b_hat = inv(D_aux.T @ D_aux) @ D_aux.T @ y_aux y_resid_aux = y_aux - D_aux @ b_hat # Estimate g0 alg_fitted = algorithm.fit(X=X_aux, y=y_resid_aux.ravel()) g0 = alg_fitted.predict(X_main).reshape(-1,1) # Compute v_hat u_hat = y_main - g0 # Estimate m0 alg_fitted = algorithm.fit(X=Z_aux, y=D_aux.ravel()) m0 = algorithm.predict(Z_main).reshape(-1,1) # Compute u_hat v_hat = D_main - m0 # Estimate beta beta = inv(v_hat.T @ D_main) @ v_hat.T @ u_hat return beta  def ddml(algorithm, alg_name, D, X, y, Z, p=0.5, verbose=False): # Expand X if Lasso or Ridge if alg_name in ['Lasso ','Ridge ']: X = PolynomialFeatures(degree=2).fit_transform(X) # Generate split (fixed proportions) split = np.array([i in train_test_split(range(len(D)), test_size=p)[0] for i in range(len(D))]) # Compute beta beta = [estimate_beta(algorithm, alg_name, D, X, y, Z, split==k) for k in range(2)] beta = np.mean(beta) # Print and return if verbose: print('%s : %.4f' % (alg_name, beta)) return beta  # Generate sample split p = 0.5 split = np.random.binomial(1, p, len(D))  We inspect different algorithms. In particular, we consider:\n Lasso Regression Ridge Regression Regression Trees Random Forest Boosted Forests  # List all algorithms algorithms = {'Ridge ': Ridge(alpha=.1), 'Lasso ': Lasso(alpha=.01), 'Tree ': DecisionTreeRegressor(), 'Forest ': RandomForestRegressor(n_estimators=30), 'Boosting': GradientBoostingRegressor(n_estimators=30)}  Let\u0026rsquo;s compare the results.\n# Loop over algorithms for alg_name, algorithm in algorithms.items(): ddml(algorithm, alg_name, D, X, y, Z, verbose=True)  Ridge : 0.1289 Lasso : -8.7963 Tree : 1.2879 Forest : 2.4938 Boosting : 0.5977  The results are extremely volatile.\n# Repeat K times def estimate_beta_median(algorithms, D, X, y, Z, K): # Loop over algorithms for alg_name, algorithm in algorithms.items(): betas = [] # Iterate n times for k in range(K): beta = ddml(algorithm, alg_name, D, X, y, Z) betas = np.append(betas, beta) print('%s : %.4f' % (alg_name, np.median(betas)))  Let\u0026rsquo;s try using the median to have a more stable estimator.\nnp.random.seed(123) # Repeat 100 times and take median estimate_beta_median(algorithms, D, X, y, Z, 100)  Ridge : 0.6670 Lasso : 1.2511 Tree : 0.9605 Forest : 0.5327 Boosting : 1.0327  The results differ slightly from the ones in the paper, but they are at least closer.\n","date":1646784000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1646784000,"objectID":"18f7a71852cf7865ca2bcf5562ea90ac","permalink":"https://matteocourthoud.github.io/course/ml-econ/09_postdoubleselection/","publishdate":"2022-03-09T00:00:00Z","relpermalink":"/course/ml-econ/09_postdoubleselection/","section":"course","summary":"# Remove warnings import warnings warnings.filterwarnings('ignore')  # Import everything import pandas as pd import numpy as np import seaborn as sns import statsmodels.api as sm from numpy.linalg import inv from statsmodels.","tags":null,"title":"Post-Double Selection","type":"book"},{"authors":null,"categories":null,"content":"Lasso Issue Lasso (Least Absolute Shrinkage and Selection Operator) is a popular method for high dimensional regression. It does variable selection and estimation simultaneously. It is a non-parametric (series) estimation technique part of a general class of estimators called penalized estimators. It allows the number of regressors, $p$, to be larger than the sample size, $n$.\nConsider data $D = \\lbrace x_i, y_i \\rbrace_{i=1}^n$ with $\\dim (x_i) = p$. Assume that $p$ is large relative to $n$. Two possible reasons:\n we have an intrinsic problem of high dimensionality $p$ indicates the number of expansion terms of small number of underlying important variables (e.g. series estimation)  Assumption: $y_i = x_i' \\beta_0 + r_i + \\varepsilon_i$ where $\\beta_0$ depends on $p$, $r_i$ is a remainder term.\nNote that in classic non-parametrics, we have $x_i'\\beta_0$ as $p_1(x_i) \\beta_{1,K} + \\dots + p_K(x_i) \\beta_{K,K}$. For simplicity, we assume $r_i = 0$, as if we had extreme undersmoothing. Hence the model becomes: $$ y_i = x_i' \\beta_0 + \\varepsilon_i, \\qquad p \\geq n $$ We cannot run OLS because $p \\geq n$, thus the rank condition is violated.\nDefinition We define the Lasso estimator as $$ \\hat{\\beta}_L = \\arg \\min \\quad \\underbrace{\\mathbb E_n \\Big[ (y_i - x_i' \\beta)^2 \\Big]} _ {\\text{SSR term}} + \\underbrace{\\frac{\\lambda}{n} \\sum _ {j=1}^{P} | \\beta_j |} _ {\\text{Penalty term}} $$ where $\\lambda$ is called penalty parameter.\nThe penalty term discourages large values of $| \\beta_j |$. The choice of $\\lambda$ is analogous to the choice of $K$ in series estimation and $h$ in kernel estimation.\nPenalties The shrinkage to zero of the coefficients directly follows from the $|| \\cdot ||_1$ norm. On the contrary, another famous penalized estimator, ridge regression, uses the $|| \\cdot ||_2$ norm and does not have this property.\n Minimizing SSR + penalty is equivalent to minimize SSR $s.t.$ pen $\\leq c$ (clear from the picture).\n Sparsity Let $S_0 = \\lbrace j: \\beta_{0,j} \\ne 0 \\rbrace$, we define $s_0 = |S_0|$ as the sparsity of $\\beta_0$. If $s_0/n \\to 0$, we are dealing with a sparse regression (analogous of smooth regression).\n Remark on sparsity:\n In words, sparsity means that even if we have a lot of variables, only a small number of them (relative to $n$) have an effect on the dependent variable. Approximate sparsity imposes a restriction that only $s_0$ variables among all of $x_{ij}$, where $s_0$ is much smaller than $n$, have associated coefficients $\\beta_{0j}$ that are different from zero, while permitting a nonzero approximation error. Thus, estimators for this kind of model attempt to learn the identities of the variables with large nonzero coefficients, while simultaneously estimating these coefficients. (Belloni et al., 2004) Sparsity is an assumption. $\\beta_0$ is said to be $s_0$-sparse with $s_0 \u0026lt; n$ if $$ | \\lbrace j: \\beta_{0j} \\neq 0 \\rbrace | \\leq s_0 $$   Lasso Theorem Theorem\nSuppose that for data $D_n = (y_i, x_i){i=1}^N$ with $y_i = x_i' \\beta + \\varepsilon_i$. Let $\\hat{\\beta}L$ be the Lasso estimator. Let $\\mathcal{S} = 2 \\max_j | \\mathbb E[ x{ij} \\varepsilon_i] |$. Suppose $|support(\\beta_0) \\leq s_0$ (sparsity assumption). Let $c_0 = (\\mathcal{S} + \\lambda/n )/(-\\mathcal{S} + \\lambda/n )$. Let $$ \\kappa{c_0, s_0} = \\min_{ d \\in \\mathbb R^p, A \\subseteq \\lbrace 1, \u0026hellip; , p \\rbrace : |A| \\leq s_0 , || d_{A^c}|| \\leq c_0 || d_A ||_1 } \\sqrt{ \\frac{ s_0 d' \\mathbb E_n [x_i x_i'] d }{|| d_A ||_1^2} } $$ Then\n$$ \\mathbb I_{ \\left\\lbrace \\frac{\\lambda}{n} \u0026gt; \\mathcal{S} \\right\\rbrace} \\mathbb E_n [(x_i \\beta_0 - x_i \\beta_L)^2]^{\\frac{1}{2}} \\leq 2 \\frac{\\lambda}{n} \\frac{\\sqrt{s_0}}{\\kappa_{c_0, s_0}} $$\nIntuition: for a sufficiently high lambda the root mean squared error of Lasso is approximately zero.\n$$ \\text{ RMSE }: \\mathbb E_n [(x_i \\beta_0 - x_i \\beta_L)^2]^{\\frac{1}{2}} \\simeq 0 \\quad \\Leftrightarrow \\quad \\frac{\\lambda}{n} \u0026gt; \\mathcal{S} $$\nRemarks  The minimization region is the set of “essentially sparse” vectors $d \\in \\mathbb R^p$, where “essentially sparse” is defined by $\\mathcal{C}, \\mathcal{S}$. In particular the condition $k_{\\mathcal{C}, \\mathcal{S}}\u0026gt;0$ means that no essentially sparse vector $d$ has $\\mathbb E[x_i x_i']d = 0$, i.e. regressors were not added multiple times. Need to dominate the score with the penalty term $\\lambda$. Need no collinearity on a small ($\\leq s_0$) subset of regressors ($\\to k_{c_0, s_0}\u0026gt;0$).  When Lasso? For prediction problems in high dimensional environments. NB! Lasso is not good for inference, only for prediction.\nIn particular, in econometrics it’s used for selecting either\n instruments (predicting $\\hat{x}$ in the first stage) control variables (next section: double prediction problem, in the first stage and in the reduced form)  Choosing the Optimal Lambda The choice of $\\lambda$ determines the bias-variance tradeoff:\n if $\\lambda$ is too big: $\\lambda \\approx \\infty \\mathbb \\Rightarrow \\hat{\\beta} \\approx 0$; if $\\lambda$ is too small: $\\lambda \\approx 0 \\mathbb \\Rightarrow$ overfitting.  Possible solutions: Bonferroni correction, bootstrapping or $\\frac{\\lambda}{n} \\asymp \\sqrt{\\frac{\\log(p)}{n}}$ (asymptotically equal to), $\\mathcal{S}$ behaves like the maximum of gaussians.\nLasso Path How the estimated $\\hat{\\beta}$ depends on the penalty parameter $\\lambda$?\nPost Lasso: fit OLS without the penalty with all the nonzero coeficients selected by Lasso in the first step.\nRemarks  Do not do inference with post-Lasso because standard errors are not uniformely valid. As $n \\to \\infty$ the CV and the score domination bounds converge to a unique bound. What is the problem of cross-validation? In high dimensional settings you can overfit in so many ways that CV doesn’t work and still overfits. Using $\\lambda$ with $\\frac{\\lambda}{n} \u0026gt; \\mathcal{S}$ small coefficients get shrunk to zero with high probability. In this case with small we mean $\\propto \\frac{1}{\\sqrt{n}}$ or $2 \\max_j | \\mathbb E_n[\\varepsilon_i x_{ij}] |$. If $| \\beta_{0j}| \\leq \\frac{c}{\\sqrt{n}}$ for a sufficiently small constant $c$, then $\\hat{\\beta}_{LASSO} \\overset{p}{\\to} 0$. In standard t-tests $c = 1.96$. $\\sqrt{n}$ factor is important since it is the demarcation line for reliable statistical detection.  Optimal Lambda What is the criterium that should guide the selection of $\\lambda$? $$ \\frac{\\lambda}{n} \\geq 2 \\mathbb E_n[x_{ij} \\varepsilon_i] \\qquad \\forall j \\quad \\text{ if } Var(x_{ij} \\varepsilon_i) = 1 $$\nHow to choose the optimal $\\lambda$:\n Decide the coverage of the confidence intervals ($1-\\alpha$): $$ \\Pr \\left( \\sqrt{n} \\Big| \\mathbb E_n [x_{ij} \\varepsilon_i] \\Big| \u0026gt; t \\right) = 1- \\alpha $$ Solve for $t$ Get $\\lambda$ such that all scores are dominated by $\\frac{\\lambda}{n}$ with $\\alpha%$ probability.   It turns out that the optimal $t \\propto \\sqrt{\\log(p)}$\n Pre-Testing Omitted Variable Bias Consider two separate statistical models. Assume the following long regression of interest:\n$$ y_i = x_i' \\alpha_0+ z_i' \\beta_0 + \\varepsilon_i $$\nDefine the corresponding short regression as\n$$ y_i = x_i' \\alpha_0 + v_i \\quad \\text{ with } v_i = z_i' \\beta_0 + \\varepsilon_i $$\nTheorem\nSuppose that the DGP for the long regression corresponds to $\\alpha_0$, $\\beta_0$. Suppose further that $\\mathbb E[x_i] = 0$, $\\mathbb E[z_i] = 0$, $\\mathbb E[\\varepsilon_i |x_i,z_i] = 0$. Then, unless $\\beta_0 = 0$ or $z_i$ is orthogonal to $x_i$, the (sole) stochastic regressor $x_i$ is correlated with the error term in the short regression which implies that the OLS estimator of the short regression is inconsistent for $\\alpha_0$ due to the omitted variable bias. In particular, one can show that the plim of the OLS estimator of $\\hat{\\alpha}{SHORT}$ from the short regression is $$ \\hat{\\alpha}{SHORT} \\overset{p}{\\to} \\frac{Cov(y_i, x_i)}{Var(x_i)} = \\alpha_0 + \\beta_0 \\frac{Cov(z_i, x_i)}{Var(x_i)} $$\nPre-test bias Consider data $D= (y_i, x_i, z_i)_{i=1}^n$, where the true model is: $$ \\begin{aligned} \u0026amp; y_i = x_i' \\alpha_0 + z_i' \\beta_0 + \\varepsilon_i \\newline \u0026amp; x_i = z_i' \\gamma_0 + u_i \\end{aligned} $$\nWhere $x_i$ is the variable of interest (we want to make inference on $\\alpha_0$) and $z_i$ is a high dimensional set of control variables.\nFrom now on, we will work under the following assumptions:\n $\\dim(x_i)=1$ for all $n$ $\\beta_0$ uniformely bounded in $n$ Strict exogeneity: $\\mathbb E[\\varepsilon_i | x_i, z_i] = 0$ and $\\mathbb E[u_i | z_i] = 0$ $\\beta_0$ and $\\gamma_0$ have dimension (and hence value) that depend on $n$  Pre-Testing procedure  Regress $y_i$ on $x_i$ and $z_i$ For each $j = 1, \u0026hellip;, p = \\dim(z_i)$ calculate a test statistic $t_j$ Let $\\hat{T} = \\lbrace j: |t_j| \u0026gt; C \u0026gt; 0 \\rbrace$ for some constant $C$ (set of statistically significant coefficients). Re-run the new “model” using $(x_i, z_{\\hat{T},i})$ (i.e. using the selected covariates with statistically significant coefficients). Perform statistical inference (i.e. confidence intervals and hypothesis tests) as if no model selection had been done.  Bias As we can see from the figure above (code below), running the short regression instead of the long one introduces Omitted Variable Bias (second column). Instead, the Pre-Testing estimator is consistent but not normally distributed (third column).\nIssue Pre-testing is problematic because the post-selection estimator is not asymptotically normal. Moreover, for particular data generating processes, it even fails to be consistent at the rate of $\\sqrt{n}$ (Belloni et al., 2014).\n Intuition: when performing pre-testing, we might have an Omitted Variable Bias problem when $\\beta_0\u0026gt;0$ but we fail to reject the null hypothesis $H_0 : \\beta_0 = 0$ because of lack of statistical power, i.e. $|\\beta_0|$ is small with respect to the sample size. In particular, we fail to reject the null hypothesis for $\\beta_0(n) = O \\left( \\frac{1}{\\sqrt{n}}\\right)$. However, note that the problem vanishes asymptotically, as the resulting estimator is consistent. In fact, if $\\beta_0(n) = O \\left( \\frac{1}{\\sqrt{n}}\\right)$, then $\\alpha_0 - \\hat \\alpha_{PRETEST} \\overset{p}{\\to} \\lim_{n \\to \\infty} \\beta_0 \\gamma_0 = \\lim_{n \\to \\infty} O \\left( \\frac{1}{\\sqrt{n}} \\right) = 0$. We now clarify what it means to have a coefficient depending on the sample size, $\\beta_0(n)$.\n Uniformity Concept of uniformity: the DGP varies with $n$. Instead of having a fixed “true” parameter $\\beta_0$, you have a sequence $\\beta_0(n)$. Having a cofficient that depends on the sample size $n$ is useful to preserve the concept of “small with respect to the sample size” in asymptotic theory.\nIn the context of Pre-Testing, all problems vanish asymptotically since we are able to always reject the null hypothesis $H_0 : \\beta_0 = 0$ when $\\beta_0 \\neq 0$. In the figure below, I plot simulation results for $\\hat \\alpha_{PRETESTING}$ for a fixed coefficient $\\beta_0$ (first row) and variable coefficient $\\beta_0(n)$ that depends on the sample size (second row), for different sample sizes (columns). We see that if $\\beta_0$ is independent from the sample size (first row), the distribution of $\\hat \\alpha_{PRETEST}$ is not normal in small samples and it displays the bimodality that characterizes pre-testing. However, it becomes normal in large samples. On the other hand, when $\\beta_0(n)$ depends on the sample size, and in particular $\\beta_0 = O \\left( \\frac{1}{\\sqrt{n}} \\right)$ (second row), the distribution of $\\hat \\alpha_{PRETEST}$ stays bimodal even when the sample size increases.\n Note that the estimator is always consistent!\n Where is Pre-Testing a Problem? If we were to draw a map of where the gaussianity assumption of $\\beta_0(n)$ holds well and where it fails, it would look like the following figure.\nIntuition The intuition for the three different regions (from bottom to top) is the following.\n When $\\beta_0 = o \\left( \\frac{1}{\\sqrt{n}} \\right)$, $z_i$ is excluded with probability $p \\to 1$. But, given that $\\beta_0$ is small enough, failing to control for $z_i$ does not introduce large omitted variables bias (Belloni et al., 2014). If however the coefficient on the control is “moderately close to zero”, $\\beta_0 = O \\left( \\frac{1}{\\sqrt{n}} \\right)$, the t-test set-up above cannot distinguish this coefficient from $0$, and the control $z_i$ is dropped with probability $p \\to 1$. However, in this case the omitted variable bias generated by excluding $z_i$ scaled by $\\sqrt{n}$ does not converge to zero. That is, the standard post-selection estimator is not asymptotically normal and even fails to be consistent at the rate of $\\sqrt{n}$ (Belloni et al., 2014). Lastly, when $\\beta_0$ is large enough, the null pre-testing hypothesis $H_0 : \\beta_0 = 0$ will be rejected sufficiently often so that the bias is negligible.  Post-Double Selection The post-double-selection estimator, $\\hat{\\alpha}_{PDS}$ solves this problem by doing variable selection via standard t-tests or Lasso-type selectors with the two “true model” equations (first stage and reduced form) that contain the information from the model and then estimating $\\alpha_0$ by regressing $y_i$ on $x_i$ and the union of the selected controls. By doing so, $z_i$ is omitted only if its coefficient in both equations is small which greatly limits the potential for omitted variables bias (Belloni et al., 2014).\n Intuition: by performing post-double selection, we ensure that both $\\beta_0 = O \\left( \\frac{1}{\\sqrt{n}} \\right)$ and $\\gamma_0 = O \\left( \\frac{1}{\\sqrt{n}} \\right)$ so that $\\sqrt{n} ( \\hat \\alpha _ {PRETEST} - \\alpha _ 0) \\overset{p}{\\to} \\lim_{n \\to \\infty} \\sqrt{n} \\beta_0 \\gamma_0 = \\lim_{n \\to \\infty} \\sqrt{n} O \\left( \\frac{1}{n} \\right) = 0$ and the estimator is gaussian.\n Frisch-Waugh Theorem Theorem\nConsider the data $D = \\lbrace x_i, y_i, z_i \\rbrace_{i=1}^\\infty$ with DGP: $Y = X \\alpha + Z \\beta + \\varepsilon$. The following estimators of $\\alpha$ are numerically equivalent (if $[X, Z]$ has full rank):\n $\\hat{\\alpha}$ from regressing $Y$ on $X, Z$ $\\tilde{\\alpha}$ from regressing $Y$ on $\\tilde{X}$ $\\bar{\\alpha}$ from regressing $\\tilde{Y}$ on $\\tilde{X}$  where the operation of passing to $Y, X$ to $\\tilde{Y}, \\tilde{X}$ is called projection out $Z$, e.g.$\\tilde{X}$ are the residuals from regressing $X$ on $Z$.\nProof (1) We want to show that $\\hat{\\alpha} = \\tilde{\\alpha}$.\nClaim: $\\hat{\\alpha } = \\tilde{\\alpha} \\Leftrightarrow \\tilde{X}' \\left[ (X - \\tilde{X})\\hat{\\alpha} + Z \\hat{\\beta} +\\hat{\\varepsilon} \\right] = 0$.\nProof of the claim: if $\\hat{\\alpha} = \\tilde{\\alpha}$, we can write $Y$ as $$ Y = X \\hat{\\alpha} + Z \\hat{\\beta} + \\hat{\\varepsilon} = \\tilde{X} \\hat{\\alpha} + \\underbrace{(X - \\tilde{X}) \\hat{\\alpha } + Z \\hat{\\beta} + \\hat{\\varepsilon}}_\\text{residual of $Y$ on $\\tilde{X} $} = \\tilde{X} \\tilde{\\alpha} + \\nu_i $$\nTherefore, by the orthogonality property of the OLS residual, it must be that $\\tilde{X}'\\nu_i= 0$. $$\\tag*{$\\blacksquare$}$$\nProof (1) Having established the claim, we want to show that the normal equation $\\tilde{X}' \\left[ (X - \\tilde{X})\\hat{\\alpha} + Z \\hat{\\beta} +\\hat{\\varepsilon} \\right] = 0$ is satisfied. We follow 3 steps:\n  First we have that $\\tilde{X}' (X - \\tilde{X})\\hat{\\alpha} = 0$. This follows from the fact that $\\tilde{X}' = X' M_Z$ and hence: $$ \\begin{aligned} \\tilde{X}' (X - \\tilde{X}) \u0026amp; = X' M_Z (X - M_Z) = X' M_Z X - X' \\overbrace{M_Z M_Z}^{M_Z} X \\newline \u0026amp; = X\u0026rsquo;M_Z X - X' M_Z X = 0 \\end{aligned} $$\n  $\\tilde{X}' Z \\hat{\\beta} = 0$ since $\\tilde{X}$ is the residual from the regression of $X$ on $Z$, by normal equation it holds that $\\tilde{X}' Z = 0$.\n  $\\tilde{X}' \\hat{\\varepsilon} = 0$. This follows from (i) $M_Z ' M_{X, Z} = M_{X,Z}$ and (ii) $X' M_{X, Z} = 0$: $$ \\tilde{X}' \\hat{\\varepsilon} = (M_Z X)' (M_{X, Z} \\varepsilon) = X\u0026rsquo;M_Z' M_{X, Z} \\varepsilon = \\underbrace{X' M_{X, Z}}_0 \\varepsilon = 0. $$ $$\\tag*{$\\blacksquare$}$$\n  The coefficient $\\hat{\\alpha}$ is a partial regression coefficient identified from the variation in $X$ that is orthogonal to $Z$. This is often known as residual variation.\nPost Double Selection Setting Consider again data $D= (y_i, x_i, z_i)_{i=1}^n$, where the true model is: $$ \\begin{aligned} \u0026amp; y_i = x_i' \\alpha_0 + z_i' \\beta_0 + \\varepsilon_i \\newline \u0026amp; x_i = z_i' \\gamma_0 + u_i \\end{aligned} $$\nWe would like to guard against pretest bias if possible, in order to handle high dimensional models. A good pathway towards motivating procedures which guard against pretest bias is a discussion of classical partitioned regression.\nConsider a regression $y_i$ on $x_i$ and $z_i$. $x_i$ is the 1-dimensional variable of interest, $z_i$ is a high-dimensional set of control variables. We have the following procedure:\n First Stage selection: lasso $x_i$ on $z_i$. Let the selected variables be collected in the set $S_{FS} \\subseteq z_i$ Reduced Form selection: lasso $y_i$ on $z_i$. Let the selected variables be collected in the set $S_{RF} \\subseteq z_i$ Regress $y_i$ on $x_i$ and $S_{FS} \\cup S_{RF}$  PDS Theorem Theorem\nLet $\\lbrace P^n\\rbrace$ be a sequence of data-generating processes for $D_n = (y_i, x_i, z_i)^n_{i=1} \\in (\\mathbb R \\times \\mathbb R \\times \\mathbb R^p) ^n$ where $p$ depends on $n$. For each $n$, the data are iid with $yi = x_i'\\alpha_0^{(n)} + z_i' \\beta_0^{(n)} + \\varepsilon_i$ and $x_i = z_i' \\gamma_0^{(n)} + u_i$ where $\\mathbb E[\\varepsilon_i | x_i,z_i] = 0$ and $\\mathbb E[u_i|z_i] = 0$. The sparsity of the vectors $\\beta_0^{(n)}$, $\\gamma_0^{(n)}$ is controlled by $|| \\beta_0^{(n)} ||_0 \\leq s$ with $s^2 (\\log p)^2/n \\to 0$. Suppose that additional regularity conditions on the model selection procedures and moments of the random variables $y_i$ , $x_i$ , $z_i$ as documented in Belloni et al. (2014). Then the confidence intervals, CI, from the post double selection procedure are uniformly valid. That is, for any confidence level $\\xi \\in (0, 1)$ $$ \\Pr(\\alpha_0 \\in CI) \\to 1- \\xi $$\nIn order to have valid confidence intervals you want their bias to be negligibly. Since $$ CI = \\left[ \\hat{\\alpha} \\pm \\frac{1.96 \\cdot \\hat{\\sigma}}{\\sqrt{n}} \\right] $$\nIf the bias is $o \\left( \\frac{1}{\\sqrt{n}} \\right)$ then there is no problem since it is asymptotically negligible w.r.t. the magnitude of the confidence interval. If however the the bias is $O \\left( \\frac{1}{\\sqrt{n}} \\right)$ then it has the same magnitude of the confidence interval and it does not asymptotically vanish.\nProof (Idea) The idea of the proof is to use partitioned regression. An alternative way to think about the argument is: bound the omitted variables bias. Omitted variable bias comes from the product of 2 quantities related to the omitted variable:\n Its partial correlation with the outcome, and Its partial correlation with the variable of interest.  If both those partial correlations are $O( \\sqrt{\\log p/n})$, then the omitted variables bias is $(s \\times O( \\sqrt{\\log p/n})^2 = o \\left( \\frac{1}{\\sqrt{n}} \\right)$, provided $s^2 (\\log p)^2/n \\to 0$. Relative to the $\\frac{1}{\\sqrt{n}}$ convergence rate, the omitted variables bias is negligible.\nIn our omitted variable bias case, we want $| \\beta_0 \\gamma_0 | = o \\left( \\frac{1}{\\sqrt{n}} \\right)$. Post-double selection guarantees that\n Reduced form selection (pre-testing): any “missing” variable has $|\\beta_{0j}| \\leq \\frac{c}{\\sqrt{n}}$ First stage selection (additional): any “missing” variable has $|\\gamma_{0j}| \\leq \\frac{c}{\\sqrt{n}}$  As a consequence, as long as the number of omitted variables is finite, the omitted variable bias is $$ OVB(\\alpha) = |\\beta_{0j}| \\cdot|\\gamma_{0j}| \\leq \\frac{c}{\\sqrt{n}} \\cdot \\frac{c}{\\sqrt{n}} = \\frac{c^2}{n} = o \\left(\\frac{1}{\\sqrt{n}}\\right) $$\nDistribution We can plot the distribution of the post-double selection estimator against the pre-testing one.\n Remark: under homoskedasticity, the above estimator achieves the semiparametric efficiency bound.\n ","date":1635465600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1635465600,"objectID":"836b9d8d9e375e7fccbc006323494047","permalink":"https://matteocourthoud.github.io/course/metrics/09_selection/","publishdate":"2021-10-29T00:00:00Z","relpermalink":"/course/metrics/09_selection/","section":"course","summary":"Lasso Issue Lasso (Least Absolute Shrinkage and Selection Operator) is a popular method for high dimensional regression. It does variable selection and estimation simultaneously. It is a non-parametric (series) estimation technique part of a general class of estimators called penalized estimators.","tags":null,"title":"Variable Selection","type":"book"},{"authors":null,"categories":null,"content":"# Setup from utils.lecture10 import * %matplotlib inline  Supervised vs Unsupervised Learning The difference between supervised learning and unsupervised learning is that in the first case we have a variable $y$ which we want to predict, given a set of variables ${ X_1, . . . , X_p }$.\nIn unsupervised learning are not interested in prediction, because we do not have an associated response variable $y$. Rather, the goal is to discover interesting properties about the measurements on ${ X_1, . . . , X_p }$.\nQuestions that we are usually interested in are\n Clustering Dimensionality reduction  In general, unsupervised learning can be viewed as an extention of exploratory data analysis.\nDimensionality Reduction Working in high-dimensional spaces can be undesirable for many reasons; raw data are often sparse as a consequence of the curse of dimensionality, and analyzing the data is usually computationally intractable (hard to control or deal with).\nDimensionality reduction can also be useful to plot high-dimensional data.\nClustering Clustering refers to a very broad set of techniques for finding subgroups, or clusters, in a data set. When we cluster the observations of a data set, we seek to partition them into distinct groups so that the observations within each group are quite similar to each other, while observations in different groups are quite different from each other.\nIn this section we focus on the following algorithms:\n K-means clustering Hierarchical clustering Gaussian Mixture Models  Principal Component Analysis Suppose that we wish to visualize $n$ observations with measurements on a set of $p$ features, ${X_1, . . . , X_p}$, as part of an exploratory data analysis.\nWe could do this by examining two-dimensional scatterplots of the data, each of which contains the n observations’ measurements on two of the features. However, there are $p(p−1)/2$ such scatterplots; for example, with $p = 10$ there are $45$ plots!\nPCA provides a tool to do just this. It finds a low-dimensional represen- tation of a data set that contains as much as possible of the variation.\nFirst Principal Component The first principal component of a set of features ${X_1, . . . , X_p}$ is the normalized linear combination of the features $Z_1$\n$$ Z_1 = \\phi_{11} X_1 + \\phi_{21} X_2 + \u0026hellip; + \\phi_{p1} X_p $$\nthat has the largest variance.\nBy normalized, we mean that $\\sum_{i=1}^p \\phi^2_{i1} = 1$.\nPCA Computation In other words, the first principal component loading vector solves the optimization problem\n$$ \\underset{\\phi_{11}, \\ldots, \\phi_{p 1}}{\\max} \\ \\Bigg \\lbrace \\frac{1}{n} \\sum _ {i=1}^{n}\\left(\\sum _ {j=1}^{p} \\phi _ {j1} x _ {ij} \\right)^{2} \\Bigg \\rbrace \\quad \\text { subject to } \\quad \\sum _ {j=1}^{p} \\phi _ {j1}^{2}=1 $$\nThe objective that we are maximizing is just the sample variance of the $n$ values of $z_{i1}$.\nAfter the first principal component $Z_1$ of the features has been determined, we can find the second principal component $Z_2$. The second principal component is the linear combination of ${X_1, . . . , X_p}$ that has maximal variance out of all linear combinations that are uncorrelated with $Z_1$.\nExample We illustrate the use of PCA on the USArrests data set.\nFor each of the 50 states in the United States, the data set contains the number of arrests per $100,000$ residents for each of three crimes: Assault, Murder, and Rape. We also record the percent of the population in each state living in urban areas, UrbanPop.\n# Load crime data df = pd.read_csv('data/USArrests.csv', index_col=0) df.head()   .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }  \n  Murder Assault UrbanPop Rape   State         Alabama 13.2 236 58 21.2   Alaska 10.0 263 48 44.5   Arizona 8.1 294 80 31.0   Arkansas 8.8 190 50 19.5   California 9.0 276 91 40.6     Data Scaling To make all the features comparable, we first need to scale them. In this case, we use the sklearn.preprocessing.scale() function to normalize each variable to have zero mean and unit variance.\n# Scale data X_scaled = pd.DataFrame(scale(df), index=df.index, columns=df.columns).values  We will see later what are the practical implications of (not) scaling.\nFitting Let\u0026rsquo;s fit PCA with 2 components.\n# Fit PCA with 2 components pca2 = PCA(n_components=2).fit(X_scaled)  # Get weights weights = pca2.components_.T df_weights = pd.DataFrame(weights, index=df.columns, columns=['PC1', 'PC2']) df_weights   .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }  \n  PC1 PC2     Murder 0.535899 0.418181   Assault 0.583184 0.187986   UrbanPop 0.278191 -0.872806   Rape 0.543432 -0.167319     Projecting the data What does the trasformed data looks like?\n# Transform X to get the principal components X_dim2 = pca2.transform(X_scaled) df_dim2 = pd.DataFrame(X_dim2, columns=['PC1', 'PC2'], index=df.index) df_dim2.head()   .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }  \n  PC1 PC2   State       Alabama 0.985566 1.133392   Alaska 1.950138 1.073213   Arizona 1.763164 -0.745957   Arkansas -0.141420 1.119797   California 2.523980 -1.542934     Visualization The advantage og PCA is that it allows us to see the variation in lower dimesions.\nmake_figure_10_1a(df_dim2, df_weights)  PCA and Spectral Analysis In case you haven\u0026rsquo;t noticed, calculating principal components, is equivalent to calculating the eigenvectors of the design matrix $X\u0026rsquo;X$, i.e. the variance-covariance matrix of $X$. Indeed what we performed above is a decomposition of the variance of $X$ into orthogonal components.\nThe constrained maximization problem above can be re-written in matrix notation as\n$$ \\max \\ \\phi' X\u0026rsquo;X \\phi \\quad \\text{ s. t. } \\quad \\phi'\\phi = 1 $$\nWhich has the following dual representation\n$$ \\mathcal L (\\phi, \\lambda) = \\phi' X\u0026rsquo;X \\phi - \\lambda (\\phi'\\phi - 1) $$\nIf we take the first order conditions\n$$ \\begin{align} \u0026amp; \\frac{\\partial \\mathcal L}{\\partial \\lambda} = \\phi'\\phi - 1 \\ \u0026amp; \\frac{\\partial \\mathcal L}{\\partial \\phi} = 2 X\u0026rsquo;X \\phi - 2 \\lambda \\phi \\end{align} $$\nSetting the derivatives to zero at the optimum, we get\n$$ \\begin{align} \u0026amp; \\phi'\\phi = 1 \\ \u0026amp; X\u0026rsquo;X \\phi = \\lambda \\phi \\end{align} $$\nThus, $\\phi$ is an eigenvector of the covariance matrix $X\u0026rsquo;X$, and the maximizing vector will be the one associated with the largest eigenvalue $\\lambda$.\nEigenvalues and eigenvectors We can now double-check it using numpy linear algebra package.\neigenval, eigenvec = np.linalg.eig(X_scaled.T @ X_scaled) data = np.concatenate((eigenvec,eigenval.reshape(1,-1))) idx = list(df.columns) + ['Eigenvalue'] df_eigen = pd.DataFrame(data, index=idx, columns=['PC1', 'PC2','PC3','PC4']) df_eigen   .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }  \n  PC1 PC2 PC3 PC4     Murder 0.535899 0.418181 0.649228 -0.341233   Assault 0.583184 0.187986 -0.743407 -0.268148   UrbanPop 0.278191 -0.872806 0.133878 -0.378016   Rape 0.543432 -0.167319 0.089024 0.817778   Eigenvalue 124.012079 49.488258 8.671504 17.828159     The spectral decomposition of the variance of $X$ generates a set of orthogonal vectors (eigenvectors) with different magnitudes (eigenvalues). The eigenvalues tell us the amount of variance of the data in that direction.\nIf we combine the eigenvectors together, we form a projection matrix $P$ that we can use to transform the original variables: $\\tilde X = P X$\nX_transformed = X_scaled @ eigenvec df_transformed = pd.DataFrame(X_transformed, index=df.index, columns=['PC1', 'PC2','PC3','PC4']) df_transformed.head()   .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }  \n  PC1 PC2 PC3 PC4   State         Alabama 0.985566 1.133392 0.156267 -0.444269   Alaska 1.950138 1.073213 -0.438583 2.040003   Arizona 1.763164 -0.745957 -0.834653 0.054781   Arkansas -0.141420 1.119797 -0.182811 0.114574   California 2.523980 -1.542934 -0.341996 0.598557     This is exactly the dataset that we obtained before.\nScaling the Variables The results obtained when we perform PCA will also depend on whether the variables have been individually scaled. In fact, the variance of a variable depends on its magnitude.\n# Variables variance df.var(axis=0)  Murder 18.970465 Assault 6945.165714 UrbanPop 209.518776 Rape 87.729159 dtype: float64  Consequently, if we perform PCA on the unscaled variables, then the first principal component loading vector will have a very large loading for Assault, since that variable has by far the highest variance.\n# Fit PCA with unscaled varaibles X = df.values pca2_u = PCA(n_components=2).fit(X)  # Get weights weights_u = pca2_u.components_.T df_weights_u = pd.DataFrame(weights_u, index=df.columns, columns=['PC1', 'PC2']) df_weights_u   .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }  \n  PC1 PC2     Murder 0.041704 0.044822   Assault 0.995221 0.058760   UrbanPop 0.046336 -0.976857   Rape 0.075156 -0.200718     # Transform X to get the principal components X_dim2_u = pca2_u.transform(X) df_dim2_u = pd.DataFrame(X_dim2_u, columns=['PC1', 'PC2'], index=df.index)  Plotting We can compare the lower dimensional representations with and without scaling.\nmake_figure_10_1b(df_dim2, df_dim2_u, df_weights, df_weights_u)  As predicted, the first principal component loading vector places almost all of its weight on Assault, while the second principal component loading vector places almost all of its weight on UrpanPop. Comparing this to the left-hand plot, we see that scaling does indeed have a substantial effect on the results obtained. However, this result is simply a consequence of the scales on which the variables were measured.\nThe Proportion of Variance Explained We can now ask a natural question: how much of the information in a given data set is lost by projecting the observations onto the first few principal components? That is, how much of the variance in the data is not contained in the first few principal components? More generally, we are interested in knowing the proportion of variance explained (PVE) by each principal component.\n# Four components pca4 = PCA(n_components=4).fit(X_scaled)  # Variance of the four principal components pca4.explained_variance_  array([2.53085875, 1.00996444, 0.36383998, 0.17696948])  Interpretation We can compute it in percentage of the total variance.\n# As a percentage of the total variance pca4.explained_variance_ratio_  array([0.62006039, 0.24744129, 0.0891408 , 0.04335752])  In the Arrest dataset, the first principal component explains $62.0%$ of the variance in the data, and the next principal component explains $24.7%$ of the variance. Together, the first two principal components explain almost $87%$ of the variance in the data, and the last two principal components explain only $13%$ of the variance.\nPlotting We can plot in a graph the percentage of the variance explained, relative to the number of components.\nmake_figure_10_2(pca4)  How Many Principal Components? In general, a $n \\times p$ data matrix $X$ has $\\min{n − 1, p}$ distinct principal components. However, we usually are not interested in all of them; rather, we would like to use just the first few principal components in order to visualize or interpret the data.\nWe typically decide on the number of principal components required to visualize the data by examining a scree plot.\nHowever, there is no well-accepted objective way to decide how many principal com- ponents are enough.\nK-Means Clustering The idea behind K-means clustering is that a good clustering is one for which the within-cluster variation is as small as possible. Hence we want to solve the problem\n$$ \\underset{C_{1}, \\ldots, C_{K}}{\\operatorname{minimize}} \\Bigg\\lbrace \\sum_{k=1}^{K} W\\left(C_{k}\\right) \\Bigg\\rbrace $$\nwhere $C_k$ is a cluster and $ W(C_k)$ is a measure of the amount by which the observations within a cluster differ from each other.\nThere are many possible ways to define this concept, but by far the most common choice involves squared Euclidean distance. That is, we define\n$$ W\\left(C_{k}\\right)=\\frac{1}{\\left|C_{k}\\right|} \\sum_{i, i^{\\prime} \\in C_{k}} \\sum_{j=1}^{p}\\left(x_{i j}-x_{i^{\\prime} j}\\right)^2 $$\nwhere $|C_k|$ denotes the number of observations in the $k^{th}$ cluster.\nAlgorithm   Randomly assign a number, from $1$ to $K$, to each of the observations. These serve as initial cluster assignments for the observations.\n  Iterate until the cluster assignments stop changing:\na) For each of the $K$ clusters, compute the cluster centroid. The kth cluster centroid is the vector of the $p$ feature means for the observations in the $k^{th}$ cluster.\nb) Assign each observation to the cluster whose centroid is closest (where closest is defined using Euclidean distance).\n  Generate the data We first generate a 2-dimensional dataset.\n# Simulate data np.random.seed(123) X = np.random.randn(50,2) X[0:25, 0] = X[0:25, 0] + 3 X[0:25, 1] = X[0:25, 1] - 4  make_new_figure_1(X)  Step 1: random assignement Now let\u0026rsquo;s randomly assign the data to two clusters, at random.\n# Init clusters K = 2 clusters0 = np.random.randint(K,size=(np.size(X,0)))  make_new_figure_2(X, clusters0)  Step 2: estimate distributions What are the new centroids?\n# Compute new centroids def compute_new_centroids(X, clusters): K = len(np.unique(clusters)) centroids = np.zeros((K,np.size(X,1))) for k in range(K): if sum(clusters==k)\u0026gt;0: centroids[k,:] = np.mean(X[clusters==k,:], axis=0) else: centroids[k,:] = np.mean(X, axis=0) return centroids  # Print centroids0 = compute_new_centroids(X, clusters0) print(centroids0)  [[ 1.54179703 -1.65922379] [ 1.67917325 -2.36272948]]  Plotting the centroids Let\u0026rsquo;s add the centroids to the graph.\n# Plot plot_assignment(X, centroids0, clusters0, 0, 0)  Step 3: assign data to clusters Now we can assign the data to the clusters, according to the closest centroid.\n# Assign X to clusters def assign_to_cluster(X, centroids): K = np.size(centroids,0) dist = np.zeros((np.size(X,0),K)) for k in range(K): dist[:,k] = np.mean((X - centroids[k,:])**2, axis=1) clusters = np.argmin(dist, axis=1) # Compute inertia inertia = 0 for k in range(K): if sum(clusters==k)\u0026gt;0: inertia += np.sum((X[clusters==k,:] - centroids[k,:])**2) return clusters, inertia  Plotting assigned data # Get cluster assignment [clusters1,d] = assign_to_cluster(X, centroids0)  # Plot plot_assignment(X, centroids0, clusters1, d, 1)  Full Algorithm We now have all the components to proceed iteratively.\ndef kmeans_manual(X, K): # Init i = 0 d0 = 1e4 d1 = 1e5 clusters = np.random.randint(K,size=(np.size(X,0))) # Iterate until convergence while np.abs(d0-d1) \u0026gt; 1e-10: d1 = d0 centroids = compute_new_centroids(X, clusters) [clusters, d0] = assign_to_cluster(X, centroids) plot_assignment(X, centroids, clusters, d0, i) i+=1  Plotting k-means clustering # Test kmeans_manual(X, K)  Here the observations can be easily plotted because they are two-dimensional. If there were more than two variables then we could instead perform PCA and plot the first two principal components score vectors.\nMore clusters In the previous example, we knew that there really were two clusters because we generated the data. However, for real data, in general we do not know the true number of clusters. We could instead have performed K-means clustering on this example with K = 3. If we do this, K-means clustering will split up the two \u0026ldquo;real\u0026rdquo; clusters, since it has no information about them:\n# K=3 kmeans_manual(X, 3)  Sklearn package The automated function in sklearn to persorm $K$-means clustering is KMeans.\n# SKlearn algorithm km1 = KMeans(n_clusters=3, n_init=1, random_state=1) km1.fit(X)  KMeans(n_clusters=3, n_init=1, random_state=1)  Plotting We can plot the asssignment generated by the KMeans function.\n# Plot plot_assignment(X, km1.cluster_centers_, km1.labels_, km1.inertia_, km1.n_iter_)  As we can see, the results are different in the two algorithms? Why? $K$-means is susceptible to the initial values. One way to solve this problem is to run the algorithm multiple times and report only the best results\nInitial Assignment To run the Kmeans() function in python with multiple initial cluster assignments, we use the n_init argument (default: 10). If a value of n_init greater than one is used, then K-means clustering will be performed using multiple random assignments, and the Kmeans() function will report only the best results.\n# 30 runs km_30run = KMeans(n_clusters=3, n_init=30, random_state=1).fit(X) plot_assignment(X, km_30run.cluster_centers_, km_30run.labels_, km_30run.inertia_, km_30run.n_iter_)  Best Practices It is generally recommended to always run K-means clustering with a large value of n_init, such as 20 or 50 to avoid getting stuck in an undesirable local optimum.\nWhen performing K-means clustering, in addition to using multiple initial cluster assignments, it is also important to set a random seed using the random_state parameter. This way, the initial cluster assignments can be replicated, and the K-means output will be fully reproducible.\nHierarchical Clustering One potential disadvantage of K-means clustering is that it requires us to pre-specify the number of clusters $K$.\nHierarchical clustering is an alternative approach which does not require that we commit to a particular choice of $K$.\nThe Dendogram Hierarchical clustering has an added advantage over K-means clustering in that it results in an attractive tree-based representation of the observations, called a dendrogram.\nd = dendrogram( linkage(X, \u0026quot;complete\u0026quot;), leaf_rotation=90., # rotates the x axis labels leaf_font_size=8., # font size for the x axis labels )  Interpretation Each leaf of the dendrogram represents one observation.\nAs we move up the tree, some leaves begin to fuse into branches. These correspond to observations that are similar to each other. As we move higher up the tree, branches themselves fuse, either with leaves or other branches. The earlier (lower in the tree) fusions occur, the more similar the groups of observations are to each other.\nWe can use de dendogram to understand how similar two observations are: we can look for the point in the tree where branches containing those two obse rvations are first fused. The height of this fusion, as measured on the vertical axis, indicates how different the two observations are. Thus, observations that fuse at the very bottom of the tree are quite similar to each other, whereas observations that fuse close to the top of the tree will tend to be quite different.\nThe term hierarchical refers to the fact that clusters obtained by cutting the dendrogram at a given height are necessarily nested within the clusters obtained by cutting the dendrogram at any greater height.\nThe Hierarchical Clustering Algorithm   Begin with $n$ observations and a measure (such as Euclidean distance) of all the $n(n − 1)/2$ pairwise dissimilarities. Treat each 2 observation as its own cluster.\n  For $i=n,n−1,\u0026hellip;,2$\na) Examine all pairwise inter-cluster dissimilarities among the $i$ clusters and identify the pair of clusters that are least dissimilar (that is, most similar). Fuse these two clusters. The dissimilarity between these two clusters indicates the height in the dendrogram at which the fusion should be placed.\nb) Compute the new pairwise inter-cluster dissimilarities among the $i−1$ remaining clusters.\n  The Linkage Function We have a concept of the dissimilarity between pairs of observations, but how do we define the dissimilarity between two clusters if one or both of the clusters contains multiple observations?\nThe concept of dissimilarity between a pair of observations needs to be extended to a pair of groups of observations. This extension is achieved by developing the notion of linkage, which defines the dissimilarity between two groups of observations.\nLinkages The four most common types of linkage are:\n Complete: Maximal intercluster dissimilarity. Compute all pairwise dissimilarities between the observations in cluster A and the observations in cluster B, and record the largest of these dissimilarities. Single: Minimal intercluster dissimilarity. Compute all pairwise dissimilarities between the observations in cluster A and the observations in cluster B, and record the smallest of these dissimilarities. Average: Mean intercluster dissimilarity. Compute all pairwise dissimilarities between the observations in cluster A and the observations in cluster B, and record the average of these dissimilarities. Centroid: Dissimilarity between the centroid for cluster A (a mean vector of length p) and the centroid for cluster B. Centroid linkage can result in undesirable inversions.  Average, complete, and single linkage are most popular among statisticians. Average and complete linkage are generally preferred over single linkage, as they tend to yield more balanced dendrograms. Centroid linkage is often used in genomics, but suffers from a major drawback in that an inversion can occur, whereby two clusters are fused at a height below either of the individual clusters in the dendrogram. This can lead to difficulties in visualization as well as in interpretation of the dendrogram.\n# Init linkages = [hierarchy.complete(X), hierarchy.average(X), hierarchy.single(X)] titles = ['Complete Linkage', 'Average Linkage', 'Single Linkage']  Plotting make_new_figure_4(linkages, titles)  For this data, both complete and average linkage generally separates the observations into their correct groups.\nGaussian Mixture Models Clustering methods such as hierarchical clustering and K-means are based on heuristics and rely primarily on finding clusters whose members are close to one another, as measured directly with the data (no probability model involved).\nGaussian Mixture Models assume that the data was generated by multiple multivariate gaussian distributions. The objective of the algorithm is to recover these latent distributions.\nThe advantages with respect to K-means are\n a structural interpretaion of the parameters automatically generates class probabilities can generate clusters of observations that are not necessarily close to each other  Algorithm   Randomly assign a number, from $1$ to $K$, to each of the observations. These serve as initial cluster assignments for the observations.\n  Iterate until the cluster assignments stop changing:\na) For each of the $K$ clusters, compute its mean and variance. The main difference with K-means is that we also compute the variance matrix.\nb) Assign each observation to its most likely cluster.\n  Dataset Let\u0026rsquo;s use the same data we have used for k-means, for a direct comparison.\nmake_new_figure_1(X)  Step 1: random assignement Let\u0026rsquo;s also use the same random assignment of the K-means algorithm.\nmake_new_figure_2(X, clusters0)  Step 2: compute distirbutions What are the new distributions?\n# Compute new centroids def compute_distributions(X, clusters): K = len(np.unique(clusters)) distr = [] for k in range(K): if sum(clusters==k)\u0026gt;0: distr += [multivariate_normal(np.mean(X[clusters==k,:], axis=0), np.cov(X[clusters==k,:].T))] else: distr += [multivariate_normal(np.mean(X, axis=0), np.cov(X.T))] return distr  # Print distr0 = compute_distributions(X, clusters0) print(\u0026quot;Mean of the first distribution: \\n\u0026quot;, distr0[0].mean) print(\u0026quot;\\nVariance of the first distribution: \\n\u0026quot;, distr0[0].cov)  Mean of the first distribution: [ 1.54179703 -1.65922379] Variance of the first distribution: [[ 3.7160256 -2.27290036] [-2.27290036 4.67223237]]  Plotting the distributions Let\u0026rsquo;s add the distributions to the graph.\n# Plot plot_assignment_gmm(X, clusters0, distr0, i=0, logL=0.0)  Likelihood The main difference with respect with K-means is that we can now compute the probability that each observation belongs to each cluster. This is the probability that each observation was generated by one of the two bi-variate normal distributions. These probabilities are called likelihoods.\n# Print first 5 likelihoods pdfs0 = np.stack([d.pdf(X) for d in distr0], axis=1) pdfs0[:5]  array([[0.03700522, 0.05086876], [0.00932081, 0.02117353], [0.04092453, 0.04480732], [0.00717854, 0.00835799], [0.01169199, 0.01847373]])  Step 3: assign data to clusters Now we can assign the data to the clusters, via maximum likelihood.\n# Assign X to clusters def assign_to_cluster_gmm(X, distr): pdfs = np.stack([d.pdf(X) for d in distr], axis=1) clusters = np.argmax(pdfs, axis=1) log_likelihood = 0 for k, pdf in enumerate(pdfs): log_likelihood += np.log(pdf[clusters[k]]) return clusters, log_likelihood  # Get cluster assignment clusters1, logL1 = assign_to_cluster_gmm(X, distr0)  Plotting assigned data # Compute new distributions distr1 = compute_distributions(X, clusters1)  # Plot plot_assignment_gmm(X, clusters1, distr1, 1, logL1);  Expectation - Maximization The two steps we have just seen, are part of a broader family of algorithms to maximize likelihoods called expectation-maximization algorithms.\nIn the expectation step, we computed the expectation of the parameters, given the current cluster assignment.\nIn the maximization step, we assigned observations to the cluster that maximized the likelihood of the single observation.\nThe alternative, and more computationally intensive procedure, would have been to specify a global likelihood function and find the mean and variance paramenters of the two normal distributions that maximized those likelihoods.\nFull Algorithm We can now deploy the full algorithm.\ndef gmm_manual(X, K): # Init i = 0 logL0 = 1e4 logL1 = 1e5 clusters = np.random.randint(K,size=(np.size(X,0))) # Iterate until convergence while np.abs(logL0-logL1) \u0026gt; 1e-10: logL1 = logL0 distr = compute_distributions(X, clusters) clusters, logL0 = assign_to_cluster_gmm(X, distr) plot_assignment_gmm(X, clusters, distr, i, logL0) i+=1  Plotting k-means clustering # Test gmm_manual(X, K)  In this case, GMM does a very poor job identifying the original clusters.\nOverlapping Clusters Let\u0026rsquo;s now try with a different dataset, where the data is drawn from two overlapping bi-variate gaussian distributions, forming a cross.\n# Simulate data X = np.random.randn(50,2) X[0:25, :] = np.random.multivariate_normal([0,0], [[50,0],[0,1]], size=25) X[25:, :] = np.random.multivariate_normal([0,0], [[1,0],[0,50]], size=25)  make_new_figure_1(X)  GMM with overlapping distributions # GMM gmm_manual(X, K)  As we can see, GMM is able to correctly recover the original clusters.\nK-means with overlapping distributions # K-means kmeans_manual(X, K)  K-means generates completely different clusters.\n","date":1646784000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1646784000,"objectID":"7ac490af7d8081e38d0778e24284cbbb","permalink":"https://matteocourthoud.github.io/course/ml-econ/10_unsupervised/","publishdate":"2022-03-09T00:00:00Z","relpermalink":"/course/ml-econ/10_unsupervised/","section":"course","summary":"# Setup from utils.lecture10 import * %matplotlib inline  Supervised vs Unsupervised Learning The difference between supervised learning and unsupervised learning is that in the first case we have a variable $y$ which we want to predict, given a set of variables ${ X_1, .","tags":null,"title":"Unsupervised Learning","type":"book"},{"authors":null,"categories":null,"content":"Intro In this session, I am going to cover demand estimation.\n Compute equilibrium outcomes with Logit demand Simulate a dataset Estimate Logit demand Compare different instruments Include supply  Model In this first part, we are going to assume that consumer $i \\in \\lbrace1,\u0026hellip;,I\\rbrace$ utility from good $j \\in \\lbrace1,\u0026hellip;,J\\rbrace$ in market $t \\in \\lbrace1,\u0026hellip;,T\\rbrace$ takes the form\n$$ u_{ijt} = \\boldsymbol x_{jt} \\boldsymbol \\beta_{it} - \\alpha p_{jt} + \\xi_{jt} + \\epsilon_{ijt} $$\nwhere\n $\\xi_{jt}$ is type-1 extreme value distributed $\\boldsymbol \\beta$ has dimension $K$  i.e. goods have $K$ characteristics    Setup We have $J$ firms and each product has $K$ characteristics\nJ = 3; # 3 firms == products K = 2; # 2 product characteristics c = rand(Uniform(0, 1), J); # Random uniform marginal costs ξ = rand(Normal(0, 1), J+1); # Random normal individual shocks X = rand(Exponential(1), J, K); # Random exponential product characteristics β = [.5, 2, -1]; # Preferences (last one is for prices, i.e. alpha)  Code Demand function demand(p::Vector, X::Matrix, β::Vector, ξ::Vector)::Tuple{Vector, Number} \u0026quot;\u0026quot;\u0026quot;Compute demand\u0026quot;\u0026quot;\u0026quot; δ = 1 .+ [X p] * β # Mean value u = [δ; 0] + ξ # Utility e = exp.(u) # Take exponential q = e ./ sum(e) # Compute demand return q[1:end-1], q[end] end;  We can try with an example.\np = 2 .* c; demand(p, X, β, ξ)  ## ([0.4120077746005573, 0.26650568009936, 0.24027826270165709], 0.08120828259842561)  Code Supply function profits(p::Vector, c::Vector, X::Matrix, β::Vector, ξ::Vector)::Vector \u0026quot;\u0026quot;\u0026quot;Compute profits\u0026quot;\u0026quot;\u0026quot; q, _ = demand(p, X, β, ξ) # Compute demand pr = (p - c) .* q # Compute profits return pr end;  We can try with an example.\nprofits(p, c, X, β, ξ)  ## 3-element Array{Float64,1}: ## 0.20289186252172428 ## 0.1596422305025479 ## 0.1270874470740512  Code Best Reply We first code the best reply of firm $j$\nfunction profits_j(pj::Number, j::Int, p::Vector, c::Vector, X::Matrix, β::Vector, ξ::Vector)::Number \u0026quot;\u0026quot;\u0026quot;Compute profits of firm j\u0026quot;\u0026quot;\u0026quot; p[j] = pj # Insert price of firm j pr = profits(p, c, X, β, ξ) # Compute profits return pr[j] end;  Let’s test it.\nj = 1; obj_fun(pj) = - profits_j(pj[1], j, copy(p), c, X, β, ξ); pj = optimize(x -\u0026gt; obj_fun(x), [1.0], LBFGS()).minimizer[1]  ## 1.8019637881982011  What are the implied profits now?\nprint(\u0026quot;Profits old: \u0026quot;, round.(profits(p, c, X, β, ξ), digits=4))  ## Profits old: [0.2029, 0.1596, 0.1271]  p_new = copy(p); p_new[j] = pj; print(\u0026quot;Profits new: \u0026quot;, round.(profits(p_new, c, X, β, ξ), digits=4))  ## Profits new: [0.3095, 0.2073, 0.1651]  Indeed firm 1 has increased its profits.\nCode Equilibrium We can now compute equilibrium prices\nfunction equilibrium(c::Vector, X::Matrix, β::Vector, ξ::Vector)::Vector \u0026quot;\u0026quot;\u0026quot;Compute equilibrium prices and profits\u0026quot;\u0026quot;\u0026quot; p = 2 .* c; dist = 1; iter = 0; # Until convergence while (dist \u0026gt; 1e-8) \u0026amp;\u0026amp; (iter\u0026lt;1000) # Compute best reply for each firm p1 = copy(p); for j=1:length(p) obj_fun(pj) = - profits_j(pj[1], j, p, c, X, β, ξ); optimize(x -\u0026gt; obj_fun(x), [1.0], LBFGS()).minimizer[1]; end # Update distance dist = max(abs.(p - p1)...); iter += 1; end return p end;  Code Equilibrium Let’s test it\n# Compute equilibrium prices p_eq = equilibrium(c, X, β, ξ); print(\u0026quot;Equilibrium prices: \u0026quot;, round.(p_eq, digits=4))  ## Equilibrium prices: [1.9764, 1.9602, 1.8366]  # And profits pi_eq = profits(p_eq, c, X, β, ξ); print(\u0026quot;Equilibrium profits: \u0026quot;, round.(pi_eq, digits=4))  ## Equilibrium profits: [0.484, 0.3612, 0.3077]  As expected the prices of the first 2 firms are lower and their profits are higher.\nDGP Let’s generate our Data Generating Process (DGP).\n $\\boldsymbol x \\sim exp(V_{x})$ $\\xi \\sim N(0, V_{\\xi})$ $w \\sim N(0, 1)$ $\\omega \\sim N(0, 1)$  function draw_data(J::Int, K::Int, rangeJ::Vector, varX::Number, varξ::Number)::Tuple \u0026quot;\u0026quot;\u0026quot;Draw data for one market\u0026quot;\u0026quot;\u0026quot; J_ = rand(rangeJ[1]:rangeJ[2]) # Number of firms (products) X_ = rand(Exponential(varX), J_, K) # Product characteristics ξ_ = rand(Normal(0, varξ), J_+1) # Product-level utility shocks w_ = rand(Uniform(0, 1), J_) # Cost shifters ω_ = rand(Uniform(0, 1), J_) # Cost shocks c_ = w_ + ω_ # Cost j_ = sort(sample(1:J, J_, replace=false)) # Subset of firms return X_, ξ_, w_, c_, j_ end;  Equilibrium We first compute the equilibrium in one market.\nfunction compute_mkt_eq(J::Int, b::Vector, rangeJ::Vector, varX::Number, varξ::Number)::DataFrame \u0026quot;\u0026quot;\u0026quot;Compute equilibrium one market\u0026quot;\u0026quot;\u0026quot; # Initialize variables K = size(β, 1) - 1 X_, ξ_, w_, c_, j_ = draw_data(J, K, rangeJ, varX, varξ) # Compute equilibrium p_ = equilibrium(c_, X_, β, ξ_) # Equilibrium prices q_, q0 = demand(p_, X_, β, ξ_) # Demand with shocks pr_ = (p_ - c_) .* q_ # Profits # Save to data q0_ = ones(length(j_)) .* q0 df = DataFrame(j=j_, w=w_, p=p_, q=q_, q0=q0_, pr=pr_) for k=1:K df[!,\u0026quot;x$k\u0026quot;] = X_[:,k] df[!,\u0026quot;z$k\u0026quot;] = sum(X_[:,k]) .- X_[:,k] end return df end;  Simulate Dataset We can now write the code to simulate the whole dataset.\nfunction simulate_data(J::Int, b::Vector, T::Int, rangeJ::Vector, varX::Number, varξ::Number) \u0026quot;\u0026quot;\u0026quot;Simulate full dataset\u0026quot;\u0026quot;\u0026quot; df = compute_mkt_eq(J, β, rangeJ, varX, varξ) df[!, \u0026quot;t\u0026quot;] = ones(nrow(df)) * 1 for t=2:T df_temp = compute_mkt_eq(J, β, rangeJ, varX, varξ) df_temp[!, \u0026quot;t\u0026quot;] = ones(nrow(df_temp)) * t append!(df, df_temp) end CSV.write(\u0026quot;../data/logit.csv\u0026quot;, df) end;  Simulate Dataset (2) We generate the dataset by simulating many markets that differ by\n number of firms (and their identity) their marginal costs their product characteristics  # Set parameters J = 10; # Number of firms K = 2; # Product caracteristics T = 500; # Markets β = [.5, 2, -1]; # Preferences rangeJ = [2, 6]; # Min and max firms per market varX = 1; # Variance of X varξ = 2; # Variance of xi # Simulate df = simulate_data(J, β, T, rangeJ, varX, varξ);  The Data What does the data look like? Let’s switch to R!\n# Read data df = fread(\u0026quot;../data/logit.csv\u0026quot;) kable(df[1:6,], digits=4)     j w p q q0 pr x1 z1 x2 z2 t     1 0.1491 1.9616 0.0932 0.0013 0.1028 2.4517 0.8219 1.6918 3.6779 1   2 0.8352 2.1112 0.0193 0.0013 0.0197 0.1328 3.1408 0.2075 5.1622 1   7 0.2749 2.2789 0.2710 0.0013 0.3717 0.1449 3.1287 1.1493 4.2205 1   10 0.4118 3.6386 0.6151 0.0013 1.5982 0.5442 2.7294 2.3212 3.0485 1   5 0.6071 2.1457 0.0886 0.0003 0.0972 0.9239 2.4182 3.6551 10.0474 2   6 0.1615 1.1626 0.0000 0.0003 0.0000 0.1763 3.1657 0.3629 13.3396 2    Estimation First we need to compute the dependent variable\ndf$y = log(df$q) - log(df$q0)  Now we can estimate the logit model. The true values are $alpha=1$.\nols \u0026lt;- lm(y ~ x1 + x2 + p, data=df) kable(tidy(ols), digits=4)     term estimate std.error statistic p.value     (Intercept) -1.3558 0.1476 -9.1874 0e+00   x1 0.4176 0.0537 7.7782 0e+00   x2 1.1494 0.0719 15.9903 0e+00   p 0.2406 0.0656 3.6664 3e-04    The estimate of $\\alpha = 1$ is biased (positive and significant) since $p$ is endogenous. We need instruments.\nIV 1: Cost Shifters First set of instruments: cost shifters.\nfm_costiv \u0026lt;- ivreg(y ~ x1 + x2 + p | x1 + x2 + w, data=df) kable(tidy(fm_costiv), digits=4)     term estimate std.error statistic p.value     (Intercept) 0.2698 0.4923 0.5480 0.5837   x1 0.5249 0.0643 8.1679 0.0000   x2 1.8178 0.2064 8.8059 0.0000   p -0.7034 0.2800 -2.5123 0.0121    Now the estimate of $\\alpha$ is negative and significant.\nIV 2: BLP Instruments Second set of instruments: product characteristics of other firms in the same market.\nfm_blpiv \u0026lt;- ivreg(y ~ x1 + x2 + p | x1 + x2 + z1 + z2, data=df) kable(tidy(fm_blpiv), digits=4)     term estimate std.error statistic p.value     (Intercept) 1.6616 0.5014 3.3139 9e-04   x1 0.6167 0.0698 8.8380 0e+00   x2 2.3901 0.2110 11.3279 0e+00   p -1.5117 0.2840 -5.3221 0e+00    Also the BLP instruments deliver an estimate of $\\alpha$ is negative and significant.\nAppendix References ","date":1636502400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1636502400,"objectID":"289c62cc59ec0e5e817ebc58bc3283ae","permalink":"https://matteocourthoud.github.io/course/empirical-io/11_logit_demand/","publishdate":"2021-11-10T00:00:00Z","relpermalink":"/course/empirical-io/11_logit_demand/","section":"course","summary":"Intro In this session, I am going to cover demand estimation.\n Compute equilibrium outcomes with Logit demand Simulate a dataset Estimate Logit demand Compare different instruments Include supply  Model In this first part, we are going to assume that consumer $i \\in \\lbrace1,\u0026hellip;,I\\rbrace$ utility from good $j \\in \\lbrace1,\u0026hellip;,J\\rbrace$ in market $t \\in \\lbrace1,\u0026hellip;,T\\rbrace$ takes the form","tags":null,"title":"Coding: Logit Demand","type":"book"},{"authors":null,"categories":null,"content":"Intro In this session, I am going to cover demand estimation.\n Compute equilibrium outcomes with RCL demand Simulate market-level data  Extremely similar to the logit demand simulation   Build the BLP estimator from Berry, Levinsohn, and Pakes (1995)  Model In this first part, we are going to assume that consumer $i \\in \\lbrace1,\u0026hellip;,I\\rbrace$ utility from good $j \\in \\lbrace1,\u0026hellip;,J\\rbrace$ in market $t \\in \\lbrace1,\u0026hellip;,T\\rbrace$ takes the form\n$$ u_{ijt} = \\boldsymbol x_{jt} \\boldsymbol \\beta_{it} - \\alpha p_{jt} + \\xi_{jt} + \\epsilon_{ijt} $$\nwhere\n $\\xi_{jt}$ is type-1 extreme value distributed $\\boldsymbol \\beta_{it}$: has dimension $K$ $$\\beta_{it}^k = \\beta_0^k + \\sigma_k \\zeta_{it}^k$$  $\\beta_0^k$: fixed taste for characteristic $k$ (the usual $\\beta$) $\\zeta_{it}^k$: random taste, i.i.d. across consumers and markets $t$    Setup We have $J$ firms and each product has $K$ characteristics.\ni = 100; # Number of consumers J = 10; # Number of firms K = 2; # Product characteristics T = 100; # Number of markets β = [.5, 2, -1]; # Preferences varζ = 5; # Variance of the random taste rangeJ = [2, 6]; # Min and max firms per market varX = 1; # Variance of X varξ = 2; # Variance of xi  Demand Demand is the main difference w.r.t. the logit model. Now we have individual shocks $\\zeta$ we have to integrate over.\nfunction demand(p::Vector, X::Matrix, β::Vector, ξ::Matrix, ζ::Matrix)::Tuple{Vector, Number} \u0026quot;\u0026quot;\u0026quot;Compute demand\u0026quot;\u0026quot;\u0026quot; δ = [X p] * (β .+ ζ) # Mean value δ0 = zeros(1, size(ζ, 2)) # Mean value of the outside option u = [δ; δ0] + ξ # Utility e = exp.(u) # Take exponential q = mean(e ./ sum(e, dims=1), dims=2) # Compute demand return q[1:end-1], q[end] end;  Supply Computing profits is instead exactly the same as before. We just have to save the shocks $\\zeta$ to be sure demand is stable.\nfunction profits(p::Vector, c::Vector, X::Matrix, β::Vector, ξ::Matrix, ζ::Matrix)::Vector \u0026quot;\u0026quot;\u0026quot;Compute profits\u0026quot;\u0026quot;\u0026quot; q, _ = demand(p, X, β, ξ, ζ) # Compute demand pr = (p - c) .* q # Compute profits return pr end;  function profits_j(pj::Number, j::Int, p::Vector, c::Vector, X::Matrix, β::Vector, ξ::Matrix, ζ::Matrix)::Number \u0026quot;\u0026quot;\u0026quot;Compute profits of firm j\u0026quot;\u0026quot;\u0026quot; p[j] = pj # Insert price of firm j pr = profits(p, c, X, β, ξ, ζ) # Compute profits return pr[j] end;  Equilibrium We can now compute the equilibrium for a specific market, as before.\nfunction equilibrium(c::Vector, X::Matrix, β::Vector, ξ::Matrix, ζ::Matrix)::Vector \u0026quot;\u0026quot;\u0026quot;Compute equilibrium prices and profits\u0026quot;\u0026quot;\u0026quot; p = 2 .* c; dist = 1; iter = 0; # Iterate until convergence while (dist \u0026gt; 1e-8) \u0026amp;\u0026amp; (iter\u0026lt;1000) # Compute best reply for each firm p_old = copy(p); for j=1:length(p) obj_fun(pj) = - profits_j(pj[1], j, p, c, X, β, ξ, ζ); optimize(x -\u0026gt; obj_fun(x), [1.0], LBFGS()); end # Update distance dist = max(abs.(p - p_old)...); iter += 1; end return p end;  Simulating Data We are now ready to simulate the data, i.e. equilibrium outcomes across different markets. We first draw all the variables.\nfunction draw_data(I::Int, J::Int, K::Int, rangeJ::Vector, varζ::Number, varX::Number, varξ::Number)::Tuple \u0026quot;\u0026quot;\u0026quot;Draw data for one market\u0026quot;\u0026quot;\u0026quot; J_ = rand(rangeJ[1]:rangeJ[2]) # Number of firms (products) X_ = rand(Exponential(varX), J_, K) # Product characteristics ξ_ = rand(Normal(0, varξ), J_+1, I) # Product-level utility shocks # Consumer-product-level preference shocks ζ_ = [rand(Normal(0,1), 1, I) * varζ; zeros(K,I)] w_ = rand(Uniform(0, 1), J_) # Cost shifters ω_ = rand(Uniform(0, 1), J_) # Cost shocks c_ = w_ + ω_ # Cost j_ = sort(sample(1:J, J_, replace=false)) # Subset of firms return X_, ξ_, ζ_, w_, c_, j_ end;  Simulating Data Then we simulate the data for one market.\nfunction compute_mkt_eq(I::Int, J::Int, β::Vector, rangeJ::Vector, varζ::Number, varX::Number, varξ::Number)::DataFrame \u0026quot;\u0026quot;\u0026quot;Compute equilibrium one market\u0026quot;\u0026quot;\u0026quot; # Initialize variables K = size(β, 1) - 1 X_, ξ_, ζ_, w_, c_, j_ = draw_data(I, J, K, rangeJ, varζ, varX, varξ) # Compute equilibrium p_ = equilibrium(c_, X_, β, ξ_, ζ_) # Equilibrium prices q_, q0 = demand(p_, X_, β, ξ_, ζ_) # Demand with shocks pr_ = (p_ - c_) .* q_ # Profits # Save to data q0_ = ones(length(j_)) .* q0 df = DataFrame(j=j_, w=w_, p=p_, q=q_, q0=q0_, pr=pr_) for k=1:K df[!,\u0026quot;x$k\u0026quot;] = X_[:,k] df[!,\u0026quot;z$k\u0026quot;] = sum(X_[:,k]) .- X_[:,k] end return df end;  Simultate the Data (2) We repeat for $T$ markets.\nfunction simulate_data(I::Int, J::Int, β::Vector, T::Int, rangeJ::Vector, varζ::Number, varX::Number, varξ::Number) \u0026quot;\u0026quot;\u0026quot;Simulate full dataset\u0026quot;\u0026quot;\u0026quot; df = compute_mkt_eq(I, J, β, rangeJ, varζ, varX, varξ) df[!, \u0026quot;t\u0026quot;] = ones(nrow(df)) * 1 for t=2:T df_temp = compute_mkt_eq(I, J, β, rangeJ, varζ, varX, varξ) df_temp[!, \u0026quot;t\u0026quot;] = ones(nrow(df_temp)) * t append!(df, df_temp) end CSV.write(\u0026quot;../data/blp.csv\u0026quot;, df) return df end;  Simulate the Data (3) Now let’s run the code\n# Simulate df = simulate_data(i, J, β, T, rangeJ, varζ, varX, varξ);  The Data What does the data look like? Let’s switch to R!\n# Read data df = fread(\u0026quot;../data/blp.csv\u0026quot;) kable(df[1:6,], digits=4)     j w p q q0 pr x1 z1 x2 z2 t     4 0.6481 3.5918 0.2929 0.4558 0.8165 0.6531 1.0002 1.7063 1.8207 1   7 0.9997 5.1926 0.2513 0.4558 0.8717 1.0002 0.6531 1.8207 1.7063 1   1 0.5842 2.6999 0.0800 0.3919 0.1572 0.3591 7.1958 0.4217 2.1996 2   4 0.5291 4.5934 0.1404 0.3919 0.4467 2.3801 5.1748 0.1283 2.4929 2   5 0.5012 4.4196 0.1368 0.3919 0.4461 2.2638 5.2911 0.4408 2.1804 2   8 0.9359 3.2923 0.0863 0.3919 0.1477 0.5182 7.0367 1.0271 1.5942 2    Estimation The BLP estimation procedure\nFrom deltas to shares First, we need to compute the shares implied by aspecific vector of $\\delta$s\nfunction implied_shares(Xt_::Matrix, ζt_::Matrix, δt_::Vector, δ0::Matrix)::Vector \u0026quot;\u0026quot;\u0026quot;Compute shares implied by deltas and shocks\u0026quot;\u0026quot;\u0026quot; u = [δt_ .+ (Xt_ * ζt_); δ0] # Utility e = exp.(u) # Take exponential q = mean(e ./ sum(e, dims=1), dims=2) # Compute demand return q[1:end-1] end;  Inner Loop We can now compute the inner loop and invert the demand function: from shares $q$ to $\\delta$s\nfunction inner_loop(qt_::Vector, Xt_::Matrix, ζt_::Matrix)::Vector \u0026quot;\u0026quot;\u0026quot;Solve the inner loop: compute delta, given the shares\u0026quot;\u0026quot;\u0026quot; δt_ = ones(size(qt_)) δ0 = zeros(1, size(ζt_, 2)) dist = 1 # Iterate until convergence while (dist \u0026gt; 1e-8) q = implied_shares(Xt_, ζt_, δt_, δ0) δt2_ = δt_ + log.(qt_) - log.(q) dist = max(abs.(δt2_ - δt_)...) δt_ = δt2_ end return δt_ end;  Compute Delta We can now repeat the inversion for every market and get the vector of mean utilities $\\delta$s from the observed market shares $q$.\nfunction compute_delta(q_::Vector, X_::Matrix, ζ_::Matrix, T::Vector)::Vector \u0026quot;\u0026quot;\u0026quot;Compute residuals\u0026quot;\u0026quot;\u0026quot; δ_ = zeros(size(T)) # Loop over each market for t in unique(T) qt_ = q_[T.==t] # Quantity in market t Xt_ = X_[T.==t,:] # Characteristics in mkt t δ_[T.==t] = inner_loop(qt_, Xt_, ζ_) # Solve inner loop end return δ_ end;  Compute Xi Now that we have $\\delta$, it is pretty straightforward to compute $\\xi$. We just need to perform a linear regression (with instruments) of mean utilities $\\delta$ on prices $p$ and product characteristics $X$ and compute the residuals $\\xi$.\nfunction compute_xi(X_::Matrix, IV_::Matrix, δ_::Vector)::Tuple \u0026quot;\u0026quot;\u0026quot;Compute residual, given delta (IV)\u0026quot;\u0026quot;\u0026quot; β_ = inv(IV_' * X_) * (IV_' * δ_) # Compute coefficients (IV) ξ_ = δ_ - X_ * β_ # Compute errors return ξ_, β_ end;  Objective Function We now have all the ingredients to set up the GMM objective function.\nfunction GMM(varζ_::Number)::Tuple \u0026quot;\u0026quot;\u0026quot;Compute GMM objective function\u0026quot;\u0026quot;\u0026quot; δ_ = compute_delta(q_, X_, ζ_ * varζ_, T) # Compute deltas ξ_, β_ = compute_xi(X_, IV_, δ_) # Compute residuals gmm = ξ_' * Z_ * Z_' * ξ_ / length(ξ_)^2 # Compute ortogonality condition return gmm, β_ end;  Estimation (1) First, we need to set up our objects\n# Retrieve data T = Int.(df.t) X_ = [df.x1 df.x2 df.p] q_ = df.q q0_ = df.q0 IV_ = [df.x1 df.x2 df.w] Z_ = [df.x1 df.x2 df.z1 df.z2]  Estimation (2) What would a logit regression estimate?\n# Compute logit estimate y = log.(df.q) - log.(df.q0); β_logit = inv(IV_' * X_) * (IV_' * y); print(\u0026quot;Estimated logit coefficients: $β_logit\u0026quot;)  ## Estimated logit coefficients: [2.063144844221613, 1.2888511782561123, -0.9824308271558686]  Estimation (3) We can now run the BLP machinery\n# Draw shocks (less) ζ_ = [rand(Normal(0,1), 1, i); zeros(K, i)]; # Minimize GMM objective function varζ_ = optimize(x -\u0026gt; GMM(x[1])[1], [2.0], LBFGS()).minimizer[1]; β_blp = GMM(varζ_)[2]; print(\u0026quot;Estimated BLP coefficients: $β_blp\u0026quot;)  ## Estimated BLP coefficients: [0.549234645269979, 1.1243451127088748, -0.6229637255651461]  Appendix References [references] Berry, Steven, James Levinsohn, and Ariel Pakes. 1995. “Automobile Prices in Market Equilibrium.” Econometrica: Journal of the Econometric Society, 841–90.\n  ","date":1635465600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1635465600,"objectID":"3c20ec0569d51ddc3466ae8054be35bd","permalink":"https://matteocourthoud.github.io/course/empirical-io/12_blp_1995/","publishdate":"2021-10-29T00:00:00Z","relpermalink":"/course/empirical-io/12_blp_1995/","section":"course","summary":"Intro In this session, I am going to cover demand estimation.\n Compute equilibrium outcomes with RCL demand Simulate market-level data  Extremely similar to the logit demand simulation   Build the BLP estimator from Berry, Levinsohn, and Pakes (1995)  Model In this first part, we are going to assume that consumer $i \\in \\lbrace1,\u0026hellip;,I\\rbrace$ utility from good $j \\in \\lbrace1,\u0026hellip;,J\\rbrace$ in market $t \\in \\lbrace1,\u0026hellip;,T\\rbrace$ takes the form","tags":null,"title":"Coding: BLP (1995)","type":"book"},{"authors":null,"categories":null,"content":"Setting From Rust (1988)\n  An agent owns a fleet to buses\n  Buses get old over time\n  The older the bus is, the most costly it is to maintain\n  The agent can decide to replace the bus engine with a new one, at a cost\n  Dynamic trade-off\n  What is the best moment to replace the engine?\n  You don’t want to replace an engine too early\n doesn’t change much    You don’t want to replace an engine too late\n avoid unnecessary maintenance costs      State   State: mileage of the bus\n$$s_t \\in \\lbrace 1, \u0026hellip;, 10 \\rbrace $$\n  State transitions: with probability $\\lambda$ the mileage of the bus increases\n$$ s_{t+1} = \\begin{cases} \\min \\lbrace s_t + 1,10 \\rbrace \u0026amp; \\text { with probability } \\lambda \\newline s_t \u0026amp; \\text { with probability } 1 - \\lambda \\end{cases} $$\nNote that $\\lambda$ does not depend on the value of the state\n  Actions   Action: replacement decision $$ a_t \\in \\lbrace 0, 1 \\rbrace $$\n  Payoffs\n  Per-period maintenance cost\n  Cost of replacement $$ u\\left(s_{t}, a_{t}, \\epsilon_{1 t}, \\epsilon_{2 t} ; \\theta\\right)= \\begin{cases} -\\theta_{1} s_{t}-\\theta_{2} s_{t}^{2}+\\epsilon_{0 t}, \u0026amp; \\text { if } a_{t}=0 \\newline -\\theta_{3} + \\epsilon_{1t}, \u0026amp; \\text { if } a_{t}=1 \\end{cases} $$\n    Solving the Model   Start with an initial expected value function $V(s_t)=0$\n  Compute the alternative-specific value function $$ \\bar V(s_t) = \\begin{cases} -\\theta_1 s_t - \\theta_2 s_t^2 + \\beta \\Big[(1-\\lambda) V(s_t) + \\lambda V(\\min \\lbrace s_t+1,10 \\rbrace ) \\Big] , \u0026amp; \\text { if } a_t=0 \\newline -\\theta_3 + \\beta \\Big[(1-\\lambda) V(0) + \\lambda V(1) \\Big] , \u0026amp; \\text { if } a_t=1 \\end{cases} $$\n  Compute the new expected value function $$ V'(a_t) = \\log \\Big( e^{\\bar V(a_t|s_t=0)} + e^{\\bar V(a_t|s_t=1)} \\Big) $$\n  Repeat until convergence\n  Code First we set the parameter values.\n# Set parameters θ = [0.13; -0.004; 3.1]; λ = 0.82; β = 0.95;  Then we set the state space.\n# State space k = 10; s = Vector(1:k);  Static Utility First, we can compute static utility. $$ u\\left(s_{t}, a_{t}, \\epsilon_{1 t}, \\epsilon_{2 t} ; \\theta\\right)= \\begin{cases} -\\theta_{1} s_{t}-\\theta_{2} s_{t}^{2}+\\epsilon_{0 t}, \u0026amp; \\text { if } a_{t}=0 \\newline -\\theta_{3} + \\epsilon_{1 t}, \u0026amp; \\text { if } a_{t}=1 \\end{cases} $$\nfunction compute_U(θ::Vector, s::Vector)::Matrix \u0026quot;\u0026quot;\u0026quot;Compute static utility\u0026quot;\u0026quot;\u0026quot; u1 = - θ[1]*s - θ[2]*s.^2 # Utility of not investing u2 = - θ[3]*ones(size(s)) # Utility of investing U = [u1 u2] # Combine in a matrix return U end;  Value Function We can now set up the value function iteration\nfunction compute_Vbar(θ::Vector, λ::Number, β::Number, s::Vector)::Matrix \u0026quot;\u0026quot;\u0026quot;Compute value function by Bellman iteration\u0026quot;\u0026quot;\u0026quot; k = length(s) # Dimension of the state space U = compute_U(θ, s) # Static utility index_λ = Int[1:k [2:k; k]]; # Mileage index index_A = Int[1:k ones(k,1)]; # Investment index γ = Base.MathConstants.eulergamma # Euler's gamma # Iterate the Bellman equation until convergence Vbar = zeros(k, 2); Vbar1 = Vbar; dist = 1; iter = 0; while dist\u0026gt;1e-8 V = γ .+ log.(sum(exp.(Vbar), dims=2)) # Compute value expV = V[index_λ] * [1-λ; λ] # Compute expected value Vbar1 = U + β * expV[index_A] # Compute v-specific dist = max(abs.(Vbar1 - Vbar)...); # Check distance iter += 1; Vbar = Vbar1 # Update value function end return Vbar end;  Solving the Model We can now solve for the value function.\n# Compute value function V_bar = compute_Vbar(θ, λ, β, s);  DGP Now that we know how to compute the equilibrium, we can simulate the data.\nfunction generate_data(θ::Vector, λ::Number, β::Number, s::Vector, N::Int)::Tuple \u0026quot;\u0026quot;\u0026quot;Generate data from primitives\u0026quot;\u0026quot;\u0026quot; Vbar = compute_Vbar(θ, λ, β, s) # Solve model ε = rand(Gumbel(0,1), N, 2) # Draw shocks St = rand(s, N) # Draw states A = (((Vbar[St,:] + ε) * [-1;1]) .\u0026gt; 0) # Compute investment decisions δ = (rand(Uniform(0,1), N) .\u0026lt; λ) # Compute mileage shock St1 = min.(St .* (A.==0) + δ, max(s...)) # Compute neSr state df = DataFrame(St=St, A=A, St1=St1) # Dataframe CSV.write(\u0026quot;../data/rust.csv\u0026quot;, df) return St, A, St1 end;  Generate the DAta We can now generate the data\n# Generate data N = Int(1e5); St, A, St1 = generate_data(θ, λ, β, s, N);  How many investment decisions do we observe?\nprint(\u0026quot;we observe \u0026quot;, sum(A), \u0026quot; investment decisions in \u0026quot;, N, \u0026quot; observations\u0026quot;)  ## we observe 19207 investment decisions in 100000 observations  The Data What does the data look like?\n# Read data df = fread(\u0026quot;../data/rust.csv\u0026quot;) kable(df[1:6,], digits=4)     St A St1     3 FALSE 4   9 TRUE 1   8 TRUE 1   3 FALSE 4   9 FALSE 10   10 FALSE 10    Estimation - Lambda   First we can estimate the value of lambda as the probability of mileage increase\n  Conditional on not investing\n  And not being in the last state (mileage cannot increase any more)\n$$ \\hat \\lambda = \\mathbb E_n \\Big[ (s_{t+1}-s_t) \\mid a_{t}=0 \\wedge s_{t}\u0026lt;10 \\Big] $$\n  # Estimate lambda Δ = St1 - St; λ_ = mean(Δ[(A.==0) .\u0026amp; (St.\u0026lt;10)]); print(\u0026quot;Estimated lambda: $λ_ (true = $λ)\u0026quot;)  ## Estimated lambda: 0.8206570869594549 (true = 0.82)  Estimation - Theta   Take a parameter guess $\\theta_0$\n  Compute the alternative-specific value function $\\bar V(s_t ; \\hat \\lambda, \\theta_0)$ by iteration\n  Compute the implied choice probabilities\n  Compute the likelihood $$ \\mathcal{L}(\\theta) = \\prod_{t=1}^{T}\\left(\\hat{\\operatorname{Pr}}\\left(a=1 \\mid s_{t}, \\theta\\right) \\mathbb{1}\\left(a_{t}=1\\right)+\\left(1-\\hat{\\operatorname{Pr}}\\left(a=0 \\mid s_{t}, \\theta\\right)\\right) \\mathbb{1}\\left(a_{t}=0\\right)\\right) $$\n  Repeat the above to find a minimum of the likelihood function\n  Likelihood Function function logL_Rust(θ0::Vector, λ::Number, β::Number, s::Vector, St::Vector, A::BitVector)::Number \u0026quot;\u0026quot;\u0026quot;Compute log-likelihood functionfor Rust problem\u0026quot;\u0026quot;\u0026quot; # Compute value Vbar = compute_Vbar(θ0, λ_, β, s) # Expected choice probabilities EP = exp.(Vbar[:,2]) ./ (exp.(Vbar[:,1]) + exp.(Vbar[:,2])) # Likelihood logL = sum(log.(EP[St[A.==1]])) + sum(log.(1 .- EP[St[A.==0]])) return -logL end;  We can check the likelihood at the true value:\n# True likelihood value logL_trueθ = logL_Rust(θ, λ, β, s, St, A); print(\u0026quot;The likelihood at the true parameter is $logL_trueθ\u0026quot;)  ## The likelihood at the true parameter is 45937.866092460084  Estimating Theta # Select starting values θ0 = Float64[0,0,0]; # Optimize θ_R = optimize(x -\u0026gt; logL_Rust(x, λ, β, s, St, A), θ0).minimizer; print(\u0026quot;Estimated thetas: $θ_R (true = $θ)\u0026quot;)  ## Estimated thetas: [0.12063838656559037, -0.003220197034620527, 3.0865668144650487] (true = [0.13, -0.004, 3.1])  Starting Values Starting values are important!\n# Not all initial values are equally good θ0 = Float64[1,1,1]; # Optimize θ_R2 = optimize(x -\u0026gt; logL_Rust(x, λ, β, s, St, A), θ0).minimizer; print(\u0026quot;Estimated thetas: $θ_R2 (true = $θ)\u0026quot;)  ## Estimated thetas: [1.0, 1.0, 1.0] (true = [0.13, -0.004, 3.1])  Hotz \u0026amp; Miller Recap Hotz \u0026amp; Miller estimation procedure works as follows\n  Estimate the CCPs from the data\n  Hotz \u0026amp; Miller inversion $$ \\hat V = \\Big[I - \\beta \\ \\sum_a P_a .* T_a \\Big]^{-1} \\ * \\ \\left( \\sum_a P_a \\ .* \\ \\bigg[ u_a + \\mathbb E [\\epsilon_a] \\bigg] \\right) $$\n  Compute EP from EV $$ \\hat \\Pr(a=1 ; \\theta) = \\frac{\\exp (u_1 +\\beta T_1 \\hat V )}{\\sum_{a} \\exp (u_a +\\beta T_a \\hat V )} $$\n  Compute the objective function: the (log)likelihood $$ \\mathcal{L}(\\theta) = \\prod_{t=1}^{T}\\left(\\hat{\\operatorname{Pr}}\\left(a=1 \\mid s_{t}; \\theta\\right) \\mathbb{1}\\left(a_{t}=1\\right)+\\left(1-\\hat{\\operatorname{Pr}}\\left(a=0 \\mid s_{t}; \\theta\\right)\\right) \\mathbb{1}\\left(a_{t}=0\\right)\\right) $$\n  CCPs First, we need to estimate the Conditional Choice Proabilities (CCP)\n can be done non-parametrically i.e. just look at the frequency of investment in each state  # Estimate CCP P = [mean(A[St.==i]) for i=s]; CCP = [(1 .- P) P]  ## 10×2 Array{Float64,2}: ## 0.952419 0.0475814 ## 0.923046 0.0769535 ## 0.894443 0.105557 ## 0.853306 0.146694 ## 0.819293 0.180707 ## 0.788935 0.211065 ## 0.747248 0.252752 ## 0.717915 0.282085 ## 0.6947 0.3053 ## 0.678452 0.321548  Transition Probabilities NeSr, we need $T$, the matrices of transition probabilities, conditional on the investment choice.\nfunction compute_T(k::Int, λ_::Number)::Array \u0026quot;\u0026quot;\u0026quot;Compute transition matrix\u0026quot;\u0026quot;\u0026quot; T = zeros(k, k, 2); # Conditional on not investing T[k,k,1] = 1; for i=1:k-1 T[i,i,1] = 1-λ_ T[i,i+1,1] = λ_ end # Conditional on investing T[:,1,2] .= 1-λ_; T[:,2,2] .= λ_; return(T) end;  T What form does the transition matrix $T$ take?\n# Compute T T = compute_T(k, λ_); # Conditional on not investing T[:,:,1]  ## 10×10 Array{Float64,2}: ## 0.179343 0.820657 0.0 0.0 … 0.0 0.0 0.0 ## 0.0 0.179343 0.820657 0.0 0.0 0.0 0.0 ## 0.0 0.0 0.179343 0.820657 0.0 0.0 0.0 ## 0.0 0.0 0.0 0.179343 0.0 0.0 0.0 ## 0.0 0.0 0.0 0.0 0.0 0.0 0.0 ## 0.0 0.0 0.0 0.0 … 0.0 0.0 0.0 ## 0.0 0.0 0.0 0.0 0.820657 0.0 0.0 ## 0.0 0.0 0.0 0.0 0.179343 0.820657 0.0 ## 0.0 0.0 0.0 0.0 0.0 0.179343 0.820657 ## 0.0 0.0 0.0 0.0 0.0 0.0 1.0  T (2) Instead, the transitions conditional on investing are\n# T Conditional on investing T[:,:,2]  ## 10×10 Array{Float64,2}: ## 0.179343 0.820657 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 ## 0.179343 0.820657 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 ## 0.179343 0.820657 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 ## 0.179343 0.820657 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 ## 0.179343 0.820657 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 ## 0.179343 0.820657 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 ## 0.179343 0.820657 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 ## 0.179343 0.820657 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 ## 0.179343 0.820657 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 ## 0.179343 0.820657 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0  Hotz \u0026amp; Miller Inversion We now have all the pieces to compute the expected value function $V$ through the Hotz \u0026amp; Miller inversion. $$ \\hat V = \\left[I - \\beta \\ \\sum_a P_a .* T_a \\right]^{-1} \\ * \\ \\left( \\sum_a P_a \\ .* \\ \\bigg[ u_a + \\mathbb E [\\epsilon_a] \\bigg] \\right) $$\nfunction HM_inversion(CCP::Matrix, T::Array, U::Matrix, β::Number)::Vector \u0026quot;\u0026quot;\u0026quot;Perform HM inversion\u0026quot;\u0026quot;\u0026quot; # Compute LHS (to be inverted) γ = Base.MathConstants.eulergamma LEFT = I - β .* (CCP[:,1] .* T[:,:,1] + CCP[:,2] .* T[:,:,2]) # Compute LHS (not to be inverted) RIGHT = γ .+ sum(CCP .* (U .- log.(CCP)) , dims=2) # Compute V EV_ = inv(LEFT) * RIGHT return vec(EV_) end;  From EV to EP We can now compute the expected policy function from the expected value function $$ \\hat \\Pr(a=1 ; \\theta) = \\frac{\\exp (u_1 +\\beta T_1 \\hat V )}{\\sum_{a} \\exp (u_a +\\beta T_a \\hat V )} $$\nfunction from_EV_to_EP(EV_::Vector, T::Array, U::Matrix, β::Number)::Vector \u0026quot;\u0026quot;\u0026quot;Compute expected policy from expected value\u0026quot;\u0026quot;\u0026quot; E = exp.( U + β .* [(T[:,:,1] * EV_) (T[:,:,2] * EV_)] ) EP_ = E[:,2] ./ sum(E, dims=2) return vec(EP_) end;  Likelihood We now have all the pieces to build the likelihood function $$ \\mathcal{L}(\\theta) = \\prod_{t=1}^{T} \\left(\\hat \\Pr \\left(a=1 \\mid s_{t}; \\theta\\right) \\mathbb{1} \\left(a_{t}=1\\right) + \\left(1-\\hat \\Pr \\left(a=0 \\mid s_{t}; \\theta\\right)\\right) \\mathbb{1} \\left(a_{t}=0\\right)\\right) $$\nfunction logL_HM(θ0::Vector, λ::Number, β::Number, s::Vector, St::Vector, A::BitVector, T::Array, CCP::Matrix)::Number \u0026quot;\u0026quot;\u0026quot;Compute log-likelihood function for HM problem\u0026quot;\u0026quot;\u0026quot; # Compute static utility U = compute_U(θ0, s) # Espected value by inversion EV_ = HM_inversion(CCP, T, U, β) # Implies choice probabilities EP_ = from_EV_to_EP(EV_, T, U, β) # Likelihood logL = sum(log.(EP_[St[A.==1]])) + sum(log.(1 .- EP_[St[A.==0]])) return -logL end;  Estimation We can now estimate the parameters\n# Optimize θ0 = Float64[0,0,0]; θ_HM = optimize(x -\u0026gt; logL_HM(x, λ, β, s, St, A, T, CCP), θ0).minimizer; print(\u0026quot;Estimated thetas: $θ_HM (true = $θ)\u0026quot;)  ## Estimated thetas: [0.12064911403839335, -0.003220614484856523, 3.086621855583483] (true = [0.13, -0.004, 3.1])  Aguirregabiria, Mira (2002) With Hotz and Miller, we have generated a mapping of the form\n$$ \\bar P(\\cdot ; \\theta) = g(h(\\hat P(\\cdot) ; \\theta); \\theta) $$\nAguirregabiria and Mira (2002): why don’t we iterate it?\nAM Likelihood Function The likelihood function in Aguirregabiria and Mira (2002) is extremely similar to Hotz and Miller (1993)\nfunction logL_AM(θ0::Vector, λ::Number, β::Number, s::Vector, St::Vector, A::BitVector, T::Array, CCP::Matrix, K::Int)::Number \u0026quot;\u0026quot;\u0026quot;Compute log-likelihood function for AM problem\u0026quot;\u0026quot;\u0026quot; # Compute static utility U = compute_U(θ0, s) EP_ = CCP[:,2] # Iterate HM mapping for _=1:K EV_ = HM_inversion(CCP, T, U, β) # Expected value by inversion EP_ = from_EV_to_EP(EV_, T, U, β) # Implies choice probabilities CCP = [(1 .- EP_) EP_] end # Likelihood logL = sum(log.(EP_[St[A.==1]])) + sum(log.(1 .- EP_[St[A.==0]])) return -logL end;  Estimation We can now estimate the parameters\n# Set number of iterations K = 2; # Optimize θ0 = Float64[0,0,0]; θ_AM = optimize(x -\u0026gt; logL_AM(x, λ, β, s, St, A, T, CCP, K), θ0).minimizer; print(\u0026quot;Estimated thetas: $θ_AM (true = $θ)\u0026quot;)  ## Estimated thetas: [0.12063890836521114, -0.0032202282942220464, 3.086571461772538] (true = [0.13, -0.004, 3.1])  Not much changes in our case.\nSpeed We can compare the methods in terms of speed.\n# Compare speed θ0 = Float64[0,0,0]; optimize(x -\u0026gt; logL_Rust(x, λ, β, s, St, A), θ0).time_run  ## 0.5477378368377686  optimize(x -\u0026gt; logL_HM(x, λ, β, s, St, A, T, CCP), θ0).time_run  ## 0.3244161605834961  optimize(x -\u0026gt; logL_AM(x, λ, β, s, St, A, T, CCP, K), θ0).time_run  ## 0.35499119758605957  Even in this simple example with a very small state space, the difference is significant.\nAppendix References [references] Aguirregabiria, Victor, and Pedro Mira. 2002. “Swapping the Nested Fixed Point Algorithm: A Class of Estimators for Discrete Markov Decision Models.” Econometrica 70 (4): 1519–43.\n Hotz, V Joseph, and Robert A Miller. 1993. “Conditional Choice Probabilities and the Estimation of Dynamic Models.” The Review of Economic Studies 60 (3): 497–529.\n Rust, John. 1988. “Maximum Likelihood Estimation of Discrete Control Processes.” SIAM Journal on Control and Optimization 26 (5): 1006–24.\n  ","date":1635465600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1635465600,"objectID":"77163084f4a11a31c955652b8755f93f","permalink":"https://matteocourthoud.github.io/course/empirical-io/17_rust_1987/","publishdate":"2021-10-29T00:00:00Z","relpermalink":"/course/empirical-io/17_rust_1987/","section":"course","summary":"Setting From Rust (1988)\n  An agent owns a fleet to buses\n  Buses get old over time\n  The older the bus is, the most costly it is to maintain","tags":null,"title":"Coding: Rust (1987)","type":"book"},{"authors":null,"categories":null,"content":"In the decade preceding the Second World War, there was a massive increase in industrial production of war materials, so there was a need to ensure that products, especially munitions, were reliable. The testing of war materials is not only expensive but also destructive since, for example, bullets need to be fired in order to be tested.\nTherefore, the U.S. government was presented with the following dilemma: how many bullets should one fire out of a batch before declaring the batch reliable? Clearly, if we were to fire all the bullets, we would know the exact amount of functioning bullets in a crate. However, there would be no bullets left to use.\nBecause of the growing relevance of these statistical problems, in 1939, a group of prominent statisticians and economists joined forces at Columbia University\u0026rsquo;s Statistical Research Group (SGR). The group included, among others, W. Allen Wallis, Jacob Wolfowitz and Abraham Wald. According to Wallis himself the SGR group was \u0026ldquo;composed of what surely must be the most extraordinary group of statisticians ever organized, taking into account both number and quality\u0026quot;[2].\nTheir work was of first order importance and classified, to the point that Wallis reports:\n It is said that as Wald worked on sequential analysis his pages were snatched away and given a security classification. Being still an \u0026ldquo;enemy alien\u0026rdquo;, he did not have a security clearance so, the story has it, he was not allowed to know of his results. [Wallis (1980)]\n Indeed, the group worked under the pressure from the U.S. Army to deliver fast practical solutions that could be readily deployed on the field. For example, Wallis reports that\n during the Battle of the Bulge in December 1944, several high-ranking Army officers flew to Washington from the battle, spent a day discussing the best settings on proximity fuzes for air bursts of artillery shells against ground troops, and flew back to the battle to put into effect advice from, among others, Milton Friedman, whose earlier studies of the fuzes had given him extensive and accurate knowledge of the way the fuzes actually performed. [Wallis (1980)]\n The most prominent result that came out of the SGR experience was undoubtedly the Sequential Probability Ratio Test. The idea first came to Wallis and Friedman that realized that\n it might pay to use a test which would not be as efficient as the classical tests if a sample of exactly N were to be taken, but which would more than offset this disadvantage by providing a good chance of terminating early when used sequentially. [Wallis (1980)]\n The two economists exposed the idea to the statistician Jacob Wolfowitz who initially\n seemed to be something distasteful about the idea of people so ignorant of mathematics as Milton and I venturing to meddle with such sacred ideas as those of most powerful statistics, etc. No doubt this antipathy was strengthened by our calling the new tests \u0026ldquo;supercolossal\u0026rdquo; on the grounds that they are more powerful than \u0026ldquo;most powerful\u0026rdquo; tests. [Wallis (1980)]\n Ultimately, the two economists managed to draw the attention of both Wolfowitz and Wald that started to formally work on the idea. The results remained top secret until the end of the war when Wald published his Sequential Tests of Statistical Hypotheses article.\nIn this post, after a quick introduction to hypothesis testing, we are going to explore the Sequential Probability Ratio Test and implement it in Python.\nHypothesis Testing When we design an A/B test or, more generally, an experiment, the standard steps are the following\n  Define a null hypothesis $H_0$, usually a zero effect of the experiment on a metric of interest\n for example, no effect of a drug on mortality    Define a significance level $\\alpha$, usually equal to 0.05, it represents the maximum probability of rejecting the null hypothesis when it is true\n for example, the probability of claiming that the drug is effective in reducing mortality, when it\u0026rsquo;s not effective    Define an alternative hypothesis $H_1$, usually the minimum effect size that we would like to detect\n for example, a decrease in mortality by 1%    Define a power level $1-\\beta$, usually equal to 0.8 ($\\beta=0.2$), it represents the minimum probability of rejecting the null hypothesis $H_0$, when the alternative $H_1$ is true\n for example, the probability of claiming that the drug is ineffective, when it\u0026rsquo;s effective    Pick a test statistic whose distribution is known under both hypotheses, usually the sample average of the metric of interest\n for example, the average mortality rate of patients    Compute the minimum sample size, in order to achieve the desired power level $1-\\beta$, given all the test parameters\n  Then, we run the test and, depending on the realized value of the test statistic, we decide whether to reject the null hypothesis or not. In particular, we reject the null hypothesis if the p-value, i.e. the probability of observing under the null hypothesis a statistic as or more extreme than the sample statistic, is lower than the significance level $\\alpha$.\nRemember that rejecting the null hypothesis does not imply accepting the alternative hypothesis.\nPeeking Suppose that halfway through the experiment we were to peek at the data and notice that, for that intermediate value of the test statistic, we would reject the null hypothesis. Should we stop the experiment? If we do, what happens?\nThe answer is that we should not stop the experiment. If we do, the test would not achieve the desired significance level or, in other terms, our confidence intervals would have the wrong coverage.\nLet\u0026rsquo;s see what I mean with an example. Suppose our data generating process is a standard normal distribution with unknown mean $\\mu$ and known variance $\\sigma=1$: $X \\sim N(\\mu,1)$.\nThe hypothesis that we wish to test is\n$$ \\begin{align} H_0: \\quad \u0026amp; \\mu = 0 \\newline H_1: \\quad \u0026amp; \\mu = 0.1 \\end{align} $$\nAfter each observation $n$, we compute the z test statistic\n$$ z = \\frac{\\bar X_n - \\mu_0}{\\frac{\\sigma}{\\sqrt{n}}} = \\frac{\\bar X_n - 0}{\\frac{1}{\\sqrt{n}}} = \\bar X_n * \\sqrt{n} $$\nwhere $\\bar X_n$ is the sample mean from a sample $X_1, X_2, \u0026hellip;, X_n$, of size $n$, $\\sigma$ is the standard deviation of the population, and $\\mu_0$ is the population mean, under the null hypothesis. The term in the denominator, $\\frac{\\sigma}{\\sqrt{n}}$, is the variance of the sample mean. Under the null hypothesis of zero mean, the test statistic is distributed as a standard normal distribution with zero mean and unit variance, $N(0,1)$.\nLet\u0026rsquo;s code the test in Python. I import some code from utils to make the plots prettier.\n%matplotlib inline %config InlineBackend.figure_format = 'retina'  from src.utils import * zstat = lambda x: np.mean(x) * np.sqrt(len(x)) zstat.__name__ = 'z-statistic'  Suppose we want a test with significance level $\\alpha=0.05$ and power $1-\\beta=0.8$. What sample size $n$ do we need?\nWe need a sample size such that\n The probability of rejecting the null hypothesis $H_0$, when $H_0$ is true, is at most $\\alpha=0.05$ The probability of not rejecting the null hypothesis $H_0$, when $H_0$ is false (i.e. $H_1$ is true), is at most $\\beta=0.2$  I.e. we need to find a critical value $c$ such that\n $c = \\mu_0 + z_{0.95} * \\frac{\\sigma}{\\sqrt{n}}$ $c = \\mu_1 - z_{0.8} * \\frac{\\sigma}{\\sqrt{n}}$  where $z_{p}$ is the CDF inverse (or percent point function) at $p$, and $\\mu_i$ are the values of the mean under the different hypotheses.\nCombining the two expressions together we can solve for the required minimum sample size.\n$$ n : \\mu_0 + z_{0.95} * \\frac{\\sigma}{\\sqrt{n}} = \\mu_1 - z_{0.8} * \\frac{\\sigma}{\\sqrt{n}} $$\nso that\n$$ n = \\left( \\sigma * \\frac{z_{0.95} + z_{0.8}}{\\mu_0 + \\mu_1} \\right)^2 = \\left( 1 * \\frac{1.64 + 0.84}{0 + 0.1} \\right)^2 = 618.25 $$\nfrom scipy.stats import norm n = ( (norm.ppf(0.95) + norm.ppf(0.8)) / 0.1 )**2 print(f\u0026quot;Minimum sample size: {n}\u0026quot;)  Minimum sample size: 618.2557232019765  We need at least 619 observations.\nWe can get a better intuition by graphically plotting the two distributions with the critical value. I wrote a function plot_test to draw a standard hypothesis testing setting.\nfrom src.figures import plot_test plot_test(mu0=0, mu1=0.1, alpha=0.05, n=n)  The critical value is such that, given the distributions under the two hypothesis, the rejection area in red is equal to $\\alpha$. The sample size $n$ is such that it shrinks the variance of the two distributions so that the area in green is equal to $\\beta$.\nLet\u0026rsquo;s now simulate an experiment in which we draw an ordered sequence of observations and, after each observation, we compute the value of the test statistic.\ndef experiment(f_stat, mu=0, n=619, seed=1): np.random.seed(seed) # Set seed I = np.arange(1, n+1) # Observation index x = np.random.normal(mu, 1, n) # Observation value stat = [f_stat(x[:i]) for i in I] # Value of the test statistic so far df = pd.DataFrame({'i': I, 'x': x, f_stat.__name__: stat}) # Generate dataframe return df  Let\u0026rsquo;s have a look at what a sample looks like.\ndf = experiment(zstat) df.head()   .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }  \n  i x z-statistic     0 1 1.624345 1.624345   1 2 -0.611756 0.716009   2 3 -0.528172 0.279678   3 4 -1.072969 -0.294276   4 5 0.865408 0.123814     We can now plot the time trend of the test statistic as we accumulate observations during the sampling process. I also mark with horizontal lines the values for rejection of the null hypothesis of a test with $\\alpha = 0.05$: $z_{0.025} = -1.96$ and $z_{0.975} = 1.96$.\ndef plot_experiment(df, ybounds, **kwargs): sns.lineplot(data=df, x='i', y=df.columns[2], **kwargs) for ybound in ybounds: sns.lineplot(x=df['i'], y=ybound, lw=1.5, color='black') plt.title(f'{df.columns[2]} with sequential sampling') plt.yticks([0, ybounds[0], ybounds[1]])  plot_experiment(df, ybounds=[-1.96, 1.96])  In this case, the test never crosses the critical values. Therefore, peeking does not have an effect. We would not have stopped the experiment prematurely.\nWhat would happen if we were repeating the experiment many times? Since the data is generated under the null hypothesis, $H_0: \\mu = 0$, we expect to reject it only $\\alpha=5%$ of the times.\nLet\u0026rsquo;s simulate the data-generating process $K=100$ times.\ndef simulate_experiments(f_stat, ybounds, xmin=0, early_stop=False, mu=0, K=100, n=619, **kwargs): # Count experiment durations stops = np.zeros(K) * n # Perform K simulations for k in range(K): # Draw data df = experiment(f_stat, mu=mu, seed=k, n=n) vals = df[f_stat.__name__].values # If early stop, check early violations (during sampling) if early_stop: violations = (vals[xmin:] \u0026gt; max(ybounds)) + (vals[xmin:] \u0026lt; min(ybounds)) if early_stop and any(violations): end = 1 + xmin + np.where(violations)[0][0] plot_experiment(df.iloc[:end, :], ybounds, **kwargs) stops[k] = end * np.sign(df[f_stat.__name__].values[end]) # Otherwise, only check violations of last value elif (vals[-1] \u0026gt; max(ybounds)) or (vals[-1] \u0026lt; min(ybounds)): plot_experiment(df, ybounds, **kwargs) stops[k] = len(df) * np.sign(vals[-1]) # Plot all other observations in grey else: plot_experiment(df, ybounds, color='grey', alpha=0.1, lw=1) # Print diagnostics pct_up = sum(stops\u0026gt;0)/sum(stops!=0)*100 print(f'Bounds crossed: {sum(stops!=0)} ({pct_up:.0f}% upper, {100-pct_up:.0f}% lower)') print(f'Average experiment duration: {(sum(np.abs(stops)) + n*sum(stops==0))/ len(stops) :.0f}')  We plot the distribution of the z-statistic over samples .\nsimulate_experiments(zstat, ybounds=[-1.96, 1.96], early_stop=False);  Bounds crossed: 3 (33% upper, 67% lower) Average experiment duration: 619  In the figure above, I have highlighted the experiments for which we reject the null hypothesis without peeking, i.e. given the value of the z test statistic at the end of the sampling process. Only in 3 experiments the final value lies outside the critical values, so that we reject the null hypothesis. This means a rejection rate of 3% which is very close to the expected rejection rate of $\\alpha=0.05$ (under the null).\nWhat if instead we were impatient and, after collecting the first 100 observations, we were stopping as soon as we saw the z-statistic crossing the boundaries?\nstops_zstat_h0 = simulate_experiments(zstsat, xmin=99, ybounds=[-1.96, 1.96], early_stop=True, lw=2); plt.vlines(100, ymin=plt.ylim()[0], ymax=plt.ylim()[1], color='k', lw=1, ls='--');  Bounds crossed: 25 (48% upper, 52% lower) Average experiment duration: 523  In the figure above, I have highlighted the experiments in which the values of the z-statistic crosses one of the boundaries, from the 100th observation onwards. This happens in 25 simulations out of 100, which implies a rejection rate of 25%, which is very far from the expected rejection rate of $\\alpha=0.05$ (under the null hypothesis). Peaking distorts the significance level of the test.\nPotential solutions are:\n sequential probability ratio tests sequential triangular testing group sequential testing  Before analyzing these sequential testing procedures, we first need to introduce the likelihood ratio test.\nLikelihood Ratio Test The likelihood ratio test is a test that tries to assess the likelihood that the observed data was generated by either one of two competing statistical models. In order to perform the likelihood ratio test for hypothesis testing, we need the data generating process to be fully specified under both hypotheses. For example, this would be the case with the following hypotheses:\n$$ \\begin{align} H_0: \\quad \u0026amp; \\mu=0 \\newline H_1: \\quad \u0026amp; \\mu=0.1 \\end{align} $$\nIn this case, we say that the statistical test is fully specified. If the alternative hypothesis was $H_1: \\mu \\neq 0$, then the data generating process would not be specified under the alternative hypothesis. When a statistical test is fully specified, we can compute the likelihood ratio as the the ratio of the likelihood function under the two hypotheses.\n$$ \\Lambda (X) = \\frac{\\mathcal L (\\theta_1 \\ | \\ X)}{\\mathcal L (\\theta_0 \\ | \\ X)} $$\nThe likelihood-ratio test provides a decision rule as follows:\n If $\\Lambda\u0026gt;c$, reject $H_{0}$; If $\\Lambda\u0026lt;c$, do not reject $H_{0}$; If $\\Lambda =c$, reject with probability $q$  The values $c$ and $q$ are usually chosen to obtain a specified significance level $\\alpha$.\nThe Neyman–Pearson lemma states that this likelihood-ratio test is the most powerful among all level $\\alpha$ tests for this case.\nSpecial Case: testing mean of normal distribution Let\u0026rsquo;s go back to our example where data is coming from a normal distribution with unknown mean $\\mu$ and known variance $\\sigma^2$ and we want to perform the following test\n$$ \\begin{align} H_0: \\quad \u0026amp; \\mu = 0 , \\newline H_1: \\quad \u0026amp; \\mu = 0.1 \\end{align} $$\nThe likelihood of the normal distribution with unknown mean $\\mu$ and known variance $\\sigma^2$ is\n$$ \\mathcal L(\\mu) = \\left( \\frac{1}{\\sqrt{2 \\pi} \\sigma } \\right)^n e^{- \\sum_{i=1}^{n} \\frac{(X_i - \\mu)^2}{2 \\sigma^2}} $$\nSo that the likelihood ratio under the two hypotheses is\n$$ \\Lambda(X) = \\frac{\\mathcal L (0.1, \\sigma^2)}{\\mathcal L (0, \\sigma^2)} = \\frac{e^{- \\sum_{i=1}^{n} \\frac{(X_i - 0.1)^2}{2 \\sigma^2}}}{e^{- \\sum_{i=1}^{n} \\frac{(X_i)^2}{2 \\sigma^2}}} $$\nWe now have all the ingredients to move on to the final purpose of this blog post: the Sequential Probability Ratio Test.\nSequential Probability Ratio Test Given a pair of fully specified hypotheses, say $H_{0}$ and $H_{1}$, the first step of the sequential probability ratio test is to calculate the log-likelihood ratio test $\\log (\\Lambda_{i})$, as new data arrive: with $S_{0}=0$, then, for $i=1,2,\u0026hellip;,$\n$$ S_{i} = S_{i-1} + \\log(\\Lambda_{i}) $$\nThe stopping rule is a simple thresholding scheme:\n $S_{i}\\geq b$: Accept $H_{1}$ $S_{i}\\leq a$: Accept $H_{0}$ $a\u0026lt;S_{i}\u0026lt;b$: continue monitoring (critical inequality)  where $a$ and $b$ ($-\\infty\u0026lt;a\u0026lt;0\u0026lt;b\u0026lt;\\infty$) depend on the desired type I and type II errors, $\\alpha$ and $\\beta$.\nWald (1945) shows that the choice of the following boundaries delivers a test with expected probability of type 1 and 2 error not greater than $\\alpha$ and $\\beta$, respectively.\n$$ a \\approx \\log {\\frac {\\beta }{1-\\alpha }} \\quad \\text{and} \\quad b \\approx \\log {\\frac {1-\\beta }{\\alpha }} $$\nThe equations are approximations because of the discrete nature of the data generating process.\nWald and Wolfowitz (1948) have proven that a test with these boundaries is the most powerful sequential probability ratio test, i.e. all SPR tests with the same power and significance require at least the same amount of observations.\nSpecial Case: testing null effect Let\u0026rsquo;s go back to our example where data is coming from a normal distribution with unknown mean $\\mu$ and known variance $\\sigma^2$ and hypotheses $H_0: \\ \\mu = 0$ and $H_1: \\ \\mu = 0.1$.\nWe have seen that the likelihood ratio with a sample of size $n$ is\n$$ \\Lambda(X) = \\frac{\\mathcal L (0.1, \\sigma^2)}{\\mathcal L (0, \\sigma^2)} = \\frac{e^{- \\sum_{i=1}^{n} \\frac{(X_i - 0.1)^2}{2 \\sigma^2}}}{e^{- \\sum_{i=1}^{n} \\frac{(X_i)^2}{2 \\sigma^2}}} $$\nTherefore, the log-likelihood (easier to compute) is\n$$ \\log (\\Lambda(X)) = \\left( \\sum_{i=1}^{n} \\frac{(X_i)^2}{2 \\sigma^2} \\right) - \\left( \\sum_{i=1}^{n} \\frac{(X_i - 0.1)^2}{2 \\sigma^2} \\right) $$\nSimulation We are now ready to perform some simulations. First, let\u0026rsquo;s code the log likelihood ratio test statistic that we have just computed.\nlog_lr = lambda x: (np.sum((x)**2) - np.sum((x-0.1)**2) ) / 2 log_lr.__name__ = 'log likelihood-ratio'  We now repeat the same experiment we did at the beginning, with one difference: we will compute the log likelihood ratio as a statistic. The data generating process has $\\mu=0$, as under the null hypothesis.\ndf = experiment(log_lr, ) df.head()   .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }  \n  i x log likelihood-ratio     0 1 1.624345 0.157435   1 2 -0.611756 0.091259   2 3 -0.528172 0.033442   3 4 -1.072969 -0.078855   4 5 0.865408 0.002686     Let\u0026rsquo;s now compute the optimal bounds, given significance level $\\alpha=0.05$ and power $1-\\beta=0.8$.\nalpha = 0.05 beta = 0.2 a = np.log( beta / (1-alpha) ) b = np.log( (1-beta) / alpha ) print(f'Optimal bounds : [{a:.3f}, {b:.3f}]')  Optimal bounds : [-1.558, 2.773]  Since significance and (one minus) power are different, the bound for the null hypothesis is much closer than the bound for the alternative hypothesis. This means that, in case of an intermediate effect of $\\mu=0.05$, we will be more likely to accept the null hypothesis $H_0: \\mu = 0$ than the alternative $H_1: \\mu = 0.1$.\nWe can plot the distribution of the likelihood ratio over samples drawn under the null hypothesis $H_0: \\mu = 0$.\nplot_experiment(df, ybounds=[a,b])  In this particular case, the test is inconclusive within our sampling framework. We need to collect more data in order to come to a decision.\nplot_experiment(experiment(log_lr, n=800), ybounds=[a,b]);  It takes 800 observations to reach to a conclusion, while before the sample size was 619. This test procedure seems much more lengthy than the previous one. Is it true on average?\nWhat would happen if we were to repeat the experiment $K=100$ times?\nsimulate_experiments(log_lr, ybounds=[a, b], early_stop=True, lw=1.5);  Bounds crossed: 91 (4% upper, 96% lower) Average experiment duration: 253  We get a decision for 91 simulations out of 100 and for 96% of them, it\u0026rsquo;s the correct decision. Therefore, our rejection rate is very close to the expected $\\alpha=0.05$ (under the null hypothesis).\nHowever, for 9 experiments, the test is inconclusive. What would happen if we were to sample until we reach a conclusion in each experiment?\nsimulate_experiments(log_lr, ybounds=[a,b], early_stop=True, lw=1.5, n=1900);  Bounds crossed: 100 (4% upper, 96% lower) Average experiment duration: 275  As we can see from the plot, in one particularly unlucky experiment, we need to collect 1900 observations before coming to a conclusion. However, despite this outlier, the average experiment duration is an astounding 275 samples, less than half of the original sample size of 619.\nWhat would happen if instead the alternative hypothesis $H_1: \\mu = 0.1$ was true?\nsimulate_experiments(log_lr, ybounds=[a,b], early_stop=True, mu=0.1, lw=1, n=2100);  Bounds crossed: 100 (84% upper, 16% lower) Average experiment duration: 443  In this case, we make the correct decision in only 84% of the simulations, which is very close to the expected value of 80% (under the alternative hypothesis), i.e. the power of the experiment, 1-β.\nMoreover, also under the alternative hypothesis we need a significantly lower sample size: just 443 observation, on average.h a conclusion in 78/100 experiments we need just 1/3 of the samples!\nConclusion In this post, we have seen the dangers of peeking during a randomized experiment. Prematurely stopping a test can be dangerous since it distorts inference, biasing the expected rejection rates.\nDoes it mean that we always need to perform tests with a pre-specified sample size? No! There exist procedures that allow for optimal stopping. These procedures were born for a specific purpose: reducing the sample size as much as possible, without sacrificing accuracy. The first and most known is the Sequential Probability Ratio Test, defined by Wallis as \u0026ldquo;the most powerful and seminal statistical ideas of the past third of a century\u0026rdquo; (in 1980).\nThe SPRT was not only a powerful tool during war time but keeps being used today for very practical purposes (see for example Netflix, Uber).\nReferences [1] A. Wald, Sequential tests of statistical hypotheses (1945), The Annal of Mathematical Statistics.\n[2] A. Wald and J Wolfowitz, Optimum character of the sequential probability ratio test (1948), The Annals of Mathematical Statistics.\n[3] W. A. Wallis, The Statistical Research Group, 1942–1945 (1980), Journal of the American Statistical Association.\nYou can find the original Jupyter Notebook here: https://github.com/matteocourthoud/Blog-Posts/blob/main/optimal_stopping.ipynb\n","date":1651795200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1651795200,"objectID":"0b40b79a667a0362644e1579efb424d3","permalink":"https://matteocourthoud.github.io/post/optimal_stopping/","publishdate":"2022-05-06T00:00:00Z","relpermalink":"/post/optimal_stopping/","section":"post","summary":"In the decade preceding the Second World War, there was a massive increase in industrial production of war materials, so there was a need to ensure that products, especially munitions, were reliable.","tags":null,"title":"Experiments, Peeking, and Optimal Stopping","type":"post"},{"authors":null,"categories":null,"content":"When analyzing causal relationships, it is very hard to understand which variables to condition the analysis on, i.e. how to \u0026ldquo;split\u0026rdquo; the data so that we are comparing apples to apples. For example, if you want to understand the effect of having a tablet in class on studenta' performance, it makes sense to compare schools where students have similar socio-economic backgrounds. Otherwise, the risk is that only wealthier students can afford a tablet and, without controlling for it, we might attribute the effect to tablets instead of the socio-economic background.\nWhen the treatment of interest comes from a proper randomized experiment, we do not need to worry about conditioning on other variables. If tablets are distributed randomly across schools, and we have enough schools in the experiment, we do not have to worry about the socio-economic background of students. The only advantage of conditioning the analysis on some so-called \u0026ldquo;control variable\u0026rdquo; could be an increase in power. However, this is a different story.\nIn this post, we are going to have a brief introduction to Directed Acyclic Graphs and how they can be useful to select variables to condition a causal analysis on. Not only DAGs provide visual intuition on which variables we need to include in the analysis, but also on which variables we should not include, and why.\nDirected Acyclic Graphs Definitions Directed acyclic graphs (DAGs) provide a visual representation of the data generating process. Random variables are represented with letters (e.g. $X$) and causal relationships are represented with arrows (e.g. $\\to$). For example, we interpret\nflowchart LR classDef white fill:#FFFFFF,stroke:#000000,stroke-width:2px X((X)):::white --\u0026gt; Y((Y)):::white  as $X$ (possibly) causes $Y$. We call a path between two variables $X$ and $Y$ any connection, independently of the direction of the arrows. If all arrows point forward, we call it a causal path, otherwise we call it a spurious path.\nflowchart LR classDef included fill:#DCDCDC,stroke:#000000,stroke-width:2px; classDef excluded fill:#ffffff,stroke:#000000,stroke-width:2px; X((X)) Y((Y)) Z1((Z1)) Z2((Z2)) Z3((Z3)) X --\u0026gt; Z1 Z1 --\u0026gt; Z2 Z3 --\u0026gt; Z2 Z3 --\u0026gt; Y class X,Y included; class Z1,Z2,Z3 excluded;  In the example above, we have a path between $X$ and $Y$ passing through the variables $Z_1$, $Z_2$, and $Z_3$. Since not all arrows point forward, the path is spurious and there is no causal relationship of $X$ on $Y$. In fact, variable $Z_2$ is caused by both $Z_1$ and $Z_3$ and therefore blocks the path.\n$Z_2$ is called a collider.\nThe purpose of our analysis is to assess the causal relationship between two variables $X$ and $Y$. Directed acyclic graphs are useful because they provide us instructions on which other variables $Z$ we need to condition our analysis on. Conditioning the analysis on a variable means that we keep it fixed and we draw our conclusions ceteris paribus. For example, in a linear regression framework, inserting another regressor $Z$ means that we are computing the best linear approximation of the conditional expectation function of $Y$ given $X$, conditional on the observed values of $Z$.\nCausality In order to assess causality, we want to close all spurious paths between $X$ and $Y$. The questions now are:\n When is a path open?  If it does not contain colliders. Otherwise, it is closed.   How do you close an open path?  You condition on at least one intermediate variable.   How do you open a closed path?  You condition on all colliders along the path.    Suppose we are again interested in the causal relationship of $X$ on $Y$. Let\u0026rsquo;s consider the following graph\nflowchart LR classDef included fill:#DCDCDC,stroke:#000000,stroke-width:2px; classDef excluded fill:#ffffff,stroke:#000000,stroke-width:2px; X((X)) Y((Y)) Z1((Z1)) Z2((Z2)) Z3((Z3)) X --\u0026gt; Y X --\u0026gt; Z2 Z2 --\u0026gt; Y Z1 --\u0026gt; X Z1 --\u0026gt; Y X --\u0026gt; Z3 Y --\u0026gt; Z3 class X,Y included; class Z1,Z2,Z3 excluded;  In this case, apart from the direct path, there are three non-direct paths between $X$ and $Y$ through the variables $Z_1$, $Z_2$, and $Z_3$.\nLet\u0026rsquo;s consider the case in which we analyze the relationship between $X$ and $Y$, ignoring all other variables.\n The path through $Z_1$ is open but it is spurious The path through $Z_2$ is open and causal The path through $Z_3$ is closed since $Z_3$ is a collider and it is spurious  Let\u0026rsquo;s draw the same graph indicating in grey variables that we are conditioning on, with dotted lines closed paths, with red lines spurious open paths, and with green lines causal open paths.\nflowchart LR classDef included fill:#DCDCDC,stroke:#000000,stroke-width:2px; classDef excluded fill:#ffffff,stroke:#000000,stroke-width:2px; X((X)) Y((Y)) Z1((Z1)) Z2((Z2)) Z3((Z3)) X --\u0026gt; Y X --\u0026gt; Z2 Z2 --\u0026gt; Y Z1 --\u0026gt; X Z1 --\u0026gt; Y X -.-\u0026gt; Z3 Y -.-\u0026gt; Z3 linkStyle 0,1,2 stroke:#00ff00,stroke-width:4px; linkStyle 3,4 stroke:#ff0000,stroke-width:4px; class X,Y included; class Z1,Z2,Z3 excluded;  In this case, to assess the causal relationship between $X$ and $Y$ we need to close the path that passes through $Z_1$. We can do that by conditioning the analysis on $Z_1$.\nflowchart LR classDef included fill:#DCDCDC,stroke:#000000,stroke-width:2px; classDef excluded fill:#ffffff,stroke:#000000,stroke-width:2px; X((X)) Y((Y)) Z1((Z1)) Z2((Z2)) Z3((Z3)) X --\u0026gt; Y X --\u0026gt; Z2 Z2 --\u0026gt; Y Z1 -.-\u0026gt; X Z1 -.-\u0026gt; Y X -.-\u0026gt; Z3 Y -.-\u0026gt; Z3 linkStyle 0,1,2 stroke:#00ff00,stroke-width:4px; class X,Y,Z1 included; class Z2,Z3 excluded;  Now we are able to recover the causal relationship between $X$ and $Y$ by conditioning on $Z_1$.\nWhat would happen if we were also conditioning on $Z_2$? In this case, we would close the path passing through $Z_2$ leaving only the direct path between $X$ and $Y$ open. We would then recover only the direct effect of $X$ on $Y$ and not the indirect one.\nflowchart LR classDef included fill:#DCDCDC,stroke:#000000,stroke-width:2px; classDef excluded fill:#ffffff,stroke:#000000,stroke-width:2px; X((X)) Y((Y)) Z1((Z1)) Z2((Z2)) Z3((Z3)) X --\u0026gt; Y X -.-\u0026gt; Z2 Z2 -.-\u0026gt; Y Z1 -.-\u0026gt; X Z1 -.-\u0026gt; Y X -.-\u0026gt; Z3 Y -.-\u0026gt; Z3 linkStyle 0 stroke:#00ff00,stroke-width:4px; class X,Y,Z1,Z2 included; class Z3 excluded;  What would happen if we were also conditioning on $Z_3$? In this case, we would open the path passing through $Z_3$ which is a spurious path. We would then not be able to recover the causal effect of $X$ on $Y$.\nflowchart LR classDef included fill:#DCDCDC,stroke:#000000,stroke-width:2px; X((X)) Y((Y)) Z1((Z1)) Z2((Z2)) Z3((Z3)) X --\u0026gt; Y X -.-\u0026gt; Z2 Z2 -.-\u0026gt; Y Z1 -.-\u0026gt; X Z1 -.-\u0026gt; Y X --\u0026gt; Z3 Y --\u0026gt; Z3 linkStyle 0 stroke:#00ff00,stroke-width:4px; linkStyle 5,6 stroke:#ff0000,stroke-width:4px; class X,Y,Z1,Z2,Z3 included;  Example: Class Size and Math Scores Suppose you are interested in the effect of class size on math scores. Are bigger classes better or worse for students' performance?\nAssume that the data generating process can be represented with the following DAG.\nflowchart TB classDef included fill:#DCDCDC,stroke:#000000,stroke-width:2px; classDef excluded fill:#ffffff,stroke:#000000,stroke-width:2px; classDef unobserved fill:#ffffff,stroke:#000000,stroke-width:2px,stroke-dasharray: 5 5; X((class size)) Y((math score)) Z1((class year)) Z2((good school)) Z3((math hours)) Z4((hist score)) U((ability)) X --\u0026gt; Y Z1 --\u0026gt; X X --\u0026gt; Z4 U --\u0026gt; Y U --\u0026gt; Z4 Z2 --\u0026gt; X Z2 --\u0026gt; Y Z2 --\u0026gt; Z4 Z3 --\u0026gt; Y class X,Y included; class Z1,Z2,Z3,Z4 excluded; class U unobserved;  The variables of interest are highlighted. Moreover, the dotted line around ability indicates that this is a variable that we do not observe in the data.\nWe can now load the data and check what it looks like.\n%matplotlib inline %config InlineBackend.figure_format = 'retina' from src.utils import *  from src.dgp import dgp_school df = dgp_school().generate_data() df.head()   .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }  \n  math_hours history_hours good_school class_year class_size math_score hist_score     0 3 3 1 3 15 13.009309 15.167024   1 2 3 1 3 19 13.047033 13.387456   2 2 4 0 1 25 8.330311 10.824070   3 3 4 1 3 22 11.322190 14.594394   4 3 3 1 4 15 12.338458 11.871626     What variables should we condition our regression on, in order to estimate the causal effect of class size on math scores?\nFirst of all, let\u0026rsquo;s look at what happens if we do not condition our analysis on any variable and we just regress math score on class size.\nsmf.ols('math_score ~ class_size', df).fit().summary().tables[1]    coef std err t P|t| [0.025 0.975]   Intercept  12.0421  0.259  46.569  0.000  11.535  12.550   class_size  -0.0399  0.013  -3.025  0.003  -0.066  -0.014   The effect of class_size is negative and statistically different from zero.\nBut should we believe this estimated effect? Without controlling for anything, this is DAG representation of the effect we are capturing.\nflowchart TB classDef included fill:#DCDCDC,stroke:#000000,stroke-width:2px; classDef excluded fill:#ffffff,stroke:#000000,stroke-width:2px; classDef unobserved fill:#ffffff,stroke:#000000,stroke-width:2px,stroke-dasharray: 5 5; X((class size)) Y((math score)) Z1((class year)) Z2((good school)) Z3((math hours)) Z4((hist score)) U((ability)) X --\u0026gt; Y Z1 --\u0026gt; X X -.-\u0026gt; Z4 U --\u0026gt; Y U -.-\u0026gt; Z4 Z2 --\u0026gt; X Z2 --\u0026gt; Y Z2 --\u0026gt; Z4 Z3 --\u0026gt; Y linkStyle 0 stroke:#00ff00,stroke-width:4px; linkStyle 5,6 stroke:#ff0000,stroke-width:4px; class X,Y included; class Z1,Z2,Z3,Z4 excluded; class U unobserved;  There is a spurious path passing through good school that biases our estimated coefficient. Intuitively, being enrolled in a better school improves the students' math scores and better schools might have smaller class sizes. We need to control for the quality of the school.\nsmf.ols('math_score ~ class_size + good_school', df).fit().summary().tables[1]    coef std err t P|t| [0.025 0.975]   Intercept  4.7449  0.247  19.176  0.000  4.259  5.230   class_size  0.2095  0.010  20.020  0.000  0.189  0.230   good_school  5.0807  0.130  39.111  0.000  4.826  5.336   Now the estimate of the effect of class size on math score is unbiased! Indeed, the true coefficient in the data generating process was $0.2$.\nflowchart TB classDef included fill:#DCDCDC,stroke:#000000,stroke-width:2px; classDef excluded fill:#ffffff,stroke:#000000,stroke-width:2px; classDef unobserved fill:#ffffff,stroke:#000000,stroke-width:2px,stroke-dasharray: 5 5; X((class size)) Y((math score)) Z1((class year)) Z2((good school)) Z3((math hours)) Z4((hist score)) U((ability)) X --\u0026gt; Y Z1 --\u0026gt; X X -.-\u0026gt; Z4 U --\u0026gt; Y U -.-\u0026gt; Z4 Z2 -.-\u0026gt; X Z2 -.-\u0026gt; Y Z2 --\u0026gt; Z4 Z3 --\u0026gt; Y linkStyle 0 stroke:#00ff00,stroke-width:4px; class X,Y,Z2 included; class Z1,Z3,Z4 excluded; class U unobserved;  What would happen if we were to instead control for all variables?\nsmf.ols('math_score ~ class_size + good_school + math_hours + class_year + hist_score', df).fit().summary().tables[1]    coef std err t P|t| [0.025 0.975]   Intercept  -0.7847  0.310  -2.529  0.012  -1.394  -0.176   class_size  0.1292  0.010  13.054  0.000  0.110  0.149   good_school  2.9815  0.170  17.533  0.000  2.648  3.315   math_hours  1.0516  0.048  21.744  0.000  0.957  1.147   class_year  0.0424  0.037  1.130  0.259  -0.031  0.116   hist_score  0.4116  0.027  15.419  0.000  0.359  0.464   The coefficient is again biased. Why?\nWe have opened a new spurious path by controlling for hist score. In fact, hist score is a collider and controlling for it has opened a path through hist score and ability that was otherwise closed.\nflowchart TB classDef included fill:#DCDCDC,stroke:#000000,stroke-width:2px; classDef excluded fill:#ffffff,stroke:#000000,stroke-width:2px; classDef unobserved fill:#ffffff,stroke:#000000,stroke-width:2px,stroke-dasharray: 5 5; X((class size)) Y((math score)) Z1((class year)) Z2((good school)) Z3((math hours)) Z4((hist score)) U((ability)) X --\u0026gt; Y Z1 --\u0026gt; X X --\u0026gt; Z4 U --\u0026gt; Y U --\u0026gt; Z4 Z2 -.-\u0026gt; X Z2 -.-\u0026gt; Y Z2 --\u0026gt; Z4 Z3 --\u0026gt; Y linkStyle 0 stroke:#00ff00,stroke-width:4px; linkStyle 2,3,4 stroke:#ff0000,stroke-width:4px; class X,Y,Z1,Z2,Z3,Z4 included; class U unobserved;  The example was inspired by the following tweet.\nWe can illustrate this with Model 16 of the \u0026quot;Crash Course in Good and Bad Controls\u0026quot; (https://t.co/GcSNzhuVt2). Here X = class size, Y = math4, Z = read4, and U = student\u0026#39;s ability. Conditioning on Z opens the path X -\u0026gt; Z \u0026lt;- U -\u0026gt; Y and it is thus a \u0026quot;bad control.\u0026quot; https://t.co/KNfqtsMWwB pic.twitter.com/lUSigNYSJj\n\u0026mdash; Análise Real (@analisereal) March 12, 2022  Conclusion In this post, we have seen how to use Directed Acyclic Graphs to select control variables in a causal analysis. DAGs are very helpful tools since they provide an intuitive graphical representation of causal relationships between random variables. Contrary to common intuition that \u0026ldquo;the more information the better\u0026rdquo;, sometimes including extra variables might bias the analysis, preventing a causal interpretation of the results. In particular, we must pay attention not to include colliders that open spurious paths that would otherwise be closed.\nReferences [1] C. Cinelli, A. Forney, J. Pearl, A Crash Course in Good and Bad Controls (2018), working paper.\n[2] J. Pearl, Causality (2009), Cambridge University Press.\n[3] S. Cunningham, Chapter 3 of The Causal Inference Mixtape (2021), Yale University Press.\n","date":1650844800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1650844800,"objectID":"fd5f8adabb41c0c3eff505f3e4584cfc","permalink":"https://matteocourthoud.github.io/post/controls/","publishdate":"2022-04-25T00:00:00Z","relpermalink":"/post/controls/","section":"post","summary":"When analyzing causal relationships, it is very hard to understand which variables to condition the analysis on, i.e. how to \u0026ldquo;split\u0026rdquo; the data so that we are comparing apples to apples.","tags":null,"title":"DAGs and Control Variables","type":"post"},{"authors":null,"categories":null,"content":"In this tutorial, we are going to see how to estimate causal effects when the treatment is not randomly assigned, we have acces to a control group and we observe few units but potentially many time periods. These settings are extremely common in observational studies and, in these cases, claims of causality are relatively weak. However, there still exist methods for causal inference, under certain assumptions.\nRequisites\nFor this tutorial, I assume you are familiar with the following concepts:\n Rubin\u0026rsquo;s potential outcome framework Ordinary least squares regression Randomization/permutation inference Difference in differences  Academic Application\nAs an academic application, we are going to replicate Synthetic Control Methods for Comparative Case Studies: Estimating the Effect of California’s Tobacco Control Program (2010) by Abadie, Diamond and Hainmueller. The authors study the effect of a tobacco control program in California on smoking habits.\nBusiness Case\nTBD\nSetting We assume that for a panel of i.i.d. subjects $i = 1, \u0026hellip;, n$ over time $t=1, \u0026hellip;,T$ we observed a tuple $(X_{it}, Y_{it})$ comprised of\n a feature vector $X_{i,t} \\in \\mathbb R^p$ a treatment assignment $D_i \\in \\lbrace 0, 1 \\rbrace$ a response $Y_{i,t} \\in \\mathbb R$  Moreover, one unit is treated at time $t^*$. We distinguish time periods before treatment and time periods after treatment.\nCrucially, treatment $D_i$ is not randomly assignment, therefore a difference in means between the treated unit(s) and the control group is not an unbiased estimator of the average treatment effect.\nSynthetic Control The problem is that, as usual, we do not observe the counterfactual outcome for treated units, i.e. we do not know what would have happened to them, if they had not been treated. This is known as the fundamental problem of causal inference.\nThe simplest approach, would be just to compare pre and post periods. This is called the event study approach.\nHowever, we can do better than this. In fact, even though treatment was not randomly assigned, we still have access to some units that were not treated.\nFor the outcome variable we have the following setup\n$$ Y = \\begin{bmatrix} Y_{t, post} \\ \u0026amp; Y_{c, post} \\newline Y_{t, pre} \\ \u0026amp; Y_{c, pre} \\end{bmatrix} $$\nwhich we can rewrite as\n$$ Y = \\begin{bmatrix} Y^{(1)} _ {t, post} \\ \u0026amp; Y^{(0)} _ {c, post} \\newline Y^{(0)} _ {t, pre} \\ \u0026amp; Y^{(0)} _ {c, pre} \\end{bmatrix} $$\nWe basically have a missing data problem since we do not observe $Y^{(0)} _ {t, post}$.\nFollowing Doudchenko and Inbens (2018), we can formulate an estimate of the counterfactual outcome for the treated unit as a linear combination of the observed outcomes for the control units.\n$$ \\hat Y^{(0)} _ {t, post} = \\alpha + \\sum_{i \\in c} \\beta_{i} Y^{(0)} _ {i, post} $$\nwhere\n the constant $\\alpha$ allows for different averages between the two groups the weights $\\beta_i$ are allowed to vary across control units $i$  otherwise it would be a diff-in-diff    Weights How should we choose which weights to use? One option could be to simply run a linear regression of the pre-treatment outcome of the treatment unit $Y^{(0)} _ {t, pre}$ on the pre-treatment outcome of the control group $Y^{(0)} _ {c, pre}$.\nThere are two problems with this approach:\n  the weights might be negative\n how should we interpret them? they make no sense in the potential outcome framework    weights could be greater or lower than one\n therefore we would not be able to interpret the synthetic control as a weighted average of untreated units    To solve this problem, Abadie et al. (2010) propose the following weights:\n$$ \\hat \\beta = \\arg \\min_{\\beta} || \\boldsymbol X_t - \\boldsymbol \\beta \\boldsymbol X_c || = \\sqrt{ \\sum_{p} \\left( X_{t, p} - \\sum_{i \\in c} \\beta_{p} X_{c, p} \\right)^2 } \\quad \\text{s.t.} \\quad \\sum_{p} \\beta_p = 1 \\quad \\text{and} \\quad \\beta_p \\geq 0 \\ \\forall p $$\nWith this approach we get an interpretable counterfactual as a weighted avarage of untreated units.\nSynthetic Control vs OLS What are the advantages and disadvantages of synthetic control methods with respect to a simple regression?\n  As long as we use positive weights that are constrained to sum to one, the method avoid extrapolation\n we will never go out of the support of the data    It can be \u0026ldquo;pre-registered\u0026rdquo; in the sense that you don\u0026rsquo;t need post-treatment observations to build the method\n could avoid p-hacking and cherry picking    Weights make explicit the counterfactual analysis\n one can look at the weights and understand which comparison we are making    It\u0026rsquo;s a bridge between quantitative and qualitative research\n can be used to inspect single-treated unit cases    Academic Application As an academic application, we are going to replicate Synthetic Control Methods for Comparative Case Studies: Estimating the Effect of California’s Tobacco Control Program (2010) by Abadie, Diamond and Hainmueller. The authors study the effect of a tobacco control program in California on smoking habits.\nThis is how the authors describe the policy:\n Its primary effect is to impose a 25-cent per pack state excise tax on the sale of tobacco cigarettes within California, with approximately equivalent excise taxes similarly imposed on the retail sale of other commercial tobacco products, such as cigars and chewing tobacco. Additional restrictions placed on the sale of tobacco include a ban on cigarette vending machines in public areas accessible by juveniles, and a ban on the individual sale of single cigarettes. Revenue generated by the act was earmarked for various environmental and health care programs, and anti-tobacco advertisements.\n Let\u0026rsquo;s start by loading the data. We have information on cigarette costs, sales, tax rate and revenues for all US states for years from 1970 to 2014.\n%matplotlib inline %config InlineBackend.figure_format = 'retina'  from src.utils import *  df = pd.read_csv('data/adh10.csv') df.head()   .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }  \n  state year cig_sales lnincome beer age15to24 retprice california after_treatment     0 Alabama 1970 89.800003 NaN NaN 0.178862 39.599998 False False   1 Alabama 1971 95.400002 NaN NaN 0.179928 42.700001 False False   2 Alabama 1972 101.099998 9.498476 NaN 0.180994 42.299999 False False   3 Alabama 1973 102.900002 9.550107 NaN 0.182060 42.099998 False False   4 Alabama 1974 108.199997 9.537163 NaN 0.183126 43.099998 False False     Is the sample balanced? Let\u0026rsquo;s check the distribution of covariates, by treatment assignment.\nfrom causalml.match import create_table_one create_table_one(df, 'california', ['cig_sales', 'lnincome', 'beer', 'age15to24', 'retprice'])   .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }  \n  Control Treatment SMD   Variable        n 1178 31    age15to24 0.18 (0.02) 0.18 (0.01) 0.0454   beer 23.46 (4.26) 22.26 (2.11) -0.3559   cig_sales 119.53 (32.60) 94.59 (30.01) -0.7962   lnincome 9.86 (0.17) 10.07 (0.08) 1.6107   retprice 108.04 (64.00) 119.92 (77.90) 0.1667     The sample is quite unbalanced. In particular, California is a richer state with a higher income, lnincome.\nWe are interested in the sales of cigarettes cig_sales over time. Let\u0026rsquo;s start by reshaping the dataset so that each state is a time series.\ndf = df.pivot(index='year', columns='state', values='cig_sales').reset_index() df.head()   .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }  \n state year Alabama Arkansas California Colorado Connecticut Delaware Georgia Idaho Illinois ... South Carolina South Dakota Tennessee Texas Utah Vermont Virginia West Virginia Wisconsin Wyoming     0 1970 89.800003 100.300003 123.000000 124.800003 120.000000 155.000000 109.900002 102.400002 124.800003 ... 103.599998 92.699997 99.800003 106.400002 65.500000 122.599998 124.300003 114.500000 106.400002 132.199997   1 1971 95.400002 104.099998 121.000000 125.500000 117.599998 161.100006 115.699997 108.500000 125.599998 ... 115.000000 96.699997 106.300003 108.900002 67.699997 124.400002 128.399994 111.500000 105.400002 131.699997   2 1972 101.099998 103.900002 123.500000 134.300003 110.800003 156.300003 117.000000 126.099998 126.599998 ... 118.699997 103.000000 111.500000 108.599998 71.300003 138.000000 137.000000 117.500000 108.800003 140.000000   3 1973 102.900002 108.000000 124.400002 137.899994 109.300003 154.699997 119.800003 121.800003 124.400002 ... 125.500000 103.500000 109.699997 110.400002 72.699997 146.800003 143.100006 116.599998 109.500000 141.199997   4 1974 108.199997 109.699997 126.699997 132.800003 112.400002 151.300003 123.699997 125.599998 131.899994 ... 129.699997 108.400002 114.800003 114.699997 75.599998 151.800003 149.600006 119.900002 111.800003 145.800003    5 rows × 40 columns\n In an ideal world, one could use the average of all the other states in the United States as a control group for California. Let\u0026rsquo;s plot the average cigarette consumption over time.\nstates = [c for c in df.columns if c not in ['year']] df['Other States'] = df[[s for s in states if s != 'California']].mean(axis=1)  def plot_lines(df, line1, line2, hline=True): sns.lineplot(x=df['year'], y=df[line1].values, label=line1) sns.lineplot(x=df['year'], y=df[line2].values, label=line2) plt.axvline(x=1988, ls=\u0026quot;:\u0026quot;, color='C2', label='Proposition 99', zorder=1) plt.legend(); plt.title(\u0026quot;Per-capita cigarette sales (in packs)\u0026quot;);  plot_lines(df, 'California', 'Other States')  People in California on average smoke less than in other states. This would not be a problem in a diff-in-diff setting if\n we had enough observations in each group trends were parallel other diff-in-diff assumptions were holding  However, we have just one treated unit and very few control units. Morover, the trends were definitely not parallel before treatment. Therefore, it feels quite a stretch to attribute the differences in cigarette sales post 1989 to Proposition 99 alone.\nThe idea is to exploit the time dimension and build a synthetic control state for California. Maybe no single state is a good control, but a combination of them could actually provide a good approximation to California. Moreover, we can exploit the time dimension to compensate for the fact that we have few observations. We are going to try to match the sales of cigarettes in california for all the pre-treatment years, from 1970 to 1988.\nLet\u0026rsquo;s start by using LinearRegression of cigsales in California onto cigsales of all the other states, pre 1989.\nfrom sklearn.linear_model import LinearRegression def synth_predict(df, state, model): y = df.loc[df['year'] \u0026lt;= 1988, state] other_states = [c for c in states if c not in ['year', state]] X = df.loc[df['year'] \u0026lt;= 1988, other_states] df[f'Synthetic {state}'] = model.fit(X, y).predict(df[other_states]) return model  coef = synth_predict(df, 'California', LinearRegression()).coef_  How well did we predict pre-1989 sales of cigarettes in California? If we believe that that\u0026rsquo;s a sensible control, what is the treatment effect?\nWe can visually answe both questions by plotting the actual sales of cigarettes in California against the predicted ones.\nplot_lines(df, 'California', 'Synthetic California')  It looks like the policy had a sensible negative effect on cigarette sales: the predicted trend is higher than the actual sales and diverges right after the approval of Proposition 99.\nWe can also observe that we are clearly overfitting: the pre-policy predicted cigarette sales line is perfectly overlapping with the actual data.\nAnother problem concerns the weights. We have not set any constraint on the weights.\ndf_states = pd.DataFrame({'state': df.columns[[i for i in range(2,40) if i!=3]], 'ols_coef': coef[1:]}) sns.barplot(data=df_states, x='ols_coef', y='state');  We have many negative weights, which do not make much sense from a causal inference perspective. Since we would like to interpret our synthetic control as a weighted average of untreated states, all weights should be positive and they should sum to one.\nTo address both concerns, we are going to build a new estimator called SyntheticControl() which constrains the weights to be positive and to sum to one.\nfrom typing import List from operator import add from toolz import reduce, partial from scipy.optimize import fmin_slsqp class SyntheticControl(): # Loss function def loss(self, W, X, y) -\u0026gt; float: return np.sqrt(np.mean((y - X.dot(W))**2)) # Fit model def fit(self, X, y): w_start = [1/X.shape[1]]*X.shape[1] self.coef_ = fmin_slsqp(partial(self.loss, X=X, y=y), np.array(w_start), f_eqcons=lambda x: np.sum(x) - 1, bounds=[(0.0, 1.0)]*len(w_start), disp=False) self.mse = self.loss(W=self.coef_, X=X, y=y) return self # Predict def predict(self, X): return X.dot(self.coef_)  We can now plot the actual and predicted cigarette sales in California using the SyntheticControl method.\ncoef_new = synth_predict(df, 'California', SyntheticControl()).coef_ plot_lines(df, 'California', 'Synthetic California')  It looks like the effect is again negative. However, let\u0026rsquo;s plot the difference between the two lines to better visualize the magnitude.\ndef plot_difference(df, state, vline=True, hline=True, **kwargs): sns.lineplot(x=df['year'], y=df[state] - df[f'Synthetic {state}'], **kwargs) if vline: plt.axvline(x=1988, ls=\u0026quot;:\u0026quot;, color='C2', label='Proposition 99', zorder=1) plt.legend() if hline: sns.lineplot(x=df['year'], y=0, lw=1, color='black', zorder=1) plt.title(\u0026quot;Normalized per-capita cigarette sales (in packs)\u0026quot;);  plot_difference(df, 'California', label='California')  The difference is clearly negative and decreasing over time. Moreover, now we see that the estimator is not overfitting the pre-period anymore. The reason is that the non-negativity constraint is constraining most coefficients to be zero (as Lasso does).\nWe can visualize it by plotting the distribution of coefficients.\ndf_states['coef_synth'] = coef_new[1:] sns.barplot(data=df_states, x='coef_synth', y='state');  What about inference? Is the estimate significantly different from zero? Or, more practically, \u0026ldquo;how unusual is this estimate under the null hypothesis of no policy effect?\u0026rdquo;.\nWe are going to perform a randomization/permutation test in order to answer this question. The idea is that if the policy has no effect, the effect we observe for California should not be significantly different from the effect we observe for any other state.\nTherefore, we are going to replicate the procedure above, but for all other states and observe how unusual is California.\nfor state in states: synth_predict(df, state, SyntheticControl()) plot_difference(df, state, vline=False, alpha=0.2, color='grey') plot_difference(df, 'California', label='California')  From the graph we notice two things. First, the effect for California is quite extreme and therefore likely not to be driven by random noise.\nSecond, we also notice that there are a couple of states for which we cannot fit the pre-trend very well. This is expected since, for each state, we are building the counterfactual trend as a convex combination of all other states. States that are quite extreme in terms of cigarette consumpion are very useful to build the counterfactuals of other states, but it\u0026rsquo;s hard to build a counterfactual for them. Not to bias the analysis, let\u0026rsquo;s exclude states for which we cannot build a \u0026ldquo;good enough\u0026rdquo; counterfectual, in terms of pre-treatment MSE.\n$$ MSE_{pre} = \\frac{1}{n} \\sum_{t \\in \\text{pre}} \\left( Y_t - \\hat Y_t \\right)^2 $$\nfor state in states: mse = synth_predict(df, state, SyntheticControl()).mse if mse \u0026lt; 15: plot_difference(df, state, vline=False, alpha=0.2, color='grey') plot_difference(df, 'California', label='California')  After exluding extreme observations, it looks like the effect for California is very unusual, especially if we consider a one-sided hypothesis test (it feels weird to assume that the policy could ever increase cigarette sales).\nOne statistic that the authors suggest to perform a randomization test is the ratio between pre-treatment MSE and post-treatment MSE.\n$$ \\lambda = \\frac{MSE_{post}}{MSE_{pre}} = \\frac{\\frac{1}{n} \\sum_{t \\in \\text{post}} \\left( Y_t - \\hat Y_t \\right)^2 }{\\frac{1}{n} \\sum_{t \\in \\text{pre}} \\left( Y_t - \\hat Y_t \\right)^2 } $$\nWe can compute a p-value as the number of observations with higher ratio.\nlambdas = {} for state in states: mse_pre = synth_predict(df, state, SyntheticControl()).mse mse_tot = np.mean((df[f'Synthetic {state}'] - df[state])**2) lambdas[state] = (mse_tot - mse_pre) / mse_pre print(f\u0026quot;p-value: {np.mean(np.fromiter(lambdas.values(), dtype='float') \u0026gt; lambdas['California']):.4}\u0026quot;)  p-value: 0.02564  It seems that only $2.5%$ of the states had a larger MSE ratio. We can visualize the distribution of the statistic under permutation with a histogram.\n_, bins, _ = plt.hist(lambdas.values(), bins=20); plt.hist([lambdas['California']], bins=bins, color=\u0026quot;C2\u0026quot;, label=\u0026quot;California\u0026quot;) plt.legend(); plt.title('Ratio of $MSE_{post}$ and $MSE_{pre}$ across states');  Indeed, the California statistic is quite extreme.\nReferences  Video lecture on Synthetic Controls by Paul Goldsmith-Pinkham (Yale) Chapter 10 of Causal Inference for the Brave and True by Matheus Facure Chapter 15 of Causal Inference: The Mixtape by Scott Cunningham  ","date":1650326400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1650326400,"objectID":"b5d7c5d08438854e3a113705c275ed8c","permalink":"https://matteocourthoud.github.io/post/synthetic_control/","publishdate":"2022-04-19T00:00:00Z","relpermalink":"/post/synthetic_control/","section":"post","summary":"In this tutorial, we are going to see how to estimate causal effects when the treatment is not randomly assigned, we have acces to a control group and we observe few units but potentially many time periods.","tags":null,"title":"Synthetic Control","type":"post"},{"authors":null,"categories":null,"content":"Introduction During the second year of my PhD, I decided that I wanted to have a personal website. After (too many) hours of research, I decided to build it using Hugo, and I picked the Wowchemy theme, also known as Hugo Academic. In this tutorial, I am going to share my guide to building a website on Github Pages so that you don’t have to go through all the pain I went through 😁.\nBefore we start, I have to warn you. If you don’t care about personalization or if you have very little time to spend on building a website, I strongly recommend Google Sites, which is, in my opinion, the fastest and easiest way to build an academic website. However, if you enjoy customizing your website, or if you like my template, then this guide might be useful.\nAlso, note that Hugo offers many other website templates. I suggest checking them out. Some interesting alternatives are:\n Resume Somrat UILite (paid)  However, in this guide, I will concentrate on the Hugo Academic theme, since it’s the one I used for my website and I believe it’s the best one for building academic profile pages. But the first part of this guide is general and it works for any Hugo theme.\nCreate Website 0. Prerequisites Before we start, I will take for granted the following:\n that you have an account on Github that you have R installed that you have RStudio installed  1. Create Github Repository First, go to your Github page and create a new repository (+ button in the top-right corner).\nName the repository username.github.io where username is your Github username.\nIn my case, my github username is matteocourthoud, therefore the repository is matteocourthoud.github.io and my personal website is https://matteocourthoud.github.io. Use the default settings when creating the repository.\n2. Install Blogdown and Hugo Now you need to install Blogdown, which is the program what will allow you to build and deploy your website, and Hugo, which is the template generator.\nSwitch to RStudio and type the following commands\n# Install blogdown install.packages(\u0026quot;blogdown\u0026quot;) # Install Hugo blogdown::install_hugo()  Now everything should be ready!\n3. Setup folder Open RStudio and select New Project.\nSelect New Directory when asked where to create the project.\nThen select Website using blogdown as project type.\nNow you have to select a couple of options:\n Directory name: here input the name of the folder which will contain all the website files. The name is irrelevant. I called mine website. Create project as a subdirectory of: select the directory in which you want to put the website folder. Theme: input wowchemy/starter-academic instead of the default theme.  Note: if you want to install a different theme, just go on the corresponding Github page (for example https://github.com/caressofsteel/hugo-story) and instead of gcushen/hugo-academic, insert the corresponding Github repository (for example caressofsteel/hugo-story).\nIf you go into the website folder, it should look something like\n4. Build website To build the website, open the RProject file website.Rproj in RStudio and type\nblogdown::hugo_build(local=TRUE)  This command will generate a public/ subfolder in which the actual code of the website is stored.\nDon’t ask me why, but the option local=TRUE seems to make a difference. Updating without it sometimes does not change the content in the public/ subfolder.\nTo preview the website, type in RStudio\nblogdown::serve_site()  The following preview should automatically open in your browser.\nPreviewing the website is very useful as it allows you to see live changes locally inside RStudio, before publishing them. This is the main advantage of working in RStudio.\nIf the preview does not automatically open in your browser, and instead it previews inside RStudio Viewer panel, you can preview it in your browser using the upper left right-most button.\n5. Publish website Importantly, before pushing the code online, you need to open the file config.yaml and change the baseurl to your future website url, which will be https://username.github.io/, where username is your Github username.\nNow that you have set the correct url, you have to push the changes from the public/ folder to your username.github.io repository on Github.\nTo do that, you need to get to the website folder. Let’s assume that the path to your folder is Documents/website. Open the Terminal and type\ncd Documents/website/public  The following code will link the public/ folder, containing the actual code of the website, to your username.github.io repository.\n# Init git in the /website/public/ folder git init # Add and commit the changes git add . git commit -m \u0026quot;first version of the website\u0026quot; # Set origin git remote add origin https://github.com/username/username.github.io.git # Rename local branch git branch -M main # And push your updates online git push -u origin main  Wait a few seconds (or minutes for heavy changes) and your website should be online!\nIf the website is not working, you can check the following:\n Is there anything in your public/ folder? (does it even exist?) If not, something went wrong when compiling the website with blogdown::hugo_build(). Inside your public/ folder, there should be an index.html file. If you double-click on it, you should see a local preview of your website in your browser. If not, something in the website code is wrong. Is the content of your public/ folder exactly the same as the content of your Github repository? If not, something went wrong when pushing to Github. Did you name your Github repository username.github.io, where username is your Github username? Did you change the baseurl option in the file config.yaml to https://username.github.io/, where username is your Github username? You can check the list of websites deployments at https://github.com/username/username.github.io/deployments. Control that they correspond with your commits.  If all the conditions are satisfied, but the website is still not online, maybe it’s just a matter of time. Have some patience.\nBasic Customization The basic files that you want to modify to customize your website are the following:\n config/_default/config.yaml: general website information config/_default/params.yaml: website customization config/_default/menus.yaml: top bar / menu customization content/authors/admin/_index.md: personal information  For what concerns images, there are two main things you might want to modify:\n Profile picture: change the content/authors/admin/avatar.jpg picture Website icon: change the assets/media/icon.png picture  In order to modify the widgets on your homepage, go to content/home/ and modify the files inside. If you want to remove a section, just open the corresponding file and select active: false. If there is no active option, just copy the line active: false in the corresponding file.\nOn my website, I have only the following sections set to true:\n about projects posts contact  To change the color palette of the website, go to data\\theme and generate a custom_theme.toml file with the following content:\n# Theme metadata name = \u0026quot;My custom theme\u0026quot; # Is theme light or dark? light = true # Primary primary = \u0026quot;#284f7a\u0026quot; # Menu menu_primary = \u0026quot;#fff\u0026quot; menu_text = \u0026quot;#34495e\u0026quot; menu_text_active = \u0026quot;#284f7a\u0026quot; menu_title = \u0026quot;#2b2b2b\u0026quot; # Home sections home_section_odd = \u0026quot;rgb(255, 255, 255)\u0026quot; home_section_even = \u0026quot;rgb(247, 247, 247)\u0026quot; [dark] link = \u0026quot;#bbdefb\u0026quot; link_hover = \u0026quot;#bbdefb\u0026quot;  Then go to the config/_default/params.yaml file and set the theme to custom_theme.\nYou can get more information on how to personalize it here.\nTo change the font, go to data\\fonts and generate a custom_font.toml file with the following content:\n# Font style metadata name = \u0026quot;My custom font\u0026quot; # Optional Google font URL google_fonts = \u0026quot;family=Roboto+Mono\u0026amp;family=Source+Sans+Pro:wght@200;300;400;700\u0026quot; # Font families heading_font = \u0026quot;Source Sans Pro\u0026quot; body_font = \u0026quot;Source Sans Pro\u0026quot; nav_font = \u0026quot;Source Sans Pro\u0026quot; mono_font = \u0026quot;Roboto Mono\u0026quot;  Then go to the config/_default/params.yaml file and set the font to custom_font.\nYou can get more information on how to personalize it here. Importantly, by default, the website supports only fonts of weight 400 and 700. If you want a lighter font, like the Source Sans Pro I use for my website, you have to dig into the advanced customization (which requires HTML and CSS skills).\nAdvanced Customization Advanced customization is possible but it’s a pain. You basically want to go inside themes\\github.com\\wowchemy\\wowchemy-hugo-modules\\wowchemy and start digging. Tip: you want to start digging in the following places:\n In layouts\\partials to edit the HTML files In assets\\scss to edit the SCSS code  If you want to copy my exact theme, I have published my custom theme here: https://github.com/matteocourthoud/custom-wowchemy-settings\nYou have to do the following:\n go inside the theme folder copy the content of the custom-wowchemy-theme repository in a folder there go to the config.yaml file into the MODULES section   change the second link to the folder with the custom settings  Now your website should look quite similar to mine! :)\nExamples Here are some examples of advanced customizations you can do. For all the examples the baseline directory is you theme directory, themes/custom-wowchemy-theme if you renamed it as in the previous paragraph.\nExample 1\nWhat to have your section titles fixed on top of the screen?\n  Go to assets/scss/wowchemy/widgets/_base.scss\n  Search for .section-heading h1\n  It should look like this\n  Add a couple of lines as follows\n  Now the section titles should stay anchored at the top of the page\n  Example 2\nDo you want to put a background image in your home page?\n  Put the selected background image, for example image.png, into the static/img folder (the location itself does not matter)\n  Go to assets/scss/wowchemy/widgets/_about.scss\n  Add the following lines anywhere in the code\n  Now go to layouts/partials/widgets/about.html\n  Add the following lines after \u0026lt;!-- About widget --\u0026gt;\n`\n  Now image.png should appear as background image in your homepage.\n  Google Analytics In order for the website to be displayed in Google searches, you need to ask Google to track it.\n Go to the Google Search Console website Use the URL Inspection tool to inspect the URL of your personal website: https://username.github.io Use Request indexing to request Google to index your website so that it will apprear in Google searches. Under Sitemap provide the link to your website sitemap to Google. It should be https://username.github.io/sitemap.xml.  In order to receive statistics on your website, you first need to get your associated tracking code.\n Go to the Google Analytics website Click Admin Select an account from the menu in the ACCOUNT column. Select a property from the menu in the PROPERTY column. Under PROPERTY, click Tracking Info \u0026gt; Tracking Code. Your tracking ID and property number are displayed at the top of the page. It should have the form UA-xxxxxxxxx-1  Now that we have the website tracking code, we need to insert it into the googleAnalytics section of the config/_default/params.yaml file.\nmarketing: google_analytics: 'UA-xxxxxxxxx-1'  The mobile application of Google Analytics is particular intuitive and allows you to monitor your website traffic in detail. You just need to link the website from the Google Sesarch Console and then you can motitor you website from this platform. There is also a very nice mobile app for both Android and iOS to monitor your website from your smartphone.\nAnother good free tool to analyze the “quality” of your website is SEO Mechanic.\nReferences Here are the main resources I used to write this guide:\n Wowchemy website: https://wowchemy.com/docs/getting-started/ Old Academic website: https://sourcethemes.com/academic/docs/install/ Guide for the Terminal: https://github.com/fliptanedo/FlipWebsite2017  ","date":1650240000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1650240000,"objectID":"50c5f0366dd809632c0e255ee6d27271","permalink":"https://matteocourthoud.github.io/post/website/","publishdate":"2022-04-18T00:00:00Z","relpermalink":"/post/website/","section":"post","summary":"Introduction During the second year of my PhD, I decided that I wanted to have a personal website. After (too many) hours of research, I decided to build it using Hugo, and I picked the Wowchemy theme, also known as Hugo Academic.","tags":null,"title":"How To Make A Personal Website with Hugo","type":"post"},{"authors":null,"categories":null,"content":"In this tutorial, we are going to see how to robustly estimate treatment effects when treatment is conditionally randomly assigned, using the Augmented Inverse Propensity Weighted estimator, also known as doubly-robust estimator.\nFor this tutorial, I assume you are familiar with the following concepts:\n Rubin\u0026rsquo;s potential outcome framework Propensity score weighting Basic machine learning  Setting We assume that for a set of i.i.d. subjects $i = 1, \u0026hellip;, n$ we observed a tuple $(X_i, T_i, Y_i)$ comprised of\n a feature vector $X_i \\in \\mathbb R^n$ a treatment assignment $T_i \\in \\lbrace 0, 1 \\rbrace$ a response $Y_i \\in \\mathbb R$  Assumption 1 : unconfoundedness (or ignorability, or selection on observables)\n$$ \\big \\lbrace Y_i^{(1)} , Y_i^{(0)} \\big \\rbrace \\ \\perp \\ T_i \\ | \\ X_i $$\ni.e. conditional on observable characteristics $X$, the treatment assignment $T$ is as good as random.\nAssumption 2: overlap (or bounded support)\n$$ \\exists \\eta \u0026gt; 0 \\ : \\ \\eta \\leq \\mathbb E \\left[ T_i = 1 \\ \\big | \\ X_i = x \\right] \\leq 1-\\eta $$\ni.e. no observation is deterministically assigned to the treatment or control group.\nThe IPW Estimator We want to estimate the average treatment effect\n$$ \\tau(x) = \\mathbb E \\left[ Y^{(1)} - Y^{(0)} \\ \\big| \\ X = x \\right] $$\nWe would like to obtain an unbiased estimator that satifies a central limit theorem of the form\n$$ \\sqrt{n} ( \\hat \\tau - \\tau) \\ \\overset{d}{\\to} \\ N(0, V) $$\nthus enabling us to construct confidence intervals.\nUnder unconfoundedness, we can rewrite the average treatment effect as\n$$ \\tau(x) = \\mathbb E \\left[ Y^{(1)} - Y^{(0)} \\ \\big| \\ X = x \\right] = \\mathbb E \\left[ \\frac{T_i Y_i}{e(X_i)} - \\frac{(1-T_i) Y_i}{1-e(X_i)} \\right] $$\nwhere $e(X_i)$ is the propensity score of observation $i$,\n$$ e(x) = \\mathbb P \\left[ T_i = 1 \\ \\big | \\ X_i = x \\right] $$\ni.e. its probability of being treated.\nNote that this formulation of the average treatment effect does not depend on the potential outcomes $Y_i^{(1)}$ and $Y_i^{(0)}$, but only on the observed outcomes $Y_i$.\nThis formulation of the average treatment effect implies the Inverse Propensity Weighted estimator which is an unbiased estimator for the average treatment effect $\\tau$\n$$ \\hat \\tau^{*}_{IPW} = \\frac{1}{n} \\sum _ {i=1}^{n} \\left( \\frac{T_i Y_i}{e(X_i)} - \\frac{(1-T_i) Y_i}{1-e(X_i)} \\right) $$\nHowever, this estimator is unfeasible since we do not observe the propensity scores $e(X_i)$.\nThe AIPW Estimator A feasible estimator of the average treatment effect $\\tau$ is\n$$ \\hat \\tau_{IPW} = \\frac{1}{n} \\sum_{i=1}^{n} \\left( \\frac{T_i Y_i}{\\hat e(X_i)} - \\frac{(1-T_i) Y_i}{1-\\hat e(X_i)} \\right) $$\nwhere we have replaced the propensity scores $e (X_i)$ with their estimates $\\hat e (X_i)$.\nWith a linear model, $Y_i = \\alpha T_i + \\beta X_i + \\varepsilon_i$, we can get a central limit theorem for $\\hat \\tau$. However, we want to take a non-parametric / machine learning approach with respect to the relationship between $Y$ and $X$.\nAdvantages\n flexible functional form for $\\mathbb E[Y | X]$  Disadvantages\n machine learning methods have slow convergence rates it impacts inference, i.e. we cannot easily get a central limit theorem result to build confidence intervals  Is there a way to get around the slow rate of convergence of machine learning methods?\nYes! Idea: combine two predicton problems instead of one.\nThe Augmented Inverse Propensity Weighted estimator is given by\n$$ \\hat \\tau_{IPW} = \\frac{1}{n} \\sum_{i=1}^{n} \\left( \\hat \\mu^{(1)}(X_i) - \\hat \\mu^{(0)}(X_i) + \\frac{T_i }{\\hat e(X_i)} \\left( Y_i - \\hat \\mu^{(1)}(X_i) \\right) - \\frac{(1-T_i) }{1-\\hat e(X_i)} \\left( Y_i - \\hat \\mu^{(0)}(X_i) \\right) \\right) $$\nwhere\n$$ \\mu^{(t)}(x) = \\mathbb E \\left[ Y_i \\ \\big | \\ X_i = x, T_i = t \\right] \\qquad ; \\qquad e(x) = \\mathbb E \\left[ T_i = 1 \\ \\big | \\ X_i = x \\right] $$\nThe formula of the AIPW estimator might seem scary at first, so let\u0026rsquo;s decompose it into two parts.\nFirst,\n$$ D = \\frac{1}{n} \\sum_{i=1}^{n} \\left( \\hat \\mu^{(1)}(X_i) - \\hat \\mu^{(0)}(X_i) \\right) $$\nis basically the direct estimate of the average treatment effect. This is a consistent estimator of the ATE but, since machine learning estimators' rate of convergence is too slow, it does not provide correct confidence intervals.\nInstead,\n$$ R = \\frac{1}{n} \\sum_{i=1}^{n} \\left(\\frac{T_i }{\\hat e(X_i)} \\left( Y_i - \\hat \\mu^{(1)}(X_i) \\right) - \\frac{(1-T_i) }{1-\\hat e(X_i)} \\left( Y_i - \\hat \\mu^{(0)}(X_i) \\right) \\right) $$\nis basically IPW applied to the residuals $Y_i - \\hat \\mu^{(t)}(X_i)$ instead of $Y_i$.\nDouble Robustness Why is the AIPW estimator so compelling? It just needs one of the two predictions, $\\hat \\mu$ and $\\hat e$, to be right in order to be unbiased. Let\u0026rsquo;s check it.\nIf $\\hat \\mu$ is correctly specified, i.e. $\\mathbb E \\left[ \\hat \\mu^{(t)}(x) \\right] = \\mathbb E \\left[ Y_i \\ \\big | \\ X_i = x, T_i = t \\right]$, then\n$$ \\begin{aligned} \\hat \\tau_{IPW} \u0026amp;\\overset{p}{\\to} \\mathbb E \\left[ \\hat \\mu^{(1)}(X_i) - \\hat \\mu^{(0)}(X_i) + \\frac{T_i \\left( Y_i - \\hat \\mu^{(1)}(X_i) \\right)}{\\hat e(X_i)} - \\frac{(1-T_i) \\left( Y_i - \\hat \\mu^{(0)}(X_i) \\right)}{1-\\hat e(X_i)} \\right] = \\newline \u0026amp;= \\mathbb E \\left[ \\hat \\mu^{(1)}(X_i) - \\hat \\mu^{(0)}(X_i) \\right] = \\newline \u0026amp;= \\mathbb E \\left[ Y^{(1)} - Y^{(0)} \\right] = \\newline \u0026amp;= \\tau \\end{aligned} $$\neven if $\\hat e$ is misspecified.\nThe intuition is that the residuals $\\left( Y_i - \\hat \\mu^{(t)}(X_i) \\right)$ converge to zero and therefore IPW has no relevance.\nOn the other hand, if $\\hat e$ is correctly specified, i.e. $\\mathbb E \\left[\\hat e(x) \\right] = \\mathbb E \\left[ T_i = 1 \\ \\big | \\ X_i = x \\right]$, then\n$$ \\begin{aligned} \\hat \\tau_{IPW} \u0026amp;\\overset{p}{\\to} \\mathbb E \\left[ \\hat \\mu^{(1)}(X_i) - \\hat \\mu^{(0)}(X_i) + \\frac{T_i \\left( Y_i - \\hat \\mu^{(1)}(X_i) \\right)}{\\hat e(X_i)} - \\frac{(1-T_i) \\left( Y_i - \\hat \\mu^{(0)}(X_i) \\right)}{1-\\hat e(X_i)} \\right] = \\newline \u0026amp;= \\mathbb E \\left[ \\frac{T_i Y_i}{\\hat e(X_i)} - \\frac{(1-T_i) Y_i }{1-\\hat e(X_i)} + \\left(1 - \\frac{T_i}{\\hat e(X_i)} \\right) \\hat \\mu^{(1)}(X_i) - \\left(1 - \\frac{1-T_i}{1-\\hat e(X_i)} \\right) \\hat \\mu^{(0)}(X_i) \\right] = \\newline \u0026amp;= \\mathbb E \\left[ \\frac{T_i Y_i}{\\hat e(X_i)} - \\frac{(1-T_i) Y_i }{1-\\hat e(X_i)}\\right] = \\newline \u0026amp;= \\mathbb E \\left[ Y^{(1)} - Y^{(0)} \\right] = \\newline \u0026amp;= \\tau \\end{aligned} $$\neven if $\\hat \\mu$ is misspecified.\nThe intuition is that, if we have a wrong model of $\\mu(x)$ for $\\mathbb E \\left[ Y_i \\ \\big | \\ X_i = x, T_i = t \\right]$, it will not capture differences between treatment and control group or, even worse, it will capture wrong ones. Running IPW on the residuals we can not only recover the treatment effect but also compensate for eventual biases introduced by $\\mu(x)$.\nBest Practices 1. Check Covariate Balance\nBoth IPW and AIPW were built for settings in which the treatment $T$ is not uconditionally randomly assigned, but might depend on some observables $X$. This information can be checked in two ways:\n Produce a balance table, summarizing the covariates across treatment arms. If undonditional randomization does not hold, we expect to see significant differences across some observables Plot the estimated propensity scores. If undonditional randomization holds, we expect the propensity scores to be constant  2. Check the Overlap Assumption\nAnother assumption that we can check is the overlap assumption, i.e. $\\exists \\eta \\ : \\ \\eta \\leq \\mathbb E \\left[ T_i = 1 \\ \\big | \\ X_i = x \\right] \\leq 1-\\eta$. To check this assumption we can simply check the bounds of the predicted propensity scores. If the overlap assumption is violated, we end up dividing some term of the estimator by zero.\n3. Use LOO Predictors\nIt is best practice, whenever we build a prediction it is best pratice to exclude observation $i$ when fitting the algorithm for predicting $\\hat \\mu^{(t)} (X_i)$ or $\\hat e (X_i)$. These predictors are called Leave One Out predictors.\nExample %matplotlib inline %config InlineBackend.figure_format = 'retina'  from src.utils import * from src.dgp import dgp_aipw  In this example, we are going to use the following data generating process\n $N = 1000$ $p = 20$ $X_i \\sim N(0, I_p)$ $e(x) = 1 / (1 + e^{-x_1})$ $\\mu^{(0)}(x) = (x_1 + x_2)_+$ $\\mu^{(1)}(x) = (x_1 + x_3)_+ \\mathbf{- 0.05}$  So that the average treatment effect is $- 0.05$.\ndgp = dgp_aipw() df = dgp.generate_data() df.head()   .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }  \n  x1 x2 x3 x4 x5 x6 x7 x8 x9 x10 ... x16 x17 x18 x19 x20 e T Y0 Y1 Y     0 1.624345 -0.611756 -0.528172 -1.072969 0.865408 -2.301539 1.744812 -0.761207 0.319039 -0.249370 ... -1.099891 -0.172428 -0.877858 0.042214 0.582815 0.835394 1 0.962589 0.996174 0.996174   1 -1.100619 1.144724 0.901591 0.502494 0.900856 -0.683728 -0.122890 -0.935769 -0.267888 0.530355 ... -0.012665 -1.117310 0.234416 1.659802 0.742044 0.249624 0 0.044105 -0.050000 0.044105   2 -0.191836 -0.887629 -0.747158 1.692455 0.050808 -0.636996 0.190915 2.100255 0.120159 0.617203 ... 0.586623 0.838983 0.931102 0.285587 0.885141 0.452188 1 -0.050000 -0.100000 -0.100000   3 -0.754398 1.252868 0.512930 -0.298093 0.488518 -0.075572 1.131629 1.519817 2.185575 -1.396496 ... -2.022201 -0.306204 0.827975 0.230095 0.762011 0.319864 1 0.448470 -0.100000 -0.100000   4 -0.222328 -0.200758 0.186561 0.410052 0.198300 0.119009 -0.670662 0.377564 0.121821 1.129484 ... 0.077340 -0.343854 0.043597 -0.620001 0.698032 0.444646 0 0.000000 -0.050000 0.000000    5 rows × 25 columns\n First, we check for covariate imbalance across treatment arms, with a balance table.\ndf.groupby('T').agg(['mean', 'std']).T.unstack(1).head()   .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead tr th { text-align: left; }  \n  T 0 1    mean std mean std     x1 -0.462644 0.920626 0.413837 0.882116   x2 0.007900 1.010610 -0.054072 0.991680   x3 -0.005718 0.950997 -0.042624 1.059308   x4 0.122930 0.936591 0.095082 1.004048   x5 0.032637 0.955479 0.119788 1.039616     We can also get a first (wrong) estimate of the treatment effect as a difference in means.\ndf.loc[df['T']==1, 'Y'].mean() - df.loc[df['T']==0, 'Y'].mean()  0.29838749886286686  We know this estimate is wrong since the treatment is not unconditionally randomized. Therefore, we estimate the average treatment effect using the AIPW estimator.\nFirst, we estimate the propensity scores $e(x)$ using LogisticRegression.\ndef estimate_e(df, X, model_e): e = model_e.fit(df[dgp.X], df['T']).predict_proba(df[dgp.X])[:,1] return e  from sklearn.linear_model import LogisticRegression as logit e = estimate_e(df, dgp.X, logit())  A best practice is to use the LeaveOneOut estimator to make predictions, i.e., for each observations, build a model using the remaining observations. This procedure helps preventing overfitting bias. The main issue is that it\u0026rsquo;s particularly slow since we need to fit one model per observation. However, it is parallelizable.\nfrom sklearn.model_selection import cross_val_predict, LeaveOneOut e = cross_val_predict(estimator=logit(), X=df[dgp.X], y=df['T'], cv=LeaveOneOut(), method='predict_proba', n_jobs=-1)[:,1]  Let\u0026rsquo;s check if the bounded support assumption is satisfied.\nprint(f'Support of e is [{min(e):.2}, {max(e):.2}]')  Support of e is [0.019, 0.97]  The support assumption is satisfied. Note that this is guaranteed with logistic regression.\nWe can further plot the distribution of scores. They definitely do not seem constant.\nsns.histplot(e, bins=100).set(title='Propensity Scores');  We can now estimate the average treatment effect using the AIPW estimator. We also separately compute its components, $D$ and $R$.\nWe use RandomForestRegressor to estimate the conditional expectation of $Y$, $\\hat \\mu^{(t)}(x)$.\ndef estimate_mu(df, X, model_mu): mu = model_mu.fit(df[X + ['T']], df['Y']) mu0 = mu.predict(df[X + ['T']].assign(T=0)) mu1 = mu.predict(df[X + ['T']].assign(T=1)) return mu0, mu1  from sklearn.ensemble import RandomForestRegressor as rfr mu0, mu1 = estimate_mu(df, dgp.X, rfr(max_features=5))  We now have all the elements to estimate AIPW\ndef AIPW(df, X, e, mu0, mu1): D = mu1 - mu0 R = df['T'] / e * (df['Y'] - mu1) - (1-df['T']) / (1-e) * (df['Y'] - mu0) tau_AIPW = D + R return D, R, tau_AIPW  D, R, hat_tau_AIPW = AIPW(df, dgp.X, e, mu0, mu1) print(f\u0026quot;Mean: {np.mean(hat_tau_AIPW):.2} and var {np.var(hat_tau_AIPW):.2}\u0026quot;)  Mean: -0.02 and var 0.48  Our estimate is now closer to the true value, $-0.05$. We plot the distribution of $\\hat \\tau_i^{AIPW}$ below.\nsns.histplot(hat_tau_AIPW, bins=20).set(title='Estimated $τ_{AIPW}$');  To visualize the impact of the AIPW correction, we simulate the distribution of the AIPW estimator and its components.\ndef simulate_AIPW(k): df = dgp_aipw().generate_data(seed=k) e = estimate_e(df, dgp.X, logit()) mu0, mu1 = estimate_mu(df, dgp.X, rfr()) aipw = AIPW(df, dgp.X, e, mu0, mu1) return np.mean(aipw, axis=1)  from joblib import Parallel, delayed def distribution_AIPW(t): r = Parallel(n_jobs=8)(delayed(simulate_AIPW)(i) for i in range(100)) sim_tau_AIPW = pd.DataFrame(r, columns=['Direct', 'Correction', '$τ_{AIPW}$']) plot = sns.boxplot(data=pd.melt(sim_tau_AIPW), x='variable', y='value', linewidth=2); plot.set(title=t, xlabel='', ylabel='') plot.axhline(-0.05, c='r', ls=':');  Below we plot the AIPW estimator and its components.\ndistribution_AIPW(\u0026quot;Distribution of $\\hat τ_{AIPW}$ and its components\u0026quot;)  Since the model is well specified, the correction is close to zero and the final estimates are very close to the direct estimates.\nWe now turn into checking the double-robustness of the estimator.\nWhat happens if we misspecify the propensity score? Let\u0026rsquo;s assume now that the propensity score is $\\hat e(X_i) = 1 /(1 + e^{x_4})$.\ndef estimate_e(df, X, model_e): e = 1 / (1 + np.exp(df['x4'])) return e  distribution_AIPW(\u0026quot;Distribution of $\\hat τ_{AIPW}$ with misspecified $\\hat e$\u0026quot;)  Not much changes. In fact, since we have a good direct model of $\\mu(x)$, the residuals are small and the correction does not play a big role.\nLet\u0026rsquo;s now misspecify $\\mu(x)$ and assume it is equal to $|x_5|$.\ndef estimate_e(df, X, model_e): e = model_e.fit(df[dgp.X], df['T']).predict_proba(df[dgp.X])[:,1] return e def estimate_mu(df, X, model_mu): mu0 = 0 mu1 = np.abs(df['x6']) return mu0, mu1  distribution_AIPW(\u0026quot;Distribution of $\\hat τ_{AIPW}$ with misspecified $\\hat μ$\u0026quot;)  In this case, the correction term is crucial and fully offsets the bias of the original estimator.\nThe EconML library offers a plug and play estimator, LinearDRLearner.\nfrom econml.drlearner import LinearDRLearner model = LinearDRLearner(model_propensity=logit(), model_regression=rfr()) model.fit(Y=df[dgp.Y], X=df[dgp.X], T=df[dgp.T]);  The model directly gives us the average treatment effect.\nmodel.ate_inference(X=df[dgp.X].values, T0=0, T1=1).summary().tables[0]  Uncertainty of Mean Point Estimate  mean_point stderr_mean zstat pvalue ci_mean_lower ci_mean_upper   -0.087 0.036 -2.438 0.015 -0.157 -0.017   The estimate is statistically different from zero and the confidence interval includes the true value of $-0.05$.\nResearch Paper Replication TBD\nBusiness Case TBD\nReferences  Original paper: An Introduction to the Augmented Inverse Propensity Weighted Estimator Video lecture by Prof. Stefan Wager (Stanford)  ","date":1649980800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1649980800,"objectID":"bc7e85f56e381bf28f3ee62ab0bf1da0","permalink":"https://matteocourthoud.github.io/post/aipw/","publishdate":"2022-04-15T00:00:00Z","relpermalink":"/post/aipw/","section":"post","summary":"In this tutorial, we are going to see how to robustly estimate treatment effects when treatment is conditionally randomly assigned, using the Augmented Inverse Propensity Weighted estimator, also known as doubly-robust estimator.","tags":null,"title":"AIPW","type":"post"},{"authors":null,"categories":null,"content":"In this tutorial, we are going to see how to estimate heterogeneous treatment effects using regression trees.\nFor this tutorial, I assume you are familiar with the following concepts:\n Rubin\u0026rsquo;s potential outcome framework Decision tree methods Propensity score matching  Setting We assume that for a set of i.i.d. subjects $i = 1, \u0026hellip;, n$ we observed a tuple $(X_i, D_i, Y_i)$ comprised of\n a feature vector $X_i \\in \\mathbb R^n$ a treatment assignment $T_i \\in \\lbrace 0, 1 \\rbrace$ a response $Y_i \\in \\mathbb R$  Our goal is to estimate the conditional average treatment effect\n$$ \\tau(x) = \\mathbb E \\Big [ Y^{(1)} - Y^{(0)} \\ \\Big| \\ X = x \\Big ] $$\nCrucially, we only get to observe $Y_i = Y_i^{(T_i)}$.\nWihtout further assumptions, we cannot estimate $\\tau(x)$.\nAssumption: unconfoundedness (or ignorability, or selection on observables)\n$$ \\lbrace Y_i^{(1)} , Y^{(0)} \\rbrace \\perp W_i \\ | \\ X_i $$\ni.e. conditional on observable characteristics the treatment assignment is as good as random.\nWhen unconfoundedness holds, matching methods usually provide consistent estimates of the conditional average treatment effect.\nPrediction Problem How can we make the inference problem a prediction problem?\nIn principle, we would like to divide the population in subgroups in order to minimize the mean squared error (MSE) of treatment effects.\nThe objective function is\n$$ \\sum_i \\Big [ ( \\tau_i - \\hat \\tau_i(X))^2 \\Big ] $$\nHowever, this objective function is unfeasible since we do not observe $\\tau_i$.\nThe idea is to transform our outcome variable as\n$$ Y_i^* = \\frac{Y_i}{D_i * p(X_i) - (1-D_i) * (1-p(X_i))} $$\nwhere $p_i$ is the propensity score of observation $i$, i.e. its probability of being treated.\nIt\u0026rsquo;s intuitive to verify that, given this specification, the expected value of $Y_i^*$ is the conditional average treatment effect.\nHere is a proof:\n$$ \\begin{aligned} \\mathbb E \\left[ Y_i^{*} \\mid X_i = x \\right] \u0026amp;= \\mathbb E \\left[ \\frac{Y_i}{T_i * p(X_i) - (1-T_i) * (1-p(X_i))} \\ \\Big | \\ X_i = x \\right] \\newline \u0026amp;= \\mathbb E \\left[ Y_i * \\frac{T_i - p(X_i)}{p(X_i) (1-p(X_i))} \\ \\Big | \\ X_i = x \\right] \\newline \u0026amp;= \\mathbb E \\left[ Y_i T_i * \\frac{T_i - p(X_i)}{p(X_i) (1 - p(X_i))} + Y_i (1-T_i) * \\frac{T_i - p(X_i)}{p(X_i) (1 - p(X_i))} \\ \\Big | \\ X_i = x \\right] \\newline \u0026amp;= \\mathbb E \\Big[ Y^{(1)}_i * \\frac{D_i (1 - p(X_i))}{p(X_i) (1 - p(X_i))} \\ \\Big | \\ X_i = x \\Big] - \\mathbb E \\left[Y^{(0)}_i * \\frac{(1 - T_i) p(X_i)}{p(X_i) (1-p(X_i))} \\ \\Big | \\ X_i = x \\right] \\newline \u0026amp;= \\frac{1}{p(X_i)} \\mathbb E \\Big[ Y^{(1)}_i * T_i \\ \\Big | \\ X_i = x \\Big] - \\frac{1}{1-p(X_i)} \\mathbb E \\left[ Y^{(0)}_i * (1 - T_i) \\ \\Big | \\ X_i = x \\right] \\newline \u0026amp;= \\frac{1}{p(X_i)} \\mathbb E \\left[ Y^{(1)}_i \\ \\Big | \\ X_i = x \\right] * \\mathbb E \\Big[ T_i \\ \\Big | \\ X_i = x \\Big] - \\frac{1}{1 - p(X_i)} \\mathbb E \\left[ Y^{(0)}_i \\ \\Big | \\ X_i = x \\right] * \\mathbb E \\left[ (1 - T_i) \\ \\Big | \\ X_i = x \\right] \\newline \u0026amp;= \\mathbb E \\Big[ Y^{(1)}_i \\ \\Big | \\ X_i = x \\Big] - \\mathbb E \\Big[Y^{(0)}_i \\ \\Big | \\ X_i = x \\Big] \\newline \u0026amp;= \\tau_i(x) \\end{aligned} $$\nHow can regression trees help estimate heterogeneous treatment effects?\nIf we fit a tree model on the modified outcome $Y^*$, we will get a partition of the data that minimizes the expected mean squared error of the conditional treatment effect. While the individual estimates are going to be inaccurate, within each leaf, we can estimate heterogeneous treatment effects.\nIn order to get an unbiased estimate however, we need to use different data to build the tree and to estimate the effect. This procedure comes at the cost of increased variance.\nExample 1: Simulated Data Let\u0026rsquo;s start with an example on synthetic data. We have the following individual characteristics:\n male: gender black: race age: age educ: education, which depends on age and race  Moreover, we maketreatment status D depend on both gender and race so that it will be important to condition on observables.\nOur outcome variable y depends on the treatment assignment differently according to education and age.\n%matplotlib inline %config InlineBackend.figure_format = 'retina'  from src.utils import * from src.dgp import dgp_ad  We generate a dataset out of our DGP.\ndgp = dgp_ad() df = dgp.generate_data() df.head()   .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }  \n  male black age educ ad_exposure revenue     0 0 0 55.0 1 False -0.327221   1 1 1 47.0 2 False 0.659393   2 0 1 31.0 2 True 2.805178   3 0 1 51.0 2 False -0.508548   4 0 0 48.0 0 True 0.762280     We can check the distribution of variables across treatment assignment.\ndf.groupby(dgp.T).agg(['mean', 'std']).T.unstack(1)   .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead tr th { text-align: left; }  \n  ad_exposure False True    mean std mean std     male 0.42000 0.494053 0.592000 0.491955   black 0.58800 0.492688 0.470000 0.499599   age 44.57600 10.350386 44.970000 9.961102   educ 2.19200 1.824773 1.968000 1.724216   revenue -0.11697 1.057941 1.156387 2.147342     As we can see, male and black are now balanced across groups.\nWe can get a first estimate of the average treatment effect as a simple comparison of means.\ndf.loc[df[dgp.T]==1, dgp.Y].mean() - df.loc[df[dgp.T]==0, dgp.Y].mean()  1.2733576182695898  We can visualize the difference with a barplot.\nsns.barplot(x=dgp.T, y=dgp.Y, data=df);  It seems there is a significant difference between the two groups. We can get a standard error around the estimate by regressing $Y$ on $D$.\nest = smf.ols(f'{dgp.Y} ~ {dgp.T}', df).fit() est.summary().tables[1]    coef std err t P|t| [0.025 0.975]   Intercept  -0.1170  0.076  -1.545  0.123  -0.266  0.032   ad_exposure[T.True]  1.2734  0.107  11.894  0.000  1.063  1.483   The coefficient is statistically significant. However, we know that the treatment assignment is not unconditionally exogenous. We need to condition on observables $X$.\nest = smf.ols(f'{dgp.Y} ~ {dgp.T} +' + ' + '.join(dgp.X), df).fit() est.summary().tables[1]    coef std err t P|t| [0.025 0.975]   Intercept  -0.1026  0.247  -0.416  0.678  -0.587  0.382   ad_exposure[T.True]  1.0342  0.103  10.044  0.000  0.832  1.236   male  0.9987  0.102  9.765  0.000  0.798  1.199   black  -0.8234  0.126  -6.555  0.000  -1.070  -0.577   age  -0.0050  0.005  -1.001  0.317  -0.015  0.005   educ  0.1240  0.035  3.533  0.000  0.055  0.193   We do not need to actually condition on the full vector of observables $X$. It is sufficient to condition on the propensity score $p(X)$, i.e. the conditional probability of treatment.\nWe can estimate the propensity score with any method we want. The more flexible, the better.\ndf['pscore'] = RandomForestRegressor().fit(df[dgp.X], df[dgp.T]).predict(df[dgp.X]) df['pscore'].head()  0 0.368726 1 0.036667 2 0.675000 3 0.701702 4 0.307456 Name: pscore, dtype: float64  We now estiamte the conditional average treatment effect regressing $Y$ on $D$ and on the p-score.\nest = smf.ols(f'{dgp.Y} ~ {dgp.T} + pscore', df).fit() est.summary().tables[1]    coef std err t P|t| [0.025 0.975]   Intercept  -0.3402  0.103  -3.315  0.001  -0.542  -0.139   ad_exposure[T.True]  0.8957  0.159  5.638  0.000  0.584  1.208   pscore  0.8235  0.257  3.204  0.001  0.319  1.328   We are now ready to estimate heterogeneous treatment effects. First, we need to compute the transformed outcome\n$$ Y_i^* = \\frac{Y_i}{T_i * e_i - (1-T_i) * (1-e_i)} $$\ndf['y_star'] = df[dgp.Y] /(df[dgp.T] * df['pscore'] - (1-df[dgp.T]) * (1-df['pscore']))  Now we train a small tree on the transformed outcome $Y^*$.\ntree = DecisionTreeRegressor(max_depth=2, min_samples_leaf=30).fit(df[dgp.X], df['y_star']) df['y_hat'] = tree.predict(df[dgp.X])  We can plot the tree and visualize the estimated groups and treatment effects.\n# Plot tree plot_tree(tree, filled=True, fontsize=12, feature_names=dgp.X, impurity=False);  We still have one issue: we have trained our tree model and estimated the treatment effects using the same data. This introduces bias in the estimates.\nfrom econml.causal_forest import CausalForest model = CausalForest(max_depth=2).fit(Y=df[dgp.Y], X=df[dgp.X], T=df[dgp.T])  We can plot a tree\nfrom econml.cate_interpreter import SingleTreeCateInterpreter intrp = SingleTreeCateInterpreter(max_depth=2).interpret(model, df[dgp.X]) intrp.plot(feature_names=dgp.X, fontsize=10)  Example 2: Lalonde Data For this tutorial, we are goind to use the data from Lalonde (1981). You can find the data here: https://users.nber.org/~rdehejia/nswdata.html\ndf = pd.read_csv('data/lalonde86.csv') df.head()   .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }  \n  treat age education black hispanic married nodegree re74 re75 re78     0 1.0 37.0 11.0 1.0 0.0 1.0 1.0 0.0 0.0 9930.0460   1 1.0 22.0 9.0 0.0 1.0 0.0 1.0 0.0 0.0 3595.8940   2 1.0 30.0 12.0 1.0 0.0 0.0 0.0 0.0 0.0 24909.4500   3 1.0 27.0 11.0 1.0 0.0 0.0 1.0 0.0 0.0 7506.1460   4 1.0 33.0 8.0 1.0 0.0 0.0 1.0 0.0 0.0 289.7899     We can summarize each variable by its treatment status.\ndf.groupby('treat').agg(['mean', 'std']).T.unstack(1)   .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead tr th { text-align: left; }  \n  treat 0.0 1.0    mean std mean std     age 25.053846 7.057745 25.816216 7.155019   education 10.088462 1.614325 10.345946 2.010650   black 0.826923 0.379043 0.843243 0.364558   hispanic 0.107692 0.310589 0.059459 0.237124   married 0.153846 0.361497 0.189189 0.392722   nodegree 0.834615 0.372244 0.708108 0.455867   re74 2107.026658 5687.905694 2095.573689 4886.620353   re75 1266.909002 3102.982044 1532.055314 3219.250870   re78 4554.801126 5483.835991 6349.143530 7867.402218     y = 're78' D = 'treat'  est = smf.ols('re78 ~ treat', df).fit() est.summary().tables[1]    coef std err t P|t| [0.025 0.975]   Intercept  4554.8011  408.046  11.162  0.000  3752.855  5356.747   treat  1794.3424  632.853  2.835  0.005  550.574  3038.110   X = ['age', 'education', 'black', 'hispanic', 'married', 'nodegree']  est = smf.ols('re78 ~ treat +' + ' + '.join(X), df).fit() est.summary().tables[1]    coef std err t P|t| [0.025 0.975]   Intercept  1168.0035  3360.588  0.348  0.728 -5436.921  7772.928   treat  1671.1304  637.973  2.619  0.009  417.254  2925.007   age  52.8219  45.255  1.167  0.244  -36.123  141.767   education  393.8213  227.114  1.734  0.084  -52.549  840.192   black -2220.2622  1168.317  -1.900  0.058 -4516.480  75.956   hispanic  83.7193  1550.348  0.054  0.957 -2963.346  3130.785   married  158.2084  850.326  0.186  0.852 -1513.029  1829.446   nodegree  -128.2203  995.416  -0.129  0.898 -2084.617  1828.177   df['pscore'] = RandomForestRegressor().fit(df[X], df[D]).predict(df[X]) df['pscore'].head()  0 0.810000 1 0.752000 2 0.485786 3 0.670023 4 0.731667 Name: pscore, dtype: float64  est = smf.ols('re78 ~ treat + pscore', df).fit() est.summary().tables[1]    coef std err t P|t| [0.025 0.975]   Intercept  4313.9001  591.433  7.294  0.000  3151.530  5476.270   treat  1461.6324  866.171  1.687  0.092  -240.692  3163.957   pscore  894.6072  1588.766  0.563  0.574 -2227.867  4017.082   Estimate CATE\nfrom econml.causal_forest import CausalForest model = CausalForest(max_depth=2).fit(Y=df[y], X=df[X], T=df[D])  from econml.cate_interpreter import SingleTreeCateInterpreter intrp = SingleTreeCateInterpreter(max_depth=2).interpret(model, df[X]) intrp.plot(feature_names=X, fontsize=10)  References  Original paper: Recursive partitioning for heterogeneous causal effects (2016) by Athey and Imbens Video lecture by Prof. Susan Athey (Stanford)  ","date":1649980800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1649980800,"objectID":"801c3459d81ea394bb44113ad5d513d6","permalink":"https://matteocourthoud.github.io/post/causal_trees/","publishdate":"2022-04-15T00:00:00Z","relpermalink":"/post/causal_trees/","section":"post","summary":"In this tutorial, we are going to see how to estimate heterogeneous treatment effects using regression trees.\nFor this tutorial, I assume you are familiar with the following concepts:\n Rubin\u0026rsquo;s potential outcome framework Decision tree methods Propensity score matching  Setting We assume that for a set of i.","tags":null,"title":"Causal Trees","type":"post"},{"authors":null,"categories":null,"content":"If you search the Wikipedia definition of Chi-Squared test, you get the following definition:\n Pearson\u0026rsquo;s chi-squared test $\\chi^2$ is a statistical test applied to sets of categorical data to evaluate how likely it is that any observed difference between the sets arose by chance.\n What does it mean? Let\u0026rsquo;s see it together.\nExample 1: is a dice fair? Suppose you want to test whether a dice is fair.\nYou throw the dice 60 times and you count the number of times you get each outcome.\nLet\u0026rsquo;s simulate some data (from a fair dice).\nimport numpy as np import pandas as pd # Data generating process def generate_data_dice(N=60, seed=1): np.random.seed(1) # Set seed for replicability dice_numbers = [1,2,3,4,5,6] # Dice numbers dice_throws = np.random.choice(dice_numbers, size=N) # Actual dice throws data = pd.DataFrame({\u0026quot;dice number\u0026quot;: dice_numbers, \u0026quot;observed\u0026quot;: [sum(dice_throws==n) for n in dice_numbers], \u0026quot;expected\u0026quot;: N / 6}) return data  # Generate data data_dice = generate_data_dice() data_dice   .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }  \n  dice number observed expected     0 1 10 10.0   1 2 14 10.0   2 3 6 10.0   3 4 8 10.0   4 5 12 10.0   5 6 10 10.0     If we were throwing the dice a lot of times, we would expect the same number of observations for each outcome. However, there is inherent noise in the process. How can we tell whether the fact that we didn\u0026rsquo;t get exactly 10 observations for each outcome is just due to randomness or it\u0026rsquo;s because the dice is unfair?\nThe idea is to compute some statistic whose distribution is known under the assumption that the dice is fair, and then check if its value is \u0026ldquo;unusual\u0026rdquo; or not. If the value is particularly \u0026ldquo;unusual\u0026rdquo;, we reject the null hypothesis that the dice is fair.\nIn our case, the statistic we choose is the chi-squared $\\chi^{2}$ test-statistic.\nThe value of the Pearson\u0026rsquo;s chi-squared test-statistic is\n$$ T_{\\chi^2} = \\sum _{i=1}^{n} \\frac{(O_i - E_i)^{2}}{E_i} = N \\sum _{i=1}^{n} \\frac{\\left(O_i/N - p_i \\right)^2 }{p_i} $$\nwhere\n $O_{i}$ = the number of observations of type i. $N$ = total number of observations $E_{i}=Np_{i}$ = the expected (theoretical) count of type $i$, asserted by the null hypothesis that the fraction of type $i$ in the population is $p_{i}$ $n$ = the number of cells in the table.  # Compute value of the statistic def compute_chi2_stat(data): return sum( (data.observed - data.expected)**2 / data.expected ) chi2_stat = compute_chi2_stat(data_dice) chi2_stat  4.0  What do we make of this number? Is it \u0026ldquo;unusual\u0026rdquo;?\nIf the dice were fair, the test Pearson\u0026rsquo;s chi-squared test-statistic $T_{\\chi^2}$ is distributed as a chi-squared distribution with $k-1$ degrees of freedom, $\\chi^2_{k-1}$. For the moment, take this claim at face value, we will verify it later, both empirically and theoretically. We will also discuss the degrees of freedom in detail later on.\nImportant! Do not confuse the chi-squared test statistic (a number) with the chi-squared distribution (a distribution).\nWhat does a chi-squared distribution with $n-1$ degrees of freedom, $\\chi^2_{k-1}$, look like?\n%matplotlib inline %config InlineBackend.figure_format = 'retina' from src.utils import *  from scipy.stats import chi2 # x-axis ranges from 0 to 20 with .001 steps x = np.arange(0, 30, 0.001) # Chi-square distribution with 5 degrees of freedom chi2_5_pdf = chi2.pdf(x, df=5) # Plot plt.plot(x, chi2_5_pdf);  How does the value of the statistic we have observed compares with its the distribution under the null hypothesis of a fair dice?\n# plot Chi-square distribution with 5 degrees of freedom plt.plot(x, chi2.pdf(x, df=5)); plt.vlines(chi2_stat, ymin=0, ymax=plt.ylim()[1], color='k', label='chi2 statistic') plt.legend();  The test statistic seems to fall well within the distribution, i.e. it does not seem to be an unusual event. Indeed, the question we want to answer is: \u0026ldquo;under the null hypothesis that the dice is fair, how unlikely is the statistic we have observed?\u0026rdquo;.\nThe last component we need in order to build a hypothesis test is a level of confidence, i.e. a threshold of \u0026ldquo;unlikeliness\u0026rdquo; of an event, below which we declare that the event is too unlikely under the model, for the model to be true. Let\u0026rsquo;s say we decide to set that threshold at 5%.\nIf the likelihood of observing an even that (or more) extreme than the one we have actually observed is less than 5%, we reject the null hypothesis that the dice is fair.\nWhat is this value for a chi-squared distribution with 5 degrees of freedom?\n# Compute the Percent Point Function of the chi-squared distribution z95 = chi2.ppf(0.95, df=5) z95  11.070497693516351  Since our value is smaller, we do not reject the null.\nWe can plot the rejection and non-rejection areas in a plot.\n# Visualize test def plot_test(x, stat, df): z95 = chi2.ppf(0.95, df=df) chi2_pdf = chi2.pdf(x, df=df) plt.plot(x, chi2_pdf); plt.fill_between(x[x\u0026gt;z95], chi2_pdf[x\u0026gt;z95], color='r', alpha=0.4, label='rejection area') plt.fill_between(x[x\u0026lt;z95], chi2_pdf[x\u0026lt;z95], color='g', alpha=0.4, label='non-rejection area') plt.vlines(chi2_stat, ymin=0, ymax=plt.ylim()[1], color='k', label='chi2 statistic') plt.legend();  # Plot plot_test(x, chi2_stat, 5)  From the plot, we can clearly see that we do not reject the null hypothesis that the dice is fair.\nWhy the Chi-squared Distribution? How do we know that that particular statistic has that particular distribution?\nBefore digging into the math, we can check this claim via simulation. We will repeat the procedure above many times, i.e.\n roll a (fair) dice 60 times compute the chi-square statistic  and then plot the distribution of chi square statistics.\n# Function to simulate data and compute chi2 stats def simulate_chi2stats(K, N, dgp): chi2_stats = [] for i in range(K): data = dgp() chi2_stats += [compute_chi2_stat(data)] return np.array(chi2_stats)  chi2_stats = simulate_chi2stats(K=100, N=60, dgp=generate_data_dice) # Plot data plt.hist(chi2_stats, density=True, bins=30, alpha=0.3, color='C0'); plt.plot(x, chi2_5_pdf);  Since we only did it 100 times, the distribution looks pretty coarse but vaguely close. Let\u0026rsquo;s now try 1000 times.\nchi2_stats = simulate_chi2stats(K=1000, N=60, dgp=generate_data_dice) # Plot data plt.hist(chi2_stats, density=True, bins=30, alpha=0.3, color='C0'); plt.plot(x, chi2_5_pdf);  The empirical distribution of the test statistic is indeed very close to its theoretical counterpart.\nSome Statistics Why does the distribution of the test statistic look like that? Let\u0026rsquo;s now dig deeper into the math.\nThere are two things we need to know in order to understand the answer:\n the Central Limit Theorem the relationship between a chi-squared and a normal distribution  The Central Limit Theorem says that\n In probability theory, the central limit theorem (CLT) establishes that, in many situations, when independent random variables are summed up, their properly normalized sum tends toward a normal distribution (informally a bell curve) even if the original variables themselves are not normally distributed.\n Where does a normal distribution come up in our case? If we look at a single row in our data, i.e. the occurrences of a specific dice throw, it can be interpreted as the sum of realization from a Bernoulli distribution with probability 1/6.\n In probability theory and statistics, the Bernoulli distribution is the discrete probability distribution of a random variable which takes the value $1$ with probability $p$ and the value $0$ with probability $q=1-p$.\n In our case, the probability of getting a particular number is exactly 1/6. What is the distribution of the sum of its realizations? The Central Limit Theorem also tells us that:\n If $X_1, X_2, \\dots , X_n, \\dots$ are random samples drawn from a population with overall mean $\\mu$ and finite variance $\\sigma^2$, and if $\\bar X_n$ is the sample mean of the first $n$ samples, then the limiting form of the distribution,\n$$ Z = \\lim_{n \\to \\infty} \\sqrt{n} \\left( \\frac{\\bar X_n - \\mu }{\\sigma} \\right) $$\nis a standard normal distribution.\n Therefore, in our case, the distribution of the sum of Bernoulli distributions with mean $p$ is distributed as a normal distribution with\n mean $p$ variance $p * (1-p)$  Therefore, we can obtain a random variable that is asymptotically standard normal distributed as\n$$ \\lim_{n \\to \\infty} \\ \\sqrt{n} \\left( \\frac {\\bar X_n - p}{\\sqrt{p * (1-p)}} \\right) \\sim N(0,1) $$\nOur last piece: what is a chi-squared distribution?\n If $Z_1, \u0026hellip;, Z_k$ are independent, standard normal random variables, then the sum of their squares,\n$$ Q = \\sum_{i=1}^k Z_i^2 $$ is distributed according to the chi-squared distribution with $k$ degrees of freedom.\n I.e. the sum of standard normal distributions is a chi-squared distribution, where the degrees of freedom indicate the number of normal distributions we are summing over. Since the normalized sum of realizations of each dice number should converge to a standard normal distribution, their sum of squares should converge to a chi-squared distribution. I.e.\n$$ \\lim_{n \\to \\infty} \\ \\sum_k n \\frac{(\\bar X_n - p)^2}{p * (1-p)} \\sim \\chi^2_k $$\nThere is just one issue: the last distribution is not really independent from the others. In fact, as soon as we know that we have thrown 60 dices and how many 1s, 2s, 3s, 4s, and 5s we got, we can compute the number of 6s. Therefore, we should exclude one distribution since only 5 (or, in general, $k-1$) are truly independent.\nIn practice, however, we sum all distributions, but then we scale them down by multiplying them by $(1-p)$ so that we have\n$$ \\lim_{n \\to \\infty} \\ \\sum_k n \\frac{(\\bar X_n - p)^2}{p} \\sim \\chi^2_{k-1} $$\nwhich is exactly the formula we used to compute the test statistic:\n$$ T_{\\chi^2} = \\sum _{i=1}^{n} \\frac{(O_i - E_i)^{2}}{E_i} = N \\sum _{i=1}^{n} \\frac{\\left(O_i/N - p_i \\right)^2 }{p_i} $$\nExample 2: are grades independent from gender? Chi-squared tests can also be used to test independence between 2 variables. The idea is fundamentally the same as the test in the previous section: checking systematic differences between observed and expected values, across different variables.\nSuppose you have data on grades in a classroom, by gender. Grades go from $1$ to $4$. Assuming males and females are equally prepared for the test, you want to test whether there has been discrimination in grading.\nThe problem is again asserting whether the observed differences are random or systematic.\nLet\u0026rsquo;s generate some data (under the no discrimination assumption).\n# Data generating process for grades def generate_data_grades(N_male=60, N_female=40): grade_scale = [1,2,3,4] p = [0.1, 0.2, 0.5, 0.2] grades_male = np.random.choice(grade_scale, size=N_male, p=p) grades_female = np.random.choice(grade_scale, size=N_female, p=p) data = pd.DataFrame({\u0026quot;grade\u0026quot;: grade_scale + grade_scale, \u0026quot;gender\u0026quot;: [\u0026quot;male\u0026quot; for i in grade_scale] + [\u0026quot;female\u0026quot; for i in grade_scale], \u0026quot;observed\u0026quot;: [sum(grades_male==n) for n in grade_scale] + [sum(grades_female==n) for n in grade_scale], }) data['expected gender'] = data.groupby(\u0026quot;gender\u0026quot;)[\u0026quot;observed\u0026quot;].transform(\u0026quot;mean\u0026quot;) data['expected grade'] = data.groupby(\u0026quot;grade\u0026quot;)[\u0026quot;observed\u0026quot;].transform(\u0026quot;mean\u0026quot;) data['expected'] = data['expected gender'] * data['expected grade'] / data['observed'].mean() return data  # Set seed for replicability np.random.seed(1) # Generate data data_grades = generate_data_grades() data_grades   .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }  \n  grade gender observed expected gender expected grade expected     0 1 male 9 15.0 5.5 6.6   1 2 male 14 15.0 10.5 12.6   2 3 male 27 15.0 24.5 29.4   3 4 male 10 15.0 9.5 11.4   4 1 female 2 10.0 5.5 4.4   5 2 female 7 10.0 10.5 8.4   6 3 female 22 10.0 24.5 19.6   7 4 female 9 10.0 9.5 7.6     Has there been discrimination?\nThe value of the test-statistic is\n$$ T_{\\chi^2} = \\sum_{i=1}^r \\sum_{j=1}^c \\frac{(O_{i,j} - E_{i,j})^2 }{ E_{i,j} } = N \\sum_{i,j} p_{i \\cdot} p_{\\cdot j} \\left( \\frac{O_{i,j}/N - p_{i \\cdot} p_{\\cdot j} }{ p_{i \\cdot} p_{\\cdot j}} \\right)^2 $$\nwhere\n  $N$ is the total sample size (the sum of all cells in the table)\n  $p_{i \\cdot} = \\frac{O_{i\\cdot }}{N} = \\sum_{j=1}^{c} \\frac{O_{i,j}}{N}$ is the fraction of observations of type i ignoring the column attribute (fraction of row totals), and\n  $p_{\\cdot j} = \\frac{O_{\\cdot j}}{N} = \\sum_{i=1}^{r} \\frac{O_{i,j}}{N}$ is the fraction of observations of type j ignoring the row attribute (fraction of column totals).\n  So the formula for the test statistic is the same\nchi2_stat = compute_chi2_stat(data_grades) chi2_stat  3.490327550477927  As before, we can double-check whether the statistic is indeed distributed as a chi-squared with $k-1$ degrees of freedom by simulating the data generating process.\n# Plot data chi2_stats = simulate_chi2stats(K=1000, N=60, dgp=generate_data_grades) plt.hist(chi2_stats, density=True, bins=30, alpha=0.3, color='C0'); plt.plot(x, chi2_5_pdf);  What happened? We forgot to change the degrees of freedom! The general formula for the degrees of freedom when testing the independence of variables is $(N_i - 1) \\times (N_j - 1)$. So in our case, it\u0026rsquo;s $(4-1) \\times (2-1) = 3$.\nchi2_3_pdf = chi2.pdf(x, df=3) # Plot data chi2_stats = simulate_chi2stats(K=1000, N=60, dgp=generate_data_grades) plt.hist(chi2_stats, density=True, bins=30, alpha=0.3, color='C0'); plt.plot(x, chi2_3_pdf);  Do we reject the null hypothesis of independent distributions of gender and grades?\n# Plot plot_test(x, chi2_stat, df=3)  No, we do not reject the null hypothesis of independend distributions of gender and grades.\nExample 3: testing a specific data generating process As we have seen, the chi-square test can be used to compare observed means/frequencies against a null hypothesis. How can we use this statistic to test a distributional assumption?\nThe answer is simple: we can construct conditional means. The easiest way to do it is to bin the data into equally sized bins and then check if the observed frequencies match the expected probabilities.\nNote: we don\u0026rsquo;t need to have equally sized bins, but it\u0026rsquo;s useful since it ensures that we have as many observations in each bin as possible.\nfrom scipy.stats import expon # Data generating process def generate_poisson_data(N=60, cuts=4): poisson_draws = np.random.exponential(size=N) cat, bins = pd.qcut(poisson_draws, cuts, retbins=True) p = [expon.cdf(bins[n+1]) - expon.cdf(bins[n]) for n in range(len(bins)-1)] data = pd.DataFrame({\u0026quot;bin\u0026quot;: cat.unique(), \u0026quot;observed\u0026quot;: [sum(cat==n) for n in cat.unique()], \u0026quot;expected\u0026quot;: np.dot(p, N)}) return data, poisson_draws  # Set seed for replicability np.random.seed(2) # Generate data data_poisson, poisson_draws = generate_poisson_data() data_poisson   .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }  \n  bin observed expected     0 (0.254, 0.573] 15 11.919795   1 (0.0253, 0.254] 15 12.706904   2 (0.573, 0.923] 15 9.967649   3 (0.923, 5.092] 15 23.481200     We can also plot the observed and realized distribution of the data.\n# Plot data plt.hist(poisson_draws, density=True, bins=30, alpha=0.3, color='C0'); exp_pdf = expon.pdf(x) plt.plot(x, exp_pdf);  The two distributions seem close but we need a test statistic in order to assess whether the dgp is indeed an exponential distribution\nchi2_stat = compute_chi2_stat(data_poisson) chi2_stat  6.813781416601728  Do we reject the null hypothesis that the data is drawn from an exponential distribution?\n# Plot plot_test(x, chi2_stat, df=3)  No, we do not reject the null hypothesis that the data is drawn from an exponential distribution.\nConclusion In this tutorial, we have seen how to perform 3 hypoteses tests\n testing if a set of means or sums is coming from the expected distribution testing if two distributions are independent or not testing a specific data generating process  The underlying principle is the same: testing discrepancies between expected and observed count data.\nThe key statistic is Pearson\u0026rsquo;s chi-square statistic and the key distribution is the chi-squared distribution. We have seen how to compute the statistic, why it has a chi-squared distribution, and how to use this information to perform a statistical hypothesis test.\n","date":1649980800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1649980800,"objectID":"6cc8886782d077be8d6a99ab075dcb4a","permalink":"https://matteocourthoud.github.io/post/chisquared/","publishdate":"2022-04-15T00:00:00Z","relpermalink":"/post/chisquared/","section":"post","summary":"If you search the Wikipedia definition of Chi-Squared test, you get the following definition:\n Pearson\u0026rsquo;s chi-squared test $\\chi^2$ is a statistical test applied to sets of categorical data to evaluate how likely it is that any observed difference between the sets arose by chance.","tags":null,"title":"Chi-Squared Test for Dummies","type":"post"},{"authors":null,"categories":null,"content":"In this tutorial, we are going to see how to estimate the causal effect of a treatment on an outcome when treatment assignment is not random, but we observe both treated and untreated units before and after treatment. Under certain structural assumptions, especially parallel outcome trends in the absence of treatment, we can recover the average treatment effect.\nRequisites\nFor this tutorial, I assume you are familiar with the following concepts:\n Rubin\u0026rsquo;s potential outcome framework Ordinary least squares regression  Academic Application\nAs an academic application, we are going to replicate Minimum Wages and Employment: A Case Study of the Fast-Food Industry in New Jersey and Pennsylvania (1994) by Card and Krueger. The authors study the effect of a minimum wage policy in New Jersey on emplyment, by using Pennsylvania as a control state.\nBusiness Case\nAs a business case, we are going to study a firm that has run a TV ad campaign. The firm would like to understand the impact of the campaign on revenue and has randomized the campaign over municipalities.\nSetting We assume that for a set of i.i.d. subjects $i = 1, \u0026hellip;, n$ over $T$ time periods $t = 1 , \u0026hellip; , T$, we observed a tuple $(X_{it}, D_{it}, Y_{it})$ comprised of\n a feature vector $X_{it} \\in \\mathbb R^{p}$ a treatment assignment $D_i \\in \\lbrace 0, 1 \\rbrace$ a response $Y_{it} \\in \\mathbb R$  We assume that treatment occurs between time $t=0$ and time $t=1$.\nAssumption 1: parallel trends\nIn the absence of treatment, the outcome $Y_{it}$ evolve in parallel across units, i.e. and their $\\gamma_{t}$ are the same.\n$$ Y_{it}^{(0)} - Y_{j,t}^{(0)} = \\alpha \\quad \\forall \\ t $$\nDiff-in-diffs In this setting, we cannot estimate any causal parameter with any other further assumption. What is the minimal number of assumptions that we could make in order to estimate a causal parameter?\nIf we were to assume that treatment was randomly assigned, we could retrieve the average treatment effect as a difference in means.\n$$ \\mathbb E[\\tau_t] = \\mathbb E \\big[ Y_{it} \\ \\big| \\ D_i = 1 \\big] - \\mathbb E \\big[ Y_{it} \\big| \\ D_i = 0 \\big] $$\nHowever, it would be a very strong assumption, and it would ignore some information that we possess: the time dimension (pre-post).\nIf we were to assume instead that no other shocks affected the treated units between period $t=0$ and $t=1$, we could retrieve the average treatment effect on the treated as a pre-post difference.\n$$ \\mathbb E[\\tau | D_i=1] = \\mathbb E \\big[ Y_{i1} \\ \\big| \\ D_i = 1 \\big] - \\mathbb E \\big[ Y_{i0} \\ \\big| \\ D_i = 1 \\big] $$\nHowever, it also this would be a very strong assumption, and it would ignore the fact that we have control units.\nCan we make less stringent assumption and still recover a causal parameter using both the availability of a (non-random) control group and the time dimension?\nDiD Model The model that is commonly assumed in diff-ind-diff settings, is the following\n$$ Y_{it} (D_{it}) = \\alpha_{i} + \\gamma_{t} + \\tau_{i} D_{it} $$\nFirst, let\u0026rsquo;s summarize the potential outcome values $Y^{(d)}_{it}$ in the simple $2 \\times 2$ setting.\n    $t=0$ $t=1$     $D=0$ $\\gamma_0 + \\alpha_i$ $\\gamma_1 + \\alpha_i$   $D=1$ $\\gamma_0 + \\alpha_i + \\tau_i$ $\\gamma_1 + \\alpha_i + \\tau_i$    For a single unit, $i$, the pre-post outcome difference is given by\n$$ Y_{i1} - Y_{i0} = (\\gamma_1 - \\gamma_0) + \\tau_i (D_{i1} - D_{i0}) $$\nIf we take the difference of the expression above between treated and untreated units, we get\n$$ \\mathbb E \\Big[ Y_{i1} - Y_{i0} \\ \\Big| \\ D_{i1} - D_{i0} = 1 \\Big] - \\mathbb E \\Big[ Y_{i1} - Y_{i0} \\ \\Big| \\ D_{i1} - D_{i0} = 0 \\Big] = \\mathbb E \\Big[ \\tau_i \\ \\Big| \\ D_{i1} - D_{i0} = 1 \\Big] = ATT $$\nwhich is the average treatment effect on the treated (ATT).\nWe can get this double difference with the folowing regressio model\n$$ Y_{it} (D_{it}) = \\alpha_{i} + \\gamma_{t} + \\beta D_{it} + \\varepsilon_{it} $$\nwhere the OLS estimator $\\hat \\beta$ will be unbiased for the ATT.\nMultiple Time Periods What if we didn\u0026rsquo;t just have one pre-treatment period and one post-treatment period? Great! We can actually do more things.\n We can partially test assumptions We can estimate dynamic effects We can run placebo tests  How do we implement it? Run a regression with multiple interactions\n$$ Y_{it} (D_{it}) = \\alpha_{i} + \\gamma_{t} + \\sum_{t=1}^{T} \\beta_t D_{it} + \\varepsilon_{it} $$\nComments Parametric Assumption\nThe diff-in-diffs method makes a lot of parametric assumptions that are is easy to forget.\nInference\nBertrand, Duflo, and Mullainathan (2004) point out that conventional robust standard errors usually overestimate the actual standard deviation of the estimator. The authors recommend clustering the standard errors at the level of randomization (e.g. classes, counties, villages, \u0026hellip;).\nTesting pre-trends\nHaving multiple pre-treatment time periods is helpful for testing the parallel trends assumption. However, this practice can lead to pre-testing bias. In particular, if one selects results based on a pre-treatment parallel trend test, inference on the ATT gets distorderd.\nAcademic Application As an academic application, we are going to replicate Minimum Wages and Employment: A Case Study of the Fast-Food Industry in New Jersey and Pennsylvania (1994) by Card and Krueger. The authors study the effect of a minimum wage policy in New Jersey on emplyment, by using Pennsylvania as a control state.\nThe authors describe the setting as follows\n On April 1, 1992, New Jersey\u0026rsquo;s minimum wage rose from $4.25 to $5.05 per hour. To evaluate the impact of the law, the authors surveyed 410 fast-food restaurants in New Jersey and eastern Pennsylvania before and after the rise. Comparisons of employment growth at stores in New Jersey and Pennsylvania (where the minimum wage was constant) provide simple estimates of the effect of the higher minimum wage.\n Let\u0026rsquo;s start by loading and inspecting the data.\n%matplotlib inline %config InlineBackend.figure_format = 'retina'  from src.utils import *  from src.data import import_ck94 df = pd.read_csv('data/ck94.csv') df.head()   .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }  \n  id after new_jersey chain employment hrsopen wage     0 3 0 0 wendys 34.0 12.0 5.00   1 4 0 0 wendys 24.0 12.0 5.50   2 6 0 0 burgerking 70.5 18.0 5.00   3 7 0 0 burgerking 23.5 24.0 5.00   4 8 0 0 kfc 11.0 10.0 5.25     We have information on fast food restaurants, indexed by i, at time t. We distinguish between before and faster treatment and between New Jersey nj and Pennsylvania restaurants. We also know the chain of the restaurant, the employment, the hours open hrsopen and the wage. We are interested on the effect on the policy on wages.\nLet\u0026rsquo;s start by producing the $2 \\times 2$ table of treatment-control before-after average outcomes.\ndf.pivot_table(index='new_jersey', columns='after', values='employment', aggfunc='mean')   .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }  \n after 0 1   new_jersey       0 23.704545 21.825758   1 20.657746 21.048415     From the table we can see that a simple before-after comparison would give a small posive effect of $21.05 - 20.66 = 0.39$.\nOn the other hand, if one was doing an ex-post treated-control comparison, would get a negative effect of $21.05 - 21.83 = - 0.78$.\nThe difference-in-differences estimator takes into account the fact that\n There is a pre-treatment level difference between New Jersey and Pennsylvania Employment was falling in Pennsylvania even without treatment  The double difference in means gives a positive effect, significantly larger than any of the two previous estimates.\n$$ \\hat \\tau_{DiD} = \\Big( 21.05 - 20.66 \\Big) - \\Big( 21.83 - 23.70 \\Big) = 0.39 + 1.87 = 2.26 $$\nWe can replicate the result with a linear regression.\nsmf.ols('employment ~ new_jersey * after', df).fit().summary().tables[1]    coef std err t P|t| [0.025 0.975]   Intercept  23.7045  1.149  20.627  0.000  21.448  25.961   new_jersey  -3.0468  1.276  -2.388  0.017  -5.552  -0.542   after  -1.8788  1.625  -1.156  0.248  -5.070  1.312   new_jersey:after  2.2695  1.804  1.258  0.209  -1.273  5.812   The effect is $2.26$, but it is not significantly different from zero.\nBusiness Case We are given the following problem:\n A firm wants to test the impact of a TV advertisement campaign on revenue. The firm releases the ad on a random sample of municipalities and track the revenue over time, before and after the ad campaign.\n We start by drawing a sample from the data generating process.\nfrom src.dgp import dgp_did dgp = dgp_did() df = dgp.generate_data() df.head()   .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }  \n  day id treated post revenue     0 1 1 0 False 3.599341   1 1 2 0 False -0.146912   2 1 3 0 False 0.696527   3 1 4 0 False 1.445169   4 1 5 0 False 1.659696     We have revenue data on a set of customers over time. We also know to which group they were assigned and whether the time is before or after the intervention.\nSince we do not have any control variable, we can directly visualize the revenue dynamics, distinguishing between treatment and control group.\nsns.lineplot(x=df['day'], y=df['revenue'], hue=df['treated']); plt.axvline(x=10, ls=\u0026quot;:\u0026quot;, color='C2'); plt.title('Revenue over time');  It seems like the treatment group was producing higher revenues before treatment and the gap has increased with treatment but it is closing over time.\nTo assess the magnitude of the effect and perform inference, we can regress revenue on a post-treatment dummy, a treatment dummy and their interaction.\nsmf.ols('revenue ~ post * treated', df).fit().summary().tables[1]    coef std err t P|t| [0.025 0.975]   Intercept  4.6357  0.078  59.428  0.000  4.483  4.789   post[T.True]  0.8928  0.110  8.093  0.000  0.676  1.109   treated  1.0558  0.110  9.571  0.000  0.839  1.272   post[T.True]:treated  0.1095  0.156  0.702  0.483  -0.196  0.415   While the coefficient for the interaction term is positive, it does not seem to be statistically significant. However, this might be due to the fact that the treatment effect is fading away over time.\nLet\u0026rsquo;s fit the same regression, with a linear time trend.\nsmf.ols('revenue ~ post * treated * day', df).fit().summary().tables[1]    coef std err t P|t| [0.025 0.975]   Intercept  4.1144  0.168  24.501  0.000  3.785  4.444   post[T.True]  0.2871  0.458  0.626  0.531  -0.612  1.186   treated  1.0788  0.237  4.543  0.000  0.613  1.544   post[T.True]:treated  1.5910  0.648  2.454  0.014  0.320  2.862   day  0.0948  0.027  3.502  0.000  0.042  0.148   post[T.True]:day  -0.0221  0.038  -0.576  0.564  -0.097  0.053   treated:day  -0.0042  0.038  -0.109  0.913  -0.079  0.071   post[T.True]:treated:day  -0.0929  0.054  -1.716  0.086  -0.199  0.013   Now the treatment effect is positive and significant at the 5% level. And indeed, we estimate a decreasing trend, post treatment, for the treated. However, it is not statistically significant.\nReferences  Video lecture on Difference-in-Differences by Paul Goldsmith-Pinkham (Yale) Chapter 13 of Causal Inference for The Brave and The True by Matheus Facure Chapter 9 of The Causal Inference Mixtape by Scott Cunningham Chapter 5 of Mostly Harmless Econometrics by Agrist and Pischke  ","date":1649980800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1649980800,"objectID":"46a37fe1c4822f8dc30b06a61cf083c7","permalink":"https://matteocourthoud.github.io/post/diff_in_diffs/","publishdate":"2022-04-15T00:00:00Z","relpermalink":"/post/diff_in_diffs/","section":"post","summary":"In this tutorial, we are going to see how to estimate the causal effect of a treatment on an outcome when treatment assignment is not random, but we observe both treated and untreated units before and after treatment.","tags":null,"title":"Difference in Differences","type":"post"},{"authors":null,"categories":null,"content":"In this tutorial, we are going to see how to estimate causal effects when the treatment is not randomly assigned, but we have access to a third variable that is as good as randomly assigned and is correlated (only) with the treatment. These variables are called instrumental variables and are a powerful tool for causal inference, especially in observational studies.\nRequisites\nFor this tutorial, I assume you are familiar with the following concepts:\n Rubin\u0026rsquo;s potential outcome framework Ordinary least squares regression  Academic Application 1\nAs a first academic application, we are going to replicate Does Compulsory School Attendance Affect Schooling and Earnings? (1991) by Angrist and Krueger. The authors study the effect of education on wages.\nAcademic Application 2\nAs a further academic application, we are going to replicate The Colonial Origins of Comparative Development (2002) by Acemoglu, Johnson, Robinson. The authors study the effect of institutions on economic development.\nBusiness Case\nAs a business case, we are going to study a company that wants to find out whether subscribing to its newsletter has an effect on revenues. Since the travel agency cannot force customers to subscribing to the newsletter, it randomly sends reminder emails to infer the effect of the newsletter on revenues.\nSetting We assume that for a set of i.i.d. subjects $i = 1, \u0026hellip;, n$ we observed a tuple $(X_i, T_i, Y_i)$ comprised of\n a feature vector $X_i \\in \\mathbb R^n$ a treatment variable $T_i \\in \\lbrace 0, 1 \\rbrace$ a response $Y_i \\in \\mathbb R$  Crucially, we do not assume unconfoundedness / strong ignorability hence\n$$ \\big \\lbrace Y_i^{(1)} , Y_i^{(0)} \\big \\rbrace \\ \\not \\perp \\ T_i \\ | \\ X_i $$\nInstrumental Variables The standard linear IV model is the following\n$$ Y_i = T_i \\alpha + X_i \\beta_1 + \\varepsilon_i \\newline T_i = Z_i \\gamma + X_i \\beta_2 + u_i $$\nWe assume there exists an instrumental variable $Z_i \\in \\mathbb R^k$ that satisfies the following assumptions.\n  Assumption 1: Exclusion: $\\mathbb E [Z \\varepsilon] = 0$\n  Assumption 2: Relevance: $\\mathbb E [Z T] \\neq 0$\n  The model can be represented by a DAG.\nfrom src.plots import dag_iv dag_iv()  The IV estimator is instead unbiased\n$$ \\hat \\beta_{IV} = (Z\u0026rsquo;X)^{-1}(Z\u0026rsquo;Y) $$\nPotential Outcomes Perspective We need to extend the potential outcomes framework in order to allow for the instrumental variable $Z$. First we define the potential outcomes as $Y^{(D(Z_i))}(Z_i)$\nThe assumptions become\n Exclusion: $Y^{(D(Z_i))}(Z_i) = Y^{(T(Z_i))}$ Relevance: $P(z) = \\mathbb E [T, Z=z]$  We assume that $Z$ is fully randomly assigned (while $T$ is not).\nWhat does IV estimate?\n$$ \\begin{aligned} \\mathbb E[Y_i | Z_i = 1] - \\mathbb E[Y_i | Z_i = 0] \u0026amp;= \\Pr (D_i^{(1)} - D_i^{(0)} = 1) \\times \\mathbb E \\Big[ Y_i^{(1)} - Y_i^{(0)} = 1 \\ \\Big | \\ D_i^{(1)} - D_i^{(0)} = 1 \\Big] - \\newline \u0026amp;- \\Pr (D_i^{(1)} - D_i^{(0)} = -1) \\times \\mathbb E \\Big[ Y_i^{(1)} - Y_i^{(0)} = 1 \\ \\Big | \\ D_i^{(1)} - D_i^{(0)} = -1 \\Big] \\end{aligned} $$\nIs this a quantity of interest? Almost. There are two issues.\nFirst, the first term is the treatment effect, but only for those individuals for whom $D_i^{(1)} - D_i^{(0)} = 1$, i.e. those that are induced into treatment by $Z_i$. These individuals are referred to as compliers.\nSecond, the second term is problematic since it removes from the first effect, the local effect of another subpopulation: $D_i^{(1)} - D_i^{(0)} = -1$, i.e. those that are induced out of treatment by $Z_i$. These individuals are referred to as defiers.\nWe can get rid of defiers with a simple assumption.\nAssumption 3: monotonocity: $D_i^{(1)} \\geq D_i^{(0)}$ (or viceversa)\n All effects must be monotone in the same direction Fundamentally untestable  Then, the IV estimator can be expressed as a ration between two differences in means\n$$ \\hat \\beta_{IV} = \\frac{\\mathbb E[Y_i | Z_i = 1] - \\mathbb E[Y_i | Z_i = 0]}{\\mathbb E[T_i | Z_i = 1] - \\mathbb E[T_i | Z_i = 0]} $$\nStructural Perspective One can interpret the IV estimator as a GMM estimator that uses the exclusion restriction as estimating equation.\n$$ \\hat \\beta_{GMM} = \\arg \\min_{\\beta} \\mathbb E \\Big[ Z (Y - \\alpha T - \\beta X) \\Big]^2 $$\nThe Algebra of IV TBD\nDemand and Supply TBD\nAcademic Application 1 As an research paper replication, we are going to replicate Does compulsory school attendance affect schooling and earnings? (1991) by Angrist and Krueger. The authors study the effect of education on wages.\nThe problem of studying the relationship of education on wages is that there might be factors that influence both education and wages but we do not observe, for example ability. Students that have higher ability might decide to stay longer in school and also get higher wages afterwards.\nThe idea of the authors is to use the quarter of birth as an instrument for education. In fact, quarter of birth is plausibly exogenous with respect to wages while, on the other hand, is correlated with education. Why? Students that are both in the last quarter of the year cannot drop out as early as other students and therefore are exposed to more eduction.\nWe can represent the DAG of their model as follows.\n%matplotlib inline %config InlineBackend.figure_format = 'retina'  from src.utils import *  dag_iv(Y=\u0026quot;wage\u0026quot;, T=\u0026quot;education\u0026quot;, Z=\u0026quot;quarter of birth\u0026quot;, U=\u0026quot;ability\u0026quot;)  A shortcoming of this instrument comes out of the fact that the population of compliers is students that drop out of school as soon as possible, we will know the treatment effect only for this population. It\u0026rsquo;s important to keep this in mind when interpreting the results.\nLet\u0026rsquo;s load the data, freely available here.\ndf = pd.read_csv('data/ak91.csv') df.head()   .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }  \n  log_wage years_of_schooling date_of_birth year_of_birth quarter_of_birth state_of_birth     0 5.790019 12.0 1930.0 1930.0 1.0 45.0   1 5.952494 11.0 1930.0 1930.0 1.0 45.0   2 5.315949 12.0 1930.0 1930.0 1.0 45.0   3 5.595926 12.0 1930.0 1930.0 1.0 45.0   4 6.068915 12.0 1930.0 1930.0 1.0 37.0     We have the variables of interest, log_wage, years_of_schooling and quarter_of_birth, together with a set of controls.\nOLS If we were to ignore the endogeneity problem we would estimate a linear regression of log_wage on years_of_schooling, plus control dummy variables for the state_of_birth and year_of_birth.\nsmf.ols('log_wage ~ years_of_schooling', df).fit().summary().tables[1]    coef std err t P|t| [0.025 0.975]   Intercept  4.9952  0.004  1118.882  0.000  4.986  5.004   years_of_schooling  0.0709  0.000  209.243  0.000  0.070  0.072   IV We now use quarter_of_birth as an instrument for years_of_schooling. We cannot check the exclusion restriction condition, but we can check the relevance condition.\nLet\u0026rsquo;s start first by plotting average years_of_schooling by date of birth.\ngroup_df = df.groupby(\u0026quot;date_of_birth\u0026quot;).mean().reset_index()  plt.figure(figsize=(15,6)) sns.lineplot(data=group_df, x=\u0026quot;date_of_birth\u0026quot;, y=\u0026quot;years_of_schooling\u0026quot;, zorder=1)\\ .set(title=\u0026quot;First Stage\u0026quot;, xlabel=\u0026quot;Year of Birth\u0026quot;, ylabel=\u0026quot;Years of Schooling\u0026quot;); for q in range(1, 5): x = group_df.loc[group_df['quarter_of_birth']==q, \u0026quot;date_of_birth\u0026quot;] y = group_df.loc[group_df['quarter_of_birth']==q, \u0026quot;years_of_schooling\u0026quot;] plt.scatter(x, y, marker=\u0026quot;s\u0026quot;, s=200, c=f\u0026quot;C{q}\u0026quot;) plt.scatter(x, y, marker=f\u0026quot;${q}$\u0026quot;, s=100, c=f\u0026quot;white\u0026quot;)  As we can see, there is an upward trend but, within each year, people both in the last quarter usually have more years of schooling than people born in other quarters of the year.\nWe can check this correlation more formally by regressing years_of_schooling of a set of dummies for quarter_of_birth.\nsmf.ols('years_of_schooling ~ C(quarter_of_birth)', df).fit().summary().tables[1]    coef std err t P|t| [0.025 0.975]   Intercept  12.6881  0.011  1105.239  0.000  12.666  12.711   C(quarter_of_birth)[T.2.0]  0.0566  0.016  3.473  0.001  0.025  0.089   C(quarter_of_birth)[T.3.0]  0.1173  0.016  7.338  0.000  0.086  0.149   C(quarter_of_birth)[T.4.0]  0.1514  0.016  9.300  0.000  0.119  0.183   The relationship between years_of_schooling and quarter_of_birth is indeed statistically significant.\nDoes it translate it into higher wages? We can have a first glimpse of potential IV effects by plotting wages against the date of birth.\nplt.figure(figsize=(15,6)) sns.lineplot(data=group_df, x=\u0026quot;date_of_birth\u0026quot;, y=\u0026quot;log_wage\u0026quot;, zorder=1)\\ .set(title=\u0026quot;Reduced Form\u0026quot;, xlabel=\u0026quot;Year of Birth\u0026quot;, ylabel=\u0026quot;Log Wage\u0026quot;); for q in range(1, 5): x = group_df.loc[group_df['quarter_of_birth']==q, \u0026quot;date_of_birth\u0026quot;] y = group_df.loc[group_df['quarter_of_birth']==q, \u0026quot;log_wage\u0026quot;] plt.scatter(x, y, marker=\u0026quot;s\u0026quot;, s=200, c=f\u0026quot;C{q}\u0026quot;) plt.scatter(x, y, marker=f\u0026quot;${q}$\u0026quot;, s=100, c=f\u0026quot;white\u0026quot;)  It seems that indeed people both in later quarters earn higher wages later in life.\nWe now turn into the estimation of the causal effect of education on wages.\ndf[['q1', 'q2', 'q3', 'q4']] = pd.get_dummies(df['quarter_of_birth'])  from linearmodels.iv import IV2SLS IV2SLS.from_formula('log_wage ~ 1 + [years_of_schooling ~ q1 + q2 + q3]', data=df).fit().summary.tables[1]  Parameter Estimates   Parameter Std. Err. T-stat P-value Lower CI Upper CI   Intercept 4.5898 0.2494 18.404 0.0000 4.1010 5.0786   years_of_schooling 0.1026 0.0195 5.2539 0.0000 0.0643 0.1409   The coefficient is slightly higher than the OLS coefficient. It\u0026rsquo;s important to remember that the estimated effect is specific to the subpopulation of people that drop out of school as soon as they can.\nResearch Paper Replication 2 In The Colonial Origins of Comparative Development (2002) by Acemoglu, Johnson, Robinson, the authors wish to determine whether or not differences in institutions can help to explain observed economic outcomes.\nHow do we measure institutional differences and economic outcomes?\nIn this paper,\n economic outcomes are proxied by log GDP per capita in 1995, adjusted for exchange rates. institutional differences are proxied by an index of protection against expropriation on average over 1985-95, constructed by the Political Risk Services Group.  The problem is that there might exist other factors that affects both the quality of institutions and GDP. The authors suggest the following problems as sources of endogeneity:\n richer countries may be able to afford or prefer better institutions variables that affect income may also be correlated with institutional differences the construction of the index may be biased; analysts may be biased towards seeing countries with higher income having better institutions  The idea of the authors is to use settler\u0026rsquo;s mortality during the colonization period as an instrument for the quality of institutions. They hypothesize that higher mortality rates of colonizers led to the establishment of institutions that were more extractive in nature (less protection against expropriation), and these institutions still persist today.\nWe can represent their DAG as follows.\ndag_iv(Y=\u0026quot;GDP\u0026quot;, T=\u0026quot;institutions\u0026quot;, Z=\u0026quot;settlers' mortality\u0026quot;, U=\u0026quot;tons of stuff\u0026quot;)  First, let\u0026rsquo;s load the data (available here) and have a look at it.\ndf = pd.read_csv('data/ajr02.csv',index_col=0) df.head()   .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }  \n  GDP Exprop Mort Latitude Neo Africa Asia Namer Samer logMort Latitude2     1 8.39 6.50 78.20 0.3111 0 1 0 0 0 4.359270 0.096783   2 7.77 5.36 280.00 0.1367 0 1 0 0 0 5.634790 0.018687   3 9.13 6.39 68.90 0.3778 0 0 0 0 1 4.232656 0.142733   4 9.90 9.32 8.55 0.3000 1 0 0 0 0 2.145931 0.090000   5 9.29 7.50 85.00 0.2683 0 0 0 1 0 4.442651 0.071985     The data contains the main variables, DGP, Exprop and Mort, plus some geographical information.\nOLS What would we get if we were to ignore the endogeneity problem? We estimate the following misspecified model by OLS\n$$ {GDP}_i = \\beta_0 + \\beta_1 {Exprop}_i + \\varepsilon_i $$\nreg1 = smf.ols('GDP ~ Exprop', df).fit() reg1.summary().tables[1]    coef std err t P|t| [0.025 0.975]   Intercept  4.6609  0.409  11.402  0.000  3.844  5.478   Exprop  0.5220  0.061  8.527  0.000  0.400  0.644   The coefficient of Exprop is positive and significant but we know it is a biased estimate of the causal effect.\nOne direction we could take in addressing the endogeneity problem could be to control for any factor that affects both GDP and Exprop. In particular, the authors consider the following sets of variables:\n climat; proxied by latitude differences that affect both economic performance and institutions, eg. cultural, historical, etc.; controlled for with the use of continent dummies  reg2 = smf.ols('GDP ~ Exprop + Latitude + Latitude2', df).fit() reg3 = smf.ols('GDP ~ Exprop + Latitude + Latitude2 + Asia + Africa + Namer + Samer', df).fit()  from statsmodels.iolib.summary2 import summary_col summary_col(results=[reg1,reg2,reg3], float_format='%0.2f', stars = True, info_dict={'No. observations' : lambda x: f\u0026quot;{int(x.nobs):d}\u0026quot;}, regressor_order=['Intercept','Exprop','Latitude','Latitude2'])    GDP I GDP II GDP III   Intercept 4.66*** 4.55*** 5.95***    (0.41) (0.45) (0.68)   Exprop 0.52*** 0.49*** 0.40***    (0.06) (0.07) (0.06)   Latitude  2.16 0.42     (1.68) (1.47)   Latitude2  -2.12 0.44     (2.86) (2.48)   Africa   -1.06**      (0.41)   Asia   -0.74*      (0.42)   Namer   -0.17      (0.40)   Samer   -0.12      (0.42)   R-squared 0.54 0.56 0.71   R-squared Adj. 0.53 0.54 0.67   No. observations 64 64 64   The coefficient of Expropr decreases in magnitude but remains positive and significant after the addition of geographical control variables. This might suggest that the endogeneity problem is not very pronounced. However, it\u0026rsquo;s hard to say given the large number of factors that could affect both institutions and GDP.\nIV In order for Mort to be a valid instrument it needs to satisfy the two IV conditions:\n Exclusion: Mort must be correlated to GDP only through Exprop Relevance: Mort must be correlated with Exprop  The exclusion restriction condition is untestable, however, we may not be satisfied if settler mortality rates in the 17th to 19th centuries have a direct effect on current GDP (in addition to their indirect effect through institutions).\nFor example, settler mortality rates may be related to the current disease environment in a country, which could affect current economic performance.\nThe authors argue this is unlikely because:\n The majority of settler deaths were due to malaria and yellow fever and had a limited effect on local people. The disease burden on local people in Africa or India, for example, did not appear to be higher than average, supported by relatively high population densities in these areas before colonization.  The relevance condition is testable and we can check it by computing the partial correlation between Mort and Exprop. Let\u0026rsquo;s start by visual inspection first.\nsns.scatterplot(data=df, x='Mort', y='Exprop')\\ .set(title='First Stage', xlabel='Settler mortality', ylabel='Risk of expropriation');  Visually, the first stage seems weak, at best. However, a regression of Exprop on Mort can help us better assess whether the relationship is significant or not.\nsmf.ols('Exprop ~ Mort', df).fit().summary().tables[1]    coef std err t P|t| [0.025 0.975]   Intercept  6.7094  0.202  33.184  0.000  6.305  7.114   Mort  -0.0008  0.000  -2.059  0.044  -0.002 -2.28e-05   The coefficient is negative, as expected, and statistically significant.\nThe second-stage regression results give us an unbiased and consistent estimate of the effect of institutions on economic outcomes.\n$$ {GDP}_i = \\beta_0 + \\beta_1 {Exprop}_i + \\varepsilon_i \\ {Exprop}_i = \\delta_0 + \\delta_1 {logMort}_i + v_i $$\nNote that while our parameter estimates are correct, our standard errors are not and for this reason, computing 2SLS ‘manually’ (in stages with OLS) is not recommended.\nWe can correctly estimate a 2SLS regression in one step using the linearmodels package, an extension of statsmodels\nNote that when using IV2SLS, the exogenous and instrument variables are split up in the function arguments (whereas before the instrument included exogenous variables)\nIV2SLS.from_formula('GDP ~ 1 + [Exprop ~ logMort]', data=df).fit().summary.tables[1]  Parameter Estimates   Parameter Std. Err. T-stat P-value Lower CI Upper CI   Intercept 2.0448 1.1273 1.8139 0.0697 -0.1647 4.2542   Exprop 0.9235 0.1691 5.4599 0.0000 0.5920 1.2550   The result suggests a stronger positive relationship than what the OLS results indicated.\nBusiness Case We are given the following problem:\n A firm would like to understand whether its newsletter is working to increase revenue. However, it cannot force customers to subscribe to the newsletter. Instead, the firm sends a reminder email to a random sample of customers for the newsletter. Estimate the effect of the newsletter on revenue.\n We start by drawing a sample from the data generating process.\nfrom src.dgp import dgp_newsletter dgp = dgp_newsletter() df = dgp.generate_data() df.head()   .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }  \n  reminder subscribe revenue     0 0 1 0.582809   1 1 0 3.427162   2 0 0 1.953731   3 0 0 2.902038   4 0 0 0.826724     From the data, we know the revenue per customer, whether it was sent a reminder for the newsletter and whether it actually decided to subscribe.\nIf we to estimate the effect of subscribe on revenue, we might get a biased estimate because the decision of subscribing is endogenous. For example, we can imagine that wealthier customers are generating more revenue but are also less likely to subscribe.\nWe can represent the model with a DAG.\ndag_iv(Y=\u0026quot;revenue\u0026quot;, T=\u0026quot;subscribe\u0026quot;, Z=\u0026quot;reminder\u0026quot;, U=\u0026quot;income\u0026quot;)  OLS By directly inspecting the data, it seems that subscribed members actually generate less revenue than normal customers.\nsns.barplot(x='subscribe', y='revenue', data=df);  A linear regression confirms the graphical intuition.\nsmf.ols('revenue ~ 1 + subscribe', df).fit().summary().tables[1]    coef std err t P|t| [0.025 0.975]   Intercept  1.7752  0.086  20.697  0.000  1.607  1.943   subscribe  -0.7441  0.140  -5.334  0.000  -1.018  -0.470   However, if indeed wealthier customers generate more revenue and are less likely to subscribe, we have a negative omitted variable bias and we can expect the true effect of the newsletter to be bigger than the OLS estimate.\nIV Let\u0026rsquo;s now exploit the random variation induced by the discount. In order for our instrument to be valid, we need it to be exogenous (untestable) and relevant. We can test the relevance with the first stage regresssion of reminder on subscribe.\nsmf.ols('subscribe ~ 1 + reminder', df).fit().summary().tables[1]    coef std err t P|t| [0.025 0.975]   Intercept  0.2368  0.021  11.324  0.000  0.196  0.278   reminder  0.2790  0.029  9.488  0.000  0.221  0.337   It seems that the instrument is relevant. We can now estimate the IV regression.\nIV2SLS.from_formula('revenue ~ 1 + [subscribe ~ reminder]', data=df).fit().summary.tables[1]  Parameter Estimates   Parameter Std. Err. T-stat P-value Lower CI Upper CI   Intercept 0.9485 0.2147 4.4184 0.0000 0.5278 1.3693   subscribe 1.4428 0.5406 2.6689 0.0076 0.3832 2.5023   The estimated coefficient has now flipped sign and turned positive! Ignoring the endogeneity problem would have lead us to the wrong conclusion.\nReferences  Instrumental Variables video lecture by Paul Goldsmith-Pinkham (Yale) Instrumental Variables section from Matheus Facure\u0026rsquo;s Causal Inference for The Brave and The True Does compulsory school attendance affect schooling and earnings? (1991) by Angrist and Krueger The Colonial Origins of Comparative Development (2002) by Acemoglu, Johnson, Robinson  ","date":1649980800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1649980800,"objectID":"c1ba7293fd6ef944fe34ab3ec2773d15","permalink":"https://matteocourthoud.github.io/post/iv/","publishdate":"2022-04-15T00:00:00Z","relpermalink":"/post/iv/","section":"post","summary":"In this tutorial, we are going to see how to estimate causal effects when the treatment is not randomly assigned, but we have access to a third variable that is as good as randomly assigned and is correlated (only) with the treatment.","tags":null,"title":"Instrumental Variables","type":"post"},{"authors":null,"categories":null,"content":"In this tutorial, we are going to explore and compare different methods that leverage machine learning to estimate heterogeneous treatment effects.\nFor this tutorial, I assume you are familiar with the following concepts:\n Rubin\u0026rsquo;s potential outcome framework Propensity score weighting Basic machine learning models  Setting We assume that for a set of i.i.d. subjects $i = 1, \u0026hellip;, n$ we observed a tuple $(X_i, D_i, Y_i)$ comprised of\n a feature vector $X_i \\in \\mathbb R^n$ a treatment assignment $T_i \\in \\lbrace 0, 1 \\rbrace$ a response $Y_i \\in \\mathbb R$  Assumption 1 : unconfoundedness (or ignorability, or selection on observables)\n$$ \\big \\lbrace Y_i^{(1)} , Y_i^{(0)} \\big \\rbrace \\ \\perp \\ T_i \\ | \\ X_i $$\ni.e. conditional on observable characteristics $X$, the treatment assignment $T$ is as good as random.\nAssumption 2: overlap (or bounded support)\n$$ \\exists \\eta \u0026gt; 0 \\ : \\ \\eta \\leq \\mathbb E \\left[ T_i = 1 \\ \\big | \\ X_i = x \\right] \\leq 1-\\eta $$\ni.e. no observation is deterministically assigned to the treatment or control group.\nMeta Learners S-Learner The simplest meta-algorithm is the single learner or S-learner. To build the S-learner estimator, we fit a single model for all observations.\n$$ \\mu(z) = \\mathbb E \\left[ Y_i \\ \\big | \\ (X_i, T_i) = z \\right] $$\nthe estimator is given by the difference between the predicted values evaluated at $t=1$ and $t=0$.\n$$ \\hat \\tau_{S} (x) = \\hat \\mu(x,1) - \\hat \\mu(x,0) $$\nProblems\n We are learning a single model so we hope that the model uncovers heterogeneity in $T$ but it might not be the case If the model is heavily regularized because of the high dimensionality of $X$, it might not recover any treatment effect  e.g. with trees, it might not split on $T$    T-learner To build the two-learner or T-learner estimator, we fit two different models, one for treated units and one for control units.\n$$ \\mu^{(1)}(x) = \\mathbb E \\left[ Y_i \\ \\big | \\ X_i = x, T_i = 1 \\right] \\qquad ; \\qquad \\mu^{(0)}(x) = \\mathbb E \\left[ Y_i \\ \\big | \\ X_i = x, T_i = 0 \\right] $$\nthe estimator is given by the difference between the predicted values of the two algorithms.\n$$ \\hat \\tau_{T} (x) = \\hat \\mu^{(1)}(x) - \\hat \\mu^{(0)}(x) $$\nProblems\n We are using just a fraction of the data for each prediction problem  S-learner was using all the data   We might get heterogeneity where there is none, just because we are forcing different models  E.g. if trees split differently, to compute the two potential outcomes we use different populations    X-learner The cross-learner or X-learner estimator is an extension of the T-learner estimator. It is built in the following way:\n  As for the T-learner, compute separate models for $\\mu^{(1)}(x)$ and $\\mu^{(0)}(x)$ using the treated and control units, respectively\n  Compute the estimated treatment effects as\n  $$ \\Delta_i (x) = \\begin{cases} Y_i - \\hat \\mu^{(0)}(x) \u0026amp;\\quad \\text{ if } T_i = 1 \\newline \\hat \\mu^{(1)}(x) - Y_i \u0026amp;\\quad \\text{ if } T_i = 0 \\end{cases} $$\n Predicting $\\Delta$ from $X$, compute $\\hat \\tau^{(0)}(x)$ from treated units and $\\hat \\tau^{(1)}(x)$ from control units\n  Estimate $e(x) = \\mathbb E \\left[ T_i = 1 \\ \\big | \\ X_i = x \\right]$\n  Compute\n  $$ \\hat \\tau_X(x) = \\hat \\tau^{(0)}(x) \\hat e(x) + \\hat \\tau^{(1)}(x) (1 - \\hat e(x)) $$\nExample In this example, we are going to use the following data generating process\n $N = 4000$ $p = 10$ $X_i \\sim N(0, I_p)$ $e(x) = 0.3$ $\\varepsilon_i \\sim N(0, 1)$ $\\mu^{(0)}(x) = (x_1 + x_2)_{+} + \\varepsilon$ $\\mu^{(1)}(x) = (x_1 + x_2)_{+} + \\frac{1}{1 + e^{-x_3}} + \\varepsilon$  So that the propensity score is constant $e(x) = 0.3$, the treatment effect is $\\frac{1}{1 + e^{-x_3}}$ and the average treatment effect is $0.5$.\n%matplotlib inline %config InlineBackend.figure_format = 'retina'  from src.utils import * from src.dgp import dgp4  We generate a dataset out of our DGP.\ndgp = dgp4() df = dgp.generate_data() df.head()   .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }  \n  x1 x2 x3 x4 x5 x6 x7 x8 x9 x10 e T tau Y     0 1.624345 -0.611756 -0.528172 -1.072969 0.865408 -2.301539 1.744812 -0.761207 0.319039 -0.249370 0.3 0 0.370943 1.116502   1 1.462108 -2.060141 -0.322417 -0.384054 1.133769 -1.099891 -0.172428 -0.877858 0.042214 0.582815 0.3 0 0.420087 -0.248671   2 -1.100619 1.144724 0.901591 0.502494 0.900856 -0.683728 -0.122890 -0.935769 -0.267888 0.530355 0.3 0 0.711276 0.651441   3 -0.691661 -0.396754 -0.687173 -0.845206 -0.671246 -0.012665 -1.117310 0.234416 1.659802 0.742044 0.3 0 0.334662 -0.913644   4 -0.191836 -0.887629 -0.747158 1.692455 0.050808 -0.636996 0.190915 2.100255 0.120159 0.617203 0.3 0 0.321441 0.121779     First, we implement the simplest machine learning method for learning heterogeneous treatment effects: the S-learner.\nWe fit a single RandomForestRegressor method to all the data.\nfrom sklearn.ensemble import RandomForestRegressor as rfr mu_S = rfr(min_samples_leaf=30) mu_S.fit(df[dgp.X + ['T']], df['Y']);  Then, we use it to predict $\\mu^{(1)}(x)$ and $\\mu^{(0)}(x)$.\ndf['hat_mu0_S'] = mu_S.predict(df[dgp.X + ['T']].assign(T=0)) df['hat_mu1_S'] = mu_S.predict(df[dgp.X + ['T']].assign(T=1))  We estimate the average treatment effect as the difference between the two predictions.\ndf['hat_tau_S'] = df['hat_mu1_S'] - df['hat_mu0_S'] print(f\u0026quot;S-learner estimate : {np.mean(df['hat_tau_S']):.4}\u0026quot;)  S-learner estimate : 0.3657  How close are we to the true treatment effect?\nsns.scatterplot(data=df, x='x3', y='hat_tau_S', alpha=0.3); sns.scatterplot(data=df, x='x3', y='tau', color='C2');  The T-learner method instead fits different model for treated and control units. The advantage is that it can\nmu0_T = rfr(min_samples_leaf=30) mu0_T.fit(df.loc[df['T']==0, dgp.X + ['T']], df.loc[df['T']==0, 'Y']) mu1_T = rfr(min_samples_leaf=30) mu1_T.fit(df.loc[df['T']==1, dgp.X + ['T']], df.loc[df['T']==1, 'Y']);  Then, we use it to predict $\\mu^{(1)}(x)$ and $\\mu^{(0)}(x)$.\ndf['hat_mu0_T'] = mu0_T.predict(df[dgp.X + ['T']]) df['hat_mu1_T'] = mu1_T.predict(df[dgp.X + ['T']])  We estimate the average treatment effect as the difference between the two predictions.\ndf['hat_tau_T'] = df['hat_mu1_T'] - df['hat_mu0_T'] print(f\u0026quot;S-learner estimate : {np.mean(df['hat_tau_T']):.4}\u0026quot;)  S-learner estimate : 0.5231  We can plot the distribution of treatment effect estimates against the true values.\nsns.scatterplot(data=df, x='x3', y='hat_tau_T', alpha=0.3); sns.scatterplot(data=df, x='x3', y='tau', color='C2');  Let\u0026rsquo;s now estimate the X-learner. The first step is exactly the same as for the T-learner: estimate $\\hat \\mu^{(1)}(x)$ and $\\hat \\mu^{(0)}(x)$ using the treated and control group, respectively.\nAfterwards, we compute the estimated treatment effect on the treated using the the estimated counterfactual outcome estimated on the control group $\\hat \\mu^{(0)}(x)$, and viceversa.\ndf['Delta'] = 0 df.loc[df['T']==0, 'Delta'] = (df['hat_mu1_T'] - df['Y'])[df['T']==0] df.loc[df['T']==1, 'Delta'] = (df['Y'] - df['hat_mu0_T'])[df['T']==1]  Then, we basically repeat the process for the T-learner, but using Delta as outcome variable.\ntau0_X = rfr(min_samples_leaf=30) tau0_X.fit(df.loc[df['T']==0, dgp.X + ['T']], df.loc[df['T']==0, 'Delta']) tau1_X = rfr(min_samples_leaf=30) tau1_X.fit(df.loc[df['T']==1, dgp.X + ['T']], df.loc[df['T']==1, 'Delta']);  df['hat_tau0_X'] = tau0_X.predict(df[dgp.X + ['T']]) df['hat_tau1_X'] = tau1_X.predict(df[dgp.X + ['T']])  Finally, we estimate the propensity score.\nfrom sklearn.linear_model import LogisticRegression as lr df['hat_e'] = lr().fit(df[dgp.X], df['T']).predict_proba(df[dgp.X])[:,1]  Now we can compute the X-learner estimate as\ndf['hat_tau_X'] = df['hat_e'] * df['hat_tau0_X'] + (1-df['hat_e']) * df['hat_tau1_X'] print(f\u0026quot;X-learner estimate : {np.mean(df['hat_tau_X']):.4}\u0026quot;)  X-learner estimate : 0.5253  We can plot the distribution of treatment effect estimates against the true values.\nX_plot = sns.scatterplot(data=df, x='x3', y='hat_tau_X', alpha=0.3); sns.scatterplot(data=df, x='x3', y='tau', color='C2');  The X-learner estimator is heavily superior to both the S-learner and the T-learner. This is particularly evident if we combine all the plots.\nfig, axs = plt.subplots(1,3, sharex=True, sharey=True, figsize=(20,6)) for i, l in enumerate(['S', 'T', 'X']): sns.scatterplot(data=df, x='x3', y=f\u0026quot;hat_tau_{l}\u0026quot;, alpha=0.3, ax=axs[i]); sns.scatterplot(data=df, x='x3', y='tau', color='C2', ax=axs[i]).\\ set(title=f\u0026quot;{l}-learner\u0026quot;, ylabel='');  References  Meta learners: Metalearners for estimating heterogeneous treatment effects using machine learning (2019) by Künzel, Sekhon, Bickel, and Yu Taxonomy of methods: Recursive partitioning for heterogeneous causal effects (2016) by Athey and Imbens Video lecture by Stefan Wager (Stanford)  ","date":1649980800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1649980800,"objectID":"23b98ec03768fc1a97168ce2e40c689a","permalink":"https://matteocourthoud.github.io/post/meta_learners/","publishdate":"2022-04-15T00:00:00Z","relpermalink":"/post/meta_learners/","section":"post","summary":"In this tutorial, we are going to explore and compare different methods that leverage machine learning to estimate heterogeneous treatment effects.\nFor this tutorial, I assume you are familiar with the following concepts:","tags":null,"title":"Meta Learners","type":"post"},{"authors":null,"categories":null,"content":"If you search \u0026ldquo;permutation test\u0026rdquo; on Wikipedia, you get the following definition:\n A permutation test (also called re-randomization test) is an exact statistical hypothesis test making use of the proof by contradiction in which the distribution of the test statistic under the null hypothesis is obtained by calculating all possible values of the test statistic under possible rearrangements of the observed data.\n What does it mean? In this tutorial we are going to see in detail what this definition means, how to implement permutation tests, and their pitfalls.\nExample 1: is a coin fair? Let\u0026rsquo;s start with an example: suppose you wanted to test whether a coin is fair. You throw the coin 10 times and you count the number of times you get heads. Let\u0026rsquo;s simulate the outcome.\nimport numpy as np np.random.seed(1) np.random.binomial(1, 0.5, 10)  array([0, 1, 0, 0, 0, 0, 0, 0, 0, 1])  Out of 10 coin throws, we got only 2 heads. Does it mean that the coin is not fair?\nThe question that permutation testing is trying to answer is \u0026ldquo;how unlikely is the observed outcome under the null hypothesis that the coin is fair?\u0026rdquo;.\nIn this case we can directly compute this answer since we have a very little number of throws. The total number of outcomes is $2^{10}$. The number of as or more extreme outcomes, under the assumption that the coin is fair (50-50) is\n 0 heads: ${10 \\choose 0} = 1$ 1 head: ${10 \\choose 1} = 10$ 2 heads: ${10 \\choose 2} = 45$  So that the probability of getting the same or a more extreme outcome is\nfrom scipy.special import comb (comb(10, 0) + comb(10, 1) + comb(10, 2)) / 2**10  0.0546875  This probability seems low but not too low.\nHowever, we have forgot one thing. We want to test whether the coin is fair in either direction. We would suspect that the coin is unfair if we were getting few heads (as we did), but also if we were getting many heads. Therefore, we should account for both extremes.\nsum([comb(10, i) for i in [0, 1, 2, 8, 9, 10]]) / 2**10  0.109375  This number should not be surprising since it\u0026rsquo;s exactly double the previous one.\nIt is common in statistics to say that an event is unusual if its probability is less than 1 in 20, i.e. $5%$. If we were adopting that threshold, we would not conclude that getting 2 heads in 10 trows is so unusual. However, getting just one, would be.\nsum([comb(10, i) for i in [0, 1, 9, 10]]) / 2**10  0.021484375  Hypothesis Testing The process we just went through is called hypothesis testing. The components of an hypothesis test are:\n  A null hypothesis $H_0$\n in our case, that the coin war fair    A test statistic $t$\n in our case, the number of zeros    A level of significance $\\alpha$\n it is common to choose 5%    The idea behind permutation testing is the following: in a setting in which we are checking whether one variable has an effect on another variable, the two variables should not be correlated, under the null hypothesis . Therefore, we could re-shuffle the treatment variable and re-compute the test statistic. Lastly, we can compute the p-value as the fraction of as or more extremes outcomes under re-shuffling of the data.\nExample 2: are women smarter? Suppose now we were interested in knowing whether females perform better in a test than men. Let\u0026rsquo;s start by writing the data generating process under the assumption of no difference in scores. However, only 30% of the sample will be female.\nimport pandas as pd # Data generating process def generate_data_gender(N=100, seed=1): np.random.seed(seed) # Set seed for replicability data = pd.DataFrame({\u0026quot;female\u0026quot;: np.random.binomial(1, 0.3, N), \u0026quot;test_score\u0026quot;: np.random.exponential(3, N)}) return data  Let\u0026rsquo;s now generate a sample of size 100.\n# Generate data data_gender = generate_data_gender() data_gender.head()   .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }  \n  female test_score     0 0 1.186447   1 1 2.246348   2 0 6.513147   3 0 1.326091   4 0 7.175402     We can compute the treatment effect by computing the difference in mean outcomes between male and females.\ndef compute_score_diff(data): T = np.mean(data.loc[data['female']==1, 'test_score']) - np.mean(data.loc[data['female']==0, 'test_score']) return T  T = compute_score_diff(data_gender) print(f\u0026quot;The estimated treatment effect is {T}\u0026quot;)  The estimated treatment effect is -1.3612262580563321  It looks that females actually did worse than males. But is the difference statistically significant? We can perform a randomization test and compute the probability of observing a more extreme outcome.\nFirst, let\u0026rsquo;s write the permutation routine that takes a variable in the data and permutes it.\ndef permute(data, var, r): temp_data = data.copy() temp_data[var] = np.random.choice(data[var], size=len(data), replace=r) return temp_data  We can now write the permutation test. It spits out a vector of statistics and prints the implied p-value.\ndef permutation_test(data, permute, var, compute_stat, K=1000, r=False): T = compute_stat(data) T_perm = [] for k in range(K): temp_data = permute(data, var, r) T_perm += [compute_stat(temp_data)] print(f\u0026quot;The p-value is {sum(np.abs(T_perm) \u0026gt;= np.abs(T))/K}\u0026quot;) return T_perm  Ts = permutation_test(data_gender, permute, 'test_score', compute_score_diff)  The p-value is 0.063  Apparently the result we have observed was quite unusual, but not at the 5% level. We can plot the distribution of statistics to visualize this result.\n%matplotlib inline %config InlineBackend.figure_format = 'retina' from src.utils import *  def plot_test(T, Ts, title): plt.hist(Ts, density=True, bins=30, alpha=0.7, color='C0') plt.vlines([-T, T], ymin=plt.ylim()[0], ymax=plt.ylim()[1], color='C2') plt.title(title);  plot_test(T, Ts, 'Distribution of score differences under permutation')  As we can see, the observed difference in scores is quite extreme with respect the distribution generate by the permutation.\nOne issue with the permutation test we just ran is that it is computationally expensive to draw without replacement. The standard and much faster procedure is to draw without replacement.\nTs_repl = permutation_test(data_gender, permute, 'test_score', compute_score_diff, r=True)  The p-value is 0.052  The p-value is virtually the same.\nHow accurate is the test? Since we have access to the data generating process, we can compute the true p-value via simulation. We draw many samples from the true data generating process and, for each, compute the difference in scores. The simulated p-value is going to be the frequency of more extreme statistics.\n# Function to simulate data and compute pvalue def simulate_stat(dgp, compute_stat, K=1000): T = compute_stat(dgp()) T_sim = [] for k in range(K): data = dgp(seed=k) T_sim += [compute_stat(data)] print(f\u0026quot;The p-value is {sum(np.abs(T_sim) \u0026gt;= np.abs(T))/K}\u0026quot;) return np.array(T_sim)  T_sim = simulate_stat(generate_data_gender, compute_score_diff)  The p-value is 0.038  Again, we can plot the distribution of simulated statistics to understand the computed p-value.\nplot_test(T, T_sim, 'Distribution of score differences under simulation')  As expected, most of the mass lies within the interval, indicating a relatively extreme result. We have just been \u0026ldquo;unlucky\u0026rdquo; with the draw, but the permutation test was accurate.\nPermutation tests vs t-tests What is the difference between a t-test and a permutation test?\nPermutation test advantages:\n does not make distributional assumptions not sensible to outliers can be computed also for statistics whose distribution is not known  Permutation test disadvantages:\n computationally intense very sample-dependent  Example 3: is university worth? Let\u0026rsquo;s now switch to a new example to compare t-tests and permutation tests.\nAssume we want to check whether university is a worthy investment. We have information about whether individuals attended university and their future salary. The problem here is that income is a particularly skewed variable.\n# Data generating process def generate_data_income(N=1000, seed=1): np.random.seed(seed) # Set seed for replicability university = np.random.binomial(1, 0.5, N) # Treatment data = pd.DataFrame({\u0026quot;university\u0026quot;: university, \u0026quot;income\u0026quot;: np.random.lognormal(university, 2.3, N)}) return data  data_income = generate_data_income() data_income.head()   .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }  \n  university income     0 0 5.305618   1 1 1.289598   2 0 6.507720   3 0 6.019961   4 0 0.034482     The distribution of income is very heavy tailed. Let\u0026rsquo;s plot its density across the two groups.\nsns.kdeplot(data=data_income, x=\u0026quot;income\u0026quot;, hue=\u0026quot;university\u0026quot;)\\ .set(title='Income density by group');  The distribution is so skewed that we cannot actually visually perceive differences between the two groups. Let\u0026rsquo;s compute the expected difference.\ndef compute_income_diff(data): T = np.mean(data.loc[data['university']==1, 'income']) - np.mean(data.loc[data['university']==0, 'income']) return T  T = compute_income_diff(data_income) T  23.546974435985444  It looks like university graduates have higher income. Is this difference statistically different from zero? Let\u0026rsquo;s perform a permutation test.\nT_perm = permutation_test(data_income, permute, 'university', compute_income_diff)  The p-value is 0.011  The permutation test is telling us that the difference is extremely unusual under the null hypothesis. In other words, it is very unlikely that university graduates earn the same income of non-university graduates.\nWhat would be the outcome of a standard t-test?\nfrom scipy.stats import ttest_ind ttest_ind(data_income.query('university==1')['income'], data_income.query('university==0')['income'])  Ttest_indResult(statistic=1.5589492598056494, pvalue=0.1193254252009701)  As we can see, the two tests provide extremely different results. The t-test is much more conservative, telling us that the unlikeliness of the data is just $12%$ compared to the $1.1%$ of the permutation test.\nThe reason is that we have extremely skewed data. The t-test is very sensible to extreme observation and will therefore compute a very high variance because of very few data points.\nThe permutation test can further address the problem of a skewed outcome distribution by using a test statistic that is more sensible to outliers. Let\u0026rsquo;s perform the permutation test using the trimmed mean instead of the mean.\nfrom scipy.stats import trim_mean def compute_income_mediandiff(data): T = np.median(data.loc[data['university']==1, 'income']) - np.median(data.loc[data['university']==0, 'income']) return T  T_perm = permutation_test(data_income, permute, 'university', compute_income_mediandiff)  The p-value is 0.0  In this case, the permutation test is extremely confident that the trimmed mean of the two groups is different.\nHowever, an advantage of the t-test is speed. Let\u0026rsquo;s compare the two tests by computing their execution time. Note that this is just a rough approximation since the permutation test could be sensible optimized.\nimport time # No replacement start = time.time() permutation_test(data_income, permute, 'university', compute_income_diff) print(f\u0026quot;Elapsed time without replacement: {time.time() - start}\u0026quot;) # Replacement start = time.time() ttest_ind(data_income.query('university==1')['income'], data_income.query('university==0')['income']) print(f\u0026quot;Elapsed time with replacement: {time.time() - start}\u0026quot;)  The p-value is 0.016 Elapsed time without replacement: 0.28911614418029785 Elapsed time with replacement: 0.00125885009765625  The permutation test is 300 times slower. This can be a particularly relevant difference for larger sample sizes.\nConclusion In this tutorial, we have seen how to perform permutation tests across different data generating processes.\nThe underlying principle is the same: permute an variable that is assumed to be random under the null hypothesis and re-compute the test statistic. Then check how unusual was the test statistic in the original dataset.\n","date":1649980800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1649980800,"objectID":"a5a6a7d39cdee5c66211055b0347a2c5","permalink":"https://matteocourthoud.github.io/post/permutation_test/","publishdate":"2022-04-15T00:00:00Z","relpermalink":"/post/permutation_test/","section":"post","summary":"If you search \u0026ldquo;permutation test\u0026rdquo; on Wikipedia, you get the following definition:\n A permutation test (also called re-randomization test) is an exact statistical hypothesis test making use of the proof by contradiction in which the distribution of the test statistic under the null hypothesis is obtained by calculating all possible values of the test statistic under possible rearrangements of the observed data.","tags":null,"title":"Permutation Tests for Dummies","type":"post"},{"authors":null,"categories":null,"content":"In this tutorial, we are going to see how to design the most welfare-improving policy in presence of treatment effect heterogeneity and treatment costs or budget constraints.\nRequisites\nFor this tutorial, I assume you are familiar with the following concepts:\n Rubin\u0026rsquo;s potential outcome framework Propensity weighting or uplifting AIPW or Double Robust Estimators Causal Trees  Academic Replication\nWe are going to replicate the paper by Hanna and Olken (2018) in which the authors study the optimal allocation of a cash transfer to households in Peru. We slightly twist the original paper by actually assigning the transfer and assuming 100% consumption.\nBusiness Case\nWe are going to study a company that has to decide which consumers to target with ads.\nSetting We assume that for a set of i.i.d. subjects $i = 1, \u0026hellip;, n$ we observed a tuple $(X_i, T_i, Y_i)$ comprised of\n a feature vector $X_i \\in \\mathbb R^n$ a treatment variable $T_i \\in \\lbrace 0, 1 \\rbrace$ a response $Y_i \\in \\mathbb R$  Assumption 1 : unconfoundedness (or ignorability, or selection on observables)\n$$ \\big \\lbrace Y_i^{(1)} , Y_i^{(0)} \\big \\rbrace \\ \\perp \\ T_i \\ | \\ X_i $$\ni.e. conditional on observable characteristics $X$, the treatment assignment $T$ is as good as random.\nAssumption 2: overlap (or bounded support)\n$$ \\exists \\eta \u0026gt; 0 \\ : \\ \\eta \\leq \\mathbb E \\left[ T_i = 1 \\ \\big | \\ X_i = x \\right] \\leq 1-\\eta $$\ni.e. no observation is deterministically assigned to the treatment or control group.\nPolicy Learning The objective of policy learning is to decide which people to treat. More explicitly, we want to learn a map from observable characteristics to a (usually binary) policy space.\n$$ \\pi : \\mathcal X \\to \\lbrace 0, 1 \\rbrace $$\nPolicy learning is closely related to the estimation of heterogeneous treatment effects. In fact, in both settings, we want to investigate how the treatment affects different individuals in different ways.\nThe main difference between policy learning and the estimation of heterogeneous treatment effects is the objective function. In policy learning, we are acting in a limited resources setting where providing treatment is costly and the cost could depend on individual characteristics. For example, it might be more costly to vaccinate individuals that live in remote areas. Therefore, one might not just want to treat individuals with the largest expected treatment effect, but the ones for whom treatment is most cost-effective.\nThe utilitarian value of a policy $\\pi$\n$$ V(\\pi) = \\mathbb E \\Big[ Y_i(\\pi(X_i)) \\Big] = \\mathbb E \\big[ Y^{(0)}_i \\big] + \\mathbb E \\big[ \\tau(X_i) \\pi(X_i) \\big] $$\nmeasures the expectation of the potential outcome $Y$ if we were to assign treatment $T$ according to policy $\\pi$. This expectation can be split into two parts:\n The baseline expected potential outcome $\\mathbb E \\big[ Y^{(0)}_i \\big]$ The expected effect of the policy $\\mathbb E \\big[ \\tau(X_i) \\pi(X_i) \\big]$  The objective of policy learning is to learn a policy with high value $V(\\pi)$. As part (2) of the formula makes clear, you get a higher value if you treat the people with a high treatment effect $\\tau(x)$.\nA simple approach could be to assign treatment according to a thresholding rule $\\tau(x) \u0026gt; c$, where $c$ is some cost below which is not worth treating individuals (or there is not enough budget).\nHowever, estimating the conditional average treatment effect (CATE) function $\\tau(x)$ and learning a good policy $\\pi(x)$ are different problems.\n the correct loss function for policy learning is not the mean squared error (MSE) on $\\tau(x)$  we want to maximize welfare!   the CATE function $\\tau(x)$ might not use some features for targeting  e.g. cannot discriminate based on race or gender   you don\u0026rsquo;t want to have feature that people can influence  e.g. use a self-reported measure that people can distort    We would like to find a loss function $L(\\pi ; Y_i, X_i, T_i)$ such that\n$$ \\mathbb E \\big[ L(\\pi ; Y_i, X_i, T_i) \\big] = - V(\\pi) $$\nIPW Loss Kitagawa and Tenenov (2018) propose to learn an empirical estimate of the value function using inverse propensity weighting (IPW).\n$$ \\hat \\pi = \\arg \\max_{\\pi} \\Big\\lbrace \\hat V(\\pi) : \\pi \\in \\Pi \\Big\\rbrace $$\nwhere\n$$ \\hat V(\\pi) = \\frac{ \\mathbb I \\big(\\lbrace T_i = \\pi(X_i) \\rbrace \\big) }{ \\mathbb P \\big[ \\lbrace T_i = \\pi(X_i) \\rbrace \\ \\big| \\ X_i \\big] } Y_i $$\nThe authors show that under unconfoundedness, if the propensity score $e(x)$ is known and $\\Pi$ is not too complex, the value of the estimated policy converges to the optimal value.\nNote that this is a very different problem from the normal optimization problem with a MSE loss. In fact, we now have a binary argument in the loss function which makes the problem similar to a classification problem, in which we want to classify people into high gain and low gain categories.\nAIPW Loss If propensity score $e(x)$ is not known, we can use a doubly robust estimator, exactly as for the average treatment effect.\n$$ \\hat V = \\frac{1}{n} \\sum_{i=1}^{n} \\begin{cases} \\hat \\Gamma_i \\quad \u0026amp;\\text{if} \\quad \\pi(X_i) = 1 \\newline\n \\hat \\Gamma_i \\quad \u0026amp;\\text{if} \\quad \\pi(X_i) = 0 \\end{cases} $$  where\n$$ \\hat \\Gamma_i = \\hat \\mu^{(1)}(X_i) - \\hat \\mu^{(0)}(X_i) + \\frac{T_i }{\\hat e(X_i)} \\left( Y_i - \\hat \\mu^{(1)}(X_i) \\right) - \\frac{(1-T_i) }{1-\\hat e(X_i)} \\left( Y_i - \\hat \\mu^{(0)}(X_i) \\right) $$\nThe relationship with AIPW is that $\\hat \\tau_{AIPW} = \\frac{1}{n} \\sum_{i=1}^{n} \\hat \\Gamma_i$. Therefore, the objective function $V(\\pi)$ is build so that when we assign treatment to a unit we \u0026ldquo;gain\u0026rdquo; the double-robust score $\\hat \\tau_{AIPW}$, while, if we do not assign treatment, we \u0026ldquo;pay\u0026rdquo; the double-robust score $\\hat \\tau_{AIPW}$.\nAcademic Application For the academic applicaiton, we are going to replicate the paper by Hanna and Olken (2018) in which the authors study the optimal allocation of a cash transfer to households in Peru. We slightly twist the original paper by actually assigning the transfer and assuming 100% consumption.\nFirst, let\u0026rsquo;s load the modified dataset.\nfrom src.utils import * from src.dgp import dgp_ao18  %matplotlib inline %config InlineBackend.figure_format = 'retina'  dgp = dgp_ao18() df = dgp.import_data() df.head()   .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }  \n  Unnamed: 0 d_fuel_other d_fuel_wood d_fuel_coal d_fuel_kerosene d_fuel_gas d_fuel_electric d_fuel_none d_water_other d_water_river ... d_lux_1 d_lux_2 d_lux_3 d_lux_4 d_lux_5 training h_hhsize cash_transfer consumption welfare     0 0 0 1 0 0 0 0 0 0 1 ... 0 0 0 0 0 0 1 0 211.0000 5.351858   1 1 0 0 0 0 1 0 0 0 0 ... 0 0 1 0 0 0 3 1 420.1389 6.040585   2 2 0 0 0 0 1 0 0 0 0 ... 0 0 1 0 0 0 4 0 390.8318 5.968277   3 3 0 0 0 0 1 0 0 0 0 ... 0 0 0 0 0 0 9 0 285.6018 5.654599   4 4 0 1 0 0 0 0 0 0 1 ... 0 0 0 0 0 0 8 0 118.0713 4.771289    5 rows × 78 columns\n As we can see, we have a lot of information about individuals in Peru. Crucially for the research question, we observe\n whether the household received a cash transfer, cash_transfer the household\u0026rsquo;s welfare afterwards, welfare_post  assuming $$ \\text{welfare} = \\log (\\text{consumption}) $$    We would like to understand which individuals should be given a transfer, given that the transfer is costly. Let\u0026rsquo;s assume the transfer costs $0.3$ units of welfare.\nfrom econml.policy import DRPolicyForest cost = 0.3 policy = DRPolicyForest(random_state=1).fit(Y=df[dgp.Y] - cost*df[dgp.T], T=df[dgp.T], X=df[dgp.X])  We can partially visualize the policy by plotting a regression tree for the most important features.\n%matplotlib inline policy.plot(tree_id=1, max_depth=2, feature_names=dgp.X, fontsize=8)  To understand if the estimated policy was effective, we can load the oracle dataset, with the potential outcomes.\ndf_oracle = dgp.import_data(oracle=True)  From the oracle dataset, we can compute the actual value of the policy.\nT_hat = policy.predict(df[dgp.X]) V_policy = (df_oracle['welfare_1'].values - cost - df_oracle['welfare_0'].values) * T_hat print(f'Estimated policy value (N_T={sum(T_hat)}): {np.mean(V_policy) :.4}')  Estimated policy value (N_T=21401): 0.05897  The value is positive, indicating that the treatment was effective. But how well did we do? We can compare the estimated policy with the oracle policy that assign treatment to each cost-effective unit.\nT_oracle = (df_oracle['welfare_1'] - df_oracle['welfare_0']) \u0026gt; cost V_oracle = (df_oracle['welfare_1'] - cost - df_oracle['welfare_0'] ) * T_oracle print(f'Oracle policy value (N_T={sum(T_oracle)}): {np.mean(V_oracle) :.4}')  Oracle policy value (N_T=17630): 0.07494  We actually achieved 79% of the potential policy gains! Also note that our policy is too generous, treating more units than optimal. But how well would we have done if the same amount of cash transfers were given at random?\nT_rand = np.random.binomial(1, sum(T_hat)/len(df), len(df)) V_rand = (df_oracle['welfare_1'] - cost - df_oracle['welfare_0'] ) * T_rand print(f'Random policy value (N_T={sum(T_rand)}): {np.mean(V_rand) :.4}')  Random policy value (N_T=21359): 0.0002698  A random assignment of the same amount of cash transfers would not achieve any effect. However, this assumes that we already know the optimal amount of funds to distribute. What if instead we had treated everyone?\nV_all = (df_oracle['welfare_1'] - cost - df_oracle['welfare_0'] ) print(f'All-treated policy value (N_T={len(df)}): {np.mean(V_all) :.4}')  All-treated policy value (N_T=45378): 0.0004019  Indiscriminate treatment would again not achieve any effect. Lastly, what if we had just estimated the treatment effect using AIPW and used it as a threshold?\nfrom econml.dr import LinearDRLearner model = LinearDRLearner(random_state=1).fit(Y=df[dgp.Y], T=df[dgp.T], X=df[dgp.X]) T_ipw = model.effect(X=df[dgp.X], T0=0, T1=1) \u0026gt; cost V_ipw = (df_oracle['welfare_1'] - cost - df_oracle['welfare_0'] ) * T_ipw print(f'IPW policy value (N_T={sum(T_ipw)}): {np.mean(V_ipw) :.4}')  IPW policy value (N_T=21003): 0.06293  We are actually doing better! Weird\u0026hellip;\nBusiness Case We are given the following problem:\n A firm would like to understand which customers to show an ad, in order to increase revenue. The firm ran a A/B test showing a random sample of customers an ad. First, try to understand if there is heterogeneity in treatment. Then, decide which customers to show the ad, given that ads are costly (1$ each). Further suppose that you cannot discriminate on gender. How do the results change?\n We start by drawing a sample from the data generating process.\nfrom src.utils import * from src.dgp import dgp_ad  dgp = dgp_ad() df = dgp.generate_data() df.head()   .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }  \n  male black age educ ad revenue     0 0 0 55.0 1 False -0.327221   1 1 1 47.0 2 False 0.659393   2 0 1 31.0 2 True 2.805178   3 0 1 51.0 2 False -0.508548   4 0 0 48.0 0 True 0.762280     We have information on the number of pages visited in the previous month, whether the user is located in the US, whether it connects by mobile and the revenue pre-intervention.\nWe are going to use the econml library to estimate the treatment effects. First, we use the DRLearner library to estimate heterogeneous treatment effects using a double robust estimator. We can specify both the model_propensity for $e(x)$ and the model_regression for $\\mu(x)$.\nfrom econml.dr import DRLearner model = DRLearner(random_state=1).fit(Y=df[dgp.Y], T=df[dgp.T], X=df[dgp.X]);  We can plot a visual representation of the treatment effect heterogeneity using the SingleTreePolicyInterpreter function, which infers a tree representation of the treatment effects learned from another model.\nfrom econml.cate_interpreter import SingleTreeCateInterpreter SingleTreeCateInterpreter(max_depth=2, random_state=1).interpret(model, X=df[dgp.X]).plot(feature_names=dgp.X)  It seems that the most relevant dimension of treatment heterogeneity is education.\nWe can now use policy learning to estimate a treatment policy. We use the DRPolicyTree from the econml package.\nfrom econml.policy import DRPolicyTree policy = DRPolicyTree(random_state=1, max_depth=2).fit(Y=df[dgp.Y], T=df[dgp.T], X=df[dgp.X]) policy.plot(feature_names=dgp.X)  We will now assume that the treatment is costly.\ncost = 1 policy = DRPolicyTree(random_state=1, max_depth=2).fit(Y=df[dgp.Y]-cost*df[dgp.T], T=df[dgp.T], X=df[dgp.X]) policy.plot(feature_names=dgp.X)  As we can see, the model decides to use race to discriminate treatment. However, let\u0026rsquo;s now suppose we cannot discriminate on race and gender.\nX_short = ['age', 'educ'] policy = DRPolicyTree(random_state=1, max_depth=2).fit(Y=df[dgp.Y]-cost*df[dgp.T], T=df[dgp.T], X=df[X_short]) policy.plot(feature_names=X_short)  In this case, the model uses education instead of race in order to assign treatment.\nReferences  Who Should Be Treated? Empirical Welfare Maximization Methods for Treatment Choice (2018) by Kitagawa and Tetenov Efficient Policy Learning (2017) by Athey and Wager Policy Learning video lecture by Stefan Wager (Stanford) Customer Segmentation case study by EconML  ","date":1649980800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1649980800,"objectID":"7ba4e24eb8ae8d8c72aa72dc308152d7","permalink":"https://matteocourthoud.github.io/post/policy_learning/","publishdate":"2022-04-15T00:00:00Z","relpermalink":"/post/policy_learning/","section":"post","summary":"In this tutorial, we are going to see how to design the most welfare-improving policy in presence of treatment effect heterogeneity and treatment costs or budget constraints.\nRequisites\nFor this tutorial, I assume you are familiar with the following concepts:","tags":null,"title":"Policy Learning","type":"post"},{"authors":null,"categories":null,"content":"In this tutorial, we are going to see how to estimate causal effects when the treatment is not unconditionally randomly assigned, but we need to condition on observable features in order to assume treatment exogeneity. This might happen either when an experiment is stratified or in observational studies.\nRequisites\nFor this tutorial, I assume you are familiar with the following concepts:\n Rubin\u0026rsquo;s potential outcome framework Ordinary least squares regression  Academic Application\nAs an academic application, we are going to replicate Evaluating the Econometric Evaluations of Training Programs with Experimental Data (1986) by Lalonde and the followup paper Causal Effects in Non-Experimental Studies: Reevaluating the Evaluation of Training Programs (1999) by Dahejia and Wahba. These papers study a randomized intervention providing work experienced to improve labor market outcomes.\nBusiness Case\nTBD\nSetting We assume that for a set of i.i.d. subjects $i = 1, \u0026hellip;, n$ we observed a tuple $(X_i, D_i, Y_i)$ comprised of\n a feature vector $X_i \\in \\mathbb R^n$ a treatment assignment $D_i \\in \\lbrace 0, 1 \\rbrace$ a response $Y_i \\in \\mathbb R$  Assumption 1 : unconfoundedness (or ignorability, or selection on observables, or conditional independence)\n$$ \\big \\lbrace Y_i^{(1)} , Y_i^{(0)} \\big \\rbrace \\ \\perp \\ D_i \\ | \\ X_i $$\ni.e. conditional on observable characteristics $X$, the treatment assignment $T$ is as good as random. What this assumption rules out is selection on unobservables. Moreover, it\u0026rsquo;s untestable.\nAssumption 2: overlap (or common support)\n$$ \\exists \\eta \u0026gt; 0 \\ : \\ \\eta \\leq \\mathbb E \\left[ D_i = 1 \\ \\big | \\ X_i = x \\right] \\leq 1-\\eta $$\ni.e. no observation is deterministically assigned to the treatment or control group. We need this assumption for counterfactual statements to make sense. If some observations had zero probability of (not) being treated, it would make no sense to try to estimate their counterfactual outcome in case they would have (not) being treated. Also this assumption is untestable.\nAssumption 3: stable unit treatment value (SUTVA)\n$$ Y_i^{(D_i)} \\perp D_j \\quad \\forall j \\neq i $$\ni.e. the potential outcome of one individual is independent from the treatment status of any other individual. Common violations of this assumption include\n general equilibrium effects spillover effects  This assumption is untestable.\nPropensity Scores Exogenous Treatment The fundamental problem of causal inference is that we do not observe counterfactual outcomes, i.e. we do not observe what would have happened to treated units if they had not received the treatment and viceversa.\nIf treatment is exogenous, we know that the difference in means identifies the average treatment effect $\\mathbb E[\\tau]$.\n$$ \\mathbb E[\\tau] = \\mathbb E \\big[ Y_i \\ \\big| \\ D_i = 1 \\big] - \\mathbb E \\big[ Y_i \\ \\big| \\ D_i = 0 \\big] = \\mathbb E \\big[ Y_{i}^{(1)} - Y_{i}^{(0)} \\big] $$\nTherefore, we can build an unbiased estimator of the average treatment effect as the empirical counterpart of the expression above\n$$ \\hat \\tau(Y, D) = \\frac{1}{n} \\sum_{i=1}^{n} \\big( D_i Y_i - (1-D_i) Y_i \\big) $$\nIn case treatment is not randomly assigned, we use the Thompson Horowitz (1952) estimator\n$$ \\hat \\tau(Y, D) = \\frac{1}{n} \\sum_{i=1}^{n} \\left( \\frac{D_i Y_i}{\\pi_{i}} - \\frac{(1-D_i) Y_i}{1 - \\pi_{i}} \\right) $$\nwhere $\\pi_{i} = \\Pr(D_i=1)$ is the probability of being treated, also known as propensity score. Sometimes the propensity score is known, for example when treatment is stratified. However, in general, it is not.\nConditionally Exogenous Treatment In many cases and especially in observational studies, treatment $D$ is not unconditionally exogenous, but it\u0026rsquo;s exogenous only after we condition on some characteristic $X$. If these characteristics are observables, we have the unconfoundedness assumption.\nUnder unconfoundedness, we can still identify the average treatment effect, as a conditional difference in means:\n$$ \\mathbb E[\\tau] = \\mathbb E \\big[ Y_{i}^{(1)} - Y_{i}^{(0)} \\ \\big| \\ X_i \\big] $$\nThe main problem is that we need to condition of the observables that actually make the unconfoundedness assumption hold. This might be tricky in two cases:\n when we have many observables when we do not know the functional form of the observables that we need to condition on  The main contribution of Rosenbaum and Rubin (1983) is to show that if unconfoundedness holds, then\n$$ \\big \\lbrace Y_i^{(1)} , Y_i^{(0)} \\big \\rbrace \\ \\perp \\ D_i \\ | \\ \\pi(X_i) $$\ni.e. you only need to condition on $\\pi(X)$ in order to recover the average treatment effect.\n$$ \\mathbb E[\\tau] = \\mathbb E \\big[ Y_{i}^{(1)} - Y_{i}^{(0)} \\ \\big| \\ \\pi(X_i) \\big] $$\nThis implies the following inverse propensity-weighted estimator:\n$$ \\hat \\tau^{IPW} = \\frac{1}{n} \\sum_{i=1}^{n} \\left( \\frac{D_i Y_i}{\\hat \\pi(X_i)} - \\frac{(1-D_i) Y_i}{1 - \\hat \\pi(X_i)} \\right) $$\nwhich, under unconfoundedness is an unbiased estimator of the average treatment effect, $\\mathbb E \\left[\\hat \\tau^{IPW} \\right] = \\tau$.\nThis is a very practically relevant result since it tells us that we need to condition on a single variable instead of a potentially infinite dimensional array. The only thing we need to do is to estimate $\\pi(X_i)$.\nComments Actual vs Estimated Scores\nHirano and Ridder (2002) show that even when you know the true propensity score $\\pi(X)$, it\u0026rsquo;s better to plug in the estimated propensity score $\\hat \\pi(X)$. Why? The idea is that the deviation between the actual and the estimated propensity score is providing some additional information. Therefore, it is best to use the actual fraction of treated rather than the theoretical one.\nPropensity Scores and Regression\nWhat is the difference between running a regression with controls vs doing propensity score matching?\nAranow and Miller (2015) investigate this comparison in depth. First of all, whenever you are inserting control variables in a regression, you are implicitly thinking about propensity scores. Both approaches are implicitly estimating counterfactual outcomes. Usually OLS extrapolates further away from the actual support than propensity score does.\nIn the tweet (and its comments) below you can find further discussion and comments.\nThank you for tolerating such a vague poll question. Let me explain why I think this is a useful thing to bring front and certain, and highlight what I think is a flaw in how much of econometrics is taught, currently. 1/n pic.twitter.com/Wm2jFereYO\n\u0026mdash; Paul Goldsmith-Pinkham (@paulgp) December 14, 2021  Academic Application As an academic application, we are going to replicate Causal Effects in Non-Experimental Studies: Reevaluating the Evaluation of Training Programs (1999) by Dahejia and Wahba.\nThis study builds on a previous study: Evaluating the Econometric Evaluations of Training Programs with Experimental Data (1986) by Lalonde. In this study, the author compares observational and experimental methods. In particular, he studies an experimental intervention called the NSW (National Supported Work demonstration). The NSW is a temporary training program to give work experience to unemployed people.\nThe exogenous variation allows us to estimate the treatment effect as a difference in means. The author then asks: what if we didn\u0026rsquo;t have access to an experiment? In particular, what if we did not have information on the control group? He takes a sample of untreated people from the PSID panel and use them as a control group.\nExperimental Data Let\u0026rsquo;s start by loading the NSW data.\n%matplotlib inline %config InlineBackend.figure_format = 'retina'  from src.utils import *  df_nsw = pd.read_csv('data/l86_nsw.csv') df_nsw.head()   .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }  \n  treat age educ black hisp marr nodegree re74 re75 re78     0 1 37 11 1 0 1 1 0.0 0.0 9930.045898   1 1 22 9 0 1 0 1 0.0 0.0 3595.894043   2 1 30 12 1 0 0 0 0.0 0.0 24909.449219   3 1 27 11 1 0 0 1 0.0 0.0 7506.145996   4 1 33 8 1 0 0 1 0.0 0.0 289.789886     The treatment variable is treat and the outcome of interest is re78, the income in 1978. We also have access to a bunch of covariates.\ny = 're78' T = 'treat' X = df_nsw.columns[2:9]  Was there selection on observables? Let\u0026rsquo;s summarize the data, according to treatment status.\ndf_nsw.groupby('treat').agg(['mean', 'std']).T.unstack(1)   .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead tr th { text-align: left; }  \n  treat 0 1    mean std mean std     age 25.053846 7.057745 25.816216 7.155019   educ 10.088462 1.614325 10.345946 2.010650   black 0.826923 0.379043 0.843243 0.364558   hisp 0.107692 0.310589 0.059459 0.237124   marr 0.153846 0.361497 0.189189 0.392722   nodegree 0.834615 0.372244 0.708108 0.455867   re74 2107.026651 5687.905639 2095.573693 4886.620354   re75 1266.909015 3102.982088 1532.055313 3219.250879   re78 4554.801120 5483.836001 6349.143502 7867.402183     It seems that covariates are balanced across treatment arms. Nothing seems to point towards selection on observables. Therefore, we can compute the average treatment effect as a simple difference in means\ndf_nsw.loc[df_nsw[T]==1, y].mean() - df_nsw.loc[df_nsw[T]==0, y].mean()  1794.3423818501024  Or equivalently in a regression\nest = smf.ols('re78 ~ treat', df_nsw).fit() est.summary().tables[1]    coef std err t P|t| [0.025 0.975]   Intercept  4554.8011  408.046  11.162  0.000  3752.855  5356.747   treat  1794.3424  632.853  2.835  0.005  550.574  3038.110   It looks like the effect is positive and significant.\nObservational Data Let\u0026rsquo;s now load a different dataset in which we have replaced the true control units with observations from the PSID sample.\ndf_psid = pd.read_csv('data/l86_psid.csv')  Is this dataset balanced?\ndf_psid.groupby('treat').agg(['mean', 'std']).T.unstack(1)   .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead tr th { text-align: left; }  \n  treat 0 1    mean std mean std     age 36.094862 12.081030 25.816216 7.155019   educ 10.766798 3.176827 10.345946 2.010650   black 0.391304 0.489010 0.843243 0.364558   hisp 0.067194 0.250853 0.059459 0.237124   marr 0.735178 0.442113 0.189189 0.392722   nodegree 0.486166 0.500799 0.708108 0.455867   re74 11027.303390 10814.670751 2095.573693 4886.620354   re75 7569.222058 9041.944403 1532.055313 3219.250879   re78 9995.949977 11184.450050 6349.143502 7867.402183     People in the PSID control group are older, more educated, white, married and generally have considerably higher pre-intervention earnings (re74). This makes sense since the people selected for the NSW program are people that are younger, less experienced and unemployed.\nLalonde (1986) argues in favor of experimental approaches by showing that using a non-experimental setting, one would not be able to estimate the true treatment effect. Actually, one could even get statistically significant results of the opposite sign.\nLet\u0026rsquo;s repeat the regression exercise for the PSID data.\nsmf.ols('re78 ~ treat', df_psid).fit().summary().tables[1]    coef std err t P|t| [0.025 0.975]   Intercept  9995.9500  623.715  16.026  0.000  8770.089  1.12e+04   treat -3646.8065  959.704  -3.800  0.000 -5533.027 -1760.586   The estimated coefficient is negative and significant. The conclusion from Lalonde (1986) is\n \u0026ldquo;This comparison shows that many of the econometric procedures d not replicate the experimentally determined results\u0026rdquo;.\n Dahejia and Wahba (1999) argue that with appropriate matching one would still be able to get a relatively precise estimate of the treatment effect. In particular, the argue in favor of controlling for pre-intervention income, re74 and re75.\nLet\u0026rsquo;s just linearly insert the control variables in the regression.\nsmf.ols('re78 ~ treat + ' + ' + '.join(X), df_psid).fit().summary().tables[1]    coef std err t P|t| [0.025 0.975]   Intercept -1089.9263  2913.224  -0.374  0.708 -6815.894  4636.042   treat  2642.1456  1039.655  2.541  0.011  598.694  4685.597   educ  521.5869  208.751  2.499  0.013  111.284  931.890   black -1026.6762  1006.433  -1.020  0.308 -3004.830  951.478   hisp  -903.1023  1726.419  -0.523  0.601 -4296.394  2490.189   marr  1026.6143  943.788  1.088  0.277  -828.410  2881.639   nodegree -1469.3712  1166.114  -1.260  0.208 -3761.379  822.637   re74  0.1928  0.058  3.329  0.001  0.079  0.307   re75  0.4976  0.070  7.068  0.000  0.359  0.636   The treatment effect is now positive, borderline significant, and close to the experimental estimate of $1794$$. Moreover, it\u0026rsquo;s hard to tell whether this is the correct functional form for the control variables.\nInverse propensity score weighting Another option is to use inverse propensity score weighting. First, we need to estimate the treatment probability. Let\u0026rsquo;s start with a very simple standard model to predict binary outcomes.\nfrom sklearn.linear_model import LogisticRegressionCV pi = LogisticRegressionCV().fit(y=df_psid[T], X=df_psid[X]) df_psid['pscore'] = pi.predict_proba(df_psid[X])[:,1]  How does the distribution of the propensity scores look like?\nsns.histplot(data=df_psid, x='pscore', hue=T, bins=20)\\ .set(title='Distribution of propensity scores, PSID data', xlabel='');  It seems that indeed we predict higher propensity scores for treated people, and viceversa, indicating a strong selection on observable. However, there is also a considerable amount of overlap.\nWe can now estimate the treatment effect by weighting by the inverse of the propensity score. First, let\u0026rsquo;s exclude observations with a very extreme predicted score.\ndf_psid1 = df_psid[(df_psid['pscore']\u0026lt;0.9) \u0026amp; (df_psid['pscore']\u0026gt;0.1)]  Now we can need to construct the weights.\ndf_psid1['weight'] = df_psid1['treat'] / df_psid1['pscore'] + (1-df_psid1['treat']) / (1-df_psid1['pscore'])  Finally, we run a weighted regression of income on the treatment program.\nest = smf.wls('re78 ~ treat', df_psid1, weights=df_psid1['weight']).fit() est.summary().tables[1]    coef std err t P|t| [0.025 0.975]   Intercept  4038.1507  512.268  7.883  0.000  3030.227  5046.074   treat  2166.8750  730.660  2.966  0.003  729.250  3604.500   The effect is positive, statistically significant and very close to the experimental estimate of $1794$$.\nWhat would have been the propensity scores if we had used the NSW experimental sample? If it\u0026rsquo;s a well done experiment with a sufficiently large sample, we would expect the propensity scores to concentrate around the percentage of people treated, $0.41$ in our data.\npi = LogisticRegressionCV().fit(y=df_nsw[T], X=df_nsw[X]) df_nsw['pscore'] = pi.predict_proba(df_nsw[X])[:,1]  sns.histplot(data=df_nsw, x='pscore', hue=T, bins=20)\\ .set(title='Distribution of propensity scores, NSW data', xlabel='');  Indeed, now the distribution of the p-scores is concentrated around the treatment frequency in the data. Remarkably, the standard deviation is extremely tight.\nReferences  The central role of the propensity score in observational studies for causal effects (1983) by Rosenbaum and Rubin Propensity Scores video lecture by Paul Goldsmith-Pinkham (Yale) Propensity Scores video lecture by Stefan Wager (Stanford)  ","date":1649980800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1649980800,"objectID":"f430af05f2329e440db7e1f50ed7187c","permalink":"https://matteocourthoud.github.io/post/propensity_score/","publishdate":"2022-04-15T00:00:00Z","relpermalink":"/post/propensity_score/","section":"post","summary":"In this tutorial, we are going to see how to estimate causal effects when the treatment is not unconditionally randomly assigned, but we need to condition on observable features in order to assume treatment exogeneity.","tags":null,"title":"Propensity Score Matching","type":"post"},{"authors":null,"categories":null,"content":"In this tutorial, we are going to see how to estimate causal effects when treatment assignment is not random, but determined by a forcing variable such as a test or a requirement. In this case, we can get a local estimate of the treatment effect by comparing units just above and just below the threshold by assuming that there is no sorting/gaming around it.\nRequisites\nFor this tutorial, I assume you are familiar with the following concepts:\n Rubin\u0026rsquo;s potential outcome framework Ordinary least squares regression Non-parametric regression Instrumental variables  Academic Application\nAs an academic application, we are going to replicate Do voters affect or elect policies? Evidence from the US House (2004) by Lee, Moretti, Butler. The authors study whether electoral strength has an effect on policies. To identify the effect, they the advantage that is given by incumbency status, and the quasi-exogeneity given by close elections.\nBusiness Case\nTBD\nSetting We assume that for a set of i.i.d. subjects $i = 1, \u0026hellip;, n$ we observed a tuple $(X_i, D_i, Y_i, Z_i)$ comprised of\n a feature vector $X_i \\in \\mathbb R^n$ a treatment assignment $D_i \\in \\lbrace 0, 1 \\rbrace$ a response $Y_i \\in \\mathbb R$  outcome of interest that depends on both $X_i$ and $D_i$   a forcing variable $Z_i \\in \\mathbb R$  variable that determines treatment assignment $D_i$    We normalize the forcing variable $Z_i$ such that $Z_i=0$ corresponds to the cutoff for treatment assignment. We will distinguish two cases for the effect of $Z_i$ on $D_i$:\n  Sharp RD: $D_i = (Z_i \\geq 0)$\n treatment is exactly determined by the cutoff    Fuzzy RD: $\\lim_{z \\to 0_{-}} \\mathbb E[D_i | Z_i=z] \\neq \\lim_{z \\to 0_{+}} \\mathbb E[D_i | Z_i=z]$\n treatment probability changes at the cutoff    Assumption 1 : CE smoothness\nAssumption 2: no sorting\nRegression Discontinuity The key behind regression discontinuity is what is called a forcing variable that determines treatment assignment. Common examples include test scores for university enrollment (you need a certain test score to get access university) or income for some policy eligibility (you need to be below a certain income threshold to be eligible for a subsidy).\nClearly, in this setting, treatment is not exogenous. However, the idea behind regression discontinuity is that units sufficiently close to the discontinuity $Z_i=0$ are sufficiently similar so that we can attribute differences in the outcome $Y_i$ to the treatment $T_i$.\nWhat does sufficiently exactly mean?\nIn practice, we are assuming a certain degree of smoothness of the conditional expectation function $\\mathbb E[D_i | Z_i=z]$. If this assumption holds, we can estimate the local average treatment effect\n$$ \\tau^{LATE} = \\lim_{z \\to 0_{+}} \\mathbb E[Y_i | Z_i=z] - \\lim_{z \\to 0_{-}} \\mathbb E[Y_i | Z_i=z] = \\mathbb E \\big[ Y_{i}^{(1)} - Y_{i}^{(0)} | Z_i=0 \\big] $$\nNote that this is the average treatment effect for a very narrow set of individuals: those that are extremely close to the cutoff.\nData Challenge Regression discontinuity design is a particularly data hungry procedure. In fact, we need to\n have a very good flexible approximation of the conditional expectation of the outcome $Y_i$ at the cutoff $Z_i=0$ while also accounting for the effect of the forcing variable $Z$ on the outcome $Y$  If we knew the functional form of $\\mathbb E[Y_i | Z_i]$, it would be easy.\nMcCrary Test Regression Kink Design Academic Application As an academic application, we are going to replicate Do voters affect or elect policies? Evidence from the US House (2004) by Lee, Moretti, Butler. The authors study whether electoral strength has an effect on policies. To identify the effect, they the advantage that is given by incumbency status, and the quasi-exogeneity given by close elections.\n%matplotlib inline %config InlineBackend.figure_format = 'retina'  from src.utils import *  df = sm.datasets.get_rdataset('close_elections_lmb', package='causaldata').data  df = pd.read_csv('data/l08.csv') df.head()   .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }  \n  state district id score year demvoteshare democrat lagdemocrat lagdemvoteshare     0 1 1 3 64.339996 1948 0.553026 1 0 0.469256   1 1 1 4 60.279999 1948 0.553026 1 0 0.469256   2 1 1 5 57.060001 1950 0.582441 1 1 0.553026   3 1 1 6 73.830002 1950 0.582441 1 1 0.553026   4 1 1 7 42.959999 1954 0.569626 1 1 0.539680     The first thing we would like to inspect, is the distribution of democratic vote shares demvoteshare, against their lagged values lagdemvoteshare.\nsns.scatterplot(df['lagdemvoteshare'], df['demvoteshare'])\\ .set(title='Vote share and incumbency status', xlabel='Dem Vote Share (t-1)', ylabel='Dem Vote Share (t)');  The plot is extremely messy. However we can already see some discontinuity at the threshold: it seems that incumbents do not get vote shares below 0.35.\nTo have a more transparent representation of the data, we can use a binscatterplot. Binscatterplots are very similar to histograms with a main difference: instead of having a fixed width, they have a fixed number of observations per bin.\nfrom scipy.stats import binned_statistic def binscatter(x, y, bins=30, area=True, **kwargs): y_bins, x_edges, _ = binned_statistic(x, y, statistic='mean', bins=bins) x_bins = (x_edges[:-1] + x_edges[1:]) / 2 p = sns.scatterplot(x_bins, y_bins, **kwargs) if area: y_std, _, _ = binned_statistic(x, y, statistic='std', bins=bins) plt.fill_between(x_bins, y_bins-y_std, y_bins+y_std, alpha=0.2, color='C0') return p  We can now plot the average vote share by previous vote share. The shades represent one standard deviation, at the bin level.\nbinscatter(df['lagdemvoteshare'], df['demvoteshare'], bins=100)\\ .set(title='Vote share and incumbency status', xlabel='Dem Vote Share (t-1)', ylabel='Dem Vote Share (t)'); plt.axvline(x=0.5, ls=\u0026quot;:\u0026quot;, color='C2'); plt.title('Vote share and incumbency status');  Now it seems quite clear that there exist a discontinuity at $0.5$. We can get a first estimate of the local average treatment effect by assuming a linear model and running a linear regression.\nsmf.ols('demvoteshare ~ lagdemvoteshare + (lagdemvoteshare\u0026gt;0.5)', df).fit().summary().tables[1]    coef std err t P|t| [0.025 0.975]   Intercept  0.2173  0.005  46.829  0.000  0.208  0.226   lagdemvoteshare  0.5[T.True]  0.0956  0.003  33.131  0.000  0.090  0.101   lagdemvoteshare  0.4865  0.011  42.539  0.000  0.464  0.509   The effect is positive and statistically significant. We can also allow the slope of the line to differ on the two sides of the discontinuity.\ndf = df.sort_values('lagdemvoteshare') model = smf.ols('demvoteshare ~ lagdemvoteshare * (lagdemvoteshare\u0026gt;0.5)', df).fit() model.summary().tables[1]    coef std err t P|t| [0.025 0.975]   Intercept  0.2256  0.007  34.588  0.000  0.213  0.238   lagdemvoteshare  0.5[T.True]  0.0747  0.012  6.334  0.000  0.052  0.098   lagdemvoteshare  0.4653  0.016  28.547  0.000  0.433  0.497   lagdemvoteshare:lagdemvoteshare  0.5[T.True]  0.0418  0.023  1.827  0.068  -0.003  0.087   Let\u0026rsquo;s plot the predicted vote share over the previous graph.\nbinscatter(df['lagdemvoteshare'], df['demvoteshare'], bins=100, alpha=0.5)\\ .set(title='Vote share and incumbency status', xlabel='Dem Vote Share (t-1)', ylabel='Dem Vote Share (t)'); plt.plot(df['lagdemvoteshare'], model.fittedvalues, color='C1') plt.axvline(x=0.5, ls=\u0026quot;:\u0026quot;, color='C2');  Now that we have established a discontinuity at the cutoff, we need to check the RD assumptions.\nFirst, is there sorting across the cutoff? In this case, are democratic politicians more or less likely to lose close elections than republicans? We can plot the distribution of (lagged) vote shares and inspect its shape at the cutoff.\nsns.histplot(df['lagdemvoteshare'], bins=100)\\ .set(title='Distribution of lagged dem vote share', xlabel='') plt.axvline(x=0.5, ls=\u0026quot;:\u0026quot;, color='C2');  If looks pretty smooth. If anything, there is a loss of density at the cutoff, plausibly indicating stronger competition when the competition is close. However, if does not seem particularly asymmetric.\nA placebo test that we can run is to check if the forcing variable has an effect on variables on which we do not expect to have an effect. In this setting, the most intuitive placebo outcome is previous elections: we do not expect that being on either side of the cutoff today is related to any past outcome.\nIn our case, we can simply swap the two variables to run the test.\nbinscatter(df['demvoteshare'], df['lagdemvoteshare'], bins=100)\\ .set(title='Vote share and incumbency status', xlabel='Dem Vote Share (t)', ylabel='Dem Vote Share (t-1)'); plt.axvline(x=0.5, ls=\u0026quot;:\u0026quot;, color='C2');  The distribution of vote shares in the past period does not seem to be discontinuous in the incumbency status today, as expected.\nReferences  Regression discontinuity video lecture by Paul Goldsmith-Pinkham (Yale)  ","date":1649980800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1649980800,"objectID":"f64bf2b2655c6c47ee0de6f4b449652a","permalink":"https://matteocourthoud.github.io/post/regression_discontinuity/","publishdate":"2022-04-15T00:00:00Z","relpermalink":"/post/regression_discontinuity/","section":"post","summary":"In this tutorial, we are going to see how to estimate causal effects when treatment assignment is not random, but determined by a forcing variable such as a test or a requirement.","tags":null,"title":"Regression Discontinuity","type":"post"},{"authors":null,"categories":null,"content":"In this page, I collect lectures and materials for graduate courses in Economics and Social Sciences.\nI will only link to lectures and materials that are freely available. I will not link to courses hosted on MOOC websites or that require university credentials to access.\nA special mention goes to the following:\n The NBER that during each Summer Institute has a lecture series The Chamberlain Seminar that since 2021 started hosting and recording tutorial sessions  Video Lectures    Course Title Author University Year Material     Machine Learning and Causal Inference Susan Athey et al. Stanford 2022 Yes   Industrial Organization Chris Conlon NYU 2021 Yes   Panel Data Econometrics Chris Conlon NYU 2021 Yes   Machine Learning with Graphs Yure Leskovec Stanford 2021 Yes   Applied Methods Paul Goldsmith-Pinkham Yale 2021 Yes   DiD Reading Group misc misc 2021 Yes   Computational Economics Kenneth Judd Stanford 2020 Yes   Reinforcement Learning Emma Brunskill Stanford 2020 Yes   Natural Language Understanding Christopher Potts Stanford 2019 No           Material    Course Title Author University Year     Computational Economics Florial Oswald Bocconi 2021   Data Science for Economists Grant McDermott Oregon 2020   Industrial Organization John Asker UCLA 2020   Topics in Empirical Industrial Organization Kohei Kawaguchi Hong Kong 2020   Industrial Organization Victor Aguirregabiria Toronto 2019   Econometrics Tyler Ransom Oklahoma 2020   Machine Learning in Econometrics Martin Spindler Munich 2020   Structural Econometrics Robert Miller Carnegie Mellon 2019          ","date":1644451200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1644451200,"objectID":"736f48d7831cb89e669f79a89666f064","permalink":"https://matteocourthoud.github.io/post/courses/","publishdate":"2022-02-10T00:00:00Z","relpermalink":"/post/courses/","section":"post","summary":"In this page, I collect lectures and materials for graduate courses in Economics and Social Sciences.\nI will only link to lectures and materials that are freely available. I will not link to courses hosted on MOOC websites or that require university credentials to access.","tags":null,"title":"Free Courses in Economics","type":"post"},{"authors":null,"categories":null,"content":"In this page, I collect information about conferences in Economics and Finance.\nIf you know about public conferences or meetings that are missing from this list, please either contact me or edit the table on Github!\nNote that conferences are ordered by deadline and not by conference date.\nJanuary    Conference Organizer Field Deadline Target Date     Annual Congress of the Swiss Society of Economics and Statistics SSES All January 31 All 23/06/21    February    Conference Organizer Field Deadline Target Date     Bergen Competition Policy Conference NHH Comp policy February 03 All 23/04/20   CEPR/JIE Conference on Applied IO CEPR IO February 10 Junior 08/06/21   Economics and Computation ACM SIGecom Theory February 10 All 11/07/21   ES North American Summer Meeting Econometric Society All February 12 Senior 16/06/21   EEA Summer Meeting European Economic Association All February 15 All 23/08/21   ES European Summer Meeting Econometric Society All February 15 Senior 23/08/21   Annual Meeting of the Society for Economic Dynamics SED Macro February 15 All 01/07/21   GAMES 2020 Game Theory Society Game Theory February 20 All 19/07/21   Annual GEP/CEPR Postgraduate Conference University of Nottingham Policy February 26 Junior 06/05/21   Doctoral Workshop on the Economics of Digitization TSE Digitalization February 28 Junior 12/05/22    March    Conference Organizer Field Deadline Target Date     Annual IIOC Northeastern University IO March 01 All 30/04/21   Young Economists' Meeting University fo Munich All March 08 Junior 01/10/20   GSE Summer Forum Barcelona GSE All March 14 All 07/06/21   EARIE NHH IO March 15 All 27/08/21   Economics of Media Workshop Queen’s University IO March 15 Junior 12/06/20   QMUL Economics and Finance Workshop Queen Mary University All March 20 Junior 26/05/20   Virtual Finance and Economics Conference Yale University Finecon March 25 All 17/04/20   DC IO Day 2020 Georgetown University IO March 31 All 15/05/20    April    Conference Organizer Field Deadline Target Date     SITE Stanford University Theory April 01 All 12/06/21   NBER Summer Meeting Workshop NBER All April 05 All 12/07/21   AEA Annual Meeting AEA All April 15 All 07/01/22   Swiss IO Day University of Bern IO April 16 All 11/06/21   Econometric Society - North American Winter Meetings ES All April 21 All 06/01/22   CRESSE CRESSE IO April 30 All 26/07/21    May    Conference Organizer Field Deadline Target Date     European Research Workshop in International Trade CEPR Trade May 02 All 22/10/21   Warsaw International Economic Meeting Warsaw University All May 03 All 01/07/20   Warwick Economics PhD Conference University of Warwick All May 09 PhD 24/06/21   Annual Conference on Antitrust Economics and Competition Policy Northwestern University Comp policy May 17 All 17/09/21   NBER Economics of AI Conference NBER AI May 31 All 23/09/21    June    Conference Organizer Field Deadline Target Date     Equity and Access in Algorithms, Mechanisms, and Optimization (EAAMO) ACM AI June 14 All 05/10/21   FTC Micro Conference FTC IO June 23 Senior 04/11/21    July    Conference Organizer Field Deadline Target Date     Empirics and Methods in Economics Conference Northwestern \u0026amp; Chicago Empirical July 30 Junior 22/10/20    August    Conference Organizer Field Deadline Target Date     AI Policy Conference ETH Zurich AI August 1 All 14/09/21   Finance, Organizations and Markets (FOM) Conference Dartmouth College Finance, IO August 14 All 28/10/21    September    Conference Organizer Field Deadline Target Date     Causal Inference \u0026amp; Machine Learning: Why now? NeurIPS Econometrics September 18 All 13/12/21   ES European Winter Meeting Econometricc Society All September 19 Senior 13/12/21   Causal Data Science Conference causalscience All September 30 All 15/11/21    October    Conference Organizer Field Deadline Target Date     Digital Economics Conference TSE Digital October 3 All 13/01/22   Asia-Pacific IO Conference Asia-Pacific IO Society IO October 22 All 13/12/21   Young Swiss Economists Meeting SSES All October 25 Junior 11/02/21    November    Conference Organizer Field Deadline Target Date     Next Generation of Antitrust, Data Privacy and Data Protection Scholars Conference NYU IO November 1 All 28/01/22   Spring Meeting of Young Economists University of Bologna All November 12 Junior 17/06/21   NBER IO Winter Meeting NBER IO November 20 Senior 12/02/21   MaCCI Annual Conference University of Mannheim IO November 30 All 12/03/21   Postal Economics Conference TSE Digital November 30 All 07/04/22    December    Conference Organizer Field Deadline Target Date     Early-Career Behavioral Economics Conference Princeton University Behavioral December 15 junior 03/06/21    Undefined    Conference Organizer Field Deadline Target Date     Annual Conference on Innovation Economics Northwestern University Innovation forthcoming All 27/08/20   Conference on Mechanism and Institution Design Universität Klagenfurt Market Design closed All 11/06/20   D-TEA Conference HEC Paris Theory closed All 16/06/20   Economics Graduate Student Conference Washington University All closed Junior 07/11/20   NBER Summer Institute NBER All invitation Senior 06/07/20   CCP Annual Conference Centre for Competition Policy Comp policy closed All 24/06/21   RES Annual Conference Royal Economics Society All closed Senior 12/04/21   JEI Student Conference Harvard University All canceled Junior 20/06/20   Annual Conference on Network Science and Economics Becker Friedman Institute Networks canceled All 27/03/20   Annual SAET Conference Society for the Advancement of Economic Theory Theory closed All 13/06/21    ","date":1641772800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1641772800,"objectID":"023c54056b60665075513e8450a12257","permalink":"https://matteocourthoud.github.io/post/conferences/","publishdate":"2022-01-10T00:00:00Z","relpermalink":"/post/conferences/","section":"post","summary":"In this page, I collect information about conferences in Economics and Finance.\nIf you know about public conferences or meetings that are missing from this list, please either contact me or edit the table on Github!","tags":null,"title":"Economics Conferences","type":"post"},{"authors":null,"categories":null,"content":"Welcome to my tutorial on how to set up a remote machine and deploy your code there. I will first analyze SSH and then look at two specific applications: coding in Python and Julia.\nSetup In order to start working on a remote server you need\n the server local shell SSH installed  SSH, or Secure Shell, is a protocol designed to transfer data between a client and a server (two computers basically) over an untrusted network.\nThe way SSH works is it encrypts the connection using a pair of keys and the server, which is the computer you would connect to, is usually waiting for an SSH connection on Port 22.\nSSH is normally installed by default. To check if you have SSH installed, open the terminal and write ssh. You should receive a message that looks like this\nusage: ssh [-1246AaCfGgKkMNnqsTtVvXxYy] [-b bind_address] [-c cipher_spec] [-D [bind_address:]port] [-E log_file] [-e escape_char] [-F configfile] [-I pkcs11] [-i identity_file] [-J [user@]host[:port]] [-L address] [-l login_name] [-m mac_spec] [-O ctl_cmd] [-o option] [-p port] [-Q query_option] [-R address] [-S ctl_path] [-W host:port] [-w local_tun[:remote_tun]] [user@]hostname [command]  If SSH is not installed, you can install it using the following commands.\nsudo apt-get install openssh-server sudo systemctl enable ssh sudo systemctl start ssh  Now that you have installed SSH, we are ready to setup a remote connection.\nFrom the computer you want to access remotey, generate the public key.\nssh-keygen -t rsa  You will be asked for a location. If you decide to enter one manually then that will be the pair’s location, if you leave the default one it will be inside the .ssh hidden folder in your home directory.\nNow you will be prompted for a password. If you enter one you will be asked for it every time you use the key, this works for added security. If you don’t want a password just press enter and continue without one.\nTwo files were created. One file ends with the ‘.pub’ extension and the other one doesn’t. The file that ends with ‘.pub’ is your public key. This key needs to be in the computer you want to connect to (the server) inside a file called authorized_keys . You can accomplish this with the following command:\nssh-copy-id username@ip  For example in my case to send the key to my computer it would be:\nssh-copy-id sergiop@132.132.132.132  If you have MacOS there’s a chance you don’t have ssh-copy-id installed, in that case you can install it using\nbrew install ssh-copy-id  If you haven’t installed brew, you can install it by following this guide.\nConnect To permanently add the SSH key, you can use the follwing command\nssh-add directory\\key.pem  Lastly, to connect, just type the following command.\nssh username@ip  Where username is the server name and ip is the public IP adress, e.g. 132.132.132.132.\nIf your server is not public, you will not be able to access it.\nIf your server is password protected, you will be prompted to insert a password when you connect. If not, you should protect it with a password.\nManaging screens While you are connected to the remote terminal, any disturbance to your connection will interrupt the code. In order to avoid that, you want to create separate screens. This will allow your code to run remotely undisturbed, irrespectively of your connection.\nFirst, you need to install screen.\nbrew install screen  To create a new screen, just type\nscreen  Now you can lunch your code.\nAfter that, you want to detach from that screen so that the code can run remotely undisturbed.\nscreen -d  Another option is to use ctrl+a followed by ctrl+d. This will detach the screen without the need to type anythin in the terminal, in case the terminal is busy (most likely).\nTo list the current active screens type\nscreen -ls  If you want to check at any time that your code is running, without re-attaching to the screen, you can just type\ntop  which is the general command to check active processes. To exit, use ctrl+z, which generally terminates processes in the terminal.\nTo reattach to your screen, type\nscreen -r  In case you have multiple screens (you can check with screen -ls), you can reattach to a specific one by typing\nscreen -r 12345  where 12345 is the id of the screen.\nTo kill a screen, type\nscreen -XS 12345 quit  where again 12345 is the id of the screen.\nPython and Pycharm If you are coding in Python, PyCharm is one of the best IDEs. Among many features, it offers the possibility to set a remote compiler for your pthon console and to sync input and output files automatically.\nFirst, you need to have setup a remote SSH connection following the steps above. Importantly, you need to have added the public key to your machine using the ssh-add command, as explained above.\nThen open Pytharm, go to the lower-right corner, where the current interpreter is listed (e.g. Pytohn 3.8), click it and select interpreter settings.\nClick on the gear icon ⚙️ on the top-right corner and select add.\nInsert the server host (IP address, e.g. 132.132.132.132) and username (e.g. sergiop).\nNext, you have to insert your credentials. If you have a password, insert it, otherwise you have to insert the path to your SSH key file.\nLastly, select the remote interpreter. If you are using a python version that is not default, browse to the preferred python installation folder. Also, check the box for execute code giving this interpreter with root privileges via sudo.\nYou can also select which remote folder to sync with your local project. By default, you are given a tmp/pycharm_project_XX folder. You can change it if you want. I recommend also to have the last option checked: automatically sync project files to the server. This will automatically synch all remote changes with your local machine, in your local project folder.\nJulia and Juno If you are coding in Julia, Juno is the best IDE around. It’s an integration with Atom with a dedicated compiler, local variables, syntax highlight, autocompletion.\nOn Atom, you first need to install the ftp-remote-edit package.\nThen go to the menu item Packages \u0026gt; Ftp-Remote-Edit \u0026gt; Toggle.\nA new Remote panel will open with the default button to Edit a new server.\nClick it and you will be able to set up your remote connection.\n Press New Insert your username in The name of the server, for example sergiop Insert your ip adress in The hostname or IP adress of the server, for example 123.123.123.123 Select SFTP - SSH File Transfer Protocol under Protocol Select your Logon option. You can either insert your password every time, just once, or use a keyfile. Insert again your username in Username for autentication, again for example sergiop If you don’t want to start from the root folder, you can change the Initial Directory  Now you will be able to see your remote directory (named for example sergiop) in the Remote panel.\nTo start using Julia remotely, just start a new remote Julia process from the menu on the left.\nNow you are ready to deploy your Julia code on your remote server!\nJupyter Notebooks If you want to have a Jupyter Notebook running remotely, the steps are the following. The main advantage of a Jupyter Notebook is that it allows you to mix text and code in a single file, similarly to RMarkdown, with the advantage of not being contrained to use a R (or Python) kernel. For example, I often use Jupyter Notebook with Julia or Matlab Kernels. Moreover, you can also make nice slides out of it!\nFirst, connect to the remote machine. Look at section 1 to set up your SSH connection.\nssh username@ip  Start a Jupyter Notebook in the remote machine.\njupyter notebook --no-browser  The command will open a jupyter notebook in the remote machine. To connect to it, we need to know which port it used. The default port is 8888. If that port is busy, it will look for another available one. We can see the port from the output in terminal.\n Jupyter Notebook is running at: http://localhost:XXXX/…\n Where XXXX is the repote port used.\nNow we need to forward the remote port XXXX to our local YYYY port.\nOpen a new local shell. Type\nssh -L localhost:YYYY:localhost:XXXX username@ip  Where YYYY can be anything. I’d use the default port 8888.\nssh -L localhost:8889:localhost:8888 username@ip  Now go to your browser and type\nlocalhost:YYYY  Which in my case is\nlocalhost:8889  This will open the remote Jupyter Notebook.\nDone!\nIn case you want to check which Jupiter notebooks are running, type\njupyter notebook list  To kill a notebook use\njupyter notebook stop XXXX  Sources  How To Setup And Use SSH For Remote Connections Connecting to a Julia session on a remote machine Running a Jupyter notebook from a remote server  ","date":1638748800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1638748800,"objectID":"2a9a4ca9e5b7b3d6e6bbc3e3e8d7c4b3","permalink":"https://matteocourthoud.github.io/post/ssh/","publishdate":"2021-12-06T00:00:00Z","relpermalink":"/post/ssh/","section":"post","summary":"Welcome to my tutorial on how to set up a remote machine and deploy your code there. I will first analyze SSH and then look at two specific applications: coding in Python and Julia.","tags":null,"title":"How to Work on a Remote Machine via SSH","type":"post"},{"authors":["Matteo Courthoud"],"categories":null,"content":"The use of algorithms to set prices is particularly popular in online marketplaces, where sellers need to take quick decisions in complex dynamic environments. In this article, I investigate the role of online marketplaces in facilitating or preventing collusion among sellers that use pricing algorithms. In particular, I investigate a platform that has the ability to give prominence to certain products and automates this decision through a reinforcement learning algorithm, that maximizes the platform\u0026rsquo;s profits. Depending on whether the business model of the platform is more aligned with consumer welfare or with sellers' profits (e.g., if it collects quantity or profit fees), the platform either prevents or facilitates collusion among algorithmic sellers. If the platform is also active as a seller, the so-called dual role, it is able to both induce sellers to set high prices and appropriate most of the profits. Importantly, self-preferencing only happens during the learning phase and not in equilibrium. I investigate a potential solution: separating the sales and marketplace divisions. The policy is effective but does not fully restore the competitive outcome when the fee is distortive, as in the case of a revenue fee.\n","date":1635120000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1635120000,"objectID":"349267903040a0c9c407d09a2d417e7c","permalink":"https://matteocourthoud.github.io/project/alg_platform/","publishdate":"2021-10-25T00:00:00Z","relpermalink":"/project/alg_platform/","section":"project","summary":"I show that online marketplaces, by algorithmically controlling consumers' attention, have the incentives and ability to either facilitate or prevent algorithmic collusion, depending on their business model. I also explore platforms' dual role.","tags":["Industrial Organization","Antitrust","Artificial Intelligence"],"title":"Algorithmic Collusion on Online Marketplaces","type":"project"},{"authors":null,"categories":null,"content":"Ok, this is a fun post. I am choosing… my color palette!\nI have decided to unify all the color palettes I have on my website, slides, graphs, etc… into a unique universal color palette.\nMain Color First of all, I have to choose my main color.\n new CoolorsPaletteWidget(\"09673912029165208\", [\"003f5c\",\"003f5c\"]);  Here are some shades of it.\n new CoolorsPaletteWidget(\"0683428549461768\", [\"002637\",\"00324a\",\"003f5c\",\"17506b\",\"2c6078\"]);  Related Palettes Now I will build a couple of colors palettes based on it.\nThe first one, is red oriented.\n new CoolorsPaletteWidget(\"06164154396260932\", [\"003f5c\",\"444e86\",\"955196\",\"dd5182\",\"ff6e54\",\"ffa600\"]);  Second one, is green oriented.\n new CoolorsPaletteWidget(\"033203286745601424\", [\"003f5c\",\"00677f\",\"00908f\",\"2db88b\",\"94dc7b\",\"f9f871\"]);  Color Sequence Now I need a high contrast scheme for graphs. I add one color at the time to check that contrast is always maximized.\n new CoolorsPaletteWidget(\"0605466695047113\", [\"003f5c\",\"ff6e54\",\"f9f871\",\"2db88b\",\"955196\"]); https://coolors.co/003f5c-ff6e54-f9f871-2db88b-955196) A milder version of the same palette is:\n new CoolorsPaletteWidget(\"007538072748725222\", [\"00798c\",\"d1495b\",\"edae49\",\"52a369\",\"756ab2\"]); https://coolors.co/00798c-d1495b-edae49-52a369-756ab2)","date":1635033600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1635033600,"objectID":"4b0693de6f93a308b40b3cf73631212d","permalink":"https://matteocourthoud.github.io/post/palette/","publishdate":"2021-10-24T00:00:00Z","relpermalink":"/post/palette/","section":"post","summary":"Ok, this is a fun post. I am choosing… my color palette!\nI have decided to unify all the color palettes I have on my website, slides, graphs, etc… into a unique universal color palette.","tags":null,"title":"My Color Palette","type":"post"},{"authors":["Matteo Courthoud","Gregory Crawford"],"categories":null,"content":"In recent merger cases across complementary markets, antitrust authorities have expressed foreclosure concerns. In particular, the presence of scale economies in one market might propagate to the complementary market, ultimately leading to the monopolization of both. In this paper, we investigate the interplay between two foreclosure practices: exclusionary bundling and predatory pricing in the setting of complementary markets with economies of scale. We show that the two practices are complementary when markets display economies of scale, exclusionary bundling is more likely and, when bundling is allowed, predatory pricing is more likely. We show that this outcome is due to exit-inducing behavior of dominant firms: shutting down predatory incentives restores competition in both markets. We investigate different policies: banning mergers between market leaders, allowing product bundling only when more than one firm is integrated and able to offer the bundle, and lastly knowledge sharing across firms in order to limit the economies of scale. All policies are effective, each for a different reason.\n","date":1633046400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1633046400,"objectID":"129fc19e6524f0b4b6a49d907df9a3de","permalink":"https://matteocourthoud.github.io/project/foreclosure/","publishdate":"2021-10-01T00:00:00Z","relpermalink":"/project/foreclosure/","section":"project","summary":"We use a computational model of market dynamics to investigate the interplay between exclusionary bundling and predatory pricing. We show that the two foreclosure practices are complementary and we investigate potential policy interventions.","tags":["Industrial Organization","Antitrust","Computation","Dynamics"],"title":"Foreclosure Complementarities","type":"project"},{"authors":null,"categories":null,"content":"In this page, I collect anquestions that I frequently asked myself during my PhD, possibly with answers.\nPersonally, the article for PhD students that helped me the most is “Doing research” by Paul Niehaus. But beware, it might not work for everyone.\nStarting the PhD Information  “PhDs: the tortuous truth”, Chris Woolston, 2019. “Why doing a PhD is often a waste of time”, The Economist, 2016 “Should you do a PhD?\u0026quot;, Daniel K. Sokol, 2012. “So, you want to go to a grad school in economics?\u0026quot;, Ceyhun Elgin and Mario Solis-Garcia, 2007.  Applying  “How to Ask Your Professor for a Letter of Recommendation”, James Tierney, 2020. “Pre-Doc Guide”, Alvin Christian, 2019. “Advice for Applying to Grad School in Economics”, Susan Athey, 2016. “The complete guide to getting into an economics PhD program”, Miles Kimball, 2013. “The 12 Step Program for Grad School”, Erik Zwick.  Starting  “Reflections on Grad School in Economics”, Nick Hagerty, 2020. “How to survive your first year of graduate school in economics”, Matthew Pearson, 2005.  During the PhD Mental Health  “Graduate Student Mental Health: Lessons from American Economics Departments”, Bolotnyy, Valentin, Matthew Basilico, and Paul Barreira, 2021. “Mental Health, Bullying, Career Uncertainty”, Colleen Flahert, 2019. “How mindfulness can help Ph.D. students deal with mental health challenges”, Katie Langin, 2019. “Managing Your Mental Health as a PhD Student”, Joanna Hughes, 2019. “What Makes It So Hard to Ask for Help?\u0026quot;, Joan Rosenberg, 2019. “Grad school depression almost took me to the end of the road—but I found a new start”, Francis Aguisanda, 2018. “Faking it”, Chris Woolston, 2016. “Panic and a PhD”, Jack Leeming, 2016. “There’s an awful cost to getting a PhD that no one talks about”, Jennifer Walker, 2015.  Research and Ideas  “Advice for Academic Research”, Ricardo Dahis, 2021. “Sins of Omission and the Practice of Economics”, George A. Akerlof, 2020. “Doing research”, Paul Niehaus, 2019. “An unofficial guidebook for PhD students in economics and education”, Alex Eble, 2018. “The Research Productivity of New PhDs in Economics: The Surprisingly High Non-Success of the Successful”, John P. Conley and Ali Sina Önder, 2014. [“How to get started on research in economics?\u0026quot;](http://econ.lse.ac.uk/staff/spischke/phds/How to start.pdf), Steve Pischke, 2009. “The Importance of Stupidity in Scientific Research”, Martin A. Schwartz, 2008. “7 Rules for Maximizing Your Creative Output”, Steve Pavlina, 2007. “How To Build An Economic Model in Your Spare Time”, Hal. R. Varian, 1998. [“Ph.D. Thesis Research: Where do I Start?\u0026quot;](http://www.columbia.edu/~drd28/Thesis Research.pdf), Don Davis.  Presenting  “Unfair Questions”, David Schindler, 2021. “Beamer Tips for Presentations”, Paul Goldsmith-Pinkham, 2020. “Public Speaking for Academic Economists”, Rachel Meager, 2017. “How to present your job market paper”, Eliana La Ferrara, 2018. “How To Give a Lunch Talk”, Adam Guren, 2018. “The Discussant’s Art”, Chris Blattman, 2010. “How to be a Great Conference Participants”, Art Carden, 2009. [“The “Big 5” and Other Ideas For Presentations”](http://econ.lse.ac.uk/staff/spischke/phds/The Big 5.pdf), Cox, Donald, 2000. “How to Give an Applied Micro Talk”, Jesse M. Shapiro. “Tips on How to Avoid Disaster in Presentations”, Monika Piazzesi. [“Seminar Slides “](https://www.ssc.wisc.edu/~bhansen/placement/Seminar Slides.pdf), Bruce Hansen.  Writing  [“5 Steps Toward a Paper”](https://www.google.com/url?q=https%3A%2F%2Fwww.dropbox.com%2Fs%2Fq7wjaidl5w91srt%2FGuest lecture FS.pdf%3Fdl%3D0\u0026amp;sa=D\u0026amp;sntz=1\u0026amp;usg=AFQjCNG_nRs6QlkZzWBHAy0PjF4jfEYBAw), Frank Schilbach, 2019. “Novelist Cormac McCarthy’s tips on how to write a great science paper”, Van Savage and Pamela Yeh, 2019. “The “Middle Bits” Formula for Applied Papers”, Marc Bellamare, 2018. “The Conclusion Formula”, Marc Bellamare, 2018. [“The Introduction Formula”](https://www.albany.edu/spatial/training/5-The Introduction Formula.pdf), Keith Head, 2015. “Writing Tips For Economics Research Papers”, Plamen Nikolov, 2013. “The Ten Most Important Rules of Writing Your Job Market Paper”, Goldin, Claudia and Lawrence Katz, 2008. “Writing Tips for Ph.D. Students”, John Cochrane, 2005. “Writing Papers: A Checklist”, Michael Kremer.  Referiing  “How To Write A Good Referee Report”, Tatyana Deryugina, 2019. “How to Review Manuscripts”, Elsevier, 2015. “Contributing to Public Goods: My 20 Rules for Refereeing”, Marc F. Bellemare, 2012  Finishing the PhD The Job Market  “A Guide and Advice for Economists on the U.S. Junior Academic Job Market 2018-2019 Edition”, John Cawley, 2018. “Academic job market advice for economics, political science, public policy, and other professional schools”, Blattman, Christopher, 2015. “How I Learned to Stop Worrying and Love the Job Market”, Erik Zwick, 2014.  The Private Sector  “My Journey from Economics PhD to Data Scientist in Tech”, Rose Tan, 2021. “Tech Industry Jobs for Econ PhDs”, Scarlet Chen, 2020. “My Journey from Econ PhD to Tech”, Scarlet Chen, 2020. “Why it is not a ‘failure’ to leave academia”, Philipp Kruger, 2018.  The Tenure Track  “The Awesomest 7-Year Postdoc or: How I Learned to Stop Worrying and Love the Tenure-Track Faculty Life”, Radhika Nagpal, 2013.  More You can find more resources here:\n AEA Mentoring Reading Materials Johannes Pfeifer Job Market Resources Kristoph Kronenberg Resources Patrick Button Resources Ryan Edwards Resources Jennifer Doleac Resources Amanda Agan Writing and Presentation Advice Random forum Resource Collection  ","date":1633046400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1633046400,"objectID":"3640fe4bf948f5a3cef62c6fa0ffdd0f","permalink":"https://matteocourthoud.github.io/post/phd_faq/","publishdate":"2021-10-01T00:00:00Z","relpermalink":"/post/phd_faq/","section":"post","summary":"In this page, I collect anquestions that I frequently asked myself during my PhD, possibly with answers.\nPersonally, the article for PhD students that helped me the most is “Doing research” by Paul Niehaus.","tags":null,"title":"PhD Frequently Asked Questions","type":"post"},{"authors":["Matteo Courthoud"],"categories":null,"content":"Reinforcement learning algorithms are gradually replacing humans in many decision-making processes, such as pricing in high-frequency markets. Recent studies on algorithmic pricing have shown that algorithms can learn sophisticated grim-trigger strategies with the intent of keeping supra-competitive prices. This paper focuses on algorithmic collusion detection. One frequent suggestion is to look at the inputs of the strategies, for example at whether the algorithms condition their prices on previous competitors' prices. The first part of the paper shows that this approach might not be sufficient to detect collusion since the algorithms can learn reward-punishment schemes that are fully independent of the rival’s actions. The mechanism that ensures the stability of supra-competitive prices is self-punishment.\nThe second part of the paper explores a novel test for algorithmic collusion detection. The test builds on the intuition that as algorithms are able to learn to collude, they might be able to learn to exploit collusive strategies. In fact, since they are not designed to learn sub-game perfect equilibrium strategies, there is the possibility that their strategies could be exploited. When one algorithm is unilaterally retrained, keeping the collusive strategies of its competitor fixed, it learns more profitable strategies. Usually, these strategies are more competitive, but not always. Since this change in strategies happens only when algorithms are colluding, retraining can be used as a test to detect algorithmic collusion.\nTo make the test implementable, the last part of the paper studies whether one could get the same insights on collusive behavior using only observational data, from a single algorithm. The result is a unilateral empirical test for algorithmic collusion that does not require any assumptions neither on the algorithms themselves nor on the underlying environment. The key insight is that algorithms, during their learning phase, produce natural experiments that allow an observer to estimate their behavior in counterfactual scenarios. The simulations show that, at least in a controlled experimental setting, the test is extremely successful in detecting algorithmic collusion.\n","date":1631404800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1631404800,"objectID":"a444b4a0948fc8347013080cc97eb487","permalink":"https://matteocourthoud.github.io/project/alg_detection/","publishdate":"2021-09-12T00:00:00Z","relpermalink":"/project/alg_detection/","section":"project","summary":"I show that algorithms can learn reward-punishment schemes that are fully independent from the rival’s actions and I propose a model-free test for algorithmic collusion based on historical data.","tags":["Industrial Organization","Antitrust","Artificial Intelligence"],"title":"Algorithmic Collusion Detection","type":"project"},{"authors":null,"categories":null,"content":"In this page, I collect useful resources for coding for researchers in social sciences. A mention goes to Maximilian Kasy that inspired me to build this page.\nA quick legend:\n 📗 book 🌐 webpage 📈 charts 🎥 videos  Econometrics and Statistics  📗Bruce Hansen’s Econometrics: By far the best freely available and regularly updated resource for Econometrics  Machine Learning  📗The Elements of Statistical Learning: General introduction to machine learning 📗Gaussian Processes for Machine Learning: Extremely useful tools for nonparametric Bayesian modeling 📗Deep Learning: The theory and implementation of neural nets 📗Understanding Machine Learning: From Theory to Algorithms: An introduction to statistical learning theory in the tradition of Vapnik 📗Reinforcement Learning - An Introduction: Adaptive learning for Markov decision problems 📗Algorithms: Introduction to the theory of algorithms 🌐Tensorflow Playground: Visualisation tool for neural networks 🌐Artificial Intelligence: Online lectures on AI 🌐The Ethical Algorithm: How to impose normative constraints on ML and other algorithms  Python  🌐RealPython: Collection of Python tutorials, from introductory to advanced. Also contains learning paths for specific topics 🌐QuantEcon Python Tutorials and economic applications in Python, especially for macroeconomics 🌐Cheat Sheets: Collection of cheat sheets for python 🌐Structuring a Python project: Advanced tutorial on how to structure a Python program 🌐IDE Guide: Comparison of IDEs for Python. Suggested: PyCharm 🌐Configuring remote interpreters via SSH: How to use Python remotely via SSH via PyCharm 📈Visualization in Python: How to make nice graphs in Python, with a dedicated jupyter notebook 📈Python Graph Gallery: Graph examples in Python  Matlab  🌐User defined classes in Matlab: How to work with classes in Matlab 🌐Julyter Notebooks: How to run a jupyter notebook with Matlab kernel 📈Graph Tips in Matlab and link2: Suggestions on how to make pretty graphs in Matlab  Julia  🌐Julia Manual: Julia unfortunately lacks a big community and tutorials, but it has a very good manual 🌐QuantEcon Julia Tutorials and economic applications in Julia, especially for macroeconomics 🌐IDE Guide: Guide for IDEs for Julia. Suggested: Juno for Atom.  R   📗An Introduction to R: Complete introduction to base R\n  📗R for Data Science Introduction to data analysis using R, focused on the tidyverse packages\n  📗Advanced R: In depth discussion of programming in R\n  📗Hands-On Machine Learning with R: Fitting ML models in R\n  🌐Bayesian statistics using Stan and link\n  🌐RStudio Cheat Sheets for various extensions, including data processing, visualization, writing web apps, …\n  📈R Graph Gallery: Graph examples in R\n  📈Nice Graphs with code\nA collection of elaborate graphs with code in R\n  Others  📗Github Advanced: Advanced guide for version control with Github 🎥The Missing Semester of Your CS Education Video lectures and notes on tools for computer scientists (version control, debugging, …) 📈PGF plots in Latex: Gallery and examples to make plots directly in Latex 🌐Work remotely from server: How to setup SSH for remote computing  ","date":1629763200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1629763200,"objectID":"47e14b4466dc75da698fe3b2f882bdb1","permalink":"https://matteocourthoud.github.io/post/coding/","publishdate":"2021-08-24T00:00:00Z","relpermalink":"/post/coding/","section":"post","summary":"In this page, I collect useful resources for coding for researchers in social sciences. A mention goes to Maximilian Kasy that inspired me to build this page.\nA quick legend:","tags":null,"title":"Coding Resources for Social Sciences","type":"post"},{"authors":null,"categories":null,"content":"In this page, I explain how to work with the WRDS database using Python.\nSetup The first thing we need to do, is to set up a connection to the WRDS database. I am assuming you have credentials to log in. Check the log in page to make sure.\nThe second requirement is the wrds Python package.\npip3 install wrds  Now, in order to connect to the WRDS database, you just need to run the following commang in Python.\nimport wrds db = wrds.Connection()  Then, you will be propted to input your WRDS username and password.\nHowever, if you are using a Python IDE such as PyCharm, you cannot run the command from the Python Console. Moreover, you might want to save your credentials once and for all, so that you don’t have to log in every time.\nFirst, walk to your home directory from the Terminal (/Users/username).\ncd  Now create an empty .pgpass file.\ntouch .pgpass  Now you write your_username and your_password into the .pgpass file.\necho \u0026quot;wrds-pgdata.wharton.upenn.edu:9737:wrds:your_username:your_password\u0026quot; \u0026gt;\u0026gt; .pgpass  You also need to restrict permissions to the file.\nchmod 600 ~/.pgpass  Now you can go back to your Python IDE and access the database by just inputing your username.\nimport wrds db = wrds.Connection(wrds_username='your_username')  If everything works, you should see the following output.\nLoading library list... Done  Query The available functions are:\n db.connection() db.list_libraries() db.list_tables() db.get_table() db.describe_table() db.raw_sql() db.close()  I make a simple example of how they work. Suppose first you want to list all the libraries in the WRDS database.\ndb.list_libraries()  Then you can list all the datasets within a given library.\ndb.list_tables(library='comp')  Before downloading a table, you can describe it.\ndf = db.describe_table(library='comp', table='funda')  To download the dataset you can use the get_table() function.\ndf = db.get_table(library='comp', table='funda')  You can restrict both the rows and the columns you want to query.\ndf_short = db.get_table(library='comp', table='funda', columns = ['conm', 'gvkey', 'cik'], obs=5)  You can also query the database directly using SQL.\ndf_sql = db.raw_sql('''select conm, gvkey, cik FROM comp.funda WHERE fyear\u0026gt;2010 AND (indfmt='INDL')''')  Sources  Querying WRDS Data using Python Using Python on WRDS Platform Introduction to the WRDS Python Package WRDS Data Access Via Python API  ","date":1615248000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1615248000,"objectID":"95c5fd9f29ea012e404664238d9d0a7d","permalink":"https://matteocourthoud.github.io/post/wrds/","publishdate":"2021-03-09T00:00:00Z","relpermalink":"/post/wrds/","section":"post","summary":"In this page, I explain how to work with the WRDS database using Python.\nSetup The first thing we need to do, is to set up a connection to the WRDS database.","tags":null,"title":"How to access WRDS in Python","type":"post"},{"authors":["Matteo Courthoud"],"categories":null,"content":"Dynamic stochastic games notoriously suffer from a curse of dimensionality that makes computing the Markov Perfect Equilibrium of large games infeasible. This article compares the existing approximation methods and alternative equilibrium concepts that have been proposed in the literature to overcome this problem. No method clearly dominates the others but some are dominated in all dimensions. In general, alternative equilibrium concepts outperform sampling-based approximation methods. I propose a new game structure, games with random order, in which players move sequentially and the order of play is unknown. The Markov Perfect equilibrium of this game consistently outperforms all existing approximation methods in terms of approximation accuracy while still being extremely efficient in terms of computational time.\n","date":1609459200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1609459200,"objectID":"942fdcc5da1de7f590684b3218f68964","permalink":"https://matteocourthoud.github.io/project/approximations/","publishdate":"2021-01-01T00:00:00Z","relpermalink":"/project/approximations/","section":"project","summary":"I compare existing approximation methods to compute Markow Perfect Equilibrium in dynamic stochastic games with large state spaces. I also propose a new approximation method called \"Games with Random Order\".","tags":["Industrial Organization","Computation","Dynamics"],"title":"Approximation Methods for Large Dynamic Stochastic Games","type":"project"},{"authors":null,"categories":null,"content":"In this page, I collect information about summer schools in Economics.\nIf you know about summer schools that are missing from this list, please either contact me or edit the table on Github!\n2020    Name and Link Field Organizer Location Instructor(s) Dates Deadline Fee Aid     Dynamic Structural Econometrics Econometrics, IO Econometrics Society Zurich John Rust et al. June 15-21 March 15 500$ no   CRESSE Comp. Policy, IO  Crete various June 20 - July 02 FCFS 3200€ -30%   Digital Economy Comp. Policy, IO Barcelona GSE Barcelona Martin Peitz July 13-17 March 10 550€ maybe   Social Networks, Platforms… Comp. Policy, IO PSE Paris from Paris June 15-19 March 31 1200€ no    2019    Name and Link Field Organizer Location Instructor(s) Dates Deadline Fee Aid     Dynamic Structural Econometrics Econometrics, IO Econometrics Society Chicago John Rust et al. July 08-14 March 15 500$ no   CRESSE Competition Policy, IO various Crete various June 20 - July 02 FCFS 3200€ -30%   Empirical Analysis of Firm Performance IO, Trade CEMFI Madrid Jan de Loecker August 19-23      Panel Data Econometrics Econometrics CEMFI Madrid Steve Bond September 02-06       2018    Name and Link Field Organizer Location Instructor(s) Dates Deadline Fee Aid     Dynamic Structural Models Econometrics, IO University of Copenhagen Copenhagen John Rust et al. May 28 - Jun 03 March 15 600€ no   Empirical Analysis of Innovation in Oligopoly Industries            ","date":1583798400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1583798400,"objectID":"0f0d27452021e254ab9744c41138bb19","permalink":"https://matteocourthoud.github.io/post/summer_schools/","publishdate":"2020-03-10T00:00:00Z","relpermalink":"/post/summer_schools/","section":"post","summary":"In this page, I collect information about summer schools in Economics.\nIf you know about summer schools that are missing from this list, please either contact me or edit the table on Github!","tags":null,"title":"Summer Schools in Economics","type":"post"},{"authors":["Matteo Courthoud"],"categories":null,"content":"I generate a time-varying measure of S\u0026amp;P500 firm similarity using a zero-shot clustering model. The model takes as input BERT embeddings of product descriptions and is trained on market definitions from the EU commission. The objective is to estimate the causal effect of common ownership on product similarity.\n","date":1580515200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1580515200,"objectID":"8945db35dd8049bb20237e33023618f5","permalink":"https://matteocourthoud.github.io/project/ownership/","publishdate":"2020-02-01T00:00:00Z","relpermalink":"/project/ownership/","section":"project","summary":"I generate a time-varying measure of S\\\u0026P500 firm similarity using a zero-shot clustering model. The model takes as input BERT embeddings of product descriptions and is trained on market definitions from the EU commission. The objective is to estimate the causal effect of common ownership on product similarity.","tags":["Industrial Organization","Antitrust","Computation","Dynamics"],"title":"Ownership and Product Similarity","type":"project"},{"authors":[],"categories":[],"content":"Create slides in Markdown with Wowchemy Wowchemy | Documentation\n Features  Efficiently write slides in Markdown 3-in-1: Create, Present, and Publish your slides Supports speaker notes Mobile friendly slides   Controls  Next: Right Arrow or Space Previous: Left Arrow Start: Home Finish: End Overview: Esc Speaker notes: S Fullscreen: F Zoom: Alt + Click PDF Export: E   Code Highlighting Inline code: variable\nCode block:\nporridge = \u0026quot;blueberry\u0026quot; if porridge == \u0026quot;blueberry\u0026quot;: print(\u0026quot;Eating...\u0026quot;)   Math In-line math: $x + y = z$\nBlock math:\n$$ f\\left( x \\right) = ;\\frac{{2\\left( {x + 4} \\right)\\left( {x - 4} \\right)}}{{\\left( {x + 4} \\right)\\left( {x + 1} \\right)}} $$\n Fragments Make content appear incrementally\n{{% fragment %}} One {{% /fragment %}} {{% fragment %}} **Two** {{% /fragment %}} {{% fragment %}} Three {{% /fragment %}}  Press Space to play!\nOne  **Two**  Three   A fragment can accept two optional parameters:\n class: use a custom style (requires definition in custom CSS) weight: sets the order in which a fragment appears   Speaker Notes Add speaker notes to your presentation\n{{% speaker_note %}} - Only the speaker can read these notes - Press `S` key to view {{% /speaker_note %}}  Press the S key to view the speaker notes!\n Only the speaker can read these notes Press S key to view    Themes  black: Black background, white text, blue links (default) white: White background, black text, blue links league: Gray background, white text, blue links beige: Beige background, dark text, brown links sky: Blue background, thin dark text, blue links    night: Black background, thick white text, orange links serif: Cappuccino background, gray text, brown links simple: White background, black text, blue links solarized: Cream-colored background, dark green text, blue links   Custom Slide Customize the slide style and background\n{{\u0026lt; slide background-image=\u0026quot;/media/boards.jpg\u0026quot; \u0026gt;}} {{\u0026lt; slide background-color=\u0026quot;#0000FF\u0026quot; \u0026gt;}} {{\u0026lt; slide class=\u0026quot;my-style\u0026quot; \u0026gt;}}   Custom CSS Example Let\u0026rsquo;s make headers navy colored.\nCreate assets/css/reveal_custom.css with:\n.reveal section h1, .reveal section h2, .reveal section h3 { color: navy; }   Questions? Ask\nDocumentation\n","date":1549324800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1549324800,"objectID":"0e6de1a61aa83269ff13324f3167c1a9","permalink":"https://matteocourthoud.github.io/slides/example/","publishdate":"2019-02-05T00:00:00Z","relpermalink":"/slides/example/","section":"slides","summary":"An introduction to using Wowchemy's Slides feature.","tags":[],"title":"Slides","type":"slides"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"f26b5133c34eec1aa0a09390a36c2ade","permalink":"https://matteocourthoud.github.io/admin/config.yml","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/admin/config.yml","section":"","summary":"","tags":null,"title":"","type":"wowchemycms"}]